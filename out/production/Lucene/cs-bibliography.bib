@inproceedings{allan2010computational,
  abstract = {The number of undergraduates entering computer science has declined in recent years. This is paralleled by a drop in the number of high school students taking the CS AP exam and the number of high schools offering computer science courses. The declines come at a time when career opportunities in CS continue to grow and computer science graduates are seen as crucial in building a globally competitive workforce for the 21st century. Efforts aimed at reversing the declining interest in computer science include curriculum revisions at the undergraduate level at many institutions, a re-design of computer science AP courses [1], and the inclusion of computational thinking into disciplines outside computer science [3]. This panel discusses four projects of computer science researchers collaborating with high school teachers on integrating computing and computational thinking into their courses. The majority of the high school teachers involved is teaching science and math courses. They are teaching a diverse group of talented and college-bound students. The goal of all projects is to integrate computing into disciplines represented in the high school curriculum and to raise the awareness of computer science as an exciting and intellectually rewarding field. This panel will outline recent and on-going activities and interaction with high school teachers. Each panelist will describe how he/she got involved and the nature of the interaction. The panelists will talk about their individual projects, outline their visions for future interactions, and how their effort can be replicated by others. The session will briefly describe NSF's RET program which provided teacher support for three of the four projects. The session will then be opened for discussion; the audience will be encouraged to ask questions and contribute additional ideas for the inclusion of computational thinking in high school courses.},
  added-at = {2017-01-01T02:44:19.000+0100},
  address = {New York, NY},
  author = {Allan, Vicki and Barr, Valerie and Brylow, Dennis and Hambrusch, Susanne},
  biburl = {https://www.bibsonomy.org/bibtex/2d61220e7565deca7c7f40fa04cad2662/vngudivada},
  booktitle = {Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
  interhash = {2a6fe4109417d1c1663ad5864255a964},
  intrahash = {d61220e7565deca7c7f40fa04cad2662},
  keywords = {ComputationalThinking NsfRED},
  pages = {390--391},
  publisher = {ACM},
  series = {SIGCSE '10},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Computational Thinking in High School Courses},
  url = {http://doi.acm.org/10.1145/1734263.1734395},
  year = 2010
}

@book{barber1983logic,
  added-at = {2017-01-02T00:03:30.000+0100},
  address = {New Brunswick, N.J.},
  author = {Barber, Bernard},
  biburl = {https://www.bibsonomy.org/bibtex/23629032b5b815adabc3a95d6f8296cb7/vngudivada},
  interhash = {6f817da19f3c886db4cc60c692756857},
  intrahash = {3629032b5b815adabc3a95d6f8296cb7},
  keywords = {Leadership NsfRED Trust},
  publisher = {Rutgers University Press},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {The logic and limits of trust},
  year = 1983
}

@inproceedings{kules2016computational,
  abstract = {Computational thinking complements critical thinking as a way of reasoning to solve problems, make decisions and interact with our world. It draws concepts and techniques such as abstraction, decomposition, algorithmic design, generalization, evaluation and iteration from computer and information science, but has broad application in the arts, sciences, humanities and social sciences. This paper compares computational and critical modes of thinking, identifying concepts and terminology that support cross-disciplinary discourse, inform faculty and curriculum development efforts, and interconnect learning outcomes at the course, program and university level, thus helping programs better articulate contributions to institutional goals. A better understanding of each mode can enrich what we teach aspiring information professionals about computational and critical thinking, how we teach it, and how we apply these skills in our professional work.},
  added-at = {2017-01-01T03:11:49.000+0100},
  address = {Silver Springs, MD},
  author = {Kules, Bill},
  biburl = {https://www.bibsonomy.org/bibtex/2a52ab879b959d79c665619a87ac42821/vngudivada},
  booktitle = {Proceedings of the 79th ASIS\&T Annual Meeting: Creating Knowledge, Enhancing Lives Through Information \& Technology},
  interhash = {db8fe3dea163f0db79aad253176012b3},
  intrahash = {a52ab879b959d79c665619a87ac42821},
  keywords = {ComputationalThinking},
  pages = {92:1--92:6},
  publisher = {American Society for Information Science},
  series = {ASIST '16},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Computational Thinking is Critical Thinking: Connecting to University Discourse, Goals, and Learning Outcomes},
  year = 2016
}

@article{yu2005taxonomy,
  abstract = {With the advent of Grid and application technologies, scientists and engineers are building more and more complex applications to manage and process large data sets, and execute scientific experiments on distributed resources. Such application scenarios require means for composing and executing complex workflows. Therefore, many efforts have been made towards the development of workflow management systems for Grid computing. In this paper, we propose a taxonomy that characterizes and classifies various approaches for building and executing workflows on Grids. The taxonomy not only highlights the design and engineering similarities and differences of state-of-the-art in Grid workflow systems, but also identifies the areas that need further research.},
  acmid = {1084814},
  added-at = {2016-12-19T05:06:37.000+0100},
  address = {New York, NY, USA},
  author = {Yu, Jia and Buyya, Rajkumar},
  biburl = {https://www.bibsonomy.org/bibtex/21aebc04aabe7e042f994a214fa0a95e6/vngudivada},
  doi = {10.1145/1084805.1084814},
  interhash = {09445a37475c8552896b98e981ab40ad},
  intrahash = {1aebc04aabe7e042f994a214fa0a95e6},
  issn = {0163-5808},
  issue_date = {September 2005},
  journal = {SIGMOD Rec.},
  keywords = {GridComputing ScientificWorkflowSystem},
  month = sep,
  number = 3,
  numpages = {6},
  pages = {44--49},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {A Taxonomy of Scientific Workflow Systems for Grid Computing},
  url = {http://doi.acm.org/10.1145/1084805.1084814},
  volume = 34,
  year = 2005
}

@inproceedings{syslo2015algorithmic,
  abstract = {Computational thinking, as coined by Jeannette Wing, is a fundamental skill for all to be able to live in today's world, a mode of thought that goes well beyond computing and provides a framework for reasoning about problems and methods of their solution. It has a long tradition as algorithmic thinking which within computer science is a competence to formulate a solution of a problem in the form of an algorithm and then to implement the algorithm as a computer program. Computational thinking is not an adequate characterization of computer science as claimed by Peter Denning and he is right -- it is a collection of key mental tools and practices originated in computing but addressed to all areas far beyond computer science. As an extension of algorithmic thinking, it includes thinking with many levels of abstraction as a problem solving approach inherently connected to computer science and addressed to all students to use computers and computing skills in solving problems in various school subjects coming from various scientific and applied areas. Computational thinking involves concepts, skills and competences that lie at the heart of computing, such as abstraction, decomposition, generalization, approximation, heuristics, algorithm design, efficiency and complexity issues and therefore it is clear that basic computer science knowledge helps to systematically, correctly, and efficiently process information, perform tasks, and solve problems. Although coming from computer science, computational thinking is not only the study of computer science, though computers play an essential role in the design of problems' solutions. It is a very important and useful mode of thinking in almost all disciplines and school subjects as an insight into what can and cannot be computed. In this talk we shall discuss a new computing curriculum addressed to ALL students in K-12 in Poland which motivates them to use computational thinking in solving problems in various school subjects. Moreover its goal is to encourage and prepare students from early school years to consider computing and related fields as disciplines of their future study and professional career. To this end, the curriculum allows teachers and schools to personalize learning and teaching according to students' interests, abilities, and needs. The new computing curriculum benefits a lot from our experience in teaching informatics in our schools for almost 30 years -- the first curriculum was approved by the ministry of education in 1985, 20 years after the first regular classes on informatics were held in two high schools in Wrocław and in Warsaw. Today, informatics is an obligatory subject in middle school (grades 7-9) and high school (grades 10-12) and it will replace computer lessons (mainly on ICT) in elementary schools (grades 1-6). The new curriculum is also addressed to vocational education.},
  added-at = {2017-01-01T03:06:20.000+0100},
  address = {New York, NY},
  author = {Syslo, Maciej M.},
  biburl = {https://www.bibsonomy.org/bibtex/2831d53171683dbbe4bd2d9967f2c47a5/vngudivada},
  booktitle = {Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education},
  interhash = {44a96e2568a92b447cb9c413abfe4728},
  intrahash = {831d53171683dbbe4bd2d9967f2c47a5},
  keywords = {ComputationalThinking},
  pages = {1--1},
  publisher = {ACM},
  series = {ITiCSE '15},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {From Algorithmic to Computational Thinking: On the Way for Computing for All Students},
  url = {http://doi.acm.org/10.1145/2729094.2742582},
  year = 2015
}

@article{turpen2016perceived,
  abstract = { In order to promote sustained and impactful educational transformation, it is essential for change agents to understand more about faculty perceptions associated with either adopting or not adopting a research-based instructional strategy (RBIS). In this paper, we use interviews with 35 physics faculty to examine barriers and affordances to the use of the research-based instructional strategy of Peer Instruction. We found that the most common reasons faculty give for aligning their instruction with Peer Instruction is that it is not lecture and they have had positive experiences with Peer Instruction. The most common reasons faculty give for not using Peer Instruction are concerns about the time it will take, the loss of content coverage, and having had bad experiences with it. Additionally, we found the perceived barriers to be very different depending on whether the interviewee was a user of Peer Instruction or not, with nonusers being more concerned with time and users being more concerned with implementation difficulties. It is important for change agents to understand and address concerns faculty have about implementing researchbased instructional strategies. Based on these results we offer four recommendations for those interested in promoting educational transformation toward research-based instructional strategies: (1) do not waste a lot of time criticizing lecture-based instruction and convincing faculty of the value of research-based strategies (they are already dissatisfied with lecture), (2) understand and address concerns faculty have about implementing active learning techniques, (3) focus on supporting and encouraging faculty experiences with RBIS, (4) address concerns faculty new to RBIS have about the time and energy needed to change.},
  added-at = {2016-12-23T22:07:52.000+0100},
  author = {Turpen, Chandra and Dancy, Melissa and Henderson, Charles},
  biburl = {https://www.bibsonomy.org/bibtex/2a0621b35289f824c907e5e31191d2462/vngudivada},
  doi = {10.1103/PhysRevPhysEducRes.12.010116},
  interhash = {56fa48e54328bb0bcb70c9c7b54ce669},
  intrahash = {a0621b35289f824c907e5e31191d2462},
  journal = {Phys. Rev. Phys. Educ. Res.},
  keywords = {PeerInstruction},
  month = feb,
  number = 1,
  numpages = {18},
  pages = 010116,
  publisher = {American Physical Society},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Perceived affordances and constraints regarding instructors' use of Peer Instruction: Implications for promoting instructional change},
  url = {http://link.aps.org/doi/10.1103/PhysRevPhysEducRes.12.010116},
  volume = 12,
  year = 2016
}

@book{gambetta1988trust,
  abstract = {A collection of essays examining the idea of trust from the standpoint of different disciplines. The contributors, drawn from Europe, Britain and the USA, aim to provide illuminating insights into how "trust" is variously regarded in the world of scholarship.},
  added-at = {2017-01-02T00:19:37.000+0100},
  address = {New York, NY},
  biburl = {https://www.bibsonomy.org/bibtex/25b1e85536bb21944f299eaa30b74a3a0/vngudivada},
  editor = {Gambetta, Diego},
  interhash = {abb68b8502b93fb65e1b10db0bfbc3c4},
  intrahash = {5b1e85536bb21944f299eaa30b74a3a0},
  keywords = {Leadership NsfRED Trust},
  publisher = {Basil Blackwell},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Trust: making and breaking cooperative relations},
  year = 1988
}

@misc{day2005introduction,
  abstract = {The Darwin Information Typing Architecture (DITA) is an XML-based, end-to-end architecture for authoring, producing, and delivering technical information. This architecture consists of a set of design principles for creating "information-typed" modules at a topic level and for using that content in delivery modes such as online help and product support portals on the Web. This document is a roadmap for DITA: what it is and how it applies to technical documentation.},
  added-at = {2016-12-07T01:33:27.000+0100},
  author = {Day, Don and Priestley, Michael and Schell, David},
  biburl = {https://www.bibsonomy.org/bibtex/218e9571cdf80f54fa4761522c27cc885/vngudivada},
  interhash = {9cc3d0355b8ab3241b3a0c28d93b3c3d},
  intrahash = {18e9571cdf80f54fa4761522c27cc885},
  journal = {developerWorks},
  keywords = {DITA},
  pages = {1 - 12},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Introduction to the Darwin Information Typing Architecture},
  year = 2005
}

@inproceedings{lu2009thinking,
  abstract = {Jeannette Wing's call for teaching Computational Thinking (CT) as a formative skill on par with reading, writing, and arithmetic places computer science in the category of basic knowledge. Just as proficiency in basic language arts helps us to effectively communicate and in basic math helps us to successfully quantitate, proficiency in computational thinking helps us to systematically and efficiently process information and tasks. But while teaching everyone to think computationally is a noble goal, there are pedagogical challenges. Perhaps the most confounding issue is the role of programming, and whether we can separate it from teaching basic computer science. How much programming, if any, should be required for CT proficiency?

We believe that to successfully broaden participation in computer science, efforts must be made to lay the foundations of CT long before students experience their first programming language. We posit that programming is to Computer Science what proof construction is to mathematics, and what literary analysis is to English. Hence by analogy, programming should be the entrance into higher CS, and not the student's first encounter in CS. We argue that in the absence of programming, teaching CT should focus on establishing vocabularies and symbols that can be used to annotate and describe computation and abstraction, suggest information and execution, and provide notation around which mental models of processes can be built. Lastly, we conjecture that students with sustained exposure to CT in their formative education will be better prepared for programming and the CS curriculum, and, furthermore, that they might choose to major in CS not only for career opportunities, but also for its intellectual content.},
  added-at = {2017-01-01T02:48:38.000+0100},
  address = {New York, NY},
  author = {Lu, James J. and Fletcher, George H.L.},
  biburl = {https://www.bibsonomy.org/bibtex/264c0e836d1a3f4137706cca046f1f062/vngudivada},
  booktitle = {Proceedings of the 40th ACM Technical Symposium on Computer Science Education},
  interhash = {a21d655310390a42bdc0977c0d228236},
  intrahash = {64c0e836d1a3f4137706cca046f1f062},
  keywords = {ComputationalThinking NsfRED},
  pages = {260--264},
  publisher = {ACM},
  series = {SIGCSE '09},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Thinking About Computational Thinking},
  url = {http://doi.acm.org/10.1145/1508865.1508959},
  year = 2009
}

@article{boyle2016beyond,
  abstract = {Cognitive scientists have identified numerous fundamental principles that influence learning; these include deliberate practice, interleaving, retrieval practice, spacing, metacognition, desirable difficulties, limited working memory, the curse of knowledge, schema generation, and constructivism. STEM (science, technology, engineering, and mathematics) education researchers have repeatedly shown improved learning when instruction employs these principles. In particular, teaching methods like flipping and clickers work best when implemented using them. These ”research-based teaching methods” are becoming the norm in STEM teaching. A macro principles course at Penn State was redesigned with these principles in mind. 508 students in this course achieved .77 standard deviations more learning than principles students normally do on the macroeconomic Test of Understanding of College Economics (TUCE).},
  added-at = {2016-12-23T23:21:19.000+0100},
  author = {Boyle, Austin and Goffe, William L.},
  biburl = {https://www.bibsonomy.org/bibtex/282c432b5f350e4e958160aaf732eeec3/vngudivada},
  interhash = {4615d2b6515b7b36de92ab8cca46abaf},
  intrahash = {82c432b5f350e4e958160aaf732eeec3},
  keywords = {FlippedClassroom NsfRED ResearchBasedTeachingMethods},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Beyond the Flipped Class: the Impact of Research-Based Teaching Methods in a Macroeconomics Principles Class},
  year = 2016
}

@book{paar2009understanding,
  abstract = {Cryptography is now ubiquitous – moving beyond the traditional environments, such as government communications and banking systems, we see cryptographic techniques realized in Web browsers, e-mail programs, cell phones, manufacturing systems, embedded software, smart buildings, cars, and even medical implants. Today's designers need a comprehensive understanding of applied cryptography.

After an introduction to cryptography and data security, the authors explain the main techniques in modern cryptography, with chapters addressing stream ciphers, the Data Encryption Standard (DES) and 3DES, the Advanced Encryption Standard (AES), block ciphers, the RSA cryptosystem, public-key cryptosystems based on the discrete logarithm problem, elliptic-curve cryptography (ECC), digital signatures, hash functions, Message Authentication Codes (MACs), and methods for key establishment, including certificates and public-key infrastructure (PKI). Throughout the book, the authors focus on communicating the essentials and keeping the mathematics to a minimum, and they move quickly from explaining the foundations to describing practical implementations, including recent topics such as lightweight ciphers for RFIDs and mobile devices, and current key-length recommendations.

The authors have considerable experience teaching applied cryptography to engineering and computer science students and to professionals, and they make extensive use of examples, problems, and chapter reviews, while the book’s website offers slides, projects and links to further resources. This is a suitable textbook for graduate and advanced undergraduate courses and also for self-study by engineers.},
  added-at = {2016-12-10T23:06:06.000+0100},
  address = {New York, NY},
  author = {Paar, Christof and Pelzl, Jan},
  biburl = {https://www.bibsonomy.org/bibtex/24e48048c7d88a606f830817d39d18859/vngudivada},
  interhash = {928b69aa3016f79e86610cd1524a2229},
  intrahash = {4e48048c7d88a606f830817d39d18859},
  keywords = {Book Cryptography},
  publisher = {Springer},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Understanding cryptography:  a textbook for students and practitioners},
  year = 2009
}

@book{drake2014android,
  abstract = {The first comprehensive guide to discovering and preventing attacks on the Android OS. As the Android operating system continues to increase its share of the smartphone market, smartphone hacking remains a growing threat. Written by experts who rank among the world's foremost Android security researchers, this book presents vulnerability discovery, analysis, and exploitation tools for the good guys. Following a detailed explanation of how the Android OS works and its overall security architecture, the authors examine how vulnerabilities can be discovered and exploits developed for various system components, preparing you to defend against them. If you are a mobile device administrator, security researcher, Android app developer, or consultant responsible for evaluating Android security, you will find this guide is essential to your toolbox. A crack team of leading Android security researchers explain Android security risks, security design and architecture, rooting, fuzz testing, and vulnerability analysis; Covers Android application building blocks and security as well as debugging and auditing Android apps; Prepares mobile device administrators, security researchers, Android app developers, and security consultants to defend Android systems against attack. Android Hacker's Handbook is the first comprehensive resource for IT professionals charged with smartphone security.--},
  added-at = {2016-12-10T22:37:58.000+0100},
  address = {Indianapolis, IN},
  author = {Drake, Joshua J. and Lanier, Zach and Mulliner, Collin and Oliva, Pau and Ridley, Stephen A. and Wicherski, Georg},
  biburl = {https://www.bibsonomy.org/bibtex/2f1fc9a597f38705fc8cf3d13a665bc81/vngudivada},
  interhash = {6bb30babdeaa2df0782f6e513f80056a},
  intrahash = {f1fc9a597f38705fc8cf3d13a665bc81},
  keywords = {Android Book Cryptography},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Android hacker's handbook},
  year = 2014
}

@misc{butler2017integrating,
  abstract = {The most effective educational interventions often face significant barriers to widespread implementation because they are highly specific, resource-intense, and/or require comprehensive reform. We argue for an alternative approach to improving education: leveraging technology and cognitive science to develop interventions that generalize, scale, and can be easily implemented within any curriculum. In a classroom experiment, we investigated whether three simple, but powerful principles from cognitive science could be combined to improve learning. Although implementing these principles only required a few small changes to standard practice in a college engineering course, it significantly increased student performance on exams. Our findings highlight the potential for developing inexpensive, yet effective educational interventions that can be implemented worldwide.},
  added-at = {2016-12-23T23:17:24.000+0100},
  author = {Butler, Andrew C. and Marsh, Elizabeth J. and Slavinsky, J. P. and Baraniu, Richard G.},
  biburl = {https://www.bibsonomy.org/bibtex/2915b5a2dfb5ebd8b22869d9a18d5e5eb/vngudivada},
  interhash = {a8b213b451cf2909e3acb5ce6d55d985},
  intrahash = {915b5a2dfb5ebd8b22869d9a18d5e5eb},
  keywords = {CognitiveScience Learning NsfRED},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Integrating Cognitive Science and Technology Improves Learning in a STEM Classroom},
  year = 2017
}

@inproceedings{barr2010everyone,
  added-at = {2017-01-01T02:40:02.000+0100},
  address = {New York, NY},
  author = {Barr, John and Cooper, Steve and Goldweber, Michael and Walker, Henry},
  biburl = {https://www.bibsonomy.org/bibtex/2cdca0c8450e71970dfdc6bff924637f3/vngudivada},
  booktitle = {Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
  interhash = {4dc3ba2af493d903e415b542f715ca99},
  intrahash = {cdca0c8450e71970dfdc6bff924637f3},
  keywords = {ComputationalThinking NsfRED},
  pages = {127--128},
  publisher = {ACM},
  series = {SIGCSE '10},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {What Everyone Needs to Know About Computation},
  url = {http://dblp.uni-trier.de/db/conf/sigcse/sigcse2010.html#BarrCGW10},
  year = 2010
}

@book{pascarella2005college,
  abstract = {This is the long-awaited second volume of Pascarella and Terenzini's 1991 award-winning review of the research on the impacts of college on students. The authors review their earlier findings and then synthesize what has been learned since 1990 about college's influences on students' learning. The book also discusses the implications of the findings for research, practice, and public policy. This authoritative and comprehensive analysis of the literature on college-impact is required reading for anyone interested in higher education practice, policy, and promise--faculty, administrators, researchers, policy analysts, and decision-makers at every level.},
  added-at = {2017-01-01T01:31:49.000+0100},
  address = {San Francisco},
  author = {Pascarella, Ernest T. and Terenzini, Patrick T.},
  biburl = {https://www.bibsonomy.org/bibtex/2de1b6c67c50a34a5170959c7d08d95bd/vngudivada},
  interhash = {89e2d84ed593c6548249192a2bcaf200},
  intrahash = {de1b6c67c50a34a5170959c7d08d95bd},
  keywords = {Book CommunityCollege NsfRED TransferStudent},
  publisher = {Jossey-Bass},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {How College Affects Students: A third decade of research, Volume 2},
  year = 2005
}

@article{mishra1994mutual,
  abstract = {Several factors that explain differences in the downsizing strategies utilized in organizations are examined in this study of 91 organizations in the automotive industry. Specifically, mutual trust within a top management team is positively associated with a strategy based on organization redesign. Mutual trust between members of an organization and its key customers and suppliers is positively associated with a strategy based on systemic change. Moreover, these two strategies are positively associated with performance outcomes in the areas of cost reduction and quality improvement.},
  added-at = {2017-01-01T23:36:12.000+0100},
  author = {Mishra, Aneil K. and Mishra, Karen E.},
  biburl = {https://www.bibsonomy.org/bibtex/2e7e5e0c06833bd9a0ed19e0606dd4953/vngudivada},
  interhash = {0ae446beb6a4ae649edc90d8b53e14e0},
  intrahash = {e7e5e0c06833bd9a0ed19e0606dd4953},
  journal = {Human Resource Management},
  keywords = {NsfRED Trust},
  number = 2,
  pages = {261--279},
  publisher = {Wiley Subscription Services, Inc.,},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {The role of mutual trust in effective downsizing strategies},
  url = {http://dx.doi.org/10.1002/hrm.3930330207},
  volume = 33,
  year = 1994
}

@book{meeker2015source,
  abstract = {Heather Meeker’s Open Source for Business is a practical, readable guide to help businesspeople, engineers, and lawyers understand open source software licensing. Based on the author’s twenty years as an attorney working at the crossroads of intellectual property and technology, this guide explains the legal and technical principles behind open source licensing so you can make the right decisions for your business. It offers tips on using open source, contributing to open source projects, and releasing your own open source software. You’ll also find quick-reference tables on the major open source licenses, plus forms and checklists you can use to promote compliance. In this book, you will learn . . . • Why open source is not a “virus” • What the GPL is and how to handle it • When and how to conduct open source audits • What a user-friendly open source policy looks like • How to avoid and respond to open source enforcement claims • How to use open source to fight patent infringement claims • How to manage trademarks for open source products},
  added-at = {2016-12-25T19:04:30.000+0100},
  author = {Meeker, Heather J.},
  biburl = {https://www.bibsonomy.org/bibtex/2c2442a0e10fce7b11a87eca8bfde3e59/vngudivada},
  edition = {First},
  interhash = {8c0f0c33a8f5a8c8564aa18280a34284},
  intrahash = {c2442a0e10fce7b11a87eca8bfde3e59},
  keywords = {Book FOSS},
  publisher = {CreateSpace Independent Publishing Platform},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Open source for business: a practical guide to open source software licensing},
  year = 2015
}

@article{talia2013workflow,
  abstract = {The wide availability of high-performance computing systems, Grids and Clouds, allowed scientists and engineers to implement more and more complex applications to access and process large data repositories and run scientific experiments in silico on distributed computing platforms. Most of these applications are designed as workflows that include data analysis, scientific computation methods, and complex simulation techniques. Scientific applications require tools and high-level mechanisms for designing and executing complex workflows. For this reason, in the past years, many efforts have been devoted towards the development of distributed workflow management systems for scientific applications. This paper discusses basic concepts of scientific workflows and presents workflow system tools and frameworks used today for the implementation of application in science and engineering on high-performance computers and distributed systems. In particular, the paper reports on a selection of workflow systems largely used for solving scientific problems and discusses some open issues and research challenges in the area.},
  added-at = {2016-12-19T05:14:36.000+0100},
  author = {Talia, Domenico},
  biburl = {https://www.bibsonomy.org/bibtex/265d449f887a06443919094e814385a57/vngudivada},
  interhash = {3f311ad63c0f2ad0fb0dd31ddc73229d},
  intrahash = {65d449f887a06443919094e814385a57},
  journal = {ISRN Software Engineering},
  keywords = {ScientificWorkflowSystem},
  pages = 15,
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Workflow Systems for Science: Concepts and Tools},
  url = {http://dx.doi.org/10.1155/2013/404525},
  volume = 2013,
  year = 2013
}

@book{cooperrider2012appreciative,
  abstract = {Written by the originators and leaders of the Appreciative Inquiry (AI) movement itself, this short, practical guide offers an approach to organizational change based on the possibility of a more desirable future, experience with the whole system, and activities that signal ''something different is happening this time.'' That difference systematically taps the potential of human beings to make themselves, their organizations, and their communities more adaptive and more effective. AI, a theory of collaborative change, erases the winner/loser paradigm in favor of coordinated actions and closer relationships that lead to solutions at once simpler and more effective.},
  added-at = {2017-01-05T00:43:58.000+0100},
  address = {San Francisco, CA},
  author = {Cooperrider, David L. and Whitney, Diana Kaplin},
  biburl = {https://www.bibsonomy.org/bibtex/21e9e54dfb324d32e9d3983e5720c034b/vngudivada},
  edition = {Sixteenth},
  interhash = {ae2339953453157e336b74a78026cd2c},
  intrahash = {1e9e54dfb324d32e9d3983e5720c034b},
  keywords = {AppreciativeInquiry Book Change Leadership NsfRED},
  publisher = {Berrett-Koehler},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Appreciative inquiry: a positive revolution in change},
  year = 2012
}

@article{nguyen2016students,
  abstract = {Engineering instructors' adoption of active learning has been slow, despite significant evidence supporting its efficacy. A common instructor concern is that students will respond negatively. This study measures the relationship between student response to instruction and 1) students' expectations for types of instruction, 2) students’ experiences of different types of instruction, and 3) instructor strategies for using in-class activities. Student Response to Instructional Practices (StRIP) survey data from 179 students at three U.S. institutions were analyzed using hierarchical linear regression modeling. Significant predictors in the final models of student response were student expectations of active learning lecture and passive lecture, experiences of group based activities, and instructor strategies for explaining and facilitating active learning. These empirical results support recommendations in prior literature about best practices for reducing student resistance and demonstrate that instructors have great power to influence student reactions to active learning and ultimately reduce student resistance. There was no evidence in this data set to support the common concern that instructor or course evaluations are negatively affected by adopting active learning strategies.},
  added-at = {2016-12-23T21:55:29.000+0100},
  author = {Nguyen, K. and Husman, J. and Borrego, M. and Shekhar, P. and Prince, M. and Demonbrun, M. and Finelli, C.J. and Henderson, C. and Waters, C.},
  biburl = {https://www.bibsonomy.org/bibtex/2c952adcc7af3eb6e3f1c3a96c7d481fc/vngudivada},
  interhash = {e49f09a55e645edff344a5a96b801727},
  intrahash = {c952adcc7af3eb6e3f1c3a96c7d481fc},
  journal = {International Journal of Engineering Education},
  keywords = {sys:relevantfor:ecu-cc-research ActiveLearning NsfRED},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Students' Expectations, Types of Instruction, and Instructor Strategies Predicting Student Response to Active Learning},
  year = 2016
}

@article{khatri2016designing,
  abstract = {The physics education research community has produced a wealth of knowledge about effective teaching and learning of college level physics. Based on this knowledge, many research-proven instructional strategies and teaching materials have been developed and are currently available to instructors. Unfortunately, these intensive research and development activities have failed to influence the teaching practices of many physics instructors. This paper describes interim results of a larger study to develop a model of designing materials for successful propagation. The larger study includes three phases, the first two of which are reported here. The goal of the first phase was to characterize typical propagation practices of education developers, using data from a survey of 1284 National Science Foundation (NSF) principal investigators and focus group data from eight disciplinary groups of NSF program directors. The goal of the second phase was to develop an understanding of successful practice by studying three instructional strategies that have been well propagated. The result of the first two phases is a tentative model of designing for successful propagation, which will be further validated in the third phase through purposeful sampling of additional well-propagated instructional strategies along with typical education development projects. We found that interaction with potential adopters was one of the key missing ingredients in typical education development activities. Education developers often develop a polished product before getting feedback, rely on mass-market communication channels for dissemination, and do not plan for supporting adopters during implementation. The tentative model resulting from this study identifies three key propagation activities: interactive development, interactive dissemination, and support of adopters. Interactive development uses significant feedback from potential adopters to develop a strong product suitable for use in many settings. Interactive dissemination uses personal interactions to reach and motivate potential users. Support of adopters is missing from typical propagation practice and is important to reduce the burden of implementation and increases the likelihood of successful adoption.},
  added-at = {2016-12-23T19:17:55.000+0100},
  author = {Khatri, Raina and Henderson, Charles and Cole, Ren\'ee and Froyd, Jeffrey E. and Friedrichsen, Debra and Stanford, Courtney},
  biburl = {https://www.bibsonomy.org/bibtex/23f4bee874cf071927df8755886ea7a16/vngudivada},
  interhash = {b449efe5c0e194e8d3ce3b5f933d3f19},
  intrahash = {3f4bee874cf071927df8755886ea7a16},
  journal = {Phys. Rev. Phys. Educ. Res.},
  keywords = {EducationalInnovation NsfRED STEM SuccessfulPropagation},
  month = feb,
  number = 1,
  pages = {010112-1 -- 010112-22},
  publisher = {American Physical Society},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Designing for sustained adoption: A model of developing educational innovations for successful propagation},
  url = {http://link.aps.org/doi/10.1103/PhysRevPhysEducRes.12.010112},
  volume = 12,
  year = 2016
}

@article{vanderijt2014field,
  abstract = {Seemingly similar individuals often experience drastically different success trajectories, with some repeatedly failing and others consistently succeeding. One explanation is preexisting variability along unobserved fitness dimensions that is revealed gradually through differential achievement. Alternatively, positive feedback operating on arbitrary initial advantages may increasingly set apart winners from losers, producing runaway inequality. To identify social feedback in human reward systems, we conducted randomized experiments by intervening in live social environments across the domains of funding, status, endorsement, and reputation. In each system we consistently found that early success bestowed upon arbitrarily selected recipients produced significant improvements in subsequent rates of success compared with the control group of nonrecipients. However, success exhibited decreasing marginal returns, with larger initial advantages failing to produce much further differentiation. These findings suggest a lesser degree of vulnerability of reward systems to incidental or fabricated advantages and a more modest role for cumulative advantage in the explanation of social inequality than previously thought},
  added-at = {2017-01-03T04:20:40.000+0100},
  author = {van de Rijt, Arnout and Kang, Soong Moon and Restivo, Michael and Patil, Akshay},
  biburl = {https://www.bibsonomy.org/bibtex/258f0da0c85351569fadc4bc5fbe76798/vngudivada},
  interhash = {47dfabe97b8b8bf1596e6b7486569c74},
  intrahash = {58f0da0c85351569fadc4bc5fbe76798},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {Leadership NsfRED SuccessBreedsSuccess},
  number = 19,
  pages = {6934-6939},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Field experiments of success-breeds-success dynamics},
  volume = 111,
  year = 2014
}

@article{bush2016fostering,
  abstract = {Globally, calls for the improvement of science education are frequent and fervent. In parallel, the phenomenon of having Science Faculty with Education Specialties (SFES) within science departments appears to have grown in recent decades. In the context of an interview study of a randomized, stratified sample of SFES from across the United States, we discovered that most SFES interviewed (82%) perceived having professional impacts in the realm of improving undergraduate science education, more so than in research in science education or K-12 science education. While SFES reported a rich variety of efforts towards improving undergraduate science education, the most prevalent reported impact by far was influencing the teaching practices of their departmental colleagues. Since college and university science faculty continue to be hired with little to no training in effective science teaching, the seeding of science departments with science education specialists holds promise for fostering change in science education from within biology, chemistry, geoscience, and physics departments.},
  added-at = {2016-12-23T18:53:21.000+0100},
  author = {Bush, Seth D. and {Rudd, II}, James A. and Stevens, Michael T. and Tanner, Kimberly D. and Williams, Kathy S.},
  biburl = {https://www.bibsonomy.org/bibtex/27ecb12d1c703d663c3e974ae4319847a/vngudivada},
  doi = {10.1371/journal.pone.0150914},
  interhash = {ae37d70458a7509f8c7785016dc06767},
  intrahash = {7ecb12d1c703d663c3e974ae4319847a},
  journal = {PLOS ONE},
  keywords = {Change NsfRED TeachingPractice},
  month = {03},
  number = 3,
  pages = {1-20},
  publisher = {Public Library of Science},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Fostering Change from Within: Influencing Teaching Practices of Departmental Colleagues by Science Faculty with Education Specialties},
  url = {http://dx.doi.org/10.1371%2Fjournal.pone.0150914},
  volume = 11,
  year = 2016
}

@article{besterfieldsacre2014changing,
  abstract = {Background
Many reports present a vision of what engineering education should look like, but few describe how this should happen. An American Society for Engineering Education initiative in 2006 attempted to bridge this gap by engaging faculty, chairs, and deans in discussion of change in engineering education; results were reported in a Phase I report (2009). In a second phase, survey data were integrated into a Phase II report (2012).

Purpose
This article uses the ASEE survey results to identify promising pathways for transforming engineering undergraduate education.

Design/Method
The survey asked faculty, chairs, and deans at engineering departments at 156 U.S. institutions to reflect on the recommendations of the Phase I report. Quantitative and qualitative responses were separately analyzed and then mixed by mapping findings to the Four Categories of Change Strategies model developed by Henderson et al. (2011), which frames the results and illustrates gaps and opportunities.

Results
Responses mapped to three of the four categories of the model that were most commonly used in other STEM education efforts: developing and disseminating new instructional approaches, supporting faculty members in their own scholarly teaching, and implementing policies that enable and reward teaching innovation. No responses mapped to developing a shared vision through activities such as strategic planning.

Conclusions
The greatest promise for transformative change in engineering education lies in developing a shared vision for educational innovation. The findings of this article provide a foundation for ongoing discussion and evaluating progress.
},
  added-at = {2016-12-23T21:35:03.000+0100},
  author = {Besterfield-Sacre, Mary and Cox, Monica F. and Borrego, Maura and Beddoes, Kacey and Zhu, Jiabin},
  biburl = {https://www.bibsonomy.org/bibtex/28850fe231e5157c4b0fd00f4bb36843c/vngudivada},
  doi = {10.1002/jee.20043},
  interhash = {078a911d23c096c9f558696a9da73623},
  intrahash = {8850fe231e5157c4b0fd00f4bb36843c},
  issn = {2168-9830},
  journal = {Journal of Engineering Education},
  keywords = {Change EngineeringEducation NsfRED},
  number = 2,
  pages = {193--219},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Changing Engineering Education: Views of U.S. Faculty, Chairs, and Deans},
  url = {http://dx.doi.org/10.1002/jee.20043},
  volume = 103,
  year = 2014
}

@book{singh2016secrets,
  abstract = {"As gripping as a good thriller." --The Washington Post

Unpack the science of secrecy and discover the methods behind cryptography--the encoding and decoding of information--in this clear and easy-to-understand young adult adaptation of the national bestseller that's perfect for this age of WikiLeaks, the Sony hack, and other events that reveal the extent to which our technology is never quite as secure as we want to believe.

Coders and codebreakers alike will be fascinated by history's most mesmerizing stories of intrigue and cunning--from Julius Caesar and his Caeser cipher to the Allies' use of the Enigma machine to decode German messages during World War II.

Accessible, compelling, and timely, The Code Book is sure to make readers see the past--and the future--in a whole new way.

"Singh's power of explaining complex ideas is as dazzling as ever." --The Guardian
It's known as the science of secrecy. Cryptography: the encoding and decoding of private information. And it is history's most fascinating story of intrigue and cunning. From Julius Caesar and his Caesar Cipher to the code used by Mary Queen of Scots and her conspiracy to the use of the Engima machine during the Second World War, Simon Singh follows the evolution of secret writing. Accessible, compelling, and timely, this international bestseller, now adapted for young people, is sure to make readers see the past -- and the future -- in a whole new way.},
  added-at = {2016-12-29T01:54:58.000+0100},
  address = {New York},
  author = {Singh, Simon},
  biburl = {https://www.bibsonomy.org/bibtex/2692f4896c0f4a75730be867099a1af57/vngudivada},
  interhash = {344bc6dd9a68fb47f79936bc1f90bed2},
  intrahash = {692f4896c0f4a75730be867099a1af57},
  keywords = {Book CodeBook Cryptography},
  publisher = {Delacorte Press},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {The Code Book: The Secrets Behind Codebreaking},
  year = 2016
}

@book{bellamy2012practices,
  abstract = {Darwin Information Typing Architecture (DITA) is today’s most powerful toolbox for constructing information. By implementing DITA, organizations can gain more value from their technical documentation than ever before. Now, three DITA pioneers offer the first complete roadmap for successful DITA adoption, implementation, and usage.

Drawing on years of experience helping large organizations adopt DITA, the authors answer crucial questions the “official” DITA documents ignore, including: Where do you start? What should you know up front? What are the pitfalls in implementing DITA? How can you avoid those pitfalls?

The authors begin with topic-based writing, presenting proven best practices for developing effective topics and short descriptions. Next, they address content architecture, including how best to set up and implement DITA maps, linking strategies, metadata, conditional processing, and content reuse. Finally, they offer “in the trenches” solutions for ensuring quality implementations, including guidance on content conversion.

Coverage includes:
Knowing how and when to use each DITA element–and when not to
Writing “minimalist,” task-oriented information that quickly meets users’ needs
Creating effective task, concept, and reference topics for any product, technology, or service
Writing effective short descriptions that work well in all contexts
Structuring DITA maps to bind topics together and provide superior navigation
Using links to create information webs that improve retrievability and navigation
Gaining benefits from metadata without getting lost in complexity
Using conditional processing to eliminate redundancy and rework
Systematically promoting reuse to improve quality and reduce costs
Planning, resourcing, and executing effective content conversion
Improving quality by editing DITA content and XML markup
If you’re a writer, editor, information architect, manager, or consultant who evaluates, deploys, or uses DITA, this book will guide you all the way to success.},
  added-at = {2016-12-07T01:02:50.000+0100},
  address = {Upper Saddle River, NJ},
  author = {Bellamy, Laura and Carey, Michelle and Schlotfeldt, Jenifer},
  biburl = {https://www.bibsonomy.org/bibtex/21ddc56d925868018acb2cf68d2af7037/vngudivada},
  interhash = {7a616df60b556cf447dc713360dce21c},
  intrahash = {1ddc56d925868018acb2cf68d2af7037},
  keywords = {Book DITA},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {DITA best practices : a roadmap for writing, editing, and architecting in DITA},
  year = 2012
}

@book{fogel2005producing,
  abstract = {The corporate market is now embracing free, "open source" software like never before, as evidenced by the recent success of the technologies underlying LAMP (Linux, Apache, MySQL, and PHP). Each is the result of a publicly collaborative process among numerous developers who volunteer their time and energy to create better software. The truth is, however, that the overwhelming majority of free software projects fail. To help you beat the odds, O'Reilly has put together Producing Open Source Software, a guide that recommends tried and true steps to help free software developers work together toward a common goal. Not just for developers who are considering starting their own free software project, this book will also help those who want to participate in the process at any level. The book tackles this very complex topic by distilling it down into easily understandable parts. Starting with the basics of project management, it details specific tools used in free software projects, including version control, IRC, bug tracking, and Wikis. Author Karl Fogel, known for his work on CVS and Subversion, offers practical advice on how to set up and use a range of tools in combination with open mailing lists and archives. He also provides several chapters on the essentials of recruiting and motivating developers, as well as how to gain much-needed publicity for your project.},
  added-at = {2016-12-25T18:54:57.000+0100},
  address = {Sebastopol, CA},
  author = {Fogel, Karl},
  biburl = {https://www.bibsonomy.org/bibtex/2377e58eef154f82441e5bf14e81855b0/vngudivada},
  interhash = {de098e7f615b85c05431bb69f34529c9},
  intrahash = {377e58eef154f82441e5bf14e81855b0},
  keywords = {Book FOSS},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Producing open source software: how to run a successful free software project},
  year = 2005
}

@article{henderson2015cognitive,
  abstract = {This article explores the directions needed to facilitate widespread adoption of the findings of cognitive science (CS) into undergraduate instruction in the disciplines of science, technology, engineering, and mathematics (STEM). The emerging research tradition of STEM discipline-based education research (DBER) is introduced briefly, with a focus on physics education research (PER). Examples of cognitive science research that are beginning to affect classroom practice are introduced, as well as examples that have direct implications for improving STEM instructional practices, yet remain largely unknown in the STEM community. Two barriers slow the implementation of CS findings in undergraduate STEM instruction. The first is lack of communication between cognitive science and STEM DBER researchers. The second is that, even when strong curricula and instructional practices are developed, there are many structural obstacles that make it difficult for STEM instructors to implement new instructional strategies. We provide an overview of current efforts to overcome these structural obstacles, and suggest policy implications for the cognitive science and DBER research communities that could facilitate the development, evaluation, and adoption of research-based instructional strategies in STEM undergraduate education.},
  added-at = {2016-12-23T22:22:56.000+0100},
  author = {Henderson, C. and Mestre, J. P. and Slakey, L. L.},
  biburl = {https://www.bibsonomy.org/bibtex/2915faead628c31e8cb033449fdb59271/vngudivada},
  doi = {10.1177/2372732215601115},
  interhash = {d2e9b83adf018b975928af23d696c841},
  intrahash = {915faead628c31e8cb033449fdb59271},
  journal = {Policy Insights from the Behavioral and Brain Sciences},
  keywords = {InstructionalPractice NsfRED STEM},
  month = aug,
  number = 1,
  pages = {51--60},
  publisher = {{SAGE} Publications},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Cognitive Science Research Can Improve Undergraduate {STEM} Instruction: What Are the Barriers?},
  url = {http://dx.doi.org/10.1177/2372732215601115},
  volume = 2,
  year = 2015
}

@inproceedings{espino2016gender,
  abstract = {Technologies of Information and Communication Technologies (ICT) have contributed significantly to the emergence of the Knowledge Society. However, there is a strong gender gap in the field of science and technology due to the influence of the male-centered culture that persists today. Computational thinking is presented as a way to develop in children from an early age the ability to solve problems, designing systems and understanding human behavior while using the fundamental concepts of computing. This factor allows girls to be in touch with technology in a fun and meaningful way; so as to balance the male and female presence in science and technology, achieving a more egalitarian system. In this article is presented a systematic review of the literature which indicates that few studies about computational thinking are working from a gender perspective, although some countries have already opted for teaching computer in the classroom. In addition, the review shows some methodologies which are using languages and suitable tools in order to work the computational thinking. Notwithstanding, It can be the basis for a proposal to integrate the gender perspective. In turn, it is committed to the creation of a methodological guide that encourages this teaching in national and international schools.},
  added-at = {2017-01-01T03:08:18.000+0100},
  address = {New York, NY},
  author = {Espino, Elisenda Eva Espino and Gonz\'{a}lez, Carina Gonz\'{a}lez},
  biburl = {https://www.bibsonomy.org/bibtex/20e3c405aff8e8c4aa31dfa12919b5079/vngudivada},
  booktitle = {Proceedings of the XVII International Conference on Human Computer Interaction},
  interhash = {1d3e7eb7c9f7d282db7c869377a35711},
  intrahash = {0e3c405aff8e8c4aa31dfa12919b5079},
  keywords = {ComputationalThinking},
  pages = {46:1--46:2},
  publisher = {ACM},
  series = {Interacci\ón '16},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Gender and Computational Thinking: Review of the Literature and Applications},
  url = {http://doi.acm.org/10.1145/2998626.2998665},
  year = 2016
}

@article{shekhar2015development,
  abstract = {Student resistance is often cited as a major barrier to instructors' use of active learning, but there are few research-based strategies for reducing this barrier. In this paper, we describe the first phase of our research-the development and validation of a classroom observation protocol to assess student responses to instructors' use of active learning. This protocol, which draws upon other published observation protocols, allows researchers to capture data about instructors' use of and students' responses to active learning. We also present findings from four first and second year engineering courses at two institutions that demonstrate the variety of ways engineering students resist active learning and strategies that engineering instructors have employed to reduce student resistance.},
  added-at = {2016-12-23T22:24:31.000+0100},
  author = {Shekhar, Prateek and Demonbrun, Matt and Borrego, Maura and Finelli, Cynthia and Prince, Michael and Henderson, Charles and Waters, Cynthia},
  biburl = {https://www.bibsonomy.org/bibtex/2a5a8a69aec14a67f4eeb7ef62b860774/vngudivada},
  interhash = {445a9b28014bbc0f7bfda2206f46c93b},
  intrahash = {a5a8a69aec14a67f4eeb7ef62b860774},
  issn = {0949-149X},
  journal = {International Journal of Engineering Education},
  keywords = {ActiveLearning NsfRED},
  number = 2,
  pages = {597--609},
  publisher = {Tempus Publications},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Development of an observation protocol to study undergraduate engineering student resistance to active learning},
  volume = 31,
  year = 2015
}

@article{sciore2007simpledb,
  abstract = {In this paper we examine the probl of how to give hands-on assignments in a database system internals course. We argue that current approaches are inadequate, either because they are not sufficiently comprehensive or because they require using software that has a steep learning curve. We then describe SimpleDB, which is software written expressly for such a course. SimpleDB is a database syst in the spirit of Minibase. Unlike Minibase, however, it supports multiple users and transactions via JDBC, and its code is easy to read and modify. We then describe a course that we teach using SimpleDB, and discuss the educational benefits resulting from it.},
  added-at = {2016-12-29T19:26:50.000+0100},
  address = {New York, NY},
  author = {Sciore, Edward},
  biburl = {https://www.bibsonomy.org/bibtex/2d321278901565b5a8b5faf3bdae67d72/vngudivada},
  interhash = {780b6ec11366bf9eb9c9e3bdac9eeae3},
  intrahash = {d321278901565b5a8b5faf3bdae67d72},
  journal = {SIGCSE Bull.},
  keywords = {DBMSImplementation DBMSInternals SimpleDB},
  month = mar,
  number = 1,
  pages = {561--565},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {SimpleDB: A Simple Java-based Multiuser System for Teaching Database Internals},
  url = {http://doi.acm.org/10.1145/1227504.1227498},
  volume = 39,
  year = 2007
}

@book{grinstead1997introduction,
  abstract = {text is designed for an introductory probability course at the university level for sophomores, juniors, and seniors in mathematics, physical and social sciences, engineering, and computer science. It presents a thorough treatment of ideas and techniques necessary for a firm understanding of the subject. The text is also recommended for use in discrete probability courses. The material is organized so that the discrete and continuous probability discussions are presented in a separate, but parallel, manner. This organization does not emphasize an overly rigorous or formal view of probabililty and therefore offers some strong pedagogical value. Hence, the discrete discussions can sometimes serve to motivate the more abstract continuous probability discussions. Features: Key ideas are developed in a somewhat leisurely style, providing a variety of interesting applications to probability and showing some nonintuitive ideas. Over 600 exercises provide the opportunity for practicing skills and developing a sound understanding of ideas. Numerous historical comments deal with the development of discrete probability. The text includes many computer programs that illustrate the algorithms or the methods of computation for important problems.},
  added-at = {2016-12-07T13:52:30.000+0100},
  address = {Providence, RI},
  author = {Grinstead, Charles M. and Snell, J. Laurie and Snell, J. Laurie},
  biburl = {https://www.bibsonomy.org/bibtex/231d6dbecfa75c90a095cbb422c1b318a/vngudivada},
  interhash = {960991aca753f34cb285ec365ce2d546},
  intrahash = {31d6dbecfa75c90a095cbb422c1b318a},
  keywords = {Book ParExcellence Probability},
  publisher = {American Mathematical Society},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Introduction to probability},
  year = 1997
}

@article{cervantes2016architectural,
  abstract = {An examination of the security approaches in industrial and open source projects shows that a strategic, systemwide architectural approach, implemented as a security framework or as a platform built using these frameworks, results in the highest security and lowest maintenance costs.},
  added-at = {2016-12-10T00:59:33.000+0100},
  author = {Cervantes, H. and Kazman, R. and Ryoo, J. and Choi, D. and Jang, D.},
  biburl = {https://www.bibsonomy.org/bibtex/28c9b6640323c672d9b0d835264b24499/vngudivada},
  doi = {10.1109/MC.2016.332},
  interhash = {182e4ed374ffe7dcbc91de4393ccb54e},
  intrahash = {8c9b6640323c672d9b0d835264b24499},
  journal = {Computer},
  keywords = {Architecture Cybersecurity},
  month = nov,
  number = 11,
  pages = {60-67},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Architectural Approaches to Security: Four Case Studies},
  volume = 49,
  year = 2016
}

@article{wilson2002study,
  abstract = {This study was conducted to determine factors that promote success in an introductory college computer science course and to determine what, if any, differences appear between genders on those factors. The model included math background, attribution for success/failure, self-efficacy, encouragement, comfort level in the course, work style preference, previous programming experience, previous non-programming computer experience, and gender as possible predictive factors for success in the computer science course. Subjects included 105 students enrolled in an introductory computer science course. The study revealed three predictive factors in the following order of importance: comfort level (with a positive influence), math background (with a positive influence), and attribution to luck (with a negative influence). No significant gender differences were found in these three factors. The study also revealed that both a formal class in programming (which had a positive correlation) and game playing (which had a negative correlation) were predictive of success. The study revealed a significant gender difference in game playing with males reporting more experience with playing games on the computer than females reported.},
  added-at = {2017-01-04T21:26:51.000+0100},
  author = {Wilson, Brenda Cantwell},
  biburl = {https://www.bibsonomy.org/bibtex/2da339759e1de1382030636ff72cf45e6/vngudivada},
  interhash = {fb885c9f8e13df82fa365312b5364cdb},
  intrahash = {da339759e1de1382030636ff72cf45e6},
  journal = {Computer Science Education},
  keywords = {CSEducation Gender NsfRED},
  number = {1-2},
  pages = {141--164},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {A Study of Factors Promoting Success in Computer Science Including Gender Differences},
  volume = 12,
  year = 2002
}

@inproceedings{heller2011visualizing,
  abstract = {We apply visualization techniques to user profiles and repository metadata from the GitHub source code hosting service. Our motivation is to identify patterns within this development community that might otherwise remain obscured. Such patterns include the effect of geographic distance on developer relationships, social connectivity and influence among cities, and variation in projectspecific contribution styles (e.g., centralized vs. distributed). Our analysis examines directed graphs in which nodes represent users’ geographic locations and edges represent (a) follower relationships, (b) successive commits, or (c) contributions to the same project. We inspect this data using a set of visualization techniques: geo-scatter maps, small multiple displays, and matrix diagrams. Using these representations, and tools based on them, we develop hypotheses about the larger GitHub community that would be difficult to discern using traditional lists, tables, or descriptive statistics. These methods are not intended to provide conclusive answers; instead, they provide a way for researchers to explore the question space and communicate initial insights.},
  added-at = {2016-12-26T00:16:58.000+0100},
  author = {Heller, Brandon and Marschner, Eli and Rosenfeld, Evan and Heer, Jeffrey},
  biburl = {https://www.bibsonomy.org/bibtex/2c4a586bdb624edd712df327317c5b29b/vngudivada},
  booktitle = {MSR},
  editor = {van Deursen, Arie and Xie, Tao and Zimmermann, Thomas},
  interhash = {920f6b09ff01442475017d51fcbc0a1d},
  intrahash = {c4a586bdb624edd712df327317c5b29b},
  keywords = {FOSS},
  pages = {223-226},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Visualizing collaboration and influence in the open-source software community},
  year = 2011
}

@book{tanimoto2012interdisciplinary,
  abstract = {This book explores image processing from several perspectives: the creative, the theoretical (mainly mathematical), and the programmatical. It explains the basic principles of image processing, drawing on key concepts and techniques from mathematics, psychology of perception, computer science, and art, and introduces computer programming as a way to get more control over image processing operations. It does so without requiring college-level mathematics or prior programming experience. The content is supported by PixelMath, a freely available software program that helps the reader understand images as both visual and mathematical objects. The first part of the book covers such topics as digital image representation, sampling, brightness and contrast, color models, geometric transformations, synthesizing images, stereograms, photomosaics, and fractals. The second part of the book introduces computer programming using an open-source version of the easy-to-learn Python language. It covers the basics of image analysis and pattern recognition, including edge detection, convolution, thresholding, contour representation, and K-nearest-neighbor classification. A chapter on computational photography explores such subjects as high-dynamic-range imaging, autofocusing, and methods for automatically inpainting to fill gaps or remove unwanted objects in a scene. Applications described include the design and implementation of an image-based game. The PixelMath software provides a "transparent" view of digital images by allowing the user to view the RGB values of pixels by zooming in on an image. PixelMath provides three interfaces: the pixel calculator; the formula page, and advanced extension of the calculator; and the Python window.},
  added-at = {2016-12-29T01:58:16.000+0100},
  address = {Cambridge, MA},
  author = {Tanimoto, S.},
  biburl = {https://www.bibsonomy.org/bibtex/2b3436d31846353bb71e51de1ac22153f/vngudivada},
  interhash = {967dc540bb2f92a6ac1fcb20fd896276},
  intrahash = {b3436d31846353bb71e51de1ac22153f},
  keywords = {Book DIP NsfRED},
  publisher = {MIT Press},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {An interdisciplinary introduction to image processing: pixels, numbers, and programs},
  year = 2012
}

@article{kramer2010collective,
  abstract = {This paper introduces a conception of collective trust applied to organizational settings. In contrast with widely studied interpersonal forms of trust, collective trust is conceptualized as a kind of generalized trust conferred on other organizational members. The paper elaborates on the psychological, relational, and structural underpinnings of collective trust. It then explicates individual and organizational consequences. The paper also discusses methodological approaches to studying collective trust, ranging from laboratory simulations to field-based, qualitative studies. Representative findings from such studies are summarized.},
  added-at = {2017-01-02T00:05:53.000+0100},
  author = {Kramer, Roderick M},
  biburl = {https://www.bibsonomy.org/bibtex/2e4c738901c36c5dbb1c673f722a3e6d6/vngudivada},
  interhash = {9925a1155652508b570ac746116b453f},
  intrahash = {e4c738901c36c5dbb1c673f722a3e6d6},
  journal = {Corporate Reputation Review},
  keywords = {Leadership NsfRED Trust},
  number = 2,
  pages = {82--97},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Collective Trust within Organizations: Conceptual Foundations and Empirical Insights},
  volume = 13,
  year = 2010
}

@book{schneier2015applied,
  abstract = {From the world's most renowned security technologist, Bruce Schneier, this 20th Anniversary Edition is the most definitive reference on cryptography ever published and is the seminal work on cryptography. Cryptographic techniques have applications far beyond the obvious uses of encoding and decoding information. For developers who need to know about capabilities, such as digital signatures, that depend on cryptographic techniques, there's no better overview than Applied Cryptography, the definitive book on the subject. Bruce Schneier covers general classes of cryptographic protocols and then specific techniques, detailing the inner workings of real-world cryptographic algorithms including the Data Encryption Standard and RSA public-key cryptosystems. The book includes source-code listings and extensive advice on the practical aspects of cryptography implementation, such as the importance of generating truly random numbers and of keeping keys secure.

". . .the best introduction to cryptography I've ever seen. . . .The book the National Security Agency wanted never to be published. . . ." -Wired Magazine

". . .monumental . . . fascinating . . . comprehensive . . . the definitive work on cryptography for computer programmers . . ." -Dr. Dobb's Journal

". . .easily ranks as one of the most authoritative in its field." -PC Magazine

The book details how programmers and electronic communications professionals can use cryptography-the technique of enciphering and deciphering messages-to maintain the privacy of computer data. It describes dozens of cryptography algorithms, gives practical advice on how to implement them into cryptographic software, and shows how they can be used to solve security problems. The book shows programmers who design computer applications, networks, and storage systems how they can build security into their software and systems.

With a new Introduction by the author, this premium edition will be a keepsake for all those committed to computer and cyber security.},
  added-at = {2016-12-10T22:27:02.000+0100},
  address = {New York, NY},
  author = {Schneier, Bruce},
  biburl = {https://www.bibsonomy.org/bibtex/26e330d1bfcf4b53c0c7cec09b5fb2134/vngudivada},
  description = {Dated book.},
  interhash = {fa1f5d180074504bc2b5bea11ebb4348},
  intrahash = {6e330d1bfcf4b53c0c7cec09b5fb2134},
  keywords = {Book Cryptography Cybersecurity},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Applied cryptography: protocols, algorithms, and source code in C},
  year = 2015
}

@book{kramer2004trust,
  abstract = {The effective functioning of a democratic society - including social, business, and political interactions - largely depends on trust. Yet trust remains a fragile and elusive resource in many of the organizations that make up society's building blocks. In their timely volume, Trust and Distrust in Organizations, editors Roderick M. Kramer and Karen S. Cook have compiled the most important research on trust in organizations, illuminating the complex nature of how trust develops, functions, and often is thwarted in organizational settings. With contributions from social psychologists, sociologists, political scientists, economists, and organizational theorists, the volume examines trust and distrust within a variety of settings - from employer-employee and doctor-patient relationships, to geographically dispersed work teams and virtual teams on the Internet. "Trust and Distrust in Organizations opens with an in-depth examination of hierarchical relationships to determine how trust is established and maintained between people with unequal power. Kurt Dirks and Daniel Skarlicki find that trust between leaders and their followers is established when people perceive a shared background or identity and interact well with their leader. After trust is established, people are willing to assume greater risks and to work harder. In part II, the contributors focus on trust between people in teams and networks. Roxanne Zolin and Pamela Hinds discover that trust is more easily established in geographically dispersed teams when they are able to meet face-to-face initially. Trust and Distrust in Organizations moves on to an examination of how people create and foster trust and of the effects of power and betrayal on trust. Kimberly Elsbach reports that managers achieve trust by demonstrating concern, maintaining open communication, and behaving consistently. The final chapter by Roderick Kramer and Dana Gavrieli includes recently declassified data from secret conversations between President Lyndon Johnson and his advisors that provide a rich window into a leader's struggles with problems of trust and distrust in his administration. "Broad in scope, Trust and Distrust in Organizations provides a captivating and insightful look at trust, power, and betrayal, and is essential reading for anyone wishing to understand the underpinnings of trust within a relationship or an organization.-- Publisher description.},
  added-at = {2017-01-01T23:58:38.000+0100},
  address = {New York, NY},
  author = {Kramer, Roderick M. and Cook, Karen S.},
  biburl = {https://www.bibsonomy.org/bibtex/2dbd0e2851ce53040d5dd6ce9459dd353/vngudivada},
  interhash = {6655e28af4ac77d9bc9278a49a0171dc},
  intrahash = {dbd0e2851ce53040d5dd6ce9459dd353},
  keywords = {Leadership NsfRED Trust},
  publisher = {Russell Sage Foundation},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Trust and distrust in organizations : dilemmas and approaches},
  year = 2004
}

@article{haythornthwaite2012media,
  abstract = {This article summarizes the new media and literacy themes that inform the articles in this special issue of IJLM. The articles all address the topic of how new media are transforming what it means to be literate in today’s society and how these media are creating new conditions and forms for learning. The articles come together around the view that digital media are fundamentally changing learning practices and that the transition to digital media is not just a transfer of class content to online venues, nor just an online-only effect, but instead represents a change in learning practice for the digital age. The articles and this special issue result from a workshop that provided the opportunity to integrate knowledge across multiple perspectives to address how new media affect how, where, and with whom we learn and what it means to be literate in the 21st century.},
  added-at = {2016-12-18T13:29:06.000+0100},
  author = {Haythornthwaite, Caroline},
  biburl = {https://www.bibsonomy.org/bibtex/27e5cd97ac8991ae3553b4668ef52d867/vngudivada},
  doi = {10.1162/ijlm_e_00097},
  interhash = {eeec5522c08ff6b2aacb4d88c57b336f},
  intrahash = {7e5cd97ac8991ae3553b4668ef52d867},
  journal = {International Journal of Learning and Media},
  keywords = {Learning},
  month = jul,
  number = {3-4},
  pages = {1--8},
  publisher = {{MIT} Press - Journals},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {New Media, New Literacies, and New Forms of Learning},
  url = {http://dx.doi.org/10.1162/IJLM_e_00097},
  volume = 4,
  year = 2012
}

@article{morelli2009revitalizing,
  abstract = {The humanitarian focus of socially useful projects promises to motivate community-minded undergrads in and out of CS.},
  added-at = {2016-12-30T02:57:57.000+0100},
  address = {New York, NY, USA},
  author = {Morelli, Ralph and Tucker, Allen and Danner, Norman and De Lanerolle, Trishan R. and Ellis, Heidi J. C. and Izmirli, Ozgur and Krizanc, Danny and Parker, Gary},
  biburl = {https://www.bibsonomy.org/bibtex/246764e85f8b5432c200618260e1b4ddc/vngudivada},
  interhash = {f81c403cc222729e7a651395ea2137bc},
  intrahash = {46764e85f8b5432c200618260e1b4ddc},
  journal = {Commun. ACM},
  keywords = {FOSS HFOSS NsfRED},
  month = aug,
  number = 8,
  pages = {67--75},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Revitalizing Computing Education Through Free and Open Source Software for Humanity},
  volume = 52,
  year = 2009
}

@article{henderson2007computational,
  abstract = {Computational Thinking [1] is a universal metaphor of reasoning used by both mankind and machines. From this perspective it has the potential to be a comprehensive umbrella for capturing the intrinsic nature of computing and conveying this in an understandable way to students and the general public. It represents a broad spectrum of reasoning across time and disciplines. Learning to count is a beginning of human computational thinking, followed naturally by arithmetic computation and abstract levels of symbol based thinking, often starting with algebra. Counting, arithmetic, symbols and abstract thinking are fundamental to the study of computing. Computational reasoning is the core of all modern Science, Technology, Engineering and Mathematics (STEM) disciplines and is intrinsic to all other disciplines from A to Z. It is used in our everyday lives from baking a cake, changing a tire or brushing our teeth. The human brain is wired to think computationally, as are modern computing devices. As educators, a Computational Thinking perspective can help us to convey fundamental computing ideas to all students. This special session will outline the principles of Computational Thinking, offer suggestions on ways to promote Computational Thinking at all educational levels, and provide ample time for audience participation and discussion.},
  added-at = {2017-01-01T02:42:11.000+0100},
  address = {New York, NY},
  author = {Henderson, Peter B. and Cortina, Thomas J. and Wing, Jeannette M.},
  biburl = {https://www.bibsonomy.org/bibtex/211a7ee920659c9798316ca145bdc3641/vngudivada},
  interhash = {9730d0fc2ab4263bbcb8a956f88e8e40},
  intrahash = {11a7ee920659c9798316ca145bdc3641},
  journal = {SIGCSE Bull.},
  keywords = {ComputationalThinking NsfRED},
  month = mar,
  number = 1,
  pages = {195--196},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Computational Thinking},
  url = {http://doi.acm.org/10.1145/1227504.1227378},
  volume = 39,
  year = 2007
}

@article{ohara2003source,
  abstract = {We investigate the role of open source software in computer science education. We begin with a brief tutorial on open source software including a description of four popular open source licenses. Next we discuss the use of open source software in education. Finally, we focus on the use of open source software in computer science education.},
  acmid = {771716},
  added-at = {2016-12-25T20:14:22.000+0100},
  address = {USA},
  author = {O'Hara, Keith J. and Kay, Jennifer S.},
  biburl = {https://www.bibsonomy.org/bibtex/23d97e9b2e29cf76aaaeffc4f46730e50/vngudivada},
  interhash = {c3383dd84eefbfb047f5ef670c40183e},
  intrahash = {3d97e9b2e29cf76aaaeffc4f46730e50},
  issn = {1937-4771},
  issue_date = {February 2003},
  journal = {J. Comput. Sci. Coll.},
  keywords = {ComputerScienceEducation FOSS},
  month = feb,
  number = 3,
  numpages = {7},
  pages = {1--7},
  publisher = {Consortium for Computing Sciences in Colleges},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Open Source Software and Computer Science Education},
  url = {http://dl.acm.org/citation.cfm?id=771712.771716},
  volume = 18,
  year = 2003
}

@article{deschryver2015creative,
  abstract = { For too long, creativity in schools has been almost solely associated with art, music, and writing classes. Now, creative thinking skills are increasingly emphasized across the disciplines. At the same time, technological progress has brought about calls for the integration of new literacies and computational thinking to prepare students as problem solvers and critical thinkers. However, in teaching and learning, all three perspectives most often manifest in isolation.  We believe this is to the detriment of both educators and students alike.

	In this paper, we develop an argument for the use of new literacies and computational thinking to promote creative thinking. First, we explore the various elements that comprise computational thinking, while demonstrating their overlap with the theoretical constructs of creative thinking. Second, we identify the design decisions that guided our plan for integrating all three perspectives in a sequence of educational technology courses designed for in-service teachers. Finally, we provide examples of the classroom activities that best facilitated creative thinking, and address how we achieved them.},
  added-at = {2017-01-03T15:11:53.000+0100},
  address = { Chesapeake, VA },
  author = {DeSchryver, Michael D. and Yadav, Aman},
  biburl = {https://www.bibsonomy.org/bibtex/2b401dc434a8589cda90099216ae1f083/vngudivada},
  interhash = {2d28359e77a4754a5c7520190eaa79f5},
  intrahash = {b401dc434a8589cda90099216ae1f083},
  journal = { Journal of Technology and Teacher Education },
  keywords = {ComputationalThinking NsfRED},
  month = { July },
  number = { 3 },
  pages = { 411--431 },
  publisher = { Society for Information Technology \& Teacher Education },
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = { Creative and Computational Thinking in the Context of New Literacies: Working with Teachers to Scaffold Complex Technology-Mediated Approaches to Teaching and Learning },
  volume = { 23 },
  year = { 2015 }
}

@article{reinholz2014towards,
  abstract = {Despite numerous calls for the transformation of undergraduate STEM
education, there is still a lack of successful models for creating large-scale,
systemic cultural changes in STEM departments. To date, change efforts have
generally focused on one of three areas: developing reflective teachers,
disseminating curricula and pedagogy, or enacting institutional policy. These
efforts illustrate many of the challenges of departmental change; in
particular, they highlight the need for a holistic approach that integrates
across all three of these levels: individual faculty, whole departments, and
university policymakers. To address these challenges, as part of our
campus-wide AAU-sponsored effort in STEM education transformation, we import
and integrate models of change from multiple perspectives. We draw from models
in organizational change, from departmental and disciplinary change in STEM
education, and from efforts to support individual efforts such as the
development and dissemination model. As a result, our departmental cultural
change efforts are an attempt at holistic reform. We will discuss our
theoretical underpinnings and ground this theory in a sample of approaches in
two departments.},
  added-at = {2016-12-23T18:58:00.000+0100},
  author = {Reinholz, Daniel L. and Corbo, Joel C. and Dancy, Melissa H. and Finkelstein, Noah and Deetz, Stanley},
  biburl = {https://www.bibsonomy.org/bibtex/28ffbcde725fb7bf94f23422a3e986a22/vngudivada},
  interhash = {0dbbc56a3a0cd35fa3c450bde91cbf98},
  intrahash = {8ffbcde725fb7bf94f23422a3e986a22},
  keywords = {Change NsfRED STEM},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Towards a Model of Systemic Change in University STEM Education},
  url = {http://arxiv.org/abs/1412.3037},
  year = 2014
}

@book{cooperrider2008appreciative,
  abstract = {The Appreciative Inquiry Handbook explains in-depth what AI is and how it works, and includes stories of AI interventions and classic articles, sample project plans, interview guidelines, participant worksheets, a list of resources, a glossary of terms, and more.},
  added-at = {2017-01-05T00:46:46.000+0100},
  address = {San Francisco, CA},
  author = {Cooperrider, David L. and Whitney, Diana Kaplin and Stavros, Jacqueline M.},
  biburl = {https://www.bibsonomy.org/bibtex/2455935273306af0144389356e6bf2472/vngudivada},
  interhash = {f72b0b151621b1f0a14850ac2fa4830a},
  intrahash = {455935273306af0144389356e6bf2472},
  keywords = {Book Change Leadership NsfRED},
  publisher = {Berrett-Koehler},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Appreciative inquiry handbook: for leaders of change},
  year = 2008
}

@book{closs2016topicbased,
  abstract = {This book presents a concise, real-world description of DITA principles. Explanations are provided on the basis of simple, applicable examples. The book will be an excellent introduction for DITA novices and is ideal as a first orientation for optimizing your information environment.},
  added-at = {2016-12-07T00:54:34.000+0100},
  author = {Closs, Sissi},
  biburl = {https://www.bibsonomy.org/bibtex/2fe5a9926a62084d5968c1558673e5d78/vngudivada},
  interhash = {3556a48cf739d8334f59798cab9a4ebf},
  intrahash = {fe5a9926a62084d5968c1558673e5d78},
  keywords = {Book DITA},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {DITA -- the topic-based XML standard: a quick start},
  year = 2016
}

@article{karpicke2011retrieval,
  abstract = {Educators rely heavily on learning activities that encourage elaborative studying, whereas activities that require students to practice retrieving and reconstructing knowledge are used less frequently. Here, we show that practicing retrieval produces greater gains in meaningful learning than elaborative studying with concept mapping. The advantage of retrieval practice generalized across texts identical to those commonly found in science education. The advantage of retrieval practice was observed with test questions that assessed comprehension and required students to make inferences. The advantage of retrieval practice occurred even when the criterial test involved creating concept maps. Our findings support the theory that retrieval practice enhances learning by retrieval-specific mechanisms rather than by elaborative study processes. Retrieval practice is an effective tool to promote conceptual learning about science.},
  added-at = {2016-12-29T22:12:29.000+0100},
  author = {Karpicke, Jeffrey D. and Blunt, Janell R.},
  biburl = {https://www.bibsonomy.org/bibtex/22b748b0f891ff343a008a6b9d5f930d3/vngudivada},
  interhash = {0d2f8917ecac6f6f69d75e9139cfd043},
  intrahash = {2b748b0f891ff343a008a6b9d5f930d3},
  journal = {Science},
  keywords = {ConceptMapping Learning RetrievalPractice},
  month = feb,
  number = 6018,
  pages = {772 -- 775},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Retrieval Practice Produces More Learning than Elaborative Studying with Concept Mapping},
  volume = 331,
  year = 2011
}

@inproceedings{meerbaumsalant2015computer,
  abstract = {The Computer Science, Academia and Industry extra-curricular program has been operated for Israeli high school students majoring in computer science. A case study was conducted aimed to identify computational thinking in students' project development processes.},
  added-at = {2017-01-01T02:37:41.000+0100},
  author = {Meerbaum-Salant, Orni and Haberman, Bruria and Pollack, Sarah},
  biburl = {https://www.bibsonomy.org/bibtex/27e12dc4a7cc953c8528b0fad48a18f94/vngudivada},
  booktitle = {ITiCSE},
  editor = {Dagiene, Valentina and Schulte, Carsten and Jevsikova, Tatjana},
  interhash = {efe657c87f6733b00c68cb4a153d7ca3},
  intrahash = {7e12dc4a7cc953c8528b0fad48a18f94},
  keywords = {ComputationalThinking},
  pages = 341,
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {"Computer Science, Academia and Industry" as pedagogical model to enhance Computational thinking.},
  year = 2015
}

@inproceedings{cajander2011assessing,
  abstract = {This paper addresses the issue of developing and assessing professional skills in higher education programs. This includes defining and assessing these skills, in the contexts of an individual course unit and for an entire degree program. Identifying forms of assessment that are seen as authentic, meaningful and understandable by the students, teaching staff and curriculum developers are of utmost importance if professional skills are to be accepted and included in the formal curriculum. This can be particularly important in programs that aim to offer students a truly collaborative learning experience in a culturally diverse team. Reflections are presented as one example of an assessment method that fits this requirement. Building assessment based on the notion of threshold concepts is introduced in the context of an open ended group project course unit at Uppsala University.},
  added-at = {2016-12-30T18:59:43.000+0100},
  address = {Darlinghurst, Australia},
  author = {Cajander, \r{A}sa and Daniels, Mats and McDermott, Roger and von Konsky, Brian R.},
  biburl = {https://www.bibsonomy.org/bibtex/2141d4dca4352460d8b37d51d13456804/vngudivada},
  booktitle = {Proceedings of the Thirteenth Australasian Computing Education Conference - Volume 114},
  interhash = {f92a524f830c08dbae900a5c6ababc08},
  intrahash = {141d4dca4352460d8b37d51d13456804},
  keywords = {Assessment ProfessionalSkill},
  pages = {145--154},
  publisher = {Australian Computer Society, Inc.},
  series = {ACE '11},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Assessing Professional Skills in Engineering Education},
  url = {http://dl.acm.org/citation.cfm?id=2459936.2459954},
  year = 2011
}

@article{dolphin2005internal,
  abstract = { This empirical study of major UK firms and organizations focuses on the role of internal communications within the total communications strategy and on the management of relationships between the organization and its employees: the internal audience. It examines the organizational role of internal communications in building and nourishing employee relations, establishing trust, providing timely and reliable information and thereby contributing to general motivation, particularly in times of change and stress. The conclusion is that communication with the internal audience makes a significant contribution to a fully developed corporate communications strategy. },
  added-at = {2017-01-01T23:38:54.000+0100},
  author = {Dolphin, Richard R.},
  biburl = {https://www.bibsonomy.org/bibtex/243693fbeb7afe80ae099ad32cd0c2950/vngudivada},
  interhash = {b484aaa21c9ba029e1a9290a592fbce3},
  intrahash = {43693fbeb7afe80ae099ad32cd0c2950},
  journal = {Journal of Marketing Communications},
  keywords = {Communication NsfRED},
  number = 3,
  pages = {171-190},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Internal Communications: Today's Strategic Imperative},
  volume = 11,
  year = 2005
}

@article{borrego2014increasing,
  abstract = {Background
Prior efforts have built a knowledge base of effective undergraduate STEM pedagogies, yet rates of implementation remain low. Theories from higher education, management, communication, and other fields can inform change efforts but remain largely inaccessible to STEM education leaders, who are just beginning to view change as a scholarly endeavor informed by the research literature.

Purpose
This article describes the goals, assumptions, and underlying logic of selected change strategies with potential relevance to STEM higher education settings for a target audience of change agents, leaders, and researchers.

Scope/Method
This review is organized according to the Four Categories of Change Strategies model developed by Henderson, Beach, and Finkelstein (2011). We describe eight strategies of potential practical relevance to STEM education change efforts (two from each category). For each change strategy, we present a summary with key references, discuss their applicability to STEM higher education, provide a STEM education example, and discuss implications for change efforts and research.

Conclusions
Change agents are guided, often implicitly, by a single change strategy. These eight strategies will expand the repertoire of change agents by helping them consider change from a greater diversity of perspectives. Change agents can use these descriptions to design more robust change efforts. Improvements in the knowledge and theory base underlying change strategies will occur when change agents situate their writing about change initiatives using shared models, such as the one presented in this article, to make their underlying assumptions about change more explicit.
},
  added-at = {2016-12-23T19:07:16.000+0100},
  author = {Borrego, Maura and Henderson, Charles},
  biburl = {https://www.bibsonomy.org/bibtex/2f786588b23d29fd930fe9fc5789c12f1/vngudivada},
  interhash = {5634f53805bd10cea5454a5615b7174c},
  intrahash = {f786588b23d29fd930fe9fc5789c12f1},
  journal = {Journal of Engineering Education},
  keywords = {InstrctionalPractice NsfRED STEM},
  number = 2,
  pages = {220--252},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Increasing the Use of Evidence-Based Teaching in STEM Higher Education: A Comparison of Eight Change Strategies},
  url = {http://dx.doi.org/10.1002/jee.20040},
  volume = 103,
  year = 2014
}

@inproceedings{thuraisingham2008mining,
  abstract = {In this paper we discuss various data mining techniques that we have successfully applied for cyber security. These applications include but are not limited to malicious code detection by mining binary executables, network intrusion detection by mining network traffic, anomaly detection, and data stream mining. We summarize our achievements and current works at the University of Texas at Dallas on intrusion detection, and cyber-security research.},
  added-at = {2016-12-10T01:18:38.000+0100},
  author = {Thuraisingham, Bhavani and Khan, Latifur and Masud, Mohammad M. and Hamlen, Kevin W.},
  biburl = {https://www.bibsonomy.org/bibtex/29a8a4c6f95f1f8634fdbb021c9ae5210/vngudivada},
  booktitle = {Proceedings of the 2008 IEEE/IFIP International Conference on Embedded and Ubiquitous Computing},
  interhash = {6bcd2698f833ea0e487c62daccbadd9a},
  intrahash = {9a8a4c6f95f1f8634fdbb021c9ae5210},
  keywords = {Cybersecurity DataMining},
  pages = {585 - 589},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Data Mining for Security Applications},
  year = 2008
}

@inproceedings{karunasekera2007preparing,
  abstract = {The lack of preparedness of software engineering (SE) graduates for a professional career is a common complaint raised by industry practitioners. The career progression of many new graduates is severely impacted due to the lack of well rounded skills. For example, some of the technically stronger graduates lack communication and managerial skills and vise versa. Industry based capstone projects, incorporated as a part of an undergraduate degree, are a well accepted means of preparing students for their professional careers. Software Engineering undergraduates at the University of Melbourne engage in such industry based projects both in the penultimate and final years of their degree. Though aimed at providing students a real-life SE experience and preparing them for industry, we observed these projects to fail in some cases in giving the necessary breadth of skills. We believe this failure to be due to the lack of an objective framework to guide student learning outcomes during projects. To address this problem we developed an objective skill-based framework, focusing on managerial, engineering and personal skills. In this paper, we present this framework and share our experiences of using it.},
  added-at = {2016-12-24T21:20:07.000+0100},
  author = {Karunasekera, Shanika and Bedse, Kunal},
  biburl = {https://www.bibsonomy.org/bibtex/2ce386eda8efcae92dbe595475a3d6888/vngudivada},
  booktitle = {20th Conference on Software Engineering Education {\&} Training ({CSEET}{\textquotesingle}07)},
  doi = {10.1109/cseet.2007.39},
  interhash = {a663608c2355075436eab1439f154338},
  intrahash = {ce386eda8efcae92dbe595475a3d6888},
  keywords = {NsfRED SoftwareEngineeringEducation},
  month = jul,
  publisher = {IEEE},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Preparing Software Engineering Graduates for an Industry Career},
  url = {http://dx.doi.org/10.1109/CSEET.2007.39},
  year = 2007
}

@inproceedings{wright2007collaboratory,
  abstract = {In this paper we present an innovative prototype Open Source Teaching/Learning Collaboratory created at UC Merced that will provide the foundation for offering the vast majority of our Computer Science and Engineer-ing (CSE) courses, as well as courses from across our engi-neering disciplines, and, increasingly, computer courses and computer intensive courses throughout our university. This prototype facility is in operation at the present time, with one and possibly two additional Collaboratories to be installed in time for our Fall 2007 session. In addition, this facility will provide the model and framework for other local and dis-tance teaching activities planned for supporting curriculum development and computing training. We show that by rely-ing on free open source software and commodity hardware we can build an education computing environment that min-imizes administration tasks, provides effortless remote inter-action, simplifies and enhances the computer science curric-ula that can be delivered to the students and minimizes both the aquisition and operation costs.},
  added-at = {2016-12-25T20:24:09.000+0100},
  author = {Wright, Jeff and Carpin, Stefano and Cerpa, Alberto and Gavilan, German and Kallmann, Marcelo and Laird, Cameron and Laird, Kyler and Newsam, Shawn and Noelle, David},
  biburl = {https://www.bibsonomy.org/bibtex/2a7c1eb7d5b68b3cc9274057f7b1e5f10/vngudivada},
  booktitle = {International Conference on Frontiers in Education: Computer Science and Computer Engineering},
  interhash = {6a55e3fe25d7b47c8286711e1350e261},
  intrahash = {a7c1eb7d5b68b3cc9274057f7b1e5f10},
  keywords = {FOSS},
  pages = {1-6},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Collaboratory: An Open Source Teaching and Learning Facility for Computer Science and Engineering Education},
  year = 2007
}

@inproceedings{ide2016lapps,
  abstract = {The US National Science Foundation (NSF) SI2-funded LAPPS Grid project has developed an open-source platform for enabling complex analyses while hiding complexities associated with underlying infrastructure, that can be accessed through a web interface, deployed on any Unix system, or run from the cloud. It provides sophisticated tool integration and history capabilities, a workflow system for building automated multi-step analyses, state-of-the-art evaluation capabilities, and facilities for sharing and publishing analyses. This paper describes the current facilities available in the LAPPS Grid and outlines the project’s ongoing activities to enhance the framework.},
  added-at = {2016-12-18T15:14:21.000+0100},
  author = {Ide, Nancy and Suderman, Keith and Nyberg, Eric and Pustejovsky, James and Verhagen, Marc},
  biburl = {https://www.bibsonomy.org/bibtex/2f053bb829133ab8b3f231400cba7b9ed/vngudivada},
  booktitle = {Proceedings of WLSI/OIAF4HLT},
  interhash = {e523e4962ae4608e4c7d717f19f049e8},
  intrahash = {f053bb829133ab8b3f231400cba7b9ed},
  keywords = {Galaxy LAPPS NLP NLPPipeline},
  month = dec,
  pages = {11 - 18},
  publisher = {aclweb.org},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {The LAPPS Grid: Current State and Next Steps},
  year = 2016
}

@article{boylanashraf2015scaffolding,
  abstract = {In the past ten years, engineering classrooms have seen an exponential growth in the use of technology, more than during any other previous decade. Unprecedented advancements, such as the advent of innovative gadgets and fundamental instructional alterations in engineering classrooms, have introduced changes in both teaching and learning. Student learning in introductory, fundamental engineering mechanics (IFEM) courses, such as statics of engineering, mechanics of materials, dynamics, and mechanics of fluids, as in any other class, is influenced by the experiences students go through in the classroom. Thus, bold new methodologies that connect science to life using student-centered approaches and scaffolding pedagogies need to be emphasized more in the learning process. This study is aimed to gain insight into the role of student-centered teaching, particularly the implementation of scaffolding pedagogies into IFEM courses. This study also attempts to contribute to the current national conversation in engineering education of the need to change its landscape—from passive learning to active learning. Demographic characteristics in this study included a total of 3,592 students, of whom 3,160 (88.0\%) are males and 432 (12.0\%) are females, over a period of six years, from 2007 to 2013. The students’ majors included aerospace engineering, agricultural engineering, civil engineering, construction engineering, industrial engineering, materials engineering, and mechanical engineering.

Results of the study, as tested using a general linear univariate model analysis, indicated that overwhelmingly the type of class in statics of engineering is a significant predictor of student “downstream” performance in tests measuring their knowledge of mechanics of materials. There is a statistically significant difference in students'performance in mechanics of materials depending on whether they were taught passively using the teacher-centered pedagogy or taught actively using the student-centered pedagogy in statics of engineering. Mechanics of materials is commonly the next immediate course, or a downstream course, following statics of engineering.},
  added-at = {2016-12-31T23:51:21.000+0100},
  author = {Boylan-Ashraf, Peggy C. and Freeman, Steven A. and Shelley, Mack C.},
  biburl = {https://www.bibsonomy.org/bibtex/27bc6b924c924b8bafd1adb32fb101271/vngudivada},
  interhash = {ba8a72f550154ab7a07951aa07c49389},
  intrahash = {7bc6b924c924b8bafd1adb32fb101271},
  journal = {Journal of STEM Education},
  keywords = {NsfRED Scaffolding},
  number = 4,
  pages = {6 -- 12},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Scaffolding in Introductory Engineering Courses},
  volume = 6,
  year = 2015
}

@inproceedings{eng2009communications,
  abstract = {This paper is about an oral communications course called "6.UAT" which is required of all students in the Department of Electrical Engineering and Computer Science at the Massachusetts Institute of Technology. We describe details about the course, discuss some high-level design decisions, and offer some results from student surveys about their communication ability before and after taking the course. Some of the curriculum ideas are not specific to EECS and thus are adoptable by the general technical community, and can serve as a possible starting point for practitioners at other institutions going through similar communications course design efforts.},
  added-at = {2016-12-30T18:52:33.000+0100},
  address = {New York, NY},
  author = {Eng, Tony},
  biburl = {https://www.bibsonomy.org/bibtex/23e1c3363412c612faffe8f2beff6303f/vngudivada},
  booktitle = {Proceedings of the 14th Western Canadian Conference on Computing Education},
  interhash = {1329aa7f60657321ffa0256fc37ab72d},
  intrahash = {3e1c3363412c612faffe8f2beff6303f},
  keywords = {NsfRED TechnicalCommunication},
  pages = {54--59},
  publisher = {ACM},
  series = {WCCCE '09},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {An Oral Communications Course for EECS Majors at MIT},
  year = 2009
}

@book{mesko2014guide,
  abstract = {A few short years ago, it would have been hard to imagine that exoskeletons could enable paralyzed people to walk again; that billions of people would rely on social media for information; and that the supercomputer Watson would be a key player in medical decision-making. Perhaps more than in any other field, technology has transformed medicine and healthcare in ways that a mere decade ago would have sounded like pure science fiction.

From his unique vantage as a trained physician, researcher, and medical futurist, Dr. Bertalan Mesko examines these developments and the many more down the pipeline. His aim is to assess how the hand of technology can continue to provide the dose of humanity that is crucial to effective healthcare. The Guide to the Future of Medicine: Technology and the Human Touch is his incisive, illuminating roundup of the technologies and trends that will shape the future of medicine.

Patients, medical professionals, and any healthcare stakeholder will find an eye opening, reassuring roadmap to tomorrow’s potential in this accessible and fact-based book. By preparing for the inevitable waves of change, you can make informed decisions about how technology will shape your own well-being.},
  added-at = {2016-12-24T21:27:59.000+0100},
  author = {Mesk\'{o}, Bertalan},
  biburl = {https://www.bibsonomy.org/bibtex/2daf8e3dd55212a5e9d8a3b66f0279f92/vngudivada},
  interhash = {296316c1931534969381534ccf551017},
  intrahash = {daf8e3dd55212a5e9d8a3b66f0279f92},
  isbn = {9789630898027 9630898020},
  keywords = {Book IBMWatsonHealth NsfRED},
  refid = {893647266},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {The guide to the future of medicine: technology and the human touch},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9789630898027},
  year = 2014
}

@book{kimber2012practitioners,
  abstract = {DITA expert Eliot Kimber takes you inside the DITA XML standard, explaining the architecture and technology that make DITA unique.
Volume 1 of his two-volume exploration of DITA starts with a hands-on explanation of end-to-end DITA processing that will get you up and running fast. Then, he explores the DITA architecture, explaining maps and topics, structural patterns, metadata, linking and addressing, keys and key references, relationship tables, conditional processing, reuse, and more.

Kimber's unique perspective unwraps the puzzle that is DITA, explaining the rationale for its design and structure, and giving you an unvarnished, detailed look inside this important technology.},
  added-at = {2016-12-07T00:59:41.000+0100},
  address = {Laguna Hills, Calif.},
  author = {Kimber, Eliot},
  biburl = {https://www.bibsonomy.org/bibtex/22857f97e90c0ddb725754a4cbb7b1c95/vngudivada},
  interhash = {326f065ef82be95a91f08d0227aad963},
  intrahash = {2857f97e90c0ddb725754a4cbb7b1c95},
  keywords = {Book DITA},
  publisher = {XML Press},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {DITA for practitioners Volume 1, Volume 1},
  year = 2012
}

@phdthesis{xu2007mining,
  abstract = {The success of Open Source Software (OSS) has attracted increased interest in many research areas. Unlike proprietary closed software, OSS projects are developed in a distributed and decentralized way. The OSS community is largely composed of part-time developers. These developers have developed a substantial number of outstanding technical achievements. A research study on how OSS developers interact with each other and how projects are developed will help researchers understand the success and failure of OSS projects. OSS developers can also benefit from this research, by being able to make more informed decisions for participating on OSS projects.
In this dissertation, we address the challenge of efficiently mining data from OSS web repositories and building models to study OSS community features. Data collection for OSS study is nontrivial since most OSS projects are developed by distributed developers using web tools. Most previous studies focus on manually creating a web crawler to collect data from OSS web sites. This method is usually implemented by creating a web crawler based on specific research goals. We design a mining process which combines web mining and database mining together to identify, extract, filter and analyze data. We address and analyze the difficulty of mining OSS data. Our work provides a general solution for researchers to implement advanced techniques, such as web mining, data mining, statistics, and algorithms to collect and analyze web repository data.

Based on our mining results, we model the OSS community as a social network, one which can be further modeled as a project network and a developer network, and study properties of these networks. Our goal is to find intrinsic mechanisms that lie in OSS networks to explain some OSS specific features such as roles of developers, communication, and reliability of the OSS community. We construct four social networks for the OSS development community at SourceForge [59]. Each social network is created by expanding the number of people with different roles in the network, moving from the core project leaders, to the core developers, to the co-developers, and finally out to active users. Social network properties such as degree distribution, diameter, cluster size, and clustering coefficient are calculated and compared for each of the expanding social networks. We elaborate on how the changing topological characteristics of the social networks may signify important capabilities for the diffusion of information, the ability to find collaborations, and the overall robustness of the OSS development community. We further find that all the social networks have scale-free properties, and the inclusion of the co-developers and active users triggers the emergence of the small-world phenomenon for the social network. We examine how these topological network properties may potentially explain the success and efficiency of OSS development practices.

To study the organization and backbones of the OSS community, we conduct the identification of the community structure on the SourceForge project network. We find that groups exist in the SourceForge project network. Furthermore, we explore possible reasons for the formation of those groups by examining assortative mixing coefficients for projects categories. Among them, we find projects with same programming languages, operating systems and topics are more likely to be grouped together. Our research provides useful information to study the interaction between projects and the communication and information flow in OSS virtual organizations.

We simulate the OSS community based on four social network models: random graphs, preferential attachment, preferential attachment with constant fitness, and preferential attachment with dynamic fitness, using two tools—Repast and Swarm. Our simulation models are fit to data from year two in the history of SourceForge. To prove the correctness of our simulations, dockingexperiments are performed on the Repast simulation and the Java/Swarm simulation. Our models simulate developers' actions and the growth of the OSS community. We compare properties of social networks such as degree distribution, diameter and clustering coefficient to dock Repast and Swarm simulations of four social network models. Our practice demonstrates the importance of verifications in scientific simulations. The simulation models we build can be used to forecast future development of OSS community.},
  added-at = {2016-12-25T20:41:16.000+0100},
  author = {Xu, Jin},
  biburl = {https://www.bibsonomy.org/bibtex/2abc5c71a5e5768a333b11c1125568241/vngudivada},
  interhash = {e863b46dd4dfdc51671878ed71e1a3f1},
  intrahash = {abc5c71a5e5768a333b11c1125568241},
  keywords = {FOSS},
  school = {University of Notre Dame},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Mining and modeling the open source software community},
  year = 2007
}

@inproceedings{hu2011computational,
  abstract = {Computational thinking has been promoted in recent years as a skill that is as fundamental as being able to read, write, and do arithmetic. However, what computational thinking really means remains speculative. While wonders, discussions and debates will likely continue, this article provides some analysis aimed to further the understanding of the notion. It argues that computational thinking is likely a hybrid thinking paradigm that must accommodate different thinking modes in terms of the way each would influence what we do in computation. Furthermore, the article makes an attempt to define computational thinking and connect the (potential) thinking elements to the known thinking paradigms. Finally, the author discusses some implications of the analysis.},
  added-at = {2017-01-01T02:46:18.000+0100},
  address = {New York, NY},
  author = {Hu, Chenglie},
  biburl = {https://www.bibsonomy.org/bibtex/23e94e2a294707b4fae911fd68f6d0ca5/vngudivada},
  booktitle = {Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education},
  interhash = {c5806d6776bdc6f9b3336968842d938a},
  intrahash = {3e94e2a294707b4fae911fd68f6d0ca5},
  keywords = {ComputationalThinking NsfRED},
  pages = {223--227},
  publisher = {ACM},
  series = {ITiCSE '11},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Computational Thinking: What It Might Mean and What We Might Do About It},
  url = {http://doi.acm.org/10.1145/1999747.1999811},
  year = 2011
}

@article{laugerman2015determining,
  abstract = {This study presents a unique synthesized set of data for community college students entering the university with the intention of earning a degree in engineering. Several cohorts of longitudinal data were combined with transcript-level data from both the community college and the university to measure graduation rates in engineering. The emphasis of the study is to determine academic variables that had significant correlations with graduation in engineering, and levels of these academic variables. The article also examines the utility of data mining methods for understanding the academic variables related to achievement in science, technology, engineering, and mathematics. The practical purpose of each model is to develop a useful strategy for policy, based on success variables, that relates to the preparation and achievement of this important group of students as they move through the community college pathway.},
  added-at = {2017-01-01T00:01:58.000+0100},
  author = {Laugerman, Marcia and Rover, Diane T. and Shelley, Mack C. and Mickelson, Steven K.},
  biburl = {https://www.bibsonomy.org/bibtex/2a506febc0ab97714b122e4d3edf66cfa/vngudivada},
  interhash = {c19568e81b05c211e85986582d9bcbb4},
  intrahash = {a506febc0ab97714b122e4d3edf66cfa},
  journal = {International Journal of Engineering Education},
  keywords = {CommunityCollege NsfRED TransferStudent},
  number = {6A},
  pages = {1448 -- 1457},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Determining Graduation Rates in Engineering for Community College Transfer Students Using Data Mining},
  volume = 31,
  year = 2015
}

@article{gouldwilliams2005using,
  abstract = {This article empirically tests the effects of exchange relationships between managers and public sector employees working in seven local government departments. Social exchange theory is used as a framework for predicting three outcomes of Human Resource Management (HRM) practice: employee commitment, employee motivation and desire to remain with the organization. The statistical models were found to predict 58 percent of the variation in employee commitment, 53 percent variation in motivation and 41 percent of the variance in respondents' desire to remain with the organization. Consistent with social exchange theory, the results highlight the importance of trust in management, which was found to predict positively all three outcomes. Team-working was found to predict employee commitment and motivation, with employee involvement, empowerment, the offer of fair rewards and job security having significant effects on worker motivation. The implications of these findings for management practice and theory are discussed. },
  added-at = {2017-01-01T23:43:16.000+0100},
  author = {Gould-Williams, Julian and Davies, Fiona},
  biburl = {https://www.bibsonomy.org/bibtex/2fc16adc7c7573a8b53f459124bed33bf/vngudivada},
  interhash = {f76b0e02d6e8ace54d332c811c500a83},
  intrahash = {fc16adc7c7573a8b53f459124bed33bf},
  journal = {Public Management Review},
  keywords = {HumanResourceManagement NsfRED},
  number = 1,
  pages = {1-24},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Using social exchange theory to predict the effects of hrm practice on employee outcomes},
  volume = 7,
  year = 2005
}

@article{dancy2016faculty,
  abstract = {The lack of knowledge about how to effectively spread and sustain the use of research-based instructional strategies is currently a significant barrier to the improvement of undergraduate physics education. In this paper we address this lack of knowledge by reporting on an interview study of 35 physics faculty, of varying institution types, who were self-reported users of, former users of, or knowledgeable nonusers of the research-based instructional strategy Peer Instruction. Interview questions included in this analysis focused on the faculty’s experiences, knowledge, and use of Peer Instruction, along with general questions about current and past teaching methods used by the interviewee. The primary findings include the following: (i) Faculty self-reported user status is an unreliable measure of their actual practice. (ii) Faculty generally modify specific instructional strategies and may modify out essential components. (iii) Faculty are often unaware of the essential features of an instructional strategy they claim to know about or use. (iv) Informal social interactions provide a significant communication channel in the dissemination process, in contrast to the formal avenues of workshops, papers, websites, etc., often promoted by change agents, and (v) experience with research-based strategies as a graduate student or through curriculum development work may be highly impactful. These findings indicate that educational transformation can be better facilitated by improving communication with faculty, supporting effective modification by faculty during implementation, and acknowledging and understanding the large impact of informal social interactions as a mode of dissemination.},
  added-at = {2016-12-23T22:03:54.000+0100},
  author = {Dancy, Melissa and Henderson, Charles and Turpen, Chandra},
  biburl = {https://www.bibsonomy.org/bibtex/25ab2e7d389992dfbd35bc52bd0edaa3c/vngudivada},
  doi = {10.1103/PhysRevPhysEducRes.12.010110},
  interhash = {38fd1ac960d8440a7a9e53a327f199ce},
  intrahash = {5ab2e7d389992dfbd35bc52bd0edaa3c},
  journal = {Phys. Rev. Phys. Educ. Res.},
  keywords = {InstructionalPractice NsfRED},
  month = feb,
  number = 1,
  numpages = {17},
  pages = 010110,
  publisher = {American Physical Society},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {How faculty learn about and implement research-based instructional strategies: The case of Peer Instruction},
  url = {http://link.aps.org/doi/10.1103/PhysRevPhysEducRes.12.010110},
  volume = 12,
  year = 2016
}

@book{james2013introduction,
  abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.  Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
  added-at = {2016-12-10T15:04:32.000+0100},
  address = {New York, NY},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/2c149ef553a584ed8f7b106e2a7ea3c4a/vngudivada},
  interhash = {b3febabdc45a8629023cee7323dfbd86},
  intrahash = {c149ef553a584ed8f7b106e2a7ea3c4a},
  isbn = {978-1461471370},
  keywords = {Book DataMining MachineLearning StatisticalLearning},
  publisher = {Springer},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {An Introduction to Statistical Learning with Applications in R},
  year = 2013
}

@inproceedings{settle2012turning,
  abstract = {Programming has a central role in the computing curriculum, and introductory programming classes have been extensively studied in the computer science education literature. However, most of the studies focus on the effectiveness of various pedagogical approaches on student learning and engagement, and relative little attention is paid to faculty development. The gap in the literature puts CS1 faculty interested in effectively implementing innovative pedagogical approaches in a difficult situation. This article argues that taking a behaviorist approach to the CS1 classroom can provide much-needed feedback. Students provide instructors with one of the best sources of information about effective programming instruction, both with respect to pedagogical approaches and with respect to less formal issues such as classroom management, student-faculty interactions, and course policies. Faculty who choose to listen and learn from the comments made by their CS1 students will find a wealth of information to guide them in their development as instructors.},
  added-at = {2017-01-01T02:30:22.000+0100},
  address = {New York, NY},
  author = {Settle, Amber},
  biburl = {https://www.bibsonomy.org/bibtex/2b7d46f8042d2b547655f56ebabb27ff4/vngudivada},
  booktitle = {Proceedings of the 13th Annual Conference on Information Technology Education},
  interhash = {add2c86a968a26185b0dfe3dcf7fe6e5},
  intrahash = {b7d46f8042d2b547655f56ebabb27ff4},
  keywords = {CS1 FacultyDevelopment NsfRED},
  pages = {133--138},
  publisher = {ACM},
  series = {SIGITE '12},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Turning the Tables: Learning from Students About Teaching {CS1}},
  url = {http://doi.acm.org/10.1145/2380552.2380594},
  year = 2012
}

@book{singh2000science,
  abstract = {In his first book since the bestselling Fermat's Enigma, Simon Singh offers the first sweeping history of encryption, tracing its evolution and revealing the dramatic effects codes have had on wars, nations, and individual lives. From Mary, Queen of Scots, trapped by her own code, to the Navajo Code Talkers who helped the Allies win World War II, to the incredible (and incredibly simple) logisitical breakthrough that made Internet commerce secure, The Code Book tells the story of the most powerful intellectual weapon ever known: secrecy.

Throughout the text are clear technical and mathematical explanations, and portraits of the remarkable personalities who wrote and broke the world's most difficult codes. Accessible, compelling, and remarkably far-reaching, this book will forever alter your view of history and what drives it.  It will also make you wonder how private that e-mail you just sent really is.},
  added-at = {2016-12-29T01:52:00.000+0100},
  author = {Singh, Simon},
  biburl = {https://www.bibsonomy.org/bibtex/20613a5f5905500d4ab391e049ddddec7/vngudivada},
  interhash = {67f1286766ef691e744bccf33dcf2d5a},
  intrahash = {0613a5f5905500d4ab391e049ddddec7},
  keywords = {Book CodeBook Cryptography},
  publisher = {Anchor},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {The code book, the science of secrecy from ancient Egypt to Quantum Cryptography},
  year = 2000
}

@article{kouzes1996envisioning,
  abstract = {A leader's step-by-step guide to envisioning the future - and communicating it to others.

You feel a strong desire to go on a challenging expedition to a place you've never been. It's a desire that you can't shake, something that you think about day and night.

At first, your desire for challenge is quite vague; you don't have a specific destination in mind. But soon you feel a need to decide on the kind of challenging journey that you want for yourself. You look at some alternatives: trekking through the mountains, sailing an ocean, hiking across a desert, going on a safari. Whether through conscious thought or unfocused meditation, you discover what appeals to you the most: You decide, for example, that you've always wanted to take a trek through the Himalayas.

So what do you do now? More than likely, you consult a travel guide, study maps, look at photographs. You talk to people who have climbed the Himalayas, read adventure stories by those who have done it before. You begin to get a real sense of the place - the weather, the dress, the customs, the food, the travel conditions - all those impressions that clarify your understanding of your destination.

Not wanting your trip to be just like others you've heard about, you decide that you'll make this something special. You decide that your expedition will be unique, one that no one else has ever undertaken - perhaps even one that National Geographic would want to cover.

Then you set a date many months or even years in the future. You know such arduous journeys aren't done alone, so you determine who else might share your desire for challenge and how those people would benefit from the experience. You recruit some colleagues, selling them on the benefits of high adventure. Then the planning begins in earnest.

Discovering a vision for your organization is similar in many ways to these initial stages of preparing for an expedition. You feel a strong inner sense of dissatisfaction with the way things are in your community, congregation, or company and have an equally strong belief that things don't have to be this way. Envisioning the future begins with a vague desire to do something that would challenge yourself and others.

Because you want what you create to be unique, you differentiate your organization or cause from others that produce the same product, provide the same service, or make the same promise. Yours is a distinctive vision, an ideal.

Visions for organizations or reformations or movements, as well as visions for journeys, are more complex than this, of course. And we don't necessarily follow such a sequential process for clarifying our visions - especially if we're attempting to achieve what no one has ever achieved before.

Four Attributes of Vision

Let's take a closer look at four attributes of vision, which we define as an ideal and unique image of the future.

1. Ideality: The Pursuit of Excellence

Visions are about possibilities, about desired futures. They're ideals, standards of excellence. As such, they're expressions of optimism and hope. A mode of thinking based on visions opens us up to considering possibilities, not simply probabilities.

Indeed, it is exactly this belief that sustains teacher Nolan Dishongh and helps him spark possibility thinking in students otherwise at risk. Many of the 14- to 16-year-olds in Dishongh's construction trades class at Alice Johnson Junior High, 25 miles east of Houston, have well-earned reputations as troublemakers, as students with short attention spans, low grades, and little interest in learning. Many are from broken or abusive homes; some are known gang members.

Dishongh sets the tone at the start of each school year by asking his students to lay their heads on their desks. Then, in his deep, soothing voice, he instructs them to think about their mother, about her holding them close as infants, feeding them and singing to them. },
  added-at = {2017-01-01T23:50:27.000+0100},
  author = {Kouzes, James M. and Posner, Barry Z.},
  biburl = {https://www.bibsonomy.org/bibtex/26c4a6c47d574805754dd32d6bb5758ac/vngudivada},
  interhash = {3b4ee56ddba814db76f0437741fc4c5a},
  intrahash = {6c4a6c47d574805754dd32d6bb5758ac},
  journal = {The Futurist},
  keywords = {Leadership NsfRED},
  month = may,
  number = 3,
  pages = {14 -- 19},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Envisioning Your Future: Imagining Ideal Scenarios},
  volume = 30,
  year = 1996
}

@article{foote2016enabling,
  abstract = {While many innovative teaching strategies exist, integration into undergraduate science teaching has been frustratingly slow. This study aims to understand the low uptake of research-based instructional innovations by studying 21 successful implementations of the Student Centered Active Learning with Upside-down Pedagogies (SCALE-UP) instructional reform. SCALE-UP significantly restructures the classroom environment and pedagogy to promote highly active and interactive instruction. Although originally designed for university introductory physics courses, SCALE-UP has spread to many other disciplines at hundreds of departments around the world. This study reports findings from in-depth, open-ended interviews with 21 key contact people involved with successful secondary implementations of SCALE-UP throughout the United States. We defined successful implementations as those who restructured their pedagogy and classroom and sustained and/or spread the change. Interviews were coded to identify the most common enabling and challenging factors during reform implementation and compared to the theoretical framework of Kotter’s 8-step Change Model. The most common enabling influences that emerged are documenting and leveraging evidence of local success, administrative support, interaction with outside SCALE-UP user(s), and funding. Many challenges are linked to the lack of these enabling factors including difficulty finding funding, space, and administrative and/or faculty support for reform. Our focus on successful secondary implementations meant that most interviewees were able to overcome challenges. Presentation of results is illuminated with case studies, quotes, and examples that can help secondary implementers with SCALE-UP reform efforts specifically. We also discuss the implications for policy makers, researchers, and the higher education community concerned with initiating structural change.},
  added-at = {2016-12-23T22:13:51.000+0100},
  author = {Foote, Kathleen and Knaub, Alexis and Henderson, Charles and Dancy, Melissa and Beichner, Robert J.},
  biburl = {https://www.bibsonomy.org/bibtex/28c938c191cad951199c28c4819fcea80/vngudivada},
  doi = {10.1103/PhysRevPhysEducRes.12.010103},
  interhash = {983f4cd75df0675420cb5efbfc607aed},
  intrahash = {8c938c191cad951199c28c4819fcea80},
  journal = {Phys. Rev. Phys. Educ. Res.},
  keywords = {InstitutionalReform NsfRED},
  month = feb,
  number = 1,
  numpages = {22},
  pages = 010103,
  publisher = {American Physical Society},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Enabling and challenging factors in institutional reform: The case of SCALE-UP},
  url = {http://link.aps.org/doi/10.1103/PhysRevPhysEducRes.12.010103},
  volume = 12,
  year = 2016
}

@book{kotter2012leading,
  abstract = {Millions worldwide have read and embraced John Kotter’s ideas on change management and leadership.
From the ill-fated dot-com bubble to unprecedented M&A activity to scandal, greed, and ultimately, recession—we’ve learned that widespread and difficult change is no longer the exception. It’s the rule. Now with a new preface, this refreshed edition of the global bestseller Leading Change is more relevant than ever.

John Kotter’s now-legendary eight-step process for managing change with positive results has become the foundation for leaders and organizations across the globe. By outlining the process every organization must go through to achieve its goals, and by identifying where and how even top performers derail during the change process, Kotter provides a practical resource for leaders and managers charged with making change initiatives work. Leading Change is widely recognized as his seminal work and is an important precursor to his newer ideas on acceleration published in Harvard Business Review.

Needed more today than at any time in the past, this bestselling business book serves as both visionary guide and practical toolkit on how to approach the difficult yet crucial work of leading change in any type of organization. Reading this highly personal book is like spending a day with the world’s foremost expert on business leadership. You’re sure to walk away inspired—and armed with the tools you need to inspire others. },
  added-at = {2017-01-05T01:04:46.000+0100},
  address = {Boston, MA},
  author = {Kotter, John P.},
  biburl = {https://www.bibsonomy.org/bibtex/2946af66a1fafc841e7eafb3485fd91e9/vngudivada},
  interhash = {27d286d0e5c5317d237668ed1289eff5},
  intrahash = {946af66a1fafc841e7eafb3485fd91e9},
  keywords = {Book Change Leadership NsfRED},
  publisher = {Harvard Business Review Press},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Leading change},
  year = 2012
}

@book{sciore2009database,
  abstract = {* Covering the traditional database system concepts from a systems perspective, this book addresses the functionality that database systems provide as well as what algorithms and design decisions will best implement their functionality
* Describes what Java tools and techniques will best help developers build an application that uses a database system
* Contains a fully functional database system that allows readers to examine and modify the code},
  added-at = {2016-12-29T19:23:45.000+0100},
  address = {Hoboken, NJ},
  author = {Sciore, Edward},
  biburl = {https://www.bibsonomy.org/bibtex/281793fe854ec8f06ab8db4d240cd61c5/vngudivada},
  interhash = {8d0df8dd284cd84140fde642436be2e1},
  intrahash = {81793fe854ec8f06ab8db4d240cd61c5},
  keywords = {Book DBMS DBMSImplementation DBMSInternals},
  publisher = {John Wiley \& Sons},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Database design and implementation},
  year = 2009
}

@book{cameron2012positive,
  abstract = {Positive Leadership shows how to reach beyond ordinary success to achieve extraordinary effectiveness, spectacular results, and what Kim Cameron calls “positively deviant performance”—performance far above the norm. Citing a wide range of research in organizational behavior, medical science, and psychology as well as real-world examples, Cameron shows that to achieve exceptional success, leaders must emphasize strengths rather than simply focus on weaknesses; foster virtuous actions such as compassion, gratitude, and forgiveness; encourage contribution goals in addition to achievement goals; and enable meaningfulness in work. In this concise, inspiring, and practical guide, Cameron describes four positive leadership strategies, lays out a proven process for implementing them, and includes a self-assessment instrument. This second edition has been updated throughout with new research findings and new ideas for implementing positive leadership},
  added-at = {2017-01-01T23:21:02.000+0100},
  address = {San Francisco, CA},
  author = {Cameron, Kim},
  biburl = {https://www.bibsonomy.org/bibtex/204692688c077caef4f7c54de6449e583/vngudivada},
  edition = {second},
  interhash = {62436a57bb40237abc17e16d1b891a6c},
  intrahash = {04692688c077caef4f7c54de6449e583},
  keywords = {Leadership NsfRED},
  publisher = {Berrett-Koehler Publishers},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Positive Leadership: Strategies for Extraordinary Performance},
  year = 2012
}

@inproceedings{miller2014integrating,
  abstract = {  Our research is based on an innovative approach that integrates computational thinking and creative thinking in CS1 to improve student learning performance. Referencing Epstein's Generativity Theory, we designed and deployed a suite of creative thinking exercises with linkages to concepts in computer science and computational thinking, with the premise that students can leverage their creative thinking skills to "unlock" their understanding of computational thinking. In this paper, we focus on our study on differential impacts of the exercises on different student populations. For all students there was a linear "dosage effect" where completion of each additional exercise increased retention of course content. The impacts on course grades, however, were more nuanced. CS majors had a consistent increase for each exercise, while non-majors benefited more from completing at least three exercises. It was also important for freshmen to complete all four exercises. We did find differences between women and men but cannot draw conclusions.},
  added-at = {2017-01-01T02:59:28.000+0100},
  address = {New York, NY},
  author = {Miller, L. D. and Soh, Leen-Kiat and Chiriacescu, Vlad and Ingraham, Elizabeth and Shell, Duane F. and Hazley, Melissa Patterson},
  biburl = {https://www.bibsonomy.org/bibtex/2a5d504a5c7aaec2701d9c81b6d634f24/vngudivada},
  booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
  interhash = {a8dd51f66d6437536c3d9df221c08da7},
  intrahash = {a5d504a5c7aaec2701d9c81b6d634f24},
  keywords = {ComputationalThinking NsfRED},
  pages = {475--480},
  publisher = {ACM},
  series = {SIGCSE '14},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Integrating Computational and Creative Thinking to Improve Learning and Performance in CS1},
  url = {http://doi.acm.org/10.1145/2538862.2538940},
  year = 2014
}

@article{knaub2016classroom,
  abstract = {Background: The characteristics of the classroom environment play an important role in shaping teaching practices and supporting research-based instructional strategies. One instructional strategy that has reimagined the classroom is the Student-Centered Active Learning Environment with Upside-Down Pedagogies (SCALE-UP). SCALE-UP uses studio-style instruction to facilitate student collaboration. Although there is significant interest in studio-style instruction, there is not much research-based guidance available for institutions interested in setting up a classroom, especially for secondary users interested in using this in different academic settings and contexts. We interviewed key informants involved in 21 successful secondary implementations of SCALE-UP about creating, using, and spreading studio-style classrooms. This paper summarizes respondent’s perceptions of (1) how these classrooms are initiated; (2) which classroom features are helpful, non-essential, and unhelpful; (3) how professional development efforts support SCALE-UP instructors; and (4) how the classroom indirectly affects the department and/or institution.

Results: Room initiation Interviewees engaged in multiple activities to obtain a studio-style classroom. The majority of interviewees worked in teams created by faculty or administrators, with the participation from both groups. Interviewees typically sought institutional funding to develop the rooms.

Classroom features When developing the room, implementers used many key characteristics of the recommended classroom, such as collaborative workspace (e.g., special tables) for students, but they generally did not replicate all of the recommended features. Interviewees had mixed opinions about the importance of classroom technology.

Professional development and support Interviewees noted the importance of professional development for teaching staff (instructors and teaching assistants) new to the SCALE-UP teaching environment.

Indirect effects Beyond direct benefits to the teachers and learners, our interviewees reported that the classrooms had larger impacts including attracting visitors to the institution and encouraging the use of active learning in non-SCALE-UP classes.

Conclusions: There are many paths to successful development of a studio-style classroom. The process can be initiated by faculty or administrators. Classroom designs can vary to suit the local environment as long as they maintain the intent of the space: to support peer collaboration. Beyond improving student outcomes, these classrooms have additional benefits for institutions that include transforming instructor approaches to teaching and symbolizing the institution’s commitment to quality teaching.},
  added-at = {2016-12-23T21:59:03.000+0100},
  author = {Knaub, Alexis V. and Foote, Kathleen T. and Henderson, Charles and Dancy, Melissa and Beichner, Robert J.},
  biburl = {https://www.bibsonomy.org/bibtex/2a0992a002e4b1c5289f941c32eb99863/vngudivada},
  doi = {10.1186/s40594-016-0042-3},
  interhash = {ac8cbcabbb4a26ab0ebf444e306487ce},
  intrahash = {a0992a002e4b1c5289f941c32eb99863},
  journal = {International Journal of {STEM} Education},
  keywords = {NsfRED StudioStyleInstruction},
  month = may,
  number = 1,
  publisher = {Springer Nature},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Get a room: the role of classroom space in sustained implementation of studio style instruction},
  url = {http://dx.doi.org/10.1186/s40594-016-0042-3},
  volume = 3,
  year = 2016
}

@techreport{mcgettrick2013toward,
  abstract = {This session reports on a workshop convened by the ACM Education Board with funding by the US National Science Foundation and invites discussion from the community on the workshop findings. The topic, curricular directions for cybersecurity, is one that resonates in many departments considering how best to prepare graduates to face the challenges of security issues in employment and future research. The session will include presentation of the workshop context and conclusions, but will be open to participant discussion. This will be the first public presentation of the results of the workshop and the first opportunity for significant response.},
  added-at = {2016-12-10T00:26:56.000+0100},
  author = {McGettrick, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/2c69ce2c22b07d90f8145ae72060b2d21/vngudivada},
  institution = {Association for Computing Machinery (ACM)},
  interhash = {1bc0de1a55d1c23b366436a9395134c8},
  intrahash = {c69ce2c22b07d90f8145ae72060b2d21},
  keywords = {Cybersecurity CybersecurityEducation},
  month = {August},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Toward Curricular Guidelines for Cybersecurity: Report of a Workshop on Cybersecurity
Education and Training},
  year = 2013
}

@article{borrego2008characteristics,
  abstract = {This article employs theory to demonstrate the characteristics of successful cross-disciplinary engineering education collaborations. Specifically, we analyzed data from interviews with 24 recent Journal of Engineering Education authors from engineer-nonengineer teams. Theoretical frameworks from education and psychology are used to ground the results and contribute to broader research on collaboration across technology and social science disciplines. The data suggest that the way an individual understands and appreciates the nature of knowledge affects the way he or she collaborates with colleagues in different academic disciplines, especially when the disciplines are fundamentally different. Although the literature criticizes engineers for not understanding or respecting other viewpoints, we found that nine engineers and eight nonengineers articulated awareness of their collaborators' perspectives, worked to integrate these into the research, and noted increased satisfaction and quality of work as a result. Recommendations for fostering this type of interdisciplinary integration in engineering education are offered along with suggestions for future research.},
  added-at = {2016-12-22T03:31:36.000+0100},
  author = {Borrego, Maura and Newswander, Lynita K.},
  biburl = {https://www.bibsonomy.org/bibtex/2690814309d6a4ec8f77692f7a407f436/vngudivada},
  doi = {10.1002/j.2168-9830.2008.tb00962.x},
  interhash = {146274d6db1b0f81c65161d84b2884b3},
  intrahash = {690814309d6a4ec8f77692f7a407f436},
  issn = {2168-9830},
  journal = {Journal of Engineering Education},
  keywords = {CrossDisciplinaryCollaboration EngineeringEducation NSFRED},
  number = 2,
  pages = {123--134},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Characteristics of Successful Cross-disciplinary Engineering Education Collaborations},
  url = {http://dx.doi.org/10.1002/j.2168-9830.2008.tb00962.x},
  volume = 97,
  year = 2008
}

@article{sadler2006impact,
  abstract = {The 2001 U.S. Supreme Court Case of Falvo v. Owasso School System (Owasso Independent School District No I-011 v. Falvo) has focused national attention on the common classroom practice of peer-grading. In a unanimous decision the court reaffirmed the popular view that students grading each others’ tests is valuable, saving teachers’ time and augmenting student learning. Our study puts these presumed benefits to the test in 4 middle school science classrooms. We compared teacher-assigned grades to those awarded either by students to themselves or by their peers. By training students to grade with the help of a scoring rubric, a very high correlation was obtained between students and their teacher on test questions (r = 0.91 to 0.94). We found patterns of bias when students assigned grades. When grading others, students awarded lower grades to the best performing students than their teacher did. When grading themselves, lower performing students tended to inflate their own low scores. Performance on an unannounced, 2nd administration of the same test 1 week later measured the degree to which student-grading resulted in any increased understanding. Students who graded their peers’tests did not gain significantly more than a control group of students who did not correct any papers but simply took the same test again. Those students who corrected their own tests improved dramatically. Self-grading and peer-grading appear to be reasonable aids to saving teachers’ time. Self-grading appears to result in increased student learning; peer-grading does not.},
  added-at = {2016-12-29T23:41:08.000+0100},
  author = {Sadler, Philip M.},
  biburl = {https://www.bibsonomy.org/bibtex/29f9d4d598f1d9a68d1cd4b4d7f0b500a/vngudivada},
  interhash = {57aa2020b6ca4251ee76732b12abab4d},
  intrahash = {9f9d4d598f1d9a68d1cd4b4d7f0b500a},
  journal = {Educational Assessment},
  keywords = {Assessment PeerGrading},
  number = 1,
  pages = {1 -- 31},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {The impact of Self- and Peer-grading on Student Learning},
  volume = 11,
  year = 2006
}

@inproceedings{wieman2010science,
  abstract = {Guided by experimental tests of theory and practice, science has advanced rapidly in the past 500 years. Guided primarily by tradition and dogma, science education meanwhile has remained largely medieval. Research on how people learn is now revealing how many teachers badly misinterpret what students are thinking and learning from traditional science classes and exams. However, research is also providing insights on how to do much better. The combination of this research with modern information technology is setting the stage for a new approach that can provide the relevant and effective science education for all students that is needed for the 21st century. I will discuss the failures of traditional educational practices, even as used by "very good" teachers, and the successes of some new practices and technology that characterize this more effective approach, and how these results are highly consistent with findings from cognitive science.},
  added-at = {2016-12-30T14:10:18.000+0100},
  address = {New York, NY, USA},
  author = {Wieman, Carl E.},
  biburl = {https://www.bibsonomy.org/bibtex/2cbef30ce076efd72f4e45231462573a0/vngudivada},
  booktitle = {Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
  interhash = {eb1c00ff3d075c9c1a687c573f6da71b},
  intrahash = {cbef30ce076efd72f4e45231462573a0},
  keywords = {NsfRED ScienceEducation},
  pages = {198--198},
  publisher = {ACM},
  series = {SIGCSE '10},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Science Education for the 21st Century: Using the Insights of Science to Teach/Learn Science},
  url = {http://doi.acm.org/10.1145/1734263.1734333},
  year = 2010
}

@article{laugerman2013engineering,
  abstract = {This paper presents the evaluation of a program designed to improve transfer outcomes for community college students pursuing an engineering degree. The program, the Engineering Admissions Partnership Program (E-APP), was designed to improve the navigational success of community college transfer students through connections to the university. These connections include coordinated academic advising, peer-mentoring, campus visits, and online social and professional networks. The objective of the study is to determine the efficacy of the E-APP and its interventions, which will be measured by increased participation rates and increased university retention rates for E-APP participants. Outcome data for the students are analyzed statistically for significant differences between the quasi-experimental groups (E-APP or not EAPP), matched based on Math ACT scores. The results show significant improvement in first-year retention rates for EAPP participants. The results of this study are both transferrable and scalable. This research may help increase the success of community college transfers to engineering through developing and implementing similar navigational programs across the country.},
  added-at = {2017-01-01T00:09:36.000+0100},
  author = {Laugerman, Marcia R. and Shelley, Mack C. and Mickelson, Steven K. and Rover, Diane T.},
  biburl = {https://www.bibsonomy.org/bibtex/2b4c8eaae41ea652dab391d7273ef99ac/vngudivada},
  interhash = {ef95b54c1112c8a08fbb3d5d0570b582},
  intrahash = {b4c8eaae41ea652dab391d7273ef99ac},
  journal = {International Journal of Engineering Education},
  keywords = {CommunityCollege NsfRED TransferStudent},
  number = 5,
  pages = {1260 -- 1269},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {The Engineering Admissions Partnership Program: A Navigation Strategy for Community College Students Seeking a Pathway into Engineering},
  volume = 29,
  year = 2013
}

@inproceedings{viegas2015engineering,
  abstract = {This paper reflects the result of a drawn challenge of enhancing the discussion related to the problematic of how to ease the transition of young engineers from academia to professional life. How can students be better prepared to face the actual demands? Can we identify some particular issues that need more attention? How can the linkage between academia and companies be strengthened? Several works were presented, reflecting concerns and sharing experiences: four papers focus on Internship projects; one papers emphasis the development of more conscientious professionals regarding laboratory safety; other called attention regarding modern market demands (necessity for professionals to integrate fields like computer science, information systems and computer engineering); and two papers centered on the importance of motivating young students to science, technology, engineering, and mathematics.},
  added-at = {2016-12-30T19:17:52.000+0100},
  address = {New York, NY},
  author = {Viegas, Clara and Marques, Arcelina and Alves, Gustavo},
  biburl = {https://www.bibsonomy.org/bibtex/2581769f2e6f3e6b52bb6eb2cd191b382/vngudivada},
  booktitle = {Proceedings of the 3rd International Conference on Technological Ecosystems for Enhancing Multiculturality},
  interhash = {8807d88aa26757ffa4278d3878eea92a},
  intrahash = {581769f2e6f3e6b52bb6eb2cd191b382},
  keywords = {NsfRED ProfessionalSkill},
  pages = {561--564},
  publisher = {ACM},
  series = {TEEM '15},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Engineering and Technological Learning in Educational and Professional Contexts},
  year = 2015
}

@inproceedings{mertz2010teaching,
  abstract = {This paper describes a course at Carnegie Mellon University that engages students as consultants working with non-profit organizations. Different from most "service learning" courses, students in the Technology Consulting in the Community course focus on building capacity within organizations to sustain IT solutions without ongoing assistance. They do not merely provide IT support, nor do they focus on system development. Rather they focus on solving organizational problems using IT solutions. In doing so, they may develop a system, or adapt open source or commercial tools as appropriate to the situation. Computing systems do not exist in isolation, but in the context of people, organizations, and their policies. We want to train leaders in our community who can not only develop new technologies, but can solve organizational and societal problems. The course has as its learning goals to build inquiry, communication and leadership skills, in addition to engaging students in project development. At the same time it provides a valuable service in the community.},
  added-at = {2016-12-30T18:56:30.000+0100},
  address = {New York, NY},
  author = {Mertz, Joseph and McElfresh, Scott},
  biburl = {https://www.bibsonomy.org/bibtex/25fbe76f0687a954b264801f712a44f24/vngudivada},
  booktitle = {Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
  interhash = {9e6ff364e620305ef41c0f7cdeb38358},
  intrahash = {5fbe76f0687a954b264801f712a44f24},
  keywords = {NsfRED TechnicalCommunication},
  pages = {77--81},
  publisher = {ACM},
  series = {SIGCSE '10},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Teaching Communication, Leadership, and the Social Context of Computing via a Consulting Course},
  year = 2010
}

@inproceedings{wang2016securing,
  abstract = {This talk discusses how to incorporate social networking sites (SNS) into IT education by suggesting specific course content areas of IT curriculum where LinkedIn assignments and training can be integrated. LinkedIn as an educational tool can not only teach soft skills, such as self-branding and social networking, but creative use of this tool can promote students' career preparedness and encourage them to begin building their career-advancing networks, which can be vital in securing their professional future.},
  added-at = {2016-12-30T19:11:42.000+0100},
  address = {New York, NY},
  author = {Wang, Ye Diana},
  biburl = {https://www.bibsonomy.org/bibtex/2f50eab722d9473ad07ababe9c8ce5f18/vngudivada},
  booktitle = {Proceedings of the 17th Annual Conference on Information Technology Education},
  interhash = {6c824f775e93a789260561a5eceb5144},
  intrahash = {f50eab722d9473ad07ababe9c8ce5f18},
  keywords = {NsfRED ProfessionalSkill},
  pages = {107--107},
  publisher = {ACM},
  series = {SIGITE '16},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Securing the Professional Future of IT Students with LinkedIn},
  year = 2016
}

@article{barba2016guest,
  added-at = {2016-12-29T21:20:14.000+0100},
  author = {Barba, Lorena A. and Kaw, Autar and LeDoux, Joseph M.},
  biburl = {https://www.bibsonomy.org/bibtex/24a9f610eaf43e8705f469ecf3092e7dd/vngudivada},
  interhash = {fae6b2ceaf1115cf41240e09d30ad738},
  intrahash = {4a9f610eaf43e8705f469ecf3092e7dd},
  journal = {Advances in Engineering Education},
  keywords = {FlippedClassroom},
  number = 3,
  pages = {1--6},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Guest Editorial: Flipped Classrooms in STEM},
  volume = 5,
  year = 2016
}

@article{demonbrun2017creating,
  abstract = {Background: Research has provided evidence on the benefits of active learning on student learning and success in the engineering classroom. Yet the adoption of such types of instruction has been slow. Prior research has suggested that students' responses may have a significant effect on instructors' willingness to adopt different types of instruction.


Purpose: We describe our method for creating an instrument to measure the effects of several variables on student response to instructional practices. We discuss the step-by-step process for creating this instrument from the initial development process through multiple stages of validity and reliability testing.


Design/Method: The process for instrument development consisted of six steps: item generation and construct development, validity testing, implementation, exploratory factor analysis, confirmatory factor analysis, and instrument modification and replication. We discuss the pilot testing of the initial instrument (n=362) as well as construct development and validation using exploratory and confirmatory factor analyses.

Results: This process resulted in the creation of 49 items measuring three parts of our framework. Types of instruction separated into four factors (interactive, constructive, active, and passive); strategies for using in-class activities into two factors (explanation and facilitation); and student responses to instruction into five factors (value, positivity, participation, distraction, and evaluation).

Conclusions: This study describes the design process and final results. for an instrument to measure Student Response to Instructional Practices, a useful tool for understanding the relationship between the type of instruction used and students' response.},
  added-at = {2016-12-23T21:44:48.000+0100},
  author = {DeMonbrun, Matt and Finelli, Cynthia J. and Prince, Michael and Borrego, Maura and Shekhar, Prateek and Henderson, Charles and Waters, Cindy},
  biburl = {https://www.bibsonomy.org/bibtex/21b7779d9e848f07d8183488981cd04d8/vngudivada},
  interhash = {7dc777b0afbe44d199c666e8c8cef303},
  intrahash = {1b7779d9e848f07d8183488981cd04d8},
  journal = {Journal of Engineering Education},
  keywords = {InstructionalPractice StudentResponse},
  month = {april},
  number = 2,
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Creating an Instrument to Measure Student Response to Instructional Practices},
  volume = 106,
  year = 2017
}

@article{patterson2006computer,
  abstract = {To draw students to CS, we must first look to create a curriculum that reflects the exciting opportunities and challenges of IT today versus the 1970s. Future students and faculty would greatly benefit from a reinvigorated CS curriculum.},
  added-at = {2016-12-30T14:27:31.000+0100},
  address = {New York, NY, USA},
  author = {Patterson, David A.},
  biburl = {https://www.bibsonomy.org/bibtex/267d67c985b026b5db425fe958f4d139d/vngudivada},
  interhash = {2210b043aa63f68ab99cddbd5c8c6c2d},
  intrahash = {67d67c985b026b5db425fe958f4d139d},
  journal = {Commun. ACM},
  keywords = {CSEducation NsfRED},
  month = mar,
  number = 3,
  pages = {27--30},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Computer Science Education in the 21st Century},
  url = {http://doi.acm.org/10.1145/1118178.1118212},
  volume = 49,
  year = 2006
}

@article{deslauriers2011improving,
  abstract = {We compared the amounts of learning achieved using two different instructional approaches under controlled conditions. We measured the learning of a specific set of topics and objectives when taught by 3 hours of traditional lecture given by an experienced highly rated instructor and 3 hours of instruction given by a trained but inexperienced instructor using instruction based on research in cognitive psychology and physics education. The comparison was made between two large sections (N = 267 and N = 271) of an introductory undergraduate physics course. We found increased student attendance, higher engagement, and more than twice the learning in the section taught using research-based instruction.},
  added-at = {2016-12-30T00:36:41.000+0100},
  author = {Deslauriers, Louis and Schelew, Ellen and Wieman, Carl},
  biburl = {https://www.bibsonomy.org/bibtex/2d037c20a85e5906e8afee233cd8b947a/vngudivada},
  interhash = {1e88a69bb3aec8746f41e1c7af40aad3},
  intrahash = {d037c20a85e5906e8afee233cd8b947a},
  journal = {Science},
  keywords = {sys:relevantfor:ecu-cc-research ActiveLearning LargeClass},
  month = may,
  number = 6031,
  pages = {862 -- 864},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Improving Learning in a Large-Enrollment Physics Class},
  volume = 332,
  year = 2011
}

@book{white2013print,
  abstract = {As DITA has become more and more popular, demand has increased for tools that can produce high quality PDFs from DITA content. The DITA Open Toolkit provides a basic PDF capability, but nearly any real-world application will require customization. Leigh White's new book, DITA for Print takes you through the process of building a print customization plugin for the DITA Open Toolkit that will give you control over your PDF output.
DITA for Print is for anybody who wants to learn how to create PDFs using the DITA Open Toolkit without learning everything there is to know about XSL-FO, XSLT, or XPath, or even about the DITA Open Toolkit itself. DITA for Print is written for non-programmers, by a non-programmer, and although it is written for people who have a good understanding of the DITA standard, you don't need a technical background to get custom PDFs up and running quickly.},
  added-at = {2016-12-07T00:57:24.000+0100},
  address = {Laguna Hills, California},
  author = {White, Leigh W.},
  biburl = {https://www.bibsonomy.org/bibtex/291c2dadb6e2cb55618005b0f3dc7a2b0/vngudivada},
  interhash = {5202ad5236ddfbde1688270486814fa4},
  intrahash = {91c2dadb6e2cb55618005b0f3dc7a2b0},
  keywords = {Book DITA},
  publisher = {XML Press},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {DITA for print: a DITA open toolkit workbook},
  year = 2013
}

@article{ellis2015project,
  abstract = {Providing students with the professional, communication, and technical skills necessary to contribute to an ongoing software project is critical, yet often difficult in higher education. Involving student teams in real-world projects developed by professional software engineers for actual users is invaluable. Free and Open Source Software (FOSS) has emerged as an important approach to creating, managing, and distributing software products. Involvement in a FOSS project provides students with experience developing within a professional environment, with a professional community, and has the additional benefit that all communication and artifacts are publicly accessible. Humanitarian Free and Open Source Software (HFOSS) projects benefit the human condition in some manner. They can range from disaster management to microfinance to election-monitoring applications. This article discusses the benefits and challenges of students participating in HFOSS projects within the context of undergraduate computing degree programs. This article reports on a 6-year study of students' self-reported attitudes and learning from participation in an HFOSS project. Results indicate that working on an HFOSS project increases interest in computing. In addition, students perceive that they are gaining experience in developing software in a distributed environment with the attendant skills of communication, distributed teamwork, and more.},
  added-at = {2016-12-30T03:05:25.000+0100},
  address = {New York, NY},
  author = {Ellis, Heidi J. C. and Hislop, Gregory W. and Jackson, Stoney and Postner, Lori},
  biburl = {https://www.bibsonomy.org/bibtex/226bf8599b64a735161e3bfcc0fc97a35/vngudivada},
  interhash = {f9ab7f9bbd372ac4b0376b2b2b430c26},
  intrahash = {26bf8599b64a735161e3bfcc0fc97a35},
  journal = {Trans. Comput. Educ.},
  keywords = {FOS HFOSS NsfRED},
  month = dec,
  number = 4,
  pages = {18:1--18:23},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Team Project Experiences in Humanitarian Free and Open Source Software (HFOSS)},
  url = {http://doi.acm.org/10.1145/2684812},
  volume = 15,
  year = 2015
}

@book{denison1997corporate,
  abstract = {This book explains how corporate culture develops and how it determines the quality of the corporation's output. It also describes methods for testing and assessing corporate culture and discusses strategies for making changes in the work enviroment that will bring about increased productivity.},
  added-at = {2017-01-02T00:08:59.000+0100},
  address = {Ann Arbor, MI},
  author = {Denison, Daniel R.},
  biburl = {https://www.bibsonomy.org/bibtex/2a8049f722c918271518f8b64a8c8fba2/vngudivada},
  interhash = {fb223dba66800b5348af1cf7bc0e3a22},
  intrahash = {a8049f722c918271518f8b64a8c8fba2},
  keywords = {Leadership NsfRED},
  publisher = {Denison Consulting},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Corporate culture and organizational effectiveness},
  year = 1997
}

@misc{chia2006measuring,
  abstract = {Public relations scholars have attempted to measure public relations relationships to determine whether they are satisfactory, whether relational partners trust each other and whether commitment is critical to successful relationships. These scholars point to the need to measure relationships but an essential first step is to have a clear and orderly accepted concept of what is to be measured and whether the measuring of relationships is a realistic goal. This paper suggests that relational characteristics such as commitment, trust and satisfaction are so subjective that attempts to measure them have been extremely difficult as they change with each situation, with different clients and organisations and with varied perceptions and interpretations of those in a relationship.

This paper looks at the contribution of interpersonal theory and scholarship in psychology and business management that suggests that personal and business relational dialogue is so complex that attempting to measure relationships or relationship characteristics maybe a fruitless exercise. Rather, interpersonal and business scholars, who focus on relational exchanges and relational development, point to the need to have rules for relationships that specify what is important to the relationship. Supporting the views of these scholars, the need for relational parameters for effective public relations relationship management was an important finding of a qualitative study of public relations consultants and their clients. The qualitative study also produced concepts of relationships and relationship management relevant to public relations that this paper argues extends the range of present perspectives beyond relationship indicators and beyond the measuring of immeasurable relationships.},
  added-at = {2017-01-01T23:33:18.000+0100},
  author = {Chia, Joy},
  biburl = {https://www.bibsonomy.org/bibtex/2d63d5af5c7886b6193a6275d5124da3a/vngudivada},
  interhash = {2fb2c5ec4d315d6c8b5d73f6046c2cd8},
  intrahash = {d63d5af5c7886b6193a6275d5124da3a},
  keywords = {Measurement NsfRED},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Measuring the immeasurable},
  url = {http://www.prismjournal.org/fileadmin/Praxis/Files/Journal_Files/Evaluation_Issue/CHIA_ARTICLE.pdf},
  year = 2006
}

@article{meyers2012comment,
  abstract = {For educators and researchers, YouTube represents a rich space for exploring the student experience of learning at the intersection of academic needs and informal sources. This article explores user-generated comments from a niche segment of YouTube’s vast database; namely, informational video that may be used by students to supplement their academic needs. Employing Buckingham’s concept of media literacy as an analytic frame and computer-mediated discourse analysis as method, this article addresses the following questions: How are students engaging with YouTube for informal learning tasks? And what role does media literacy play in this experience? The analysis focuses on evidence of meaning-making, concept negotiation, and information-sharing, -seeking, and -use practices. The findings reveal insights about the nature of instructional video as well as the ways video and user-generated comments help resolve learners’ questions. YouTube creates a unique space where students can develop learning content, find multiple representations of academic material, and make comments on the quality, authority, and usefulness of that content with regard to their information needs.},
  added-at = {2016-12-18T13:32:33.000+0100},
  author = {Meyers, Eric M.},
  biburl = {https://www.bibsonomy.org/bibtex/23e8f283ff72e47630cb507bc4a6055de/vngudivada},
  doi = {10.1162/ijlm_a_00100},
  interhash = {2097d9d94aadebb565ce242bfc3eed06},
  intrahash = {3e8f283ff72e47630cb507bc4a6055de},
  journal = {International Journal of Learning and Media},
  keywords = {InformalLearning Learning YouTube},
  month = jul,
  number = {3-4},
  pages = {33--47},
  publisher = {{MIT} Press - Journals},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {A Comment on Learning: Media Literacy Practices in {YouTube}},
  url = {http://dx.doi.org/10.1162/IJLM_a_00100},
  volume = 4,
  year = 2012
}

@inproceedings{curcin2008scientific,
  abstract = {The past decade has witnessed a growing trend in designing and using workflow systems with a focus on supporting the scientific research process in bioinformatics and other areas of life sciences. The aim of these systems is mainly to simplify access, control and orchestration of remote distributed scientific data sets using remote computational resources, such as EBI web services. In this paper we present the state of the art in the field by reviewing six such systems: Discovery Net, Taverna, Triana, Kepler, Yawl and BPEL.

We provide a high-level framework for comparing the systems based on their control flow and data flow properties with a view of both informing future research in the area by academic researchers and facilitating the selection of the most appropriate system for a specific application task by practitioners.},
  added-at = {2016-12-19T05:04:39.000+0100},
  author = {Curcin, V. and Ghanem, M.},
  biburl = {https://www.bibsonomy.org/bibtex/25a2a029d5adc13e8f10fab7fdf36643a/vngudivada},
  booktitle = {2008 Cairo International Biomedical Engineering Conference},
  doi = {10.1109/cibec.2008.4786077},
  interhash = {492ab4aee25debcb96b6f5631ca5ed36},
  intrahash = {5a2a029d5adc13e8f10fab7fdf36643a},
  keywords = {ScientificWorkflowSystem},
  month = dec,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Scientific workflow systems - can one size fit all?},
  url = {http://dx.doi.org/10.1109/CIBEC.2008.4786077},
  year = 2008
}

@book{raymond2001cathedral,
  abstract = {It all started with a series of odd statistics. The leading challenger to Microsoft's stranglehold on the computer industry is an operating system called Linux, the product of thousands of volunteer programmers who collaborate over the Internet. The software behind a majority of all the world's web sites doesn't come from a big company either, but from a loosely coordinated group of volunteer programmers called the Apache Group. The Internet itself, and much of its core software, was developed through a process of networked collaboration." "This book starts out with A Brief History of Hackerdom - the historical roots of the open-source movement - and details the events that led to the recognition of the power of open source. It contains the full text of The Cathedral and the Bazaar, updated and expanded for this book, plus Mr. Raymond's other key essays on the social and economic dynamics of open-source software development." "Open source is the competitive advantage in the Internet Age. The Cathedral and the Bazaar is a must read for anyone who cares about the computer industry or the dynamics of the information economy. Already, billions of dollars have been made and lost based on the ideas in this book. Its conclusions will be studied, debated, and implemented for years to come.},
  added-at = {2016-12-25T19:03:50.000+0100},
  address = {Sabestopol, California},
  author = {Raymond, Eric S.},
  biburl = {https://www.bibsonomy.org/bibtex/2f7c361e99be95cc0d1999f1346474a97/vngudivada},
  interhash = {3fbbba1926d6f49d1692a17aa85bb0f8},
  intrahash = {f7c361e99be95cc0d1999f1346474a97},
  keywords = {Book FOSS},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {The cathedral and the bazaar: musings on Linux and Open Source by an accidental revolutionary},
  year = 2001
}

@misc{stanford2017designing,
  abstract = {Increasing adoption and adaptation of promising instructional strategies and materials has been identified as a critical component needed to improve science, technology, engineering, and mathematics (STEM) education. This paper examines typical propagation practices and resulting outcomes of proposals written by developers of educational innovations. These proposals were analyzed using the Designing for Sustained Adoption Assessment Instrument (DSAAI), an instrument developed to evaluate propagation plans, and the results used to predict the likelihood that a successful project would result in adoption by others. We found that few education developers propose strong propagation plans. Afterwards, a follow-up analysis was conducted to see which propagation strategies developers actually used to help develop, disseminate, and support their innovations. A web search and interviews with principal investigators were used to determine the degree to which propagation plans were actually implemented and to estimate adoption of the innovations. In this study, we analyzed 71 education development proposals funded by the National Science Foundation and predicted that 80% would be unsuccessful in propagating their innovations. Follow-up data collection with a subset of these suggests that the predictions were reasonably accurate.},
  added-at = {2016-12-23T21:48:00.000+0100},
  author = {Stanford, C. and Cole, R.S. and Froyd, J. and Friedrichsen, D. and Khatri, R. and Henderson, C.},
  biburl = {https://www.bibsonomy.org/bibtex/2ff3020551bcc8fd2e4000712dfce1ec3/vngudivada},
  interhash = {9b0aa4d479cc010d927a906e3a533b47},
  intrahash = {ff3020551bcc8fd2e4000712dfce1ec3},
  keywords = {Dissemination NsfRED},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Designing for sustained adoption: An analysis of propagation plans in NSF-funded education development projects},
  year = 2017
}

@misc{eykholt2015discovering,
  added-at = {2016-12-31T20:44:14.000+0100},
  author = {Eykholt, Diane},
  biburl = {https://www.bibsonomy.org/bibtex/2b5589b9787a3c617a2419c03ff4aa27d/vngudivada},
  interhash = {7a0fb9fc816eed2e6796e853b221aec9},
  intrahash = {b5589b9787a3c617a2419c03ff4aa27d},
  keywords = {NsfRED StudentRetention},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Discovering Factors Affecting Student Retention in Computer Science at Cal Poly},
  url = {http://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1050&context=laessp},
  year = 2015
}

@article{yuen2014qualitative,
  abstract = {Critical thinking, problem solving, the use of tools, and the ability to consume and analyze information are important skills for the 21st century workforce. This article presents a qualitative case study that follows five undergraduate biology majors in a computer science course (CS0). This CS0 course teaches programming within a data-driven context and is part of a university-wide initiative to improve students' quantitative scholarship. In this course, students learn computing concepts and computational thinking by writing programs in MATLAB that compute with data, by performing meaningful analyses, and by writing about the results. The goal of the study reported here is to better understand the thought processes students use in such a data-driven approach. Findings show that students engage in an ongoing organizational process to understand the structure of the data. The computational and visualization tasks appear to be closely linked, and the visualization component appears to provide valuable feedback for students in accomplishing the programming tasks.},
  added-at = {2016-12-30T14:17:09.000+0100},
  address = {New York, NY},
  author = {Yuen, Timothy T. and Robbins, Kay A.},
  biburl = {https://www.bibsonomy.org/bibtex/28593929820975dccaefd94838e0d819b/vngudivada},
  interhash = {439fdc970b55c3f612ed63e4766c3456},
  intrahash = {8593929820975dccaefd94838e0d819b},
  journal = {Trans. Comput. Educ.},
  keywords = {CS0 ComputationalThinking DataScience},
  month = dec,
  number = 4,
  pages = {27:1--27:19},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {A Qualitative Study of Students' Computational Thinking Skills in a Data-Driven Computing Class},
  url = {http://doi.acm.org/10.1145/2676660},
  volume = 14,
  year = 2014
}

@article{williams2015describing,
  abstract = {Background
Collecting data on instructional practices is an important step in planning and enacting meaningful initiatives to improve undergraduate science instruction. Self-report survey instruments are one of the most common tools used for collecting data on instructional practices. This paper is an instrument- and item-level analysis of available instructional practice instruments to survey postsecondary instructional practices. We qualitatively analyzed the instruments to document their features and methodologically sorted their items into autonomous categories based on their content. The paper provides a detailed description and evaluation of the instruments, identifies gaps in the literature, and provides suggestions for proper instrument selection, use, and development based on these findings.

Results
The 12 instruments we analyzed use a variety of measurement and development approaches. There are two primary instrument types: those intended for all postsecondary instructors and those intended for instructors in a specific STEM discipline. The instruments intended for all instructors often focus on teaching as well as other aspects of faculty work. The number of teaching practice items and response scales varied widely. Most teaching practice items referred to the format of in-class instruction (54 %), such as group work or problem solving. Another important type of teaching practice items referred to assessment practices (35 %), frequently focusing on specific types of summative assessment items used.

Conclusions
The recent interest in describing teaching practices has led to the development of a diverse set of available self-report instruments. Many instruments lack an audit trail of their development, including rationale for response scales; whole instrument and construct reliability values; and face, construct, and content validity measures. Future researchers should consider building on these existing instruments to address some of their current weaknesses. In addition, there are important aspects of instruction that are not currently described in any of the available instruments. These include laboratory-based instruction, hybrid and online instructional environments, and teaching with elements of universal design},
  added-at = {2016-12-23T22:15:32.000+0100},
  author = {Williams, Cody T. and Walter, Emily M. and Henderson, Charles and Beach, Andrea L.},
  biburl = {https://www.bibsonomy.org/bibtex/284b4320906296150c4e15bb46d121f35/vngudivada},
  doi = {10.1186/s40594-015-0031-y},
  interhash = {bc65db057e2ab948901e34e11f55c304},
  intrahash = {84b4320906296150c4e15bb46d121f35},
  journal = {International Journal of {STEM} Education},
  keywords = {InstrctionalPractice NsfRED STEM},
  month = oct,
  number = 1,
  publisher = {Springer Nature},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Describing undergraduate {STEM} teaching practices: a comparison of instructor self-report instruments},
  url = {http://dx.doi.org/10.1186/s40594-015-0031-y},
  volume = 2,
  year = 2015
}

@book{dekoenigsberg2010teaching,
  abstract = {This textbook teaches the basic skills of open source development incrementally, through real involvement in meaningful projects, for students and self-learners.},
  added-at = {2016-12-25T18:58:00.000+0100},
  author = {DeKoenigsberg, Greg and Tyler, Chris and Wade, Karsten and Spevack, Max and Chua, Mel and Sheltren, Jeff},
  biburl = {https://www.bibsonomy.org/bibtex/2289d4a7ccc2b92d347bc92b654bea530/vngudivada},
  interhash = {86ba2c6a9b9f017f7cadb28a7330ad21},
  intrahash = {289d4a7ccc2b92d347bc92b654bea530},
  keywords = {Book FOSS},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Teaching Open Source: Practical Open Source Software Exploration},
  url = {http://teachingopensource.org/index.php/Textbook_Release_0.8},
  year = 2010
}

@article{henderson2011facilitating,
  abstract = {This article reviews current scholarship about how to promote change in instructional practices used in undergraduate science, technology, engineering, and mathematics (STEM) courses. The review is based on 191 conceptual and empirical journal articles published between 1995 and 2008. Four broad categories of change strategies were developed to capture core differences within this body of literature: disseminating curriculum and pedagogy, developing reflective teachers, enacting policy, and developing shared vision. STEM education researchers largely write about change in terms of disseminating curriculum and pedagogy. Faculty development researchers largely write about change in terms of developing reflective teachers. Higher education researchers largely write about change in terms of enacting policy. New work often does not build on prior empirical or theoretical work. Although most articles claim success of the change strategy studied, evidence presented to support these claims is typically not strong. For example, only 21\% of articles that studied implementation of a change strategy were categorized as presenting strong evidence to support claims of success or failure of the strategy. These analyses suggest that the state of change strategies and the study of change strategies are weak, and that research communities that study and enact change are largely isolated from one-another. In spite of the weak state of the literature, some conclusions related to the design of change strategies can be drawn from this review. Two commonly used change strategies are clearly not effective: developing and testing best practice curricular materials and then making these materials available to other faculty and top-down policy-making meant to influence instructional practices. Effective change strategies: are aligned with or seek to change the beliefs of the individuals involved; involve long-term interventions, lasting at least one semester; require understanding a college or university as a complex system and designing a strategy that is compatible with this system.},
  added-at = {2016-12-23T18:49:38.000+0100},
  author = {Henderson, Charles and Beach, Andrea and Finkelstein, Noah},
  biburl = {https://www.bibsonomy.org/bibtex/29ebf6f283a1d5ef588111ce6440e7347/vngudivada},
  interhash = {17d5b76563efbe63b24be765bae00908},
  intrahash = {9ebf6f283a1d5ef588111ce6440e7347},
  journal = {Journal of Research in Science Teaching},
  keywords = {InstrctionalPractice NsfRED STEM},
  number = 8,
  pages = {952--984},
  publisher = {Wiley Subscription Services, Inc.},
  timestamp = {2019-03-25T17:10:56.000+0100},
  title = {Facilitating change in undergraduate STEM instructional practices: An analytic review of the literature},
  url = {http://dx.doi.org/10.1002/tea.20439},
  volume = 48,
  year = 2011
}

@book{ambrose2010learning,
  abstract = {Any conversation about effective teaching must begin with a consideration of how students learn. However, instructors may find a gap between resources that focus on the technical research on learning and those that provide practical classroom strategies. How Learning Works provides the bridge for such a gap. In this volume, the authors introduce seven general principles of learning, distilled from the research literature as well as from twenty-seven years of experience working one-on-one with college faculty. They have drawn on research from a breadth of perspectives (cognitive, developmental, and social psychology; educational research; anthropology; demographics; and organizational behavior) to identify a set of key principles underlying learning-from how effective organization enhances retrieval and use of information to what impacts motivation. These principles provide instructors with an understanding of student learning that can help them see why certain teaching approaches are or are not supporting student learning, generate or refine teaching approaches and strategies that more effectively foster student learning in specific contexts, and transfer and apply these principles to new courses. For anyone who wants to improve his or her students' learning, it is crucial to understand how that learning works and how to best foster it. This vital resource is grounded in learning theory and based on research evidence, while being easy to understand and apply to college teaching.},
  added-at = {2017-01-16T05:13:38.000+0100},
  address = {San Francisco, CA},
  author = {Ambrose, Susan A. and Bridges, Michael W. and DiPietro, Michele and Lovett, Marsha C. and Norman, Marie K.},
  biburl = {https://www.bibsonomy.org/bibtex/2a55e79cc2b29dc8aaa7a0b94460519b1/vngudivada},
  interhash = {7f3e18b6a25f22491d101a61948af4b3},
  intrahash = {a55e79cc2b29dc8aaa7a0b94460519b1},
  keywords = {Book Learning ParExcellence SmartTeaching},
  publisher = {Jossey-Bass},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {How learning works: seven research-based principles for smart teaching},
  year = 2010
}

@incollection{bushe2013generative,
  abstract = {Generativity is defined in this chapter as the creation of new images, metaphors, physical representations, and so on that have two qualities: they change how people think so that new options for decisions and/or actions become available to them, and they are compelling images that people want to act on. Research and experiences that suggest “positivity”, particularly positive emotion, is not sufficient for transformational change, but that generativity is a key change lever in cases of transformational change, are reviewed. A model of different characteristics of generativity is offered and ways in which appreciative inquiry can be a generative process, increase generative capacity, and lead to generative outcomes, are discussed.  Ways to increase the generativity of appreciative inquiry through generative topics, generative questions, generative conversations, and generative action are offered.},
  added-at = {2017-01-05T22:37:21.000+0100},
  author = {Bushe, G.R.},
  biburl = {https://www.bibsonomy.org/bibtex/2506642f66904db34587a2c034283e677/vngudivada},
  booktitle = {Organizational Generativity: The Appreciative Inquiry Summit and a Scholarship of Transformation},
  editor = {Cooperrider, D.L. and Zandee, D.P. and Godwin, L.N. and Avital, M. and Boland, B.},
  interhash = {b6d52256fbcd647849040998e1184495},
  intrahash = {506642f66904db34587a2c034283e677},
  keywords = {AppreciativeInquiry GenerativeProcess NsfRED},
  pages = {89 -- 113},
  publisher = {Emerald Group Publishing Limited},
  series = {Advances in Appreciative Inquiry},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Generative Process, Generative Outcome: The Transformational Potential of Appreciative Inquiry},
  volume = 4,
  year = 2013
}

@inproceedings{emanueleii2015building,
  abstract = {What is data science like in a Healthcare IT startup company trying to build products to change the industry? The challenges extend far beyond the technical challenges, and we review in this talk what it takes to get a project off the ground, build a data science product, influence users of the product, and ultimately change the industry you're working in. This will require us to reflect on what makes data valuable, principles for valuating data science products, user/analyst adoption (or lack thereof), as well as the specifics of the Healthcare industry and how company culture can affect the success and failure of data science products. Included in the talk will be specific case studies about successes and failures building data science products at Wellcentive.},
  added-at = {2017-01-12T01:56:32.000+0100},
  address = {New York, NY},
  author = {{Emanuele, II}, Vincent A. and Lloyd, David},
  biburl = {https://www.bibsonomy.org/bibtex/2a60126837e392d267191f8590a117ed6/vngudivada},
  booktitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
  interhash = {6907e17c05a79ad4c3032a0d6cc2b257},
  intrahash = {a60126837e392d267191f8590a117ed6},
  keywords = {DataScience Healthcare},
  pages = {673--673},
  publisher = {ACM},
  series = {BCB '15},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Building and Implementing Data Science Products to Change Healthcare IT},
  url = {http://doi.acm.org/10.1145/2808719.2816980},
  year = 2015
}

@book{knaflic2015storytelling,
  abstract = {Storytelling with Data teaches you the fundamentals of data visualization and how to communicate effectively with data. You'll discover the power of storytelling and the way to make data a pivotal point in your story. The lessons in this illuminative text are grounded in theory, but made accessible through numerous real-world examples—ready for immediate application to your next graph or presentation.

Storytelling is not an inherent skill, especially when it comes to data visualization, and the tools at our disposal don't make it any easier. This book demonstrates how to go beyond conventional tools to reach the root of your data, and how to use your data to create an engaging, informative, compelling story. Specifically, you'll learn how to:

Understand the importance of context and audience
Determine the appropriate type of graph for your situation
Recognize and eliminate the clutter clouding your information
Direct your audience's attention to the most important parts of your data
Think like a designer and utilize concepts of design in data visualization
Leverage the power of storytelling to help your message resonate with your audience
Together, the lessons in this book will help you turn your data into high impact visual stories that stick with your audience. Rid your world of ineffective graphs, one exploding 3D pie chart at a time. There is a story in your data—Storytelling with Data will give you the skills and power to tell it!},
  added-at = {2017-01-16T02:04:32.000+0100},
  address = {New York, NY},
  author = {Knaflic, Cole Nussbaumer},
  biburl = {https://www.bibsonomy.org/bibtex/265b6232f646d39a85dc0ad15fc79722d/vngudivada},
  interhash = {81fe61523f9eb55ad717f864265e4362},
  intrahash = {65b6232f646d39a85dc0ad15fc79722d},
  keywords = {Book StoryTelling Visualization},
  publisher = {John Wiley},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Storytelling with data: a data visualization guide for business professionals},
  year = 2015
}

@article{guo2013helping,
  abstract = {The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.<br /><br />twitter<br />Follow us on Twitter at http://twitter.com/blogCACM<br /><br />http://cacm.acm.org/blogs/blog-cacm<br /><br />Philip Guo explains how programming skills can make scientists and engineers more efficient and creative.},
  added-at = {2017-01-14T23:14:18.000+0100},
  address = {New York, NY},
  author = {Guo, Philip},
  biburl = {https://www.bibsonomy.org/bibtex/2cb1d3ce317e44b5e7da6dc09d7d84c0e/vngudivada},
  interhash = {6a698d8edd3ee6e8a0e49bfa2cec5885},
  intrahash = {cb1d3ce317e44b5e7da6dc09d7d84c0e},
  journal = {Commun. ACM},
  keywords = {CS1 Programming},
  month = oct,
  number = 10,
  pages = {12--13},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Helping Scientists, Engineers to Work Up to 100 Times Faster},
  url = {http://doi.acm.org/10.1145/2507771.2507775},
  volume = 56,
  year = 2013
}

@article{fredrickson2003value,
  abstract = {Positive emotions -- joy, interest and contentment—are a puzzle to scientists. From an evolutionary point of view they don’t seem to have the same survival value as negative emotions such as fear or anger. The negative emotions elicit specific actions to run or attack, which must surely have helped our ancestors survive the dangers of life on the savannah. But what's the survival value of feeling joy or contentment? Psychologist Fredrickson argues that positive emotions allowed our ancestors to broaden their minds and build resources—intellectual, physical and social—that served them in good stead during hard times.},
  added-at = {2017-01-05T20:44:08.000+0100},
  author = {Fredrickson, Barbara},
  biburl = {https://www.bibsonomy.org/bibtex/278fe1e09787fd11aa603bc3ce655bcb9/vngudivada},
  doi = {10.1511/2003.4.330},
  interhash = {c736ac0ceae368d1983a29ba32051790},
  intrahash = {78fe1e09787fd11aa603bc3ce655bcb9},
  journal = {American Scientist},
  keywords = {NsfRED PositiveEmotions Positivity},
  number = 4,
  pages = {330 -- 335},
  publisher = {Sigma Xi},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {The Value of Positive Emotions},
  volume = 91,
  year = 2003
}

@inproceedings{kuksenok2015adoption,
  abstract = {Ocean sciences in the US have had a cultural distinction between modeling and fieldwork: a researcher either wrote MATLAB code, or went on data collection cruises. Large-scale multi-institution collaborations, and adoption of data science tools and skills, are blurring this distinction. CSCW and STS often study data: its production, maintenance, management, and use. In my dissertation, I focus not on the data but oceanographer groups incorporating data science practice into their work. By studying challenges faced by collective actors, this ethnographic research will then lead to developing design and organization implications for supporting data science practice in scientific academic collaborations.},
  added-at = {2017-01-12T01:59:24.000+0100},
  address = {New York, NY},
  author = {Kuksenok, Katerena},
  biburl = {https://www.bibsonomy.org/bibtex/2bd9f9a60babda86931297825994a6338/vngudivada},
  booktitle = {Proceedings of the 18th ACM Conference Companion on Computer Supported Cooperative Work \&\#38; Social Computing},
  interhash = {fc55cf8daf7542c1a67b9e055c5f43e0},
  intrahash = {bd9f9a60babda86931297825994a6338},
  keywords = {DataScience Oceanography},
  pages = {93--96},
  publisher = {ACM},
  series = {CSCW'15 Companion},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Adoption and Adaptation of Data Science in Oceanography},
  url = {http://doi.acm.org/10.1145/2685553.2699329},
  year = 2015
}

@book{barkley2010student,
  abstract = {From the Publisher: Keeping students involved, motivated, and actively learning is challenging educators across the country, yet good advice on how to accomplish this has not been readily available. Student Engagement Techniques is a comprehensive resource that offers college teachers a dynamic model for engaging students and includes over one hundred tips, strategies, and techniques that have been proven to help teachers from a wide variety of disciplines and institutions motivate and connect with their students. The ready-to-use format shows how to apply each of the book's techniques in the classroom and includes purpose, preparation, procedures, examples, online implementation, variations and extensions, observations and advice, and key resources.},
  added-at = {2017-01-21T18:55:55.000+0100},
  address = {San Francisco, California},
  author = {Barkley, Elizabeth F.},
  biburl = {https://www.bibsonomy.org/bibtex/2d552296c8d421cbe5d53e668e3d824c8/vngudivada},
  interhash = {1abb30f981a2d872d933e916396a17db},
  intrahash = {d552296c8d421cbe5d53e668e3d824c8},
  keywords = {Book Learning ParExcellence StudentEngagement},
  publisher = {Jossey-Bass},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Student engagement techniques: a handbook for college faculty},
  year = 2010
}

@inproceedings{anslow2016datathons,
  abstract = {Large amounts of data are becoming increasingly available through open data repositories as well as companies and governments collecting data to improve decision making and efficiencies. Consequently there is a need to increase the data literacy of computer science students. Data science is a relatively new area within computer science and the curriculum is rapidly evolving along with the tools required to perform analytics which students need to learn how to effectively use. To address the needs of students learning key data science and analytics skills we propose augmenting existing data science curriculums with hackathon events that focus on data also known as datathons. In this paper we present our experience at hosting and running four datathons that involved students and members from the community coming together to solve challenging problems with data from not-for-profit social good organizations and publicly open data. Our reported experience from our datathons will help inform other academics and community groups who also wish to host datathons to help facilitate their students and members to learn key data science and analytics skills.},
  added-at = {2017-01-12T02:45:12.000+0100},
  address = {New York, NY},
  author = {Anslow, Craig and Brosz, John and Maurer, Frank and Boyes, Mike},
  biburl = {https://www.bibsonomy.org/bibtex/2d6fd27b4b0b4aa6aafc0023d14fe42da/vngudivada},
  booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
  interhash = {d689344d229a0d3dfcee59781f414d76},
  intrahash = {d6fd27b4b0b4aa6aafc0023d14fe42da},
  keywords = {DataHackathon Datathon},
  pages = {615--620},
  publisher = {ACM},
  series = {SIGCSE '16},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Datathons: An Experience Report of Data Hackathons for Data Science Education},
  url = {http://doi.acm.org/10.1145/2839509.2844568},
  year = 2016
}

@inproceedings{howe2014should,
  abstract = {The Database Community has a unique perspective on the challenges and solutions of long-term management of data and the value of data as a resource. In current computer science curricula, however, these insights are typically locked up in the context of the traditional Intro to Databases class that was developed years (or in some cases, decades) before the modern concept of Data Science arose and embedded in the discussion of legacy data management systems. We consider how to bring these concepts front and center into the emerging wave of Data Science courses, degree programs and even departments.},
  added-at = {2017-01-12T02:05:24.000+0100},
  address = {New York, NY},
  author = {Howe, Bill and Franklin, Michael J. and Freire, Juliana and Frew, James and Kraska, Tim and Ramakrishnan, Raghu},
  biburl = {https://www.bibsonomy.org/bibtex/20958789b2801513c83af6b52e11f99d6/vngudivada},
  booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
  interhash = {f0f40938a7b52ff2598953ebb1bd876a},
  intrahash = {0958789b2801513c83af6b52e11f99d6},
  keywords = {DataScience},
  pages = {917--918},
  publisher = {ACM},
  series = {SIGMOD '14},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Should We All Be Teaching "Intro to Data Science" Instead of "Intro to Databases"?},
  url = {http://doi.acm.org/10.1145/2588555.2600092},
  year = 2014
}

@article{guo2014clarifying,
  abstract = {The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.<br /><br />twitter<br />Follow us on Twitter at http://twitter.com/blogCACM<br /><br />http://cacm.acm.org/blogs/blog-cacm<br /><br />Philip Guo teaches an undergrad through the use of examples.},
  added-at = {2017-01-14T23:06:18.000+0100},
  address = {New York, NY},
  author = {Guo, Philip},
  biburl = {https://www.bibsonomy.org/bibtex/27aedf2c62cb94a8340b8bc8bf2d25ee5/vngudivada},
  interhash = {1dcd8a794e484839e36507e6866910c1},
  intrahash = {7aedf2c62cb94a8340b8bc8bf2d25ee5},
  journal = {Commun. ACM},
  keywords = {HCI},
  month = feb,
  number = 2,
  pages = {10--11},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Clarifying Human-computer Interaction},
  url = {http://doi.acm.org/10.1145/2557448},
  volume = 57,
  year = 2014
}

@inproceedings{anderson2015facilitating,
  abstract = {This paper discusses the learning strategies adopted in a publically available, cloud-based learning environment, Learn2Mine, which facilitates student-progress as they solve data science programming problems. The learning system has been evaluated over three consecutive terms. Learn2Mine was initially introduced in an introductory course and pilot-tested for usability and effectiveness in Fall 2013. Students reported positive opinions on usability and effectiveness of the system in their completion of programming assignments. In Spring 2014, Learn2Mine was evaluated in an upper-level data mining course by comparing student submission rates and amount of programming accomplished for a group with access to the tool versus one without access. The group with access to Learn2Mine had an average assignment submission rate of 84%, while the group without had an average submission rate of only 48% (difference significant at p < 0.01). In Fall 2014, a controlled experiment was conducted in an introductory data science course: one group of students worked on a multi-part programming task with support of scaffolding and gamification as implemented in Learn2Mine, while the other section did not. The group with access performed significantly better in overall task completion (p < 0.01).},
  added-at = {2017-01-12T02:43:05.000+0100},
  address = {New York, NY},
  author = {Anderson, Paul E. and Nash, Thomas and McCauley, Ren{\'e}e},
  biburl = {https://www.bibsonomy.org/bibtex/24da30220bfa3a0b4a5a21f90825c2d40/vngudivada},
  booktitle = {Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education},
  interhash = {0fbb9edc5ff43c5e249be3b8f4842964},
  intrahash = {4da30220bfa3a0b4a5a21f90825c2d40},
  keywords = {DataScience GamifiedScaffolding Learn2Mine},
  pages = {99--104},
  publisher = {ACM},
  series = {ITiCSE '15},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Facilitating Programming Success in Data Science Courses Through Gamified Scaffolding and Learn2Mine},
  url = {http://doi.acm.org/10.1145/2729094.2742597},
  year = 2015
}

@article{fredrickson2005positive,
  abstract = {[Correction Notice: An Erratum for this article was reported in Vol 68(9) of American Psychologist (see record 2013-32937-001). The hypothesis tested in this article was motivated, in part, by the nonlinear dynamic model introduced in Losada (1999) and advanced in Losada and Heaphy (2004) and herein (Fredrickson & Losada, 2005). This model has since been called into question (Brown, Sokal, & Friedman, 2013). Losada has chosen not to defend his nonlinear dynamic model in light of the Brown et al. critique. Fredrickson’s (2013) published response to the Brown et al. critique conveys that although she had accepted Losada’s modeling as valid, she has since come to question it. As such, the modeling element of this article is formally withdrawn as invalid and, along with it, the model-based predictions about the particular positivity ratios of 2.9 and 11.6. Other elements of the article remain valid and are unaffected by this correction notice. Some of these notable elements are included in the erratum.] Extending B. L. Fredrickson's (1998) broaden-and-build theory of positive emotions and M. Losada's (1999) nonlinear dynamics model of team performance, the authors predict that a ratio of positive to negative affect at or above 2.9 will characterize individuals in flourishing mental health. Participants (N=188) completed an initial survey to identify flourishing mental health and then provided daily reports of experienced positive and negative emotions over 28 days. Results showed that the mean ratio of positive to negative affect was above 2.9 for individuals classified as flourishing and below that threshold for those not flourishing. Together with other evidence, these findings suggest that a set of general mathematical principles may describe the relations between positive affect and human flourishing. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  added-at = {2017-01-05T17:45:01.000+0100},
  author = {Fredrickson, Barbara L. and Losada, Marcial F.},
  biburl = {https://www.bibsonomy.org/bibtex/24a879a2079800f601bae4a8c3cdb5aba/vngudivada},
  interhash = {9c4fcacfa52d82bb48a9d529dad8fb3d},
  intrahash = {4a879a2079800f601bae4a8c3cdb5aba},
  journal = {American Psychologist},
  keywords = {Leadership NsfRED PositiveAffect Psychology},
  number = 7,
  pages = {678--686},
  publisher = {American Psychological Association},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Positive Affect and the Complex Dynamics of Human Flourishing},
  volume = 60,
  year = 2005
}

@article{cacmstaff2010science,
  added-at = {2017-01-12T01:35:37.000+0100},
  address = {New York, NY},
  author = {{CACM Staff}},
  biburl = {https://www.bibsonomy.org/bibtex/23206d8bb78b22434159a03e0b03d4bd1/vngudivada},
  interhash = {6034eb278a48f708d4f56364755c285e},
  intrahash = {3206d8bb78b22434159a03e0b03d4bd1},
  journal = {Commun. ACM},
  keywords = {FourthParadigm FourthPillar},
  month = dec,
  number = 12,
  pages = {6--7},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Science Has Four Legs},
  url = {http://doi.acm.org/10.1145/1859204.1859206},
  volume = 53,
  year = 2010
}

@inproceedings{morik2015driven,
  abstract = {The panel session 'Data Driven Science' discusses application and use of knowledge discovery, machine learning and data analytics in science disciplines; in natural, physical, medical and social science; from physics to geology, and from neuroscience to population health. Knowledge discovery methods are finding broad application in all areas of scientific endeavor, to explore experimental data, to discover new models, to propose new scientific theories and ideas. In addition, the availability of ever larger scientific data sets is driving a new data-driven paradigm for modeling of complex phenomena in physical, natural and social sciences. The purpose of this panel is to bring together users of knowledge discovery, machine learning and data analytics methods across the science disciplines, to understand what tools and methods are proving effective in areas such as data exploration and modeling, to uncover common problems that can be addressed in the KDD community, and to explore the emerging data-driven paradigm in science.},
  added-at = {2017-01-12T02:34:04.000+0100},
  address = {New York, NY},
  author = {Morik, Katharina and Durrant-Whyte, Hugh and Hill, Gary and M\"{u}ller, Dietmar and Berger-Wolf, Tanya},
  biburl = {https://www.bibsonomy.org/bibtex/2d51875981c82fd6001dff840f8952776/vngudivada},
  booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  interhash = {a8c60a81593aca49da757afe022a2ab5},
  intrahash = {d51875981c82fd6001dff840f8952776},
  keywords = {DataScience},
  pages = {2329--2330},
  publisher = {ACM},
  series = {KDD '15},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Data Driven Science: SIGKDD Panel},
  url = {http://doi.acm.org/10.1145/2783258.2788703},
  year = 2015
}

@book{lantz2013machine,
  abstract = {Written as a tutorial to explore and understand the power of R for machine learning. This practical guide that covers all of the need to know topics in a very systematic way. For each machine learning approach, each step in the process is detailed, from preparing the data for analysis to evaluating the results. These steps will build the knowledge you need to apply them to your own data science tasks.Intended for those who want to learn how to use R's machine learning capabilities and gain insight from your data. Perhaps you already know a bit about machine learning, but have never used R; or ...},
  added-at = {2017-01-22T04:20:40.000+0100},
  address = {Birmingham, UK},
  author = {Lantz, Brett},
  biburl = {https://www.bibsonomy.org/bibtex/2f5829b00c3afb433bd8d053341189408/vngudivada},
  interhash = {75ee51a39f749831fabbf396c645a1f8},
  intrahash = {f5829b00c3afb433bd8d053341189408},
  keywords = {Book MachineLearning ParExcellence R},
  publisher = {Packt Publishing},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Machine Learning with R},
  year = 2013
}

@inproceedings{alonso2016stack,
  abstract = {I propose to look at information retrieval applications from the perspective of the data stack infrastructure that is needed in research prototypes and production systems.},
  added-at = {2017-01-12T02:40:38.000+0100},
  address = {New York, NY},
  author = {Alonso, Omar},
  biburl = {https://www.bibsonomy.org/bibtex/2c38f9202edbb3e9d70f666b190b9223a/vngudivada},
  booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  interhash = {327fe61e8e592ba667ff9c8d999884e0},
  intrahash = {c38f9202edbb3e9d70f666b190b9223a},
  keywords = {DataStack IRWorkflow},
  pages = {597--597},
  publisher = {ACM},
  series = {SIGIR '16},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {The Data Stack in Information Retrieval},
  url = {http://doi.acm.org/10.1145/2911451.2926726},
  year = 2016
}

@article{cestarelli2015camur,
  abstract = {MOTIVATION:
Nowadays, knowledge extraction methods from Next Generation Sequencing data are highly requested. In this work, we focus on RNA-seq gene expression analysis and specifically on case-control studies with rule-based supervised classification algorithms that build a model able to discriminate cases from controls. State of the art algorithms compute a single classification model that contains few features (genes). On the contrary, our goal is to elicit a higher amount of knowledge by computing many classification models, and therefore to identify most of the genes related to the predicted class.
RESULTS:
We propose CAMUR, a new method that extracts multiple and equivalent classification models. CAMUR iteratively computes a rule-based classification model, calculates the power set of the genes present in the rules, iteratively eliminates those combinations from the data set, and performs again the classification procedure until a stopping criterion is verified. CAMUR includes an ad-hoc knowledge repository (database) and a querying tool.We analyze three different types of RNA-seq data sets (Breast, Head and Neck, and Stomach Cancer) from The Cancer Genome Atlas (TCGA) and we validate CAMUR and its models also on non-TCGA data. Our experimental results show the efficacy of CAMUR: we obtain several reliable equivalent classification models, from which the most frequent genes, their relationships, and the relation with a particular cancer are deduced.},
  added-at = {2017-01-17T15:17:07.000+0100},
  author = {Cestarelli, Valerio and Fiscon, Giulia and Felici, Giovanni and Bertolazzi, Paola and Weitschek, Emanuel},
  biburl = {https://www.bibsonomy.org/bibtex/2775da9035d56c4e24e156ff06ebe34cd/vngudivada},
  interhash = {a78d45035d1425f7f6fbaa26fdc977a7},
  intrahash = {775da9035d56c4e24e156ff06ebe34cd},
  journal = {Bioinformatics},
  keywords = {Classification RNASequenceData},
  month = oct,
  number = 5,
  pages = {697--704},
  publisher = {Oxford University Press ({OUP})},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {{CAMUR}: Knowledge extraction from {RNA}-seq cancer data through equivalent classification rules},
  url = {https://doi.org/10.1093%2Fbioinformatics%2Fbtv635},
  volume = 32,
  year = 2015
}

@article{bakri2013linking,
  abstract = {The mathematics performance of students enrolling in the engineering technology subjects such as robotics, image processing, control systems and others have been degraded at an alarming rate during the recent years. One of the reasons for this scenario is their inability to relate the mathematical knowledge with the technical applications including image processing. The inconsistency of terminologies used in mathematics and technical subjects has been identified as one of the main sources that contribute to this problem. In this paper, the mapping of the terminologies used in mathematics and image processing was done. It is found that there are different terminologies used in both subjects carry the same meaning and also some same terminologies used in both subjects represent different meanings. Thus it is recommended that lecturers teaching both subjects to introduce the variety of terminologies in defining a newly taught concept for teaching and learning of mathematics and image processing subjects.},
  added-at = {2017-01-15T19:05:33.000+0100},
  author = {Bakri, Norhayati and Ibrahim, Ratnawati and Salleh, Tuan Salwani Awang @ and Zin, Zalhan Mohd},
  biburl = {https://www.bibsonomy.org/bibtex/2fe0628f4e41a56afa5098ec473e3df66/vngudivada},
  interhash = {89a8baf574a4eacd62d6fb97ca052933},
  intrahash = {fe0628f4e41a56afa5098ec473e3df66},
  journal = {Procedia - Social and Behavioral Sciences},
  keywords = {CS1 DIP},
  pages = {454 - 463},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Linking Mathematics and Image Processing Through Common Terminologies},
  url = {http://www.sciencedirect.com/science/article/pii/S1877042813042973},
  volume = 102,
  year = 2013
}

@inproceedings{chilana2016understanding,
  abstract = {Recent research suggests that some students learn to program with the goal of becoming conversational programmers: they want to develop programming literacy skills not to write code in the future but mainly to develop conversational skills and communicate better with developers and to improve their marketability. To investigate the existence of such a population of conversational programmers in practice, we surveyed professionals at a large multinational technology company who were not in software development roles. Based on 3151 survey responses from professionals who never or rarely wrote code, we found that a significant number of them (42.6%) had invested in learning programming on the job. While many of these respondents wanted to perform traditional end-user programming tasks (e.g., data analysis), we discovered that two top motivations for learning programming were to improve the efficacy of technical conversations and to acquire marketable skillsets. The main contribution of this work is in empirically establishing the existence and characteristics of conversational programmers in a large software development context.},
  added-at = {2017-01-14T23:18:43.000+0100},
  address = {New York, NY},
  author = {Chilana, Parmit K. and Singh, Rishabh and Guo, Philip J.},
  biburl = {https://www.bibsonomy.org/bibtex/2ba6704793df1a16024ca323b48848df7/vngudivada},
  booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
  interhash = {710425bf5a4f9195c3c7eee2bcde91fe},
  intrahash = {ba6704793df1a16024ca323b48848df7},
  keywords = {Programming SoftwareIndustry},
  pages = {1462--1472},
  publisher = {ACM},
  series = {CHI '16},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Understanding Conversational Programmers: A Perspective from the Software Industry},
  url = {http://doi.acm.org/10.1145/2858036.2858323},
  year = 2016
}

@book{williams2016syntaxbased,
  abstract = {This unique book provides a comprehensive introduction to the most popular syntax-based statistical machine translation models, filling a gap in the current literature for researchers and developers in human language technologies. While phrase-based models have previously dominated the field, syntax-based approaches have proved a popular alternative, as they elegantly solve many of the shortcomings of phrase-based models. The heart of this book is a detailed introduction to decoding for syntax-based models.

The book begins with an overview of synchronous-context free grammar (SCFG) and synchronous tree-substitution grammar (STSG) along with their associated statistical models. It also describes how three popular instantiations (Hiero, SAMT, and GHKM) are learned from parallel corpora. It introduces and details hypergraphs and associated general algorithms, as well as algorithms for decoding with both tree and string input. Special attention is given to efficiency, including search approximations such as beam search and cube pruning, data structures, and parsing algorithms. The book consistently highlights the strengths (and limitations) of syntax-based approaches, including their ability to generalize phrase-based translation units, their modeling of specific linguistic phenomena, and their function of structuring the search space.

Table of Contents: Preface / Acknowledgments / Models / Learning from Parallel Text / Decoding I: Preliminaries / Decoding II: Tree Decoding / Decoding III: String Decoding / Selected Topics / Closing Remarks / Bibliography / Authors' Biographies / Author Index / Index},
  added-at = {2017-01-16T00:58:09.000+0100},
  author = {Williams, Philip and Sennrich, Rico and Post, Matt and Koehn, Philipp},
  biburl = {https://www.bibsonomy.org/bibtex/2c097830136d68426fac2409aad2e8a2b/vngudivada},
  interhash = {1ef97dd3dddcd3af52e9242656632ec3},
  intrahash = {c097830136d68426fac2409aad2e8a2b},
  journal = {Synthesis Lectures on Human Language Technologies},
  keywords = {MT NLP},
  number = 4,
  pages = {1-208},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Syntax-based Statistical Machine Translation},
  url = {http://dx.doi.org/10.2200/S00716ED1V04Y201604HLT033},
  volume = 9,
  year = 2016
}

@article{denning2009profession,
  added-at = {2017-01-08T00:37:40.000+0100},
  author = {Denning, Peter J.},
  biburl = {https://www.bibsonomy.org/bibtex/2980f7e3b05263287a3392c5e77fa0795/vngudivada},
  interhash = {ae279f4e6b3c24274920e705930f2f67},
  intrahash = {980f7e3b05263287a3392c5e77fa0795},
  journal = {Communications of the ACM},
  keywords = {CS0 ComputationalThinking NsfRED},
  month = jun,
  number = 6,
  pages = {28 -- 30},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {The Profession of IT Beyond Computational Thinking},
  volume = 52,
  year = 2009
}

@book{brown2014stick,
  abstract = {To most of us, learning something "the hard way" implies wasted time and effort. Good teaching, we believe, should be creatively tailored to the different learning styles of students and should use strategies that make learning easier. Make It Stick turns fashionable ideas like these on their head. Drawing on recent discoveries in cognitive psychology and other disciplines, the authors offer concrete techniques for becoming more productive learners. Memory plays a central role in our ability to carry out complex cognitive tasks, such as applying knowledge to problems never before encountered and drawing inferences from facts already known. New insights into how memory is encoded, consolidated, and later retrieved have led to a better understanding of how we learn. Grappling with the impediments that make learning challenging leads both to more complex mastery and better retention of what was learned. Many common study habits and practice routines turn out to be counterproductive. Underlining and highlighting, rereading, cramming, and single-minded repetition of new skills create the illusion of mastery, but gains fade quickly. More complex and durable learning come from self-testing, introducing certain difficulties in practice, waiting to re-study new material until a little forgetting has set in, and interleaving the practice of one skill or topic with another. Speaking most urgently to students, teachers, trainers, and athletes, Make It Stick will appeal to all those interested in the challenge of lifelong learning and self-improvement.},
  added-at = {2017-01-21T18:54:02.000+0100},
  address = {Cambridge, MA},
  author = {Brown, Peter C. and Roediger, Henry L. and McDaniel, Mark A.},
  biburl = {https://www.bibsonomy.org/bibtex/2af27933bfe668d02b25cad5ec0366859/vngudivada},
  interhash = {e6786e637706b8f065772b470d5de498},
  intrahash = {af27933bfe668d02b25cad5ec0366859},
  keywords = {Book Learning ParExcellence},
  publisher = {Belknap Press},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Make it stick: the science of successful learning},
  year = 2014
}

@inproceedings{gil2014teaching,
  abstract = {The goal of our work is to develop an open and modular course for data science and big data analytics that is accessible to non-programmers. The course is designed to
cover major concepts that are useful to understand the benefits
of parallel and distributed programming while not relying on a
programming background. These key concepts focus more on
algorithmic aspects rather than architecture and performance
issues. A key aspect of our work is the use of workflows to illustrate key concepts and to allow the students to practice.},
  added-at = {2017-01-14T04:22:51.000+0100},
  author = {Gil, Yolanda},
  biburl = {https://www.bibsonomy.org/bibtex/2cac7a2302cb8a1b052c4a679d87be1e2/vngudivada},
  booktitle = {2014 Workshop on Education for High Performance Computing},
  doi = {10.1109/eduhpc.2014.12},
  interhash = {9f6a3b5c15f2969a3a9208cc663b6407},
  intrahash = {cac7a2302cb8a1b052c4a679d87be1e2},
  keywords = {DataScience},
  month = nov,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Teaching Parallelism without Programming: A Data Science Curriculum for Non-{CS} Students},
  url = {https://doi.org/10.1109%2Feduhpc.2014.12},
  year = 2014
}

@inproceedings{mcandrew2005secondary,
  abstract = {For the past few years, we have run a highly successful activity teaching some elementary digital image processing to students at years 9 and 10 of secondary school. The activity involves working with a digital camera, taking, capturing and saving images, and exploring pixel values and their relationship to image brightness and colour. We also perform some elementary processing tasks: thresholding, changing spatial resolution and quantization. Students then have a brief introduction to spatial filtering, followed by some examples: image blurring and edge detection. The activity finishes with some binary morphology. Given that digital image processing is usually offered only at the upper undergraduate or postgraduate level, we have demonstrated that it is quite possible to introduce some image processing concepts in a friendly and supportive environment to students in the middle years of their secondary schooling.},
  added-at = {2017-01-15T18:46:16.000+0100},
  address = {New York, NY},
  author = {McAndrew, Alasdair and Venables, Anne},
  biburl = {https://www.bibsonomy.org/bibtex/2101d5cb815ec63a577816209446a6964/vngudivada},
  booktitle = {Proceedings of the 36th SIGCSE Technical Symposium on Computer Science Education},
  interhash = {cec6aff654595b95c789da0c9f5bcb94},
  intrahash = {101d5cb815ec63a577816209446a6964},
  keywords = {CS1 DIP},
  pages = {337--341},
  publisher = {ACM},
  series = {SIGCSE '05},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {A ``Secondary'' Look at Digital Image Processing},
  url = {http://doi.acm.org/10.1145/1047344.1047461},
  year = 2005
}

@inproceedings{begel2014analyze,
  abstract = {In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.},
  added-at = {2017-01-12T02:13:04.000+0100},
  address = {New York, NY},
  author = {Begel, Andrew and Zimmermann, Thomas},
  biburl = {https://www.bibsonomy.org/bibtex/267b6b631da04ea1b7cf9da1e6d0f87ac/vngudivada},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering},
  interhash = {ef57d40b1ea6ed0946ef0319f228a8d0},
  intrahash = {67b6b631da04ea1b7cf9da1e6d0f87ac},
  keywords = {DataScience SE},
  pages = {12--23},
  publisher = {ACM},
  series = {ICSE 2014},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Analyze This! 145 Questions for Data Scientists in Software Engineering},
  url = {http://doi.acm.org/10.1145/2568225.2568233},
  year = 2014
}

@inproceedings{chakraborty2015odyssey,
  abstract = {Children of age group 10 to 14 face difficulties to understand the concepts of the Solar System as it is vast and beyond comprehension. There is no feasible scope for direct exposure. Astronomy kits are usually expensive to afford and the current means applied in Government schools fail to address this issue. Odyssey is an interactive virtual Solar System which helps children visualize concepts of scale and distance and various events, for instance, Eclipse and Lunar Phases. Odyssey creates a realistic simulation of the movement of the planets and various other events by taking into account the Planets' positions at a given time, their mass and the gravitational forces between them. Odyssey follows an approach where students figure out concepts by trial and error, by changing different values and seeing the effects in real-time thereby learning by doing.},
  added-at = {2017-01-15T22:10:02.000+0100},
  address = {New York, NY},
  author = {Chakraborty, Abhishek},
  biburl = {https://www.bibsonomy.org/bibtex/2f5ddc0de7c5aba64f643177998471931/vngudivada},
  booktitle = {Proceedings of the 14th International Conference on Interaction Design and Children},
  interhash = {81f220bcd39189d546bf3a82c953cd35},
  intrahash = {f5ddc0de7c5aba64f643177998471931},
  keywords = {Simulation SolarSystem},
  pages = {434--437},
  publisher = {ACM},
  series = {IDC '15},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Odyssey: An Interactive Simulation to Learn Concepts of the Solar System},
  url = {http://doi.acm.org/10.1145/2771839.2771872},
  year = 2015
}

@article{fry2002conclusion,
  added-at = {2017-01-06T01:05:06.000+0100},
  address = {Westport, CT},
  author = {Fry, Ronald and Barrett, Frank},
  biburl = {https://www.bibsonomy.org/bibtex/2cb2de1359b26e7990e178858277556df/vngudivada},
  interhash = {516cb060ffcf290913a0e4da67e7185d},
  intrahash = {cb2de1359b26e7990e178858277556df},
  journal = {Appreciative inquiry and organizational transformation: reports from the field},
  keywords = {NsfRED Positivity},
  pages = {263 -- 278},
  publisher = {Quorum Books},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Conclusion: rethinking what gives life to positive change},
  year = 2002
}

@article{guzdial2014difficulty,
  abstract = {The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.<br /><br />twitter<br />Follow us on Twitter at http://twitter.com/blogCACM<br /><br />http://cacm.acm.org/blogs/blog-cacm<br /><br />Mark Guzdial considers the "poor learnability" of programming languages, while Philip Guo enumerates some practical benefits to working in a CS lab.},
  added-at = {2017-01-14T23:21:17.000+0100},
  address = {New York, NY},
  author = {Guzdial, Mark and Guo, Philip},
  biburl = {https://www.bibsonomy.org/bibtex/2b2276da6e2a6cf665c86df942b4be551/vngudivada},
  interhash = {244e5fe56aedd829c75609bf79884e8c},
  intrahash = {b2276da6e2a6cf665c86df942b4be551},
  journal = {Commun. ACM},
  keywords = {CS1 Programming},
  month = jul,
  number = 7,
  pages = {10--11},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {The Difficulty of Teaching Programming Languages, and the Benefits of Hands-on Learning},
  url = {http://doi.acm.org/10.1145/2617658},
  volume = 57,
  year = 2014
}

@book{miller2014python,
  abstract = {The user-friendly, object-oriented programming language Python is quickly becoming the most popular introductory programming language for both students and instructors. This updated Second Edition of Python Programming in Context provides a comprehensive, accessible introduction to Python fundamentals. An ideal first language for learners entering the rapidly expanding field of computer science, Python gives students a solid platform of key problem-solving skills that translate easily across programming languages. Building on essential concepts of computer science, and offering a plenitude of real-world examples, Python Programming in Context, Second Edition offers a thorough overview of multiple applied areas, including image processing, cryptography, astronomy, the Internet, and bioinformatics. The text’s emphasis on problem-solving, extrapolation, and development of independent exploration and solution-building provides students with a unique and innovative approach to learning programming. Python Programming in Context, Second Edition is the ideal introductory text for those delving into computer programming. Key Features - Utilizes Python 3 - Provides a clear, accessible, and skill-focused approach to programming with Python - Contains problem sets based on real-world examples and problem-solving rather than language features - Offers a variety of exercises that develop independent skill-building and exploration - Every new copy of the text is packaged with full student access to Turing's Craft Custom CodeLab. Customized to match the organization of the text, CodeLab offers students hands-on Python programming experience with immediate feedback. - Accompanied by a full suite of instructor support material, including solutions to the exercises in the text, downloadable source code, PowerPoint Lecture Outlines, and a complete Test Bank.},
  added-at = {2017-01-15T20:33:11.000+0100},
  address = {Burlington, Massachusetts},
  author = {Miller, Bradley N. and Ranum, David L.},
  biburl = {https://www.bibsonomy.org/bibtex/2eb3c4f11fc7c6e572dadd6808427e122/vngudivada},
  edition = {second},
  interhash = {04018ce42d6e7d6a240d4ae53a0bf30f},
  intrahash = {eb3c4f11fc7c6e572dadd6808427e122},
  keywords = {Book CS1 Python},
  publisher = {Jones \& Bartlett Learning},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Python programming in context},
  year = 2014
}

@inproceedings{carbone2016pizza,
  abstract = {Improving the employability of graduates has been a nominated priority area for successive Australian Governments. While there has been extensive research into what industry requires of graduates in the workplace, little research is available about students' perceptions of graduate employability and how they feel it should be addressed in the curriculum. This paper draws of the findings from a nationally commissioned Australian study involving multiple institutions and multiple disciplines, aimed at aligning the employability skills expectations of employers, professional bodies, academic staff, graduates and students. The focus of this paper however is around ICT students' perceptions of graduate employability and the exploration of ways in which employability skills can be developed in graduates in the discipline of ICT. As a subset of the broader commissioned study, we ran focus groups with three cohorts of information, communication and technology (ICT) students from two Australian universities. Our key research areas investigated students' perceptions of the skills ICT employers are looking for, and ways to improve ICT students' employability. In addition, insights into students' feelings of their preparedness for work between the three focus groups cohorts are provided. The research questions were analysed thematically with a qualitative open coding approach based on themes drawn from the Fullen and Scott (2014) employability framework. Overall, while students realized the importance of discipline knowledge and technical skills, other generic skills were also acknowledged. The participants also had ideas in terms of how employability skills should be enhanced.},
  added-at = {2017-01-07T20:53:56.000+0100},
  address = {New York, NY},
  author = {Carbone, Angela and Hamilton, Margaret},
  biburl = {https://www.bibsonomy.org/bibtex/218678d23f89e2f0e2eb929c213291fc6/vngudivada},
  booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
  interhash = {f91838e4fb3eecd9dfbf4289aed8bfda},
  intrahash = {18678d23f89e2f0e2eb929c213291fc6},
  keywords = {CSEducation NsfRED},
  pages = {12:1--12:9},
  publisher = {ACM},
  series = {ACSW '16},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Pizza with University ICT Students: What Do Students' Expect Employers Want from Them?},
  url = {http://doi.acm.org/10.1145/2843043.2843345},
  year = 2016
}

@inproceedings{marttilakontio2014advanced,
  abstract = {In this paper we introduce a regional education project and a positive experience of quick implementation of a new training package enabled by public funding from the European Social Fund. The project focuses on advanced data analytics (ADA) in business management. In the project, advanced data analytics is taught to both students at University of Eastern Finland and to their potential employers at the local organizations. The university teaching favors effective teaching techniques instead of conventional teaching methods, and the same topics are taught to participants from local organizations as shorter versions. The organizations also have an important role in calibrating the teaching via discussions and maturity reviews. Even-though the project is regional there has been great nationwide interest in the project. This indicates the general need for improving know-how on ADA both at universities and in companies and organizations.},
  added-at = {2017-01-12T02:29:22.000+0100},
  address = {New York, NY},
  author = {Marttila-Kontio, Maija and Kontio, Mikko and Hotti, Virpi},
  biburl = {https://www.bibsonomy.org/bibtex/2e09354f242ed749e48ddc60dc358812b/vngudivada},
  booktitle = {Proceedings of the 2014 Conference on Innovation \&\#38; Technology in Computer Science Education},
  interhash = {8dca793b1ba8c166c5bf208829a357c4},
  intrahash = {e09354f242ed749e48ddc60dc358812b},
  keywords = {DataAnalytics},
  pages = {249--254},
  publisher = {ACM},
  series = {ITiCSE '14},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Advanced Data Analytics Education for Students and Companies},
  url = {http://doi.acm.org/10.1145/2591708.2591746},
  year = 2014
}

@article{lawlor2014conceptualizing,
  abstract = {Cancer and healthy cells have distinct distributions of molecular properties and thus respond differently to drugs. Cancer drugs ideally kill cancer cells while limiting harm to healthy cells. However, the inherent variance among cells in both cancer and healthy cell populations increases the difficulty of selective drug action. Here we formalize a classification framework based on the idea that an ideal cancer drug should maximally discriminate between cancer and healthy cells. More specifically, this discrimination should be performed on the basis of measurable cell markers. We divide the problem into three parts which we explore with examples. First, molecular markers should discriminate cancer cells from healthy cells at the single-cell level. Second, the effects of drugs should be statistically predicted by these molecular markers. Third, drugs should be optimized for classification performance. We find that expression levels of a handful of genes suffice to discriminate well between individual cells in cancer and healthy tissue. We also find that gene expression predicts the efficacy of some cancer drugs, suggesting that these cancer drugs act as suboptimal classifiers using gene profiles. Finally, we formulate a framework that defines an optimal drug, and predicts drug cocktails that may target cancer more accurately than the individual drugs alone. Conceptualizing cancer drugs as solving a discrimination problem in the high-dimensional space of molecular markers promises to inform the design of new cancer drugs and drug cocktails.},
  added-at = {2017-01-17T15:26:55.000+0100},
  author = {Lawlor, Patrick Nathan and Kalisky, Tomer and Rosner, Robert and Rosner, Marsha Rich and Kording, Konrad Paul},
  biburl = {https://www.bibsonomy.org/bibtex/29779f65f8ee2bdd8a454c39ba2a6d82c/vngudivada},
  interhash = {e486f69d89bb1e11002d23c189727acb},
  intrahash = {9779f65f8ee2bdd8a454c39ba2a6d82c},
  journal = {PLOS ONE},
  keywords = {BreastCancer CancerDrugs Classification GeneExpressionData},
  month = sep,
  number = {9:e106444},
  pages = {1-10},
  publisher = {Public Library of Science},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Conceptualizing Cancer Drugs as Classifiers},
  url = {http://dx.doi.org/10.1371%2Fjournal.pone.0106444},
  volume = 9,
  year = 2014
}

@book{lewis2010medicine,
  abstract = {R is quickly becoming the number one choice for users in the fields of biology, medicine, and bioinformatics as their main means of storing, processing, sharing, and analyzing biomedical data. R for Medicine and Biology is a step-by-step guide through the use of the statistical environment R, as used in a biomedical domain. Ideal for healthcare professionals, scientists, informaticists, and statistical experts, this resource will provide even the novice programmer with the tools necessary to process and analyze their data using the R environment. Introductory chapters guide readers in how to obtain, install, and become familiar with R and provide a clear introduction to the programming language using numerous worked examples. Later chapters outline how R can be used, not just for biomedical data analysis, but also as an environment for the processing, storing, reporting, and sharing of data and results. The remainder of the book explores areas of R application to common domains of biomedical informatics, including imaging, statistical analysis, data mining/modeling, pathology informatics, epidemiology, clinical trials, and metadata usage. R for Medicine and Biology will provide you with a single desk reference for the R environment and its many capabilities.},
  added-at = {2017-01-15T21:45:50.000+0100},
  address = {Sudbury, Massachusetts},
  author = {Lewis, Paul D.},
  biburl = {https://www.bibsonomy.org/bibtex/2bac5b73d6d5c37c769b300f7a2a7f584/vngudivada},
  interhash = {7f0b1acd8afd41acb7a11c0205da3d2b},
  intrahash = {bac5b73d6d5c37c769b300f7a2a7f584},
  keywords = {Bioinformatics Medicine R},
  publisher = {Jones and Bartlett Publishers},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {R for medicine and biology},
  year = 2010
}

@book{kuhn2013applied,
  abstract = {This text is intended for a broad audience as both an introduction to predictive models as well as a guide to applying them. Non-mathematical readers will appreciate the intuitive explanations of the techniques while an emphasis on problem-solving with real data across a wide variety of applications will aid practitioners who wish to extend their expertise. Readers should have knowledge of basic statistical ideas, such as correlation and linear regression analysis. While the text is biased against complex equations, a mathematical background is needed for advanced topics. Dr. Kuhn is a Director of Non-Clinical Statistics at Pfizer Global R & D in Groton Connecticut. He has been applying predictive models in the pharmaceutical and diagnostic industries for over 15 years and is the author of a number of R packages. Dr. Johnson has more than a decade of statistical consulting and predictive modeling experience in pharmaceutical research and development. He is a co-founder of Arbor Analytics, a firm specializing in predictive modeling and is a former Director of Statistics at Pfizer Global R & D. His scholarly work centers on the application and development of statistical methodology and learning algorithms. Applied Predictive Modeling covers the overall predictive modeling process, beginning with the crucial steps of data preprocessing, data splitting and foundations of model tuning. The text then provides intuitive explanations of numerous common and modern regression and classification techniques, always with an emphasis on illustrating and solving real data problems. Addressing practical concerns extends beyond model fitting to topics such as handling class imbalance, selecting predictors, and pinpointing causes of poor model performance-all of which are problems that occur frequently in practice. The text illustrates all parts of the modeling process through many hands-on, real-life examples. And every chapter contains extensive R code for each step of the process. The data sets and corresponding code are available in the book's companion AppliedPredictiveModeling R package, which is freely available on the CRAN archive. This multi-purpose text can be used as an introduction to predictive models and the overall modeling process, a practitioner's reference handbook, or as a text for advanced undergraduate or graduate level predictive modeling courses. To that end, each chapter contains problem sets to help solidify the covered concepts and uses data available in the book's R package. Readers and students interested in implementing the methods should have some basic knowledge of R. And a handful of the more advanced topics require some mathematical knowledge.},
  added-at = {2017-01-21T18:30:50.000+0100},
  author = {Kuhn, Max and Johnson, Kjell},
  biburl = {https://www.bibsonomy.org/bibtex/2c1847b13c51297368ea3b841fd80fb22/vngudivada},
  interhash = {28fb1de6e768ed47c29f0a5d6d6dbe5d},
  intrahash = {c1847b13c51297368ea3b841fd80fb22},
  keywords = {Book Leadership ParExcellence PredictiveAnalytics},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Applied Predictive Modeling},
  url = {http://dx.doi.org/10.1007/978-1-4614-6849-3},
  year = 2013
}

@book{dawson2015language,
  abstract = {Language Files: Materials for an Introduction to Language and Linguistics has become one of the most widely adopted, consulted, and authoritative introductory textbooks to linguistics ever written. The scope of the text makes it suitable for use in a wide range of courses, while its unique organization into student-friendly, self-contained sections allows for tremendous flexibility in course design.
The twelfth edition has been significantly revised, clarified, and updated throughout—with particular attention to the chapters on phonetics, phonology, pragmatics, and especially psycholinguistics. The restructured chapter on psycholinguistics makes use of recent research on language in the brain and includes expanded coverage of language processing disorders, introducing students to current models of speech perception and production and cutting-edge research techniques. In addition, exercises have been updated, and icons have been added to the text margins throughout the book, pointing instructors and students to useful and engaging audio files, videos, and other online resources on the accompanying Language Files website, which has also been significantly expanded.},
  added-at = {2017-01-15T21:10:51.000+0100},
  address = {Columbus, Ohio},
  biburl = {https://www.bibsonomy.org/bibtex/22fd85f6bc33505462f8a25e6a777262b/vngudivada},
  edition = {twelfth},
  editor = {Dawson, Hope C. and Phelan, Michael},
  interhash = {56d4eb2dbfe2acdd53ebc9305b992f27},
  intrahash = {2fd85f6bc33505462f8a25e6a777262b},
  keywords = {Book Linguistics},
  publisher = {The Ohio State University Press},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Language files: materials for an introduction to language and linguistics},
  year = 2015
}

@inproceedings{durrantwhyte2015knowledge,
  abstract = {Increasingly it is data, vast amounts of data, that drives scientific discovery. At the heart of this so-called "fourth paradigm of science" is the rapid development of large scale statistical data fusion and machine learning methods. While these developments in "big data" methods are largely driven by commercial applications such as internet search or customer modelling, the opportunity for applying these to scientific discovery is huge. This talk will describe a number of applied machine learning projects addressing real-world inference problems in physical, life and social science areas. In particular, I will describe a major Science and Industry Endowment Fund (SIEF) project, in collaboration with the NICTA and Macquarie University, looking to apply machine learning techniques to discovery in the natural sciences. This talk will look at the key methods in machine learning that are being applied to the discovery process, especially in areas like geology, ecology and biological discovery.},
  added-at = {2017-01-12T01:47:44.000+0100},
  address = {New York, NY},
  author = {Durrant-Whyte, Hugh},
  biburl = {https://www.bibsonomy.org/bibtex/2c72a38641a47398e923486d8e04426ac/vngudivada},
  booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  interhash = {5ab7a769300436998f6882ee0dc3c3e3},
  intrahash = {c72a38641a47398e923486d8e04426ac},
  keywords = {ML NaturalScience},
  pages = {7--7},
  publisher = {ACM},
  series = {KDD '15},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Data, Knowledge and Discovery: Machine Learning Meets Natural Science},
  url = {http://doi.acm.org/10.1145/2783258.2785467},
  year = 2015
}

@inproceedings{weitschek2015software,
  abstract = {Leveraging advances in transcriptome profiling technologies (RNA-seq), biomedical scientists are collecting ever-increasing gene expression profiles data with low cost and high throughput. Therefore, automatic knowledge extraction methods are becoming essential to manage them. In this work, we present GELA (Gene Expression Logic Analyzer), a novel pipeline able to perform a knowledge discovery process in gene expression profiles data of RNA-seq. Firstly, we introduce the RNA-seq technologies, then, we illustrate our gene expression profiles data analysis method (including normalization, clustering, and classification), and finally, we test our knowledge extraction algorithm on the public RNA-seq data sets of Breast Cancer and Stomach Cancer, and on the public microarray data sets of Psoriasis and Multiple Sclerosis, obtaining in both cases promising results.},
  added-at = {2017-01-17T15:12:03.000+0100},
  author = {Weitschek, E. and Fiscon, G. and Felici, G. and Bertolazzi, P.},
  biburl = {https://www.bibsonomy.org/bibtex/2fbe49de84bacbfd117917df6ca64dc15/vngudivada},
  booktitle = {26th International Workshop on Database and Expert Systems Applications (DEXA)},
  interhash = {07bec36e8cf1c9524b6d04524f99131a},
  intrahash = {fbe49de84bacbfd117917df6ca64dc15},
  keywords = {GELA GeneExpressionData},
  month = {Sept},
  pages = {31-35},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {GELA: A Software Tool for the Analysis of Gene Expression Data},
  url = {http://ieeexplore.ieee.org/document/7406265/?arnumber=7406265},
  year = 2015
}

@article{vardi2010science,
  added-at = {2017-01-12T01:30:55.000+0100},
  address = {New York, NY},
  author = {Vardi, Moshe Y.},
  biburl = {https://www.bibsonomy.org/bibtex/24898741dc1ba3e41928d1c93ac923d7a/vngudivada},
  interhash = {bba83797772aee36b0773b0664359d8b},
  intrahash = {4898741dc1ba3e41928d1c93ac923d7a},
  journal = {Commun. ACM},
  keywords = {FourthParadigm FourthPillar},
  month = sep,
  number = 9,
  pages = {5--5},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Science Has Only Two Legs},
  url = {http://doi.acm.org/10.1145/1810891.1810892},
  volume = 53,
  year = 2010
}

@book{hofmann2013rapidminer,
  abstract = {Our primary motivation in writing this book is to share our working experience to bridge the gap between the knowledge of industry gurus and newcomers to the spoken language processing community. Many powerful techniques hide in conference proceedings and academic papers for years before becoming widely recognized by the research community or the industry. We spent many years pursuing spoken language technology research at Carnegie Mellon University before we started spoken language R&D at Microsoft. We fully understand that it is by no means a small undertaking to transfer a state-of-the-art spoken language research system into a commercially viable product that can truly help people improve their productivity. Our experience in both industry and academia is reflected in the context of this book, which presents a contemporary and comprehensive description of both theoretic and practical issues in spoken language processing. This book is intended for people of diverse academic and practical backgrounds. Speech scientists, computer scientists, linguists, engineers, physicists, and psychologists all have a unique perspective on spoken language processing. This book will be useful to all of these special interest groups.

Spoken language processing is a diverse subject that relies on knowledge of many levels, including acoustics, phonology, phonetics, linguistics, semantics, pragmatics, and discourse. The diverse nature of spoken language processing requires knowledge in computer science, electrical engineering, mathematics, syntax, and psychology. There are a number of excellent books on the subfields of spoken language processing, including speech recognition, text-to-speech conversion, and spoken language understanding, but there is no single book that covers both theoretical and practical aspects of these subfields and spoken language interface design. We devote many chapters systematically introducing fundamental theories needed to understand how speech recognition, text-to-speech synthesis, and spoken language understanding work. Even more important is the fact that the book highlights what works well in practice, which is invaluable if you want to build a practical speech recognizer, a practical text-to-speech synthesizer, or a practical spoken language system. Using numerous real examples in developing Microsoft's spoken language systems, we concentrate on showing how the fundamental theories can be applied to solve real problems in spoken language processing.},
  added-at = {2017-01-15T22:27:40.000+0100},
  address = {Boca Raton, Florida},
  author = {Hofmann, Markus and Klinkenberg, Ralf},
  biburl = {https://www.bibsonomy.org/bibtex/203de7d9b370b9da4f87fd5547f058183/vngudivada},
  interhash = {ddaf61ceebda3cf9d30385763999eacb},
  intrahash = {03de7d9b370b9da4f87fd5547f058183},
  keywords = {DataMining ML RapidMiner},
  publisher = {Chapman and Hall/CRC},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {RapidMiner: data mining use cases and business analytics applications},
  year = 2013
}

@phdthesis{guo2012software,
  abstract = {Research programming is a type of programming activity where the goal is to write computer programs to obtain insights from data. Millions of professionals in fields ranging from science, engineering, business, finance, public policy, and journalism, as well as numerous students and computer hobbyists, all perform research programming on a daily basis.

My thesis is that by understanding the unique challenges faced during research programming, it becomes possible to apply techniques from dynamic program analysis, mixed-initiative recommendation systems, and OS-level tracing to make research programmers more productive.

This dissertation characterizes the research programming process, describes typical challenges faced by research programmers, and presents five software tools that I have developed to address some key challenges. 1.) Proactive Wrangler is an interactive graphical tool that helps research programmers reformat and clean data prior to analysis. 2.) IncPy is a Python interpreter that speeds up the data analysis scripting cycle and helps programmers manage code and data dependencies. 3.) SlopPy is a Python interpreter that automatically makes existing scripts error-tolerant, thereby also speeding up the data analysis scripting cycle. 4.) Burrito is a Linux-based system that helps programmers organize, annotate, and recall past insights about their experiments. 5.) CDE is a software packaging tool that makes it easy to deploy, archive, and share research code. Taken together, these five tools enable research programmers to iterate and potentially discover insights faster by offloading the burdens of data management and provenance to the computer.},
  added-at = {2017-01-14T23:44:57.000+0100},
  author = {Guo, Philip Jia},
  biburl = {https://www.bibsonomy.org/bibtex/2f02f62feb9f3ddf2f0170e8b67f508d8/vngudivada},
  interhash = {5291e2cae9f76447997d16e2511b3237},
  intrahash = {f02f62feb9f3ddf2f0170e8b67f508d8},
  keywords = {PhD},
  month = may,
  school = {Stanford University},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Software Tools to Facilitate Research Programming},
  url = {http://pgbovine.net/publications/Philip-Guo_PhD-dissertation_software-tools-for-research-programming.pdf},
  year = 2012
}

@article{freeman2014active,
  abstract = {To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6\% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small $(n \le 50)$ classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.},
  added-at = {2017-01-06T14:39:17.000+0100},
  author = {Freeman, Scott and Eddy, Sarah L. and McDonough, Miles and Smith, Michelle K. and Okoroafor, Nnadozie and Jordt, Hannah and Wenderoth, Mary Pat},
  biburl = {https://www.bibsonomy.org/bibtex/23e83510bd441d4ed8c8de66b02375d39/vngudivada},
  interhash = {0926a06d52e2b33cabe36fe0f2d7b73a},
  intrahash = {3e83510bd441d4ed8c8de66b02375d39},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {ActiveLearning NsfRED STEM},
  number = 23,
  pages = {8410-8415},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Active learning increases student performance in science, engineering, and mathematics},
  url = {http://www.pnas.org/content/111/23/8410.abstract},
  volume = 111,
  year = 2014
}

@inproceedings{hutchings2016vismap,
  abstract = {We present VisMap, a Web-based software tool that supports student exploration of possible data visualizations during a typical process of data science practice. Specifically, we detail visualization approaches within three major kinds of data analysis (part-to-whole and rank, correlation, and geospatial) and discuss how VisMap allows students to visually explore visualization options that correspond to the kind of analysis. The discussion is rooted in our experiences developing an introductory undergraduate course in data science and in our classroom observations of the limitations of existing software tools. These limitations include inefficiency of visualization comparison, difficulty in manipulating data to achieve visualization goals, and current challenges novices face when using professional tools simultaneous with learning about visualization. We finally offer a variety of future paths to further evaluate and refine VisMap.},
  added-at = {2017-01-12T02:09:38.000+0100},
  address = {New York, NY},
  author = {Hutchings, Dugald Ralph and Squire, Megan},
  biburl = {https://www.bibsonomy.org/bibtex/230aa99349e9341f2ac9c9771e9c36b1f/vngudivada},
  booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
  interhash = {06f4fed087a47f31f5b12f897a270dc4},
  intrahash = {30aa99349e9341f2ac9c9771e9c36b1f},
  keywords = {DataScience Visualization},
  pages = {163--168},
  publisher = {ACM},
  series = {SIGCSE '16},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {VisMap: Exploratory Visualization Support for Introductory Data Science and Visualization},
  url = {http://doi.acm.org/10.1145/2839509.2844572},
  year = 2016
}

@incollection{cooperrider1987appreciative,
  abstract = {This chapter presents a conceptual refiguration of action-research based on a "sociorationalist" view of science. The position that is developed can be summarized as follows: For action-research to reach its potential as a vehicle for social innovation it needs to begin advancing theoretical knowledge of consequence; that good theory may be one of the best means human beings have for affecting change in a postindustrial world; that the discipline's steadfast commitment to a problem-solving view of the world acts as a primary constraint on its imagination and contribution to knowledge; that appreciative inquiry represents a viable complement to conventional forms of action-research; and finally, that through our assumptions and choice of method we largely create the world we later discover.},
  added-at = {2017-01-06T04:44:52.000+0100},
  address = {Greenwich, CT},
  author = {Cooperrider, D. L. and Srivastva, S.},
  biburl = {https://www.bibsonomy.org/bibtex/2c3f239100409bda11bac87c5176b0ef7/vngudivada},
  booktitle = { Research in organization change and development},
  editor = {Pasmore, W. and Woodman, R},
  interhash = {46595c83a6bc09ddcea3b0a72b0a3d0e},
  intrahash = {c3f239100409bda11bac87c5176b0ef7},
  keywords = {AppreciativeInquiry Leadership NsfRED},
  pages = {129 -- 169},
  publisher = {JAI Press},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Appreciative inquiry in organizational life},
  volume = 1,
  year = 1987
}

@book{kotu2014predictive,
  abstract = {Put Predictive Analytics into Action Learn the basics of Predictive Analysis and Data Mining through an easy to understand conceptual framework and immediately practice the concepts learned using the open source RapidMiner tool. Whether you are brand new to Data Mining or working on your tenth project, this book will show you how to analyze data, uncover hidden patterns and relationships to aid important decisions and predictions. Data Mining has become an essential tool for any enterprise that collects, stores and processes data as part of its operations. This book is ideal for business users, data analysts, business analysts, business intelligence and data warehousing professionals and for anyone who wants to learn Data Mining. You’ll be able to: 1. Gain the necessary knowledge of different data mining techniques, so that you can select the right technique for a given data problem and create a general purpose analytics process. 2. Get up and running fast with more than two dozen commonly used powerful algorithms for predictive analytics using practical use cases. 3. Implement a simple step-by-step process for predicting an outcome or discovering hidden relationships from the data using RapidMiner, an open source GUI based data mining tool

Predictive analytics and Data Mining techniques covered: Exploratory Data Analysis, Visualization, Decision trees, Rule induction, k-Nearest Neighbors, Naïve Bayesian, Artificial Neural Networks, Support Vector machines, Ensemble models, Bagging, Boosting, Random Forests, Linear regression, Logistic regression, Association analysis using Apriori and FP Growth, K-Means clustering, Density based clustering, Self Organizing Maps, Text Mining, Time series forecasting, Anomaly detection and Feature selection. Implementation files can be downloaded from the book companion site at www.LearnPredictiveAnalytics.com

Demystifies data mining concepts with easy to understand language
Shows how to get up and running fast with 20 commonly used powerful techniques for predictive analysis
Explains the process of using open source RapidMiner tools
Discusses a simple 5 step process for implementing algorithms that can be used for performing predictive analytics
Includes practical use cases and examples},
  added-at = {2017-01-15T21:51:57.000+0100},
  address = {Waltham, Massachusetts},
  author = {Kotu, Vijay and Deshpande, Balachandre},
  biburl = {https://www.bibsonomy.org/bibtex/298c22de6e217f69c0b3bd9cb3aa77008/vngudivada},
  interhash = {2cb8277d76ecf8a3b986e345713e0f39},
  intrahash = {98c22de6e217f69c0b3bd9cb3aa77008},
  keywords = {Book DataMining PredictiveAnalytics RapidMiner},
  publisher = {Morgan Kaufmann},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Predictive analytics and data mining: concepts and practice with RapidMiner},
  year = 2014
}

@inproceedings{hasan2015science,
  abstract = {Visa is the payments technology that forms the backbone of the world's financial systems by handling more than 7 trillion dollars of payments annually and our data reflects how the world spends money. We will describe technical achievements we have made in the area of fraud and cover some open challenges in data science.},
  added-at = {2017-01-12T01:51:11.000+0100},
  address = {New York, NY},
  author = {Hasan, Waqar and Wang, Min},
  biburl = {https://www.bibsonomy.org/bibtex/2789e74345ee1e922c76df13467cca708/vngudivada},
  booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  interhash = {e8ed77918eeb9d11f032195e1b5facec},
  intrahash = {789e74345ee1e922c76df13467cca708},
  keywords = {DataScience Visa},
  pages = {1627--1627},
  publisher = {ACM},
  series = {KDD '15},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Data Science at Visa},
  url = {http://doi.acm.org/10.1145/2783258.2790460},
  year = 2015
}

@book{mcandrew2011introduction,
  abstract = {Once the privilege of a secret few, cryptography is now taught at universities around the world. Introduction to Cryptography with Open-Source Software illustrates algorithms and cryptosystems using examples and the open-source computer algebra system of Sage. The author, a noted educator in the field, provides a highly practical learning experience by progressing at a gentle pace, keeping mathematics at a manageable level, and including numerous end-of-chapter exercises.

Focusing on the cryptosystems themselves rather than the means of breaking them, the book first explores when and how the methods of modern cryptography can be used and misused. It then presents number theory and the algorithms and methods that make up the basis of cryptography today. After a brief review of "classical" cryptography, the book introduces information theory and examines the public-key cryptosystems of RSA and Rabin’s cryptosystem. Other public-key systems studied include the El Gamal cryptosystem, systems based on knapsack problems, and algorithms for creating digital signature schemes.

The second half of the text moves on to consider bit-oriented secret-key, or symmetric, systems suitable for encrypting large amounts of data. The author describes block ciphers (including the Data Encryption Standard), cryptographic hash functions, finite fields, the Advanced Encryption Standard, cryptosystems based on elliptical curves, random number generation, and stream ciphers. The book concludes with a look at examples and applications of modern cryptographic systems, such as multi-party computation, zero-knowledge proofs, oblivious transfer, and voting protocols.},
  added-at = {2017-01-15T20:22:20.000+0100},
  address = {Boca Raton, FL},
  author = {McAndrew, Alasdair},
  biburl = {https://www.bibsonomy.org/bibtex/25f6e570317336c99eb28a84cee1e4bed/vngudivada},
  interhash = {83933d8cdb7b0b8cd2780bd1f2349fb2},
  intrahash = {5f6e570317336c99eb28a84cee1e4bed},
  keywords = {Book Cryptography OpenSourceSoftware},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Introduction to cryptography with open-source software},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9781439825709},
  year = 2011
}

@inproceedings{szalay2012dataintensive,
  abstract = {Scientific computing is increasingly revolving around massive amounts of data. From physical sciences to numerical simulations to high throughput genomics and homeland security, we are soon dealing with Petabytes if not Exabytes of data. This new, data-centric computing requires a new look at computing architectures and strategies. We will revisit Amdahl's Law establishing the relation between CPU and I/O in a balanced computer system, and use this to analyze current computing architectures and workloads. We will discuss how existing hardware can be used to build systems that are much closer to an ideal Amdahl machine. We have deployed various scientific applications, mostly drawn from astronomy, over different architectures and compare performance and scaling laws. We discuss a hypothetical cheap, yet high performance multi-petabyte system currently under consideration at JHU. We will also explore strategies of interacting with very large amounts of data, and compare various large scale data analysis platforms.},
  added-at = {2017-01-12T01:24:22.000+0100},
  address = {New York, NY},
  author = {Szalay, Alex},
  biburl = {https://www.bibsonomy.org/bibtex/29e03062f711b113d40cf658dd7032445/vngudivada},
  booktitle = {Proceedings of the Fifth International Workshop on Data-Intensive Distributed Computing Date},
  interhash = {9fbb08eb7279cf7557b362e0c35a4b9a},
  intrahash = {9e03062f711b113d40cf658dd7032445},
  keywords = {DataScience FourthParadigm FourthPillar},
  pages = {1--2},
  publisher = {ACM},
  series = {DIDC '12},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Data-intensive Discoveries in Science: The Fourth Paradigm},
  url = {http://doi.acm.org/10.1145/2286996.2286998},
  year = 2012
}

@inproceedings{mcandrew2003teaching,
  abstract = {Image processing is often presented as a two dimensional version of signal processing, and as such, assumes for its background several years of undergraduate engineering-style mathematics. But such heavy mathematics does not form the background of many students of computer science. In Australia, some tertiary courses are phasing formal mathematics out of their computer science courses completely. This means that the effective teaching of image processing to students of computer science must use as little mathematics as possible. In this paper we discuss the undergraduate subjects taught at Victoria University of Technology (VUT), and our methods of keeping the mathematics in them to a minimum.},
  added-at = {2017-01-15T18:49:46.000+0100},
  address = {Darlinghurst, Australia},
  author = {McAndrew, Alasdair},
  biburl = {https://www.bibsonomy.org/bibtex/2c437b96077ed802b820d23623f7212a4/vngudivada},
  booktitle = {Proceedings of the Fifth Australasian Conference on Computing Education - Volume 20},
  interhash = {447ae47c005fbc6aa582d49caffa226e},
  intrahash = {c437b96077ed802b820d23623f7212a4},
  keywords = {CS1 DIP},
  pages = {15--23},
  publisher = {Australian Computer Society, Inc.},
  series = {ACE '03},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Teaching Image Processing Using Minimal Mathematics},
  url = {http://dl.acm.org/citation.cfm?id=858403.858406},
  year = 2003
}

@book{barrett2005appreciative,
  abstract = {Within these pages is a simple yet revolutionary philosophy of organizational learning and change: discover everything that "gives life" to the cooperative capacity of a human system—and then let go.

What gets co-constructed thereafter will be anything but linear. Like jazz improvisation or a creative jam session, it will literally be impossible to predict the emerging new opportunities, options, and shared possibilities. But one thing is almost certain: whatever gets created will be good. It will be valued and valuable to the human system for one overarching reason: the quality of the "relational space" from which the new constructions of the future are nurtured makes a difference that makes the difference. Relationships, propose the authors of this wonderful introduction to Appreciative Inquiry, come alive where there is an appreciative question, when there is a deliberate search for the good and the best in one another; and in human systems the process of studying a phenomenon actually changes that phenomenon, in effect creating a new reality during the process of inquiry. That’s what this book is all about. It is all about power of "AI" as a way of creating a relational space for the cooperative construction of reality.

Based on almost twenty years of field research in organizations like the United States Navy, Roadway Express, and Nokia as well as years of scholarly commitment to social constructionist approaches to human science inquiry, the authors of this book finally put into print a pragmatic, inspiring, and simple account of an approach to strength-based change that has elevated the capacities of thousands of businesses, change leaders, and teams to realize their enormous potentials. It’s the kind of focus book that people will want to hand out at the front end of a major organization-wide change initiative. And it is a perfect companion piece for workshops, foundation courses on AI, and leadership programs.},
  added-at = {2017-01-05T03:48:11.000+0100},
  address = {Chagrin Falls, Ohio},
  author = {Barrett, Frank J. and Fry, Ronald E.},
  biburl = {https://www.bibsonomy.org/bibtex/264e7b0a5f6c921343df6450e4e0866a9/vngudivada},
  interhash = {882a1abef2b099ce3d867f17b6b9ffe0},
  intrahash = {64e7b0a5f6c921343df6450e4e0866a9},
  keywords = {AppreciativeInquiry Book Leadership NsfRED},
  publisher = {Taos Institute Publications},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Appreciative inquiry: a positive approach to building cooperative capacity},
  year = 2005
}

@inproceedings{wolpaw2012braincomputer,
  abstract = {Brain-computer interfaces (BCIs) have a promising future, with researchers in laboratories all over the world using many different brain signals, recording methods, and signal processing approaches to realize increasingly capable systems. These BCI systems can control a variety of external devices, from cursors and avatars on computer screens, to televisions and wheelchairs, to robotic arms and neuroprostheses. People with and without disabilities have tested these systems, and a few people who are severely disabled are already using them for important purposes in their daily lives. Furthermore, several groups are beginning to explore BCI-based methods for improving rehabilitation for people with strokes and other neuromuscular disorders. Thus, BCIs are poised to become a major new technology for people with disabilities, and possibly for the general population as well. At the same time, the realization of this bright future depends on advances in four critical areas. First, both non-invasive and invasive BCIs need better signal-acquisition hardware. Second, the real-life usefulness of BCI systems for people with disabilities requires convincing clinical validation. Third, effective strategies for BCI dissemination and ongoing support must be developed. Fourth, and perhaps most important, if non-invasive or invasive BCIs are to be widely used for anything more than the most basic communication functions, their reliability must be greatly improved. Better reliability may be achieved with BCI design strategies that are based on the principles underlying the excellent reliability of natural neuromuscular actions. These strategies include: effective engagement of brain adaptive capacities; task-appropriate distribution of control between the brain and the BCI; and BCI use of signals from multiple brain areas. Effective attention to these critical areas by scientists and engineers throughout the world can realize the exciting future envisioned for BCI technology.},
  added-at = {2017-01-21T23:41:42.000+0100},
  address = {New York, NY},
  author = {Wolpaw, Jonathan R.},
  biburl = {https://www.bibsonomy.org/bibtex/23adfd44ae42a211f5a5e8a85315191b7/vngudivada},
  booktitle = {Proceedings of the 2Nd ACM SIGHIT International Health Informatics Symposium},
  interhash = {b20ac58ff836280132e758e68b38ef88},
  intrahash = {3adfd44ae42a211f5a5e8a85315191b7},
  keywords = {BCI},
  pages = {3--4},
  publisher = {ACM},
  series = {IHI '12},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Brain-computer Interfaces: Progress, Problems, and Possibilities},
  url = {http://doi.acm.org/10.1145/2110363.2110366},
  year = 2012
}

@book{han2011mining,
  abstract = {The book provides the concepts and techniques in processing gathered data or information, which will be used in various applications. Specifically, it explains data mining and the tools used in discovering knowledge from the collected data. This book is referred as the knowledge discovery from data (KDD). It focuses on the feasibility, usefulness, effectiveness, and scalability of techniques of large data sets. After describing data mining, this edition explains the methods of knowing, preprocessing, processing, and warehousing data. It then presents information about data warehouses, online analytical processing (OLAP), and data cube technology. Then, the methods involved in mining frequent patterns, associations, and correlations for large data sets are described. The book details the methods for data classification and introduces the concepts and methods for data clustering. The remaining chapters discuss the outlier detection and the trends, applications, and research frontiers in data mining.},
  added-at = {2017-01-16T21:36:59.000+0100},
  address = {Amsterdam, Netherlands},
  author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  biburl = {https://www.bibsonomy.org/bibtex/2edc1a7cbc3a6c3e5544a5a53af44c1ae/vngudivada},
  edition = {Third},
  interhash = {8a1fe4e4b855565639bab355e2c57ff1},
  intrahash = {edc1a7cbc3a6c3e5544a5a53af44c1ae},
  keywords = {Book DataMining},
  publisher = {Morgan Kaufmann},
  series = {Morgan Kaufmann Series in Data Management Systems},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Data Mining: Concepts and Techniques},
  year = 2011
}

@inproceedings{conway2014science,
  abstract = {In this talk, Drew will examine data science through the lens of the social scientist. He will discuss how the various skills and disciplines combine into data science. Drew will also present a motivating example directly from his work as a senior advisor to NYC's Mayor's Office of Analytics.},
  added-at = {2017-01-12T02:20:26.000+0100},
  address = {New York, NY},
  author = {Conway, Drew},
  biburl = {https://www.bibsonomy.org/bibtex/232e5f5f118c0a6e7a9c3f44e334b8668/vngudivada},
  booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  interhash = {ed15bd7920e4c678ed10699ce4946057},
  intrahash = {32e5f5f118c0a6e7a9c3f44e334b8668},
  keywords = {DataScience SocialScience},
  pages = {1520--1520},
  publisher = {ACM},
  series = {KDD '14},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Data Science Through the Lens of Social Science},
  url = {http://doi.acm.org/10.1145/2623330.2630824},
  year = 2014
}

@inproceedings{menzies2016antipatterns,
  abstract = {Many books and papers describe how to do data science. While those texts are useful, it can also be important to reflect on anti-patterns; i.e. common classes of errors seen when large communities of researchers and commercial software engineers use, and misuse data mining tools. This technical briefing will present those errors and show how to avoid them.},
  added-at = {2017-01-12T02:03:20.000+0100},
  address = {New York, NY},
  author = {Menzies, Tim},
  biburl = {https://www.bibsonomy.org/bibtex/2104d8279b421c2c4faafb0007436785f/vngudivada},
  booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
  interhash = {c97e9b19d2ad24edfc4e51e0c9d5de4f},
  intrahash = {104d8279b421c2c4faafb0007436785f},
  keywords = {AntiPattern DataScience},
  pages = {887--887},
  publisher = {ACM},
  series = {ICSE '16},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {How Not to Do It: Anti-patterns for Data Science in Software Engineering},
  url = {http://doi.acm.org/10.1145/2889160.2891047},
  year = 2016
}

@inproceedings{fu2015datadriven,
  abstract = {Data Science is an increasingly popular area of Knowledge Discovery and Data Mining. Leading consumer Web companies such as Amazon, Facebook, eBay, Google and LinkedIn, as well as B2B companies like Salesforce, possess Petabytes of data. Through effective mining of this data, they create products and services that benefit millions of users and generate tremendous amount of business value. It is widely acknowledged that Data Scientists play key roles in the creation of these products, from pattern identification, idea generation and product prototyping to experiment design and launch decisions. Nonetheless, they also face common challenges, such as the gap between creating a prototype and turning it into a scalable product, or the frustration of generating innovative product ideas that do not get adopted. Organizers of this tutorial have many years of experience leading Data Science teams in some of the most successful consumer Web companies. In this tutorial, we introduce the framework that we created to nurture data-driven product innovations. The core of this framework is the focus on scale and impact - we take the audience through a discussion on how to balance between velocity and scale, between product innovation and product operation, and between theoretical research and practical impact. We also share some guidelines for successful data-driven product innovation with real examples from our experiences.  We end the tutorial by discussing the organizational perspective of data-driven product innovation: how to structure Data Science teams so Data Scientists collaborate effectively with other functions, and how to hire and grow talents into Data Scientist roles.},
  added-at = {2017-01-12T02:07:33.000+0100},
  address = {New York, NY},
  author = {Fu, Xin and Asorey, Hern\'{a}n},
  biburl = {https://www.bibsonomy.org/bibtex/23752ef38c3e98a7478073530137ed2e0/vngudivada},
  booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  interhash = {b56c3079538064e78dc463593a9ddcc9},
  intrahash = {3752ef38c3e98a7478073530137ed2e0},
  keywords = {DataScience ProductInnovation},
  pages = {2311--2312},
  publisher = {ACM},
  series = {KDD '15},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Data-Driven Product Innovation},
  url = {http://doi.acm.org/10.1145/2783258.2789994},
  year = 2015
}

@mastersthesis{correia2010automatic,
  added-at = {2017-01-30T04:06:49.000+0100},
  author = {Correia, Rui Pedro dos Santos},
  biburl = {https://www.bibsonomy.org/bibtex/207bdfc7e1e1c8e4dc5f3906b2b686ebd/vngudivada},
  interhash = {a4f5266a7a1e81835cb13abeffe685f2},
  intrahash = {07bdfc7e1e1c8e4dc5f3906b2b686ebd},
  keywords = {QuestionGeneration},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Automatic Question Generation for {REAP.PT} Tutoring System},
  year = 2010
}

@book{yau2011visualize,
  abstract = {Presents information on ways to present data with visual representations, covering such topics as data sources, visualization tools, proportions, and spatial relationships.},
  added-at = {2017-01-21T18:21:01.000+0100},
  address = {Indianapolis, Indiana},
  author = {Yau, Nathan},
  biburl = {https://www.bibsonomy.org/bibtex/2dbed8a4ee5342f2abfa2fd8e8fadb930/vngudivada},
  interhash = {ffccf3a49181ae39532c604a9ecb115b},
  intrahash = {dbed8a4ee5342f2abfa2fd8e8fadb930},
  keywords = {Book Leadership Visualization},
  publisher = {Wiley Pub.},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Visualize this: the FlowingData guide to design, visualization, and statistics},
  year = 2011
}

@inproceedings{cassel2016brainstorming,
  abstract = {Data Science programs are emerging in many areas and are related to many disciplines. This includes sciences, social sciences, business, journalism, history, and any other area dealing with massive amounts of data. Some institutions are establishing majors or minors in data science while others need to incorporate some level of introduction to relevant topics within the context of other coursework. The BOF will engage SIGCSE participants who have views on the content and role of courses and programs in data science. Two existing NSF sponsored projects will seed the discussion. One of the projects investigates the use of flipped classroom instruction for course material in data science, with significant attention to what can be done with a minimum of prerequisites. The other project produced a workshop to elicit opinions from a number of perspectives on what are the core elements of data science and how the subject is seen by different disciplines. Results from the workshop will be available at SIGCSE. With these as a starting point, participants in the Birds of a Feather session will explore the emerging field of data science and its relationship to computer science education.},
  added-at = {2017-01-12T01:54:14.000+0100},
  address = {New York, NY},
  author = {Cassel, Lillian N. and Goelman, Don and Dicheva, Darina and Topi, Heikki},
  biburl = {https://www.bibsonomy.org/bibtex/282eb13ae2926b3a72c6fcec27b40c916/vngudivada},
  booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
  interhash = {f512544316b88e35a7db1a000fc3a24a},
  intrahash = {82eb13ae2926b3a72c6fcec27b40c916},
  keywords = {DataScience},
  pages = {708--708},
  publisher = {ACM},
  series = {SIGCSE '16},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Brainstorming Data Science As a Fluency Course for Non-Majors and As a New Specialization},
  url = {http://doi.acm.org/10.1145/2839509.2850498},
  year = 2016
}

@article{glassman2015overcode,
  abstract = {In MOOCs, a single programming exercise may produce thousands of solutions from learners. Understanding solution variation is important for providing appropriate feedback to students at scale. The wide variation among these solutions can be a source of pedagogically valuable examples and can be used to refine the autograder for the exercise by exposing corner cases. We present OverCode, a system for visualizing and exploring thousands of programming solutions. OverCode uses both static and dynamic analysis to cluster similar solutions, and lets teachers further filter and cluster solutions based on different criteria. We evaluated OverCode against a nonclustering baseline in a within-subjects study with 24 teaching assistants and found that the OverCode interface allows teachers to more quickly develop a high-level view of students' understanding and misconceptions, and to provide feedback that is relevant to more students' solutions.},
  added-at = {2017-01-14T23:26:56.000+0100},
  address = {New York, NY},
  author = {Glassman, Elena L. and Scott, Jeremy and Singh, Rishabh and Guo, Philip J. and Miller, Robert C.},
  biburl = {https://www.bibsonomy.org/bibtex/29c61deae34ed1bd0bb90c75bd00b6dbe/vngudivada},
  interhash = {34666b0fac90e9df7cd990ca2794406c},
  intrahash = {9c61deae34ed1bd0bb90c75bd00b6dbe},
  journal = {ACM Trans. Comput.-Hum. Interact.},
  keywords = {CS1 OverCode Programming},
  month = mar,
  number = 2,
  pages = {7:1--7:35},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {OverCode: Visualizing Variation in Student Solutions to Programming Problems at Scale},
  url = {http://doi.acm.org/10.1145/2699751},
  volume = 22,
  year = 2015
}

@article{hegazy2015monitoring,
  abstract = {Urban growth is a worldwide phenomenon but the rate of urbanization is very fast in developing country like Egypt. It is mainly driven by unorganized expansion, increased immigration, rapidly increasing population. In this context, land use and land cover change are considered one of the central components in current strategies for managing natural resources and monitoring environmental changes. In Egypt, urban growth has brought serious losses of agricultural land and water bodies. Urban growth is responsible for a variety of urban environmental issues like decreased air quality, increased runoff and subsequent flooding, increased local temperature, deterioration of water quality, etc. Egypt possessed a number of fast growing cities. Mansoura and Talkha cities in Daqahlia governorate are expanding rapidly with varying growth rates and patterns. In this context, geospatial technologies and remote sensing methodology provide essential tools which can be applied in the analysis of land use change detection. This paper is an attempt to assess the land use change detection by using GIS in Mansoura and Talkha from 1985 to 2010. Change detection analysis shows that built-up area has been increased from 28 to 255 km2 by more than 30\% and agricultural land reduced by 33\%. Future prediction is done by using the Markov chain analysis. Information on urban growth, land use and land cover change study is very useful to local government and urban planners for the betterment of future plans of sustainable development of the city.},
  added-at = {2017-01-15T21:41:30.000+0100},
  author = {Hegazy, Ibrahim Rizk and Kaloop, Mosbeh Rashed},
  biburl = {https://www.bibsonomy.org/bibtex/28b7807cb21e42df0e8b5d424f0111c3b/vngudivada},
  interhash = {b2ada5c1c0323565bedd35a3655755fc},
  intrahash = {8b7807cb21e42df0e8b5d424f0111c3b},
  journal = {International Journal of Sustainable Built Environment},
  keywords = {GIS},
  number = 1,
  pages = {117 - 124},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Monitoring urban growth and land use change detection with GIS and remote sensing techniques in Daqahlia governorate Egypt},
  url = {http://www.sciencedirect.com/science/article/pii/S2212609015000060},
  volume = 4,
  year = 2015
}

@article{guo2013teaching,
  abstract = {The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.<br /><br />twitter<br />Follow us on Twitter at http://twitter.com/blogCACM<br /><br />http://cacm.acm.org/blogs/blog-cacm<br /><br />Philip Guo offers programmers 'Opportunistic Programming' tips that typically are not shared in school.},
  added-at = {2017-01-14T23:04:04.000+0100},
  address = {New York, NY},
  author = {Guo, Philip},
  biburl = {https://www.bibsonomy.org/bibtex/2eee90ef029a1c586336cc2c69717a69c/vngudivada},
  interhash = {ec55c29a27bc2eb1b41addeda3da437b},
  intrahash = {eee90ef029a1c586336cc2c69717a69c},
  journal = {Commun. ACM},
  keywords = {CS0 Programming},
  month = aug,
  number = 8,
  pages = {10--11},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Teaching Programming the Way It Works Outside the Classroom},
  url = {http://doi.acm.org/10.1145/2492007.2492012},
  volume = 56,
  year = 2013
}

@article{brown2013complex,
  abstract = {We examine critically the claims made by Fredrickson and Losada (2005) concerning the construct known as the “positivity ratio.” We find no theoretical or empirical justification for the use of differential equations drawn from fluid dynamics, a subfield of physics, to describe changes in human emotions over time; furthermore, we demonstrate that the purported application of these equations contains numerous fundamental conceptual and mathematical errors. The lack of relevance of these equations and their incorrect application lead us to conclude that Fredrickson and Losada's claim to have demonstrated the existence of a critical minimum positivity ratio of 2.9013 is entirely unfounded. More generally, we urge future researchers to exercise caution in the use of advanced mathematical tools, such as nonlinear dynamics, and in particular to verify that the elementary conditions for their valid application have been met.},
  added-at = {2017-01-05T15:22:44.000+0100},
  author = {Brown, Nicholas J. L. and Sokal, Alan D. and Friedman, Harris L.},
  biburl = {https://www.bibsonomy.org/bibtex/2c4b72845d369eb27ef3ac5e35e3669b2/vngudivada},
  interhash = {7a3b2b77e348c0abb0a3b90717e865e3},
  intrahash = {c4b72845d369eb27ef3ac5e35e3669b2},
  journal = {American Psychologist},
  keywords = {NsfRED PositivityRatio Psychology},
  note = { doi: 10.1037/a0032850},
  pages = {1 -- 13},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {The Complex Dynamics of Wishful Thinking: The Critical
Positivity Ratio},
  year = 2013
}

@inproceedings{whitelaw2009using,
  abstract = {We have designed, implemented and evaluated an end-to-end system spellchecking and autocorrection system that does not require any manually annotated training data. The World Wide Web is used as a large noisy corpus from which we infer knowledge about misspellings and word usage. This is used to build an error model and an n-gram language model. A small secondary set of news texts with artificially inserted misspellings are used to tune confidence classifiers. Because no manual annotation is required, our system can easily be instantiated for new languages. When evaluated on human typed data with real misspellings in English and German, our web-based systems outperform baselines which use candidate corrections based on hand-curated dictionaries. Our system achieves 3.8% total error rate in English. We show similar improvements in preliminary results on artificial data for Russian and Arabic.},
  added-at = {2017-01-17T00:51:12.000+0100},
  address = {Stroudsburg, PA},
  author = {Whitelaw, Casey and Hutchinson, Ben and Chung, Grace Y. and Ellis, Gerard},
  biburl = {https://www.bibsonomy.org/bibtex/25e96c59c2c5c810ae5bdc29ce465daad/vngudivada},
  booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2},
  interhash = {ac38c5620a7f8e362c60f326059a8b28},
  intrahash = {5e96c59c2c5c810ae5bdc29ce465daad},
  keywords = {NLP SpellChecking SpellCorrector},
  pages = {890--899},
  publisher = {Association for Computational Linguistics},
  series = {EMNLP '09},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Using the Web for Language Independent Spellchecking and Autocorrection},
  url = {http://dl.acm.org/citation.cfm?id=1699571.1699629},
  year = 2009
}

@book{gemignani2014fluency,
  abstract = {A dream come true for those looking to improve their data fluency Analytical data is a powerful tool for growing companies, but what good is it if it hides in the shadows? Bring your data to the forefront with effective visualization and communication approaches, and let Data Fluency: Empowering Your Organization with Effective Communication show you the best tools and strategies for getting the job done right. Learn the best practices of data presentation and the ways that reporting and dashboards can help organizations effectively gauge performance, identify areas for improvement, and communicate results. Topics covered in the book include data reporting and communication, audience and user needs, data presentation tools, layout and styling, and common design failures. Those responsible for analytics, reporting, or BI implementation will find a refreshing take on data and visualization in this resource, as will report, data visualization, and dashboard designers},
  added-at = {2017-01-21T18:24:48.000+0100},
  author = {Gemignani, Zach and Gemignani, Chris and Galentino, Richard and Schuermann, Patrick},
  biburl = {https://www.bibsonomy.org/bibtex/24c02deaf7ad3257d8383f3a2308f5163/vngudivada},
  interhash = {e58a06cf38af2161c4a021c55821dbd7},
  intrahash = {4c02deaf7ad3257d8383f3a2308f5163},
  keywords = {Book Leadership Visualization},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Data fluency: empowering your organization with effective data communication},
  year = 2014
}

@inbook{wissner2013question,
  abstract = {This paper presents a domain independent question generation and interaction procedure that automatically generates multiple-choice questions for conceptual models created with Qualitative Reasoning vocabulary. A Bayesian Network is deployed that captures the learning progress based on the answers provided by the learner. The likelihood of concepts being known or unknown on behalf of the learner determines the focus, and the question generator adjusts the contents of its questions accordingly. As a use case, the Quiz mode is introduced.},
  added-at = {2017-01-30T04:08:54.000+0100},
  address = {Berlin, Germany},
  author = {Wi{\ss}ner, Michael and Linnebank, Floris and Liem, Jochem and Bredeweg, Bert and Andr{\'e}, Elisabeth},
  biburl = {https://www.bibsonomy.org/bibtex/284f60b4f1a8dc3eec6f28eff91fbf140/vngudivada},
  booktitle = {Artificial Intelligence in Education: 16th International Conference, AIED 2013, Memphis, TN, USA, July 9-13, 2013. Proceedings},
  editor = {Lane, H. Chad and Yacef, Kalina and Mostow, Jack and Pavlik, Philip},
  interhash = {46d45b6a89f81ea777e4ae5508d3442b},
  intrahash = {84f60b4f1a8dc3eec6f28eff91fbf140},
  keywords = {BayesianNetwork QuestionGeneration},
  pages = {729--732},
  publisher = {Springer},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Question Generation and Adaptation Using a Bayesian Network of the Learner's Achievements},
  url = {http://dx.doi.org/10.1007/978-3-642-39112-5_99},
  year = 2013
}

@article{sorin2011primer,
  abstract = {Many modern computer systems and most multicore chips (chip multiprocessors) support shared memory in hardware. In a shared memory system, each of the processor cores may read and write to a single shared address space. For a shared memory machine, the memory consistency model defines the architecturally visible behavior of its memory system. Consistency definitions provide rules about loads and stores (or memory reads and writes) and how they act upon memory. As part of supporting a memory consistency model, many machines also provide cache coherence protocols that ensure that multiple cached copies of data are kept up-to-date. The goal of this primer is to provide readers with a basic understanding of consistency and coherence. This understanding includes both the issues that must be solved as well as a variety of solutions. We present both highlevel concepts as well as specific, concrete examples from real-world systems.

Table of Contents: Preface / Introduction to Consistency and Coherence / Coherence Basics / Memory Consistency Motivation and Sequential Consistency / Total Store Order and the x86 Memory Model / Relaxed Memory Consistency / Coherence Protocols / Snooping Coherence Protocols / Directory Coherence Protocols / Advanced Topics in Coherence / Author Biographies},
  added-at = {2017-01-29T18:21:50.000+0100},
  author = {Sorin, Daniel J. and Hill, Mark D. and Wood, David A.},
  biburl = {https://www.bibsonomy.org/bibtex/21c81e0e0a0a29c2f641b1d205b121043/vngudivada},
  interhash = {49d75afd76dc7ced42f399bb442109ca},
  intrahash = {1c81e0e0a0a29c2f641b1d205b121043},
  journal = {Synthesis Lectures on Computer Architecture},
  keywords = {CacheCoherence MemoryConsistency SynthesisLecture},
  number = 3,
  pages = {1-212},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {A Primer on Memory Consistency and Cache Coherence},
  url = {http://dx.doi.org/10.2200/S00346ED1V01Y201104CAC016},
  volume = 6,
  year = 2011
}

@article{marbachad2007faculty,
  abstract = {As research faculty with expertise in the area of host–pathogen interactions (HPI), we used a research group model to effect our professional development as scientific educators. We have established a working hypothesis: The implementation of a curriculum that forms bridges between our seven HPI courses allows our students to achieve deep and meaningful learning of HPI concepts. Working collaboratively, we identified common learning goals, and we chose two microorganisms to serve as anchors for student learning. We instituted variations of published active-learning methods to engage students in research-oriented learning. In parallel, we are developing an assessment tool. The value of this work is in the development of a teaching model that successfully allowed faculty who already work collaboratively in the research area of HPI to apply a “research group approach” to further scientific teaching initiatives at a research university. We achieved results that could not be accomplished by even the most dedicated instructor working in isolation.},
  added-at = {2017-01-06T15:32:14.000+0100},
  author = {Marbach-Ad, G. and Briken, V. and Frauwirth, K. and Gao, L.-Y. and Hutcheson, S. W. and Joseph, S. W. and Mosser, D. and Parent, B. and Shields, P. and Song, W. and Stein, D. C. and Swanson, K. and Thompson, K. V. and Yuan, R. and Smith, A. C.},
  biburl = {https://www.bibsonomy.org/bibtex/2027d201a266e6854be869bfca2e76188/vngudivada},
  interhash = {b3daea8b3fae4cc34331bbd082318187},
  intrahash = {027d201a266e6854be869bfca2e76188},
  journal = {Cell Biology Education},
  keywords = {ContentThread NsfRED},
  month = jun,
  number = 2,
  pages = {155--162},
  publisher = {American Society for Cell Biology ({ASCB})},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {A Faculty Team Works to Create Content Linkages among Various Courses to Increase Meaningful Learning of Targeted Concepts of Microbiology},
  url = {http://dx.doi.org/10.1187/cbe.06-12-0212},
  volume = 6,
  year = 2007
}

@inproceedings{guo2015codeopticon,
  abstract = {One-on-one tutoring from a human expert is an effective way for novices to overcome learning barriers in complex domains such as computer programming. But there are usually far fewer experts than learners. To enable a single expert to help more learners at once, we built Codeopticon, an interface that enables a programming tutor to monitor and chat with dozens of learners in real time. Each learner codes in a workspace that consists of an editor, compiler, and visual debugger. The tutor sees a real-time view of each learner's actions on a dashboard, with each learner's workspace summarized in a tile. At a glance, the tutor can see how learners are editing and debugging their code, and what errors they are encountering. The dashboard automatically reshuffles tiles so that the most active learners are always in the tutor's main field of view. When the tutor sees that a particular learner needs help, they can open an embedded chat window to start a one-on-one conversation. A user study showed that 8 first-time Codeopticon users successfully tutored anonymous learners from 54 countries in a naturalistic online setting. On average, in a 30-minute session, each tutor monitored 226 learners, started 12 conversations, exchanged 47 chats, and helped 2.4 learners.},
  added-at = {2017-01-14T23:10:05.000+0100},
  address = {New York, NY},
  author = {Guo, Philip J.},
  biburl = {https://www.bibsonomy.org/bibtex/2a78bff747db627589cba029cfed4848a/vngudivada},
  booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software \&\#38; Technology},
  interhash = {bf0b338e511d9336e463c6162d39c54c},
  intrahash = {a78bff747db627589cba029cfed4848a},
  keywords = {CS0 Codeopticon Tutorial},
  pages = {599--608},
  publisher = {ACM},
  series = {UIST '15},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Codeopticon: Real-Time, One-To-Many Human Tutoring for Computer Programming},
  url = {http://doi.acm.org/10.1145/2807442.2807469},
  year = 2015
}

@article{greene2008effortoutcome,
  abstract = {This study examined the relationship between student engagement and educational outcomes for community college students from various racial/ethnic groups (n = 3,143). Results suggest an Effort–Outcome Gap may exist for African American students—the result of having to expend more effort in attempting to overcome myriad barriers to academic success.},
  added-at = {2017-01-08T19:55:29.000+0100},
  author = {Greene, Thomas G. and Marti, C. Nathan and McClenney, Kay},
  biburl = {https://www.bibsonomy.org/bibtex/2571782904fbc72859b006dd33009bd44/vngudivada},
  interhash = {a3e21ca232bb265c901f5ec4b3dc87fa},
  intrahash = {571782904fbc72859b006dd33009bd44},
  journal = {The Journal of Higher Education},
  keywords = {EffortOutcomeGap NsfRED},
  number = 5,
  pages = {513 -- 539},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {The Effort-Outcome Gap: Differences for African American and Hispanic Community College Students in Student Engagement and Academic Achievement},
  volume = 79,
  year = 2008
}

@inproceedings{stevens2016industry,
  abstract = {Representatives of the Information Technology (IT) industry periodically question the work-readiness of university educated IT graduates. Academics typically interpret such challenges as calls to teach currently fashionable technologies. Strong demand for IT skills in Wellington, New Zealand's capital, during the past few years has resulted in this being a major area of debate. This research sought to get past the competing claims by examining job advertisements, conducting interviews and focus groups research and found that industry representatives are in fact less concerned about technical skills than 'soft' skills needed for team based, customer focused, business environments. Industry tends to seek base-level technical skills and make recruitment decisions on more personal qualities, leaving detailed training to on-the-job. Key soft skills sought by IT employers are identified to help tertiary educators clarify industry priorities.},
  added-at = {2017-01-07T20:35:56.000+0100},
  address = {New York, NY, USA},
  author = {Stevens, Matt and Norman, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/230cc2e75d4430c530c9ac3218ad4d5ff/vngudivada},
  booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
  interhash = {e464e89e5e8b7f1c93dc228c08575906},
  intrahash = {30cc2e75d4430c530c9ac3218ad4d5ff},
  keywords = {CSEducation NsfRED},
  pages = {13:1--13:9},
  publisher = {ACM},
  series = {ACSW '16},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Industry Expectations of Soft Skills in IT Graduates: A Regional Survey},
  url = {http://doi.acm.org/10.1145/2843043.2843068},
  year = 2016
}

@inproceedings{anderson2015using,
  abstract = {Data storage needs continue to grow in most fields, and the cost per byte for tape remains lower than the cost for disk, making tape storage a good candidate for cost-effective long-term storage. However, the workloads suitable for tape archives differ from those for disk file systems, and archives must handle internally generated workloads that can be more demanding than those generated by end users (e.g., migration of data from an old tape technology to a new one). To better understand the variegated workloads, we have followed the first steps in the data science methodology. For anyone considering the use or deployment of a tape-based data archive or for anyone interested in details of data archives in the context of data science, this paper describes key aspects of data archive workloads.},
  added-at = {2017-01-12T02:18:32.000+0100},
  address = {New York, NY},
  author = {Anderson, Bill and Genty, Marc and Hart, David L. and Thanhardt, Erich},
  biburl = {https://www.bibsonomy.org/bibtex/221cf9896dd25ef0eb3257c59bd2fdc90/vngudivada},
  booktitle = {Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure},
  interhash = {01afade656ba6b7b7d5ba04f43810d04},
  intrahash = {21cf9896dd25ef0eb3257c59bd2fdc90},
  keywords = {DataScience XSEDE},
  pages = {31:1--31:8},
  publisher = {ACM},
  series = {XSEDE '15},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Using Data Science to Understand Tape-based Archive Workloads},
  url = {http://doi.acm.org/10.1145/2792745.2792776},
  year = 2015
}

@inproceedings{heilman2010question,
  abstract = {We address the challenge of automatically generating questions from reading materials for educational practice and assessment. Our approach is to overgenerate questions, then rank them. We use manually written rules to perform a sequence of general purpose syntactic transformations (e.g., subject-auxiliary inversion) to turn declarative sentences into questions. These questions are then ranked by a logistic regression model trained on a small, tailored dataset consisting of labeled output from our system. Experimental results show that ranking nearly doubles the percentage of questions rated as acceptable by annotators, from 27\% of all questions to 52\% of the top ranked 20\% of questions.},
  added-at = {2017-01-30T04:17:03.000+0100},
  address = {Stroudsburg, PA},
  author = {Heilman, Michael and Smith, Noah A.},
  biburl = {https://www.bibsonomy.org/bibtex/27b983b72344c7733c20f1e4d4841074c/vngudivada},
  booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  interhash = {61b2b54aaeeaf31aeaa03f58fd4994ba},
  intrahash = {7b983b72344c7733c20f1e4d4841074c},
  keywords = {QuestionGeneration Ranking},
  pages = {609--617},
  publisher = {Association for Computational Linguistics},
  series = {HLT '10},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Good Question! Statistical Ranking for Question Generation},
  year = 2010
}

@book{buckingham2001discover,
  abstract = {Unfortunately, most of us have little sense of our talents and strengths, much less the ability to build our lives around them. Instead, guided by our parents, by our teachers, by our managers, and by psychology's fascination with pathology, we become experts in our weaknesses and spend our lives trying to repair these flaws, while our strengths lie dormant and neglected.

Marcus Buckingham, coauthor of the national bestseller First, Break All the Rules, and Donald O. Clifton, Chair of the Gallup International Research & Education Center, have created a revolutionary program to help readers identify their talents, build them into strengths, and enjoy consistent, near-perfect performance. At the heart of the book is the Internet-based StrengthsFinder® Profile, the product of a 25-year, multimillion-dollar effort to identify the most prevalent human strengths. The program introduces 34 dominant "themes" with thousands of possible combinations, and reveals how they can best be translated into personal and career success. In developing this program, Gallup has conducted psychological profiles with more than two million individuals to help readers learn how to focus and perfect these themes.

So how does it work? This book contains a unique identification number that allows you access to the StrengthsFinder Profile on the Internet. This Web-based interview analyzes your instinctive reactions and immediately presents you with your five most powerful signature themes. Once you know which of the 34 themes -- such as Achiever, Activator, Empathy, Futuristic, or Strategic -- you lead with, the book will show you how to leverage them for powerful results at three levels: for your own development, for your success as a manager, and for the success of your organization.

With accessible and profound insights on how to turn talents into strengths, and with the immediate on-line feedback of StrengthsFinder at its core, Now, Discover Your Strengths is one of the most groundbreaking and useful business books ever written.},
  added-at = {2017-01-05T01:31:05.000+0100},
  address = {New York, NY},
  author = {Buckingham, Marcus and Clifton, Donald O.},
  biburl = {https://www.bibsonomy.org/bibtex/26220ec517791be7856bcad443f2b6def/vngudivada},
  interhash = {f1df887aef5293c1e970319ab3030ab0},
  intrahash = {6220ec517791be7856bcad443f2b6def},
  keywords = {Book Change Leadership NsfRED StrengthModel},
  publisher = {Free Press},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Now, discover your strengths},
  year = 2001
}

@article{chasteen2012colorado,
  abstract = {As part of an effort to systematically improve our junior-level E&M I course, we have developed a tool to assess student conceptual learning of electrostatics at the upper division. Together with a group of physics faculty, we established a list of learning goals for the course that, with results from student observations and interviews, served as a guide in creating the Colorado Upper-Division Electrostatics (CUE) assessment. The result is a 17-question open-ended post-test diagnostic (with an optional 7-question pretest) and an accompanying grading rubric. We present measures of the validation and reliability of the instrument and grading rubric, plus results from 535 students in both standard and interactive-engagement courses across seven institutions as a baseline for the instrument. Overall, we find that the CUE is a valid and reliable measure, and the data herein are intended to be of use to researchers and faculty interested in using the CUE to measure student learning.},
  added-at = {2017-01-06T15:49:36.000+0100},
  author = {Chasteen, Stephanie V. and Pepper, Rachel E. and Caballero, Marcos D. and Pollock, Steven J. and Perkins, Katherine K.},
  biburl = {https://www.bibsonomy.org/bibtex/2e735081745555f1f779059c4835d2a8a/vngudivada},
  interhash = {8c3848ee53c8f286d9f5146885dc622b},
  intrahash = {e735081745555f1f779059c4835d2a8a},
  journal = {Physical Review Special Topics - Physics Education Research},
  keywords = {NsfRED},
  month = sep,
  number = 2,
  pages = {020108-1 -- 020108-15},
  publisher = {American Physical Society},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Colorado Upper-Division Electrostatics diagnostic: A conceptual assessment for the junior level},
  url = {http://link.aps.org/doi/10.1103/PhysRevSTPER.8.020108},
  volume = 8,
  year = 2012
}

@book{foster2017social,
  abstract = {Features

Takes an accessible, hands-on approach to handling big data in the social sciences
Presents all the key big data tools in a non-intimidating way to social and data scientists while not neglecting research questions and purposes
Illustrates social science and data science principles through real-world applications
Links computer science concepts to real social science research
Promotes good scientific practice
Provides data and code as well as programming exercises on GitHub
Summary

Both Traditional Students and Working Professionals Acquire the Skills to Analyze Social Problems.

Big Data and Social Science: A Practical Guide to Methods and Tools shows how to apply data science to real-world problems in both research and the practice. The book provides practical guidance on combining methods and tools from computer science, statistics, and social science. This concrete approach is illustrated throughout using an important national problem, the quantitative study of innovation.

The text draws on the expertise of prominent leaders in statistics, the social sciences, data science, and computer science to teach students how to use modern social science research principles as well as the best analytical and computational tools. It uses a real-world challenge to introduce how these tools are used to identify and capture appropriate data, apply data science models and tools to that data, and recognize and respond to data errors and limitations.
The world has changed for empirical social scientists. The new types of “big data” have generated an entire new research field—that of data science. That world is dominated by computer scientists who have generated new ways of creating and collecting data, developed new analytical and statistical techniques, and provided new ways of visualizing and presenting information. These new sources of data and techniques have the potential to transform the way applied social science is done.

Research has certainly changed. Researchers draw on data that are “found” rather than “made” by federal agencies; those publishing in leading academic journals are much less likely today to draw on preprocessed survey data.

The way in which data are used has also changed for both government agencies and businesses. Chief data officers are becoming as common in federal and state governments as chief economists were decades ago, and in cities like New York and Chicago, mayoral offices of data analytics have the ability to provide rapid answers to important policy questions. But since federal, state, and local agencies lack the capacity to do such analysis themselves, they must make these data available either to consultants or to the research community. Businesses are also learning that making effective use of their data assets can have an impact on their bottom line.

And the jobs have changed. The new job title of “data scientist” is highlighted in job advertisements on CareerBuilder.com and Burning-glass.com—in the same category as statisticians, economists, and other quantitative social scientists if starting salaries are useful indicators.

The goal of this book is to provide social scientists with an understanding of the key elements of this new science, its value, and the opportunities for doing better work. The goal is also to identify the many ways in which the analytical toolkits possessed by social scientists can be brought to bear to enhance the generalizability of the work done by computer scientists.

We take a pragmatic approach, drawing on our experience of working with data. Most social scientists set out to solve a real- world social or economic problem: they frame the problem, identify the data, do the analysis, and then draw inferences. At all points, of course, the social scientist needs to consider the ethical ramifications of their work, particularly respecting privacy and confidentiality. The book follows the same structure. We chose a particular problem—the link between research investments and innovation— because that is a major social science policy issue, and one in which social scientists have been addressing using big data techniques. While the example is specific and intended to show how abstract concepts apply in practice, the approach is completely generalizable. The web scraping, linkage, classification, and text analysis methods on display here are canonical in nature. The inference and privacy and confidentiality issues are no different than in any other study involving human subjects, and the communication of results through visualization is similarly generalizable.},
  added-at = {2017-01-16T01:44:52.000+0100},
  address = {Boca Raton, Florida},
  author = {Foster, Ian},
  biburl = {https://www.bibsonomy.org/bibtex/2b1eeac4bb4aa2c96cde69c59f3b7ec73/vngudivada},
  interhash = {17ddd5a4dd28ef4e093ac4a61e67261d},
  intrahash = {b1eeac4bb4aa2c96cde69c59f3b7ec73},
  keywords = {BigData Book SocialScience},
  publisher = {Chapman \& Hall/CRC},
  series = {Chapman \& Hall/CRC Statistics in the Social and Behavioral Sciences},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Big data and social science: a practical guide to methods and tools},
  year = 2017
}

@book{burton2012linguistics,
  abstract = {Linguistics is the scientific study of human language. Whether you're currently enrolled in a course or just want to explore the subject, Linguistics for Dummies helps you understand some of the primary streams of linguistics: what language is for (communication), how language works (pattern formation), what language reveals about the mind (cognition), and how written language shapes society.},
  added-at = {2017-01-15T20:57:59.000+0100},
  address = {Toronto, Canada},
  author = {Burton, Strang and D\'{e}chaine, Rose-Marie and Vatikiotis-Bateson, Eric},
  biburl = {https://www.bibsonomy.org/bibtex/2f1f131c65e9462fd5739a833f03ec6be/vngudivada},
  interhash = {b0c86ca0acb97bad378d13db32dcd605},
  intrahash = {f1f131c65e9462fd5739a833f03ec6be},
  keywords = {Book Lingusitcs},
  publisher = {J. Wiley \& Sons Canada},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Linguistics for dummies},
  year = 2012
}

@book{abumostafa2012learning,
  abstract = {This book, together with specially prepared online material freely accessible to our readers, provides a complete introduction to Machine Learning, the technology that enables computational systems to adaptively improve their performance with experience accumulated from the observed data. Such techniques are widely applied in engineering, science, finance, and commerce. This book is designed for a short course on machine learning. It is a short course, not a hurried course. From over a decade of teaching this material, we have distilled what we believe to be the core topics that every student of the subject should know. In addition, our readers are given free access to online e-Chapters that we update with the current trends in Machine Learning, such as deep learning and support vector machines. We chose the title `learning from data' that faithfully describes what the subject is about, and made it a point to cover the topics in a story-like fashion. Our hope is that the reader can learn all the fundamentals of the subject by reading the book cover to cover. Learning from data has distinct theoretical and practical tracks. In this book, we balance the theoretical and the practical, the mathematical and the heuristic. Theory that establishes the conceptual framework for learning is included, and so are heuristics that impact the performance of real learning systems. What we have emphasized are the necessary fundamentals that give any student of learning from data a solid foundation.},
  added-at = {2017-01-09T16:59:38.000+0100},
  address = {Berlin, Germany},
  author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
  biburl = {https://www.bibsonomy.org/bibtex/2377c1cf1413a1e2bf564bc4a771ec175/vngudivada},
  interhash = {48dd62a247c20739cebd95d601a42f7a},
  intrahash = {377c1cf1413a1e2bf564bc4a771ec175},
  keywords = {Book ML},
  publisher = {AMLBook.com},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Learning from data: a short course},
  year = 2012
}

@misc{guo2015grind,
  added-at = {2017-01-14T23:38:59.000+0100},
  author = {Guo, Philip J.},
  biburl = {https://www.bibsonomy.org/bibtex/2e2381ef681ef83bb00f00f25954822f3/vngudivada},
  interhash = {9c65a207bc9feea5551f6e094bdf887b},
  intrahash = {e2381ef681ef83bb00f00f25954822f3},
  keywords = {PhD Research},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {The Ph.D. Grind: A Ph.D. Student Memoir},
  url = {http://pgbovine.net/PhD-memoir/pguo-PhD-grind.pdf},
  year = 2015
}

@inproceedings{luxtonreilly2012impact,
  abstract = {Recent interest in student-centric pedagogies have resulted in the development of numerous tools that support student generated questions. Previous evaluations of such tools have reported strong correlations between student participation and exam performance, yet the level of student engagement with other learning activities in the course is a potential confounding factor. We show such correlations may be explained by other factors, and we undertake a deeper analysis that reveals evidence of the positive impact question-generation activities have on student performance.},
  added-at = {2017-01-30T04:33:33.000+0100},
  address = {New York, NY},
  author = {Luxton-Reilly, Andrew and Bertinshaw, Daniel and Denny, Paul and Plimmer, Beryl and Sheehan, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/2f8ee5a01fc285befe90cb02225f0fad8/vngudivada},
  booktitle = {Proceedings of the 43rd ACM Technical Symposium on Computer Science Education},
  interhash = {2b964992849fb3283ccd4f1cf456710c},
  intrahash = {f8ee5a01fc285befe90cb02225f0fad8},
  keywords = {QuestionGeneration},
  pages = {391--396},
  publisher = {ACM},
  series = {SIGCSE '12},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {The Impact of Question Generation Activities on Performance},
  url = {http://doi.acm.org/10.1145/2157136.2157250},
  year = 2012
}

@presentation{preville2017reaching,
  added-at = {2017-01-29T14:31:57.000+0100},
  author = {Preville, Philip},
  biburl = {https://www.bibsonomy.org/bibtex/299070f0b7eb7c7ae476c32ade2174672/vngudivada},
  interhash = {dce4194a4cadacdbc91e6b43c11045d5},
  intrahash = {99070f0b7eb7c7ae476c32ade2174672},
  keywords = {StudentEngagement TopHat},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Reaching Today's Distracted Students: A Handbook for Professors},
  year = 2017
}

@inproceedings{cassel2016science,
  abstract = {In this poster the authors report the approaches for presenting Data Science topics in Flipped Classroom mode, incorporating topics in Data Science into existing courses as well as in stand-alone courses. It provides an insight on listing of learning goals, central data science topics, content modules, and a framework for implementing a flipped classroom approach to introduce data science to students with limited technical backgrounds. The presenters are NSF-funded investigators on a collaborative team of computer scientists and statistician to create flipped material for an introductory data science class. After SIGCSE, materials described in the poster will be available in Ensemble, at http://computingportal.org/?q=VU-WSSU-DataScience},
  added-at = {2017-01-12T01:52:53.000+0100},
  address = {New York, NY},
  author = {Cassel, Lillian N. and Dicheva, Darina and Dichev, Christo and Goelman, Don and Posner, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/28111a1dabfcbbc1ed8083aaaab14c939/vngudivada},
  booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
  interhash = {dce945a5ae3b63d7130143f0ee3011a3},
  intrahash = {8111a1dabfcbbc1ed8083aaaab14c939},
  keywords = {DataScience FlippedClassroom},
  pages = {691--691},
  publisher = {ACM},
  series = {SIGCSE '16},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Data Science for All: An Introductory Course for Non-Majors; in Flipped Format (Abstract Only)},
  url = {http://doi.acm.org/10.1145/2839509.2850558},
  year = 2016
}

@inproceedings{pai2014modeling,
  abstract = {In large programming classes, MOOCs or online communities, it is challenging to find peers and mentors to help with learning specific programming concepts. In this paper we present first steps towards an automated, scalable system for matching learners with Python programmers who have expertise in different areas. The learner matching system builds a knowledge model for each programmer by analyzing their authored code and extracting features that capture domain knowledge and style. We demonstrate the feasibility of a simple model that counts the references to modules from the standard library and Python Package Index in a programmers' code. We also show that programmers exhibit self-selection using which we can extract the modules a programmer is best at, even though we may not have all of their code. In our future work we aim to extend the model to encapsulate more features, and apply it for skill matching in a programming class as well as personalizing answers on StackOverflow.},
  added-at = {2017-01-14T23:24:00.000+0100},
  address = {New York, NY},
  author = {Pai, Anvisha H. and Guo, Philip J. and Miller, Robert C.},
  biburl = {https://www.bibsonomy.org/bibtex/2dfa844f1edb9e83923badcbb52ebe780/vngudivada},
  booktitle = {Proceedings of the First ACM Conference on Learning @ Scale Conference},
  interhash = {e1bdcaeed5b584c8973e4178abd566d1},
  intrahash = {dfa844f1edb9e83923badcbb52ebe780},
  keywords = {CS1 Programming Tutorial},
  pages = {181--182},
  publisher = {ACM},
  series = {L@S '14},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Modeling Programming Knowledge for Mentoring at Scale},
  url = {http://doi.acm.org/10.1145/2556325.2567871},
  year = 2014
}

@inproceedings{watts2016computational,
  abstract = {The past 15 years have witnessed a remarkable increase in both the scale and scope of social and behavioral data available to researchers, leading some to herald the emergence of a new field: "computational social science." Against these exciting developments stands a stubborn fact: that in spite of many thousands of published papers, there has been surprisingly little progress on the "big" questions that motivated the field in the first place?questions concerning systemic risk in financial systems, problem solving in complex organizations, and the dynamics of epidemics or social movements, among others. In this talk I highlight some examples of research that would not have been possible just a handful of years ago and that illustrate the promise of CSS. At the same time, they illustrate its limitations. I then conclude with some thoughts on how CSS can bridge the gap between its current state and its potential.},
  added-at = {2017-01-14T22:48:35.000+0100},
  address = {New York, NY, USA},
  author = {Watts, Duncan},
  biburl = {https://www.bibsonomy.org/bibtex/2b4310d5b3a389ce206f908bc908191f8/vngudivada},
  booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  interhash = {eecabda377627884c8e84ac81f0bc7d1},
  intrahash = {b4310d5b3a389ce206f908bc908191f8},
  keywords = {DataScience SocialScience},
  pages = {419--419},
  publisher = {ACM},
  series = {KDD '16},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Computational Social Science: Exciting Progress and Future Challenges},
  url = {http://doi.acm.org/10.1145/2939672.2945366},
  year = 2016
}

@inproceedings{olabarriaga2013understanding,
  abstract = {Scientific workflow management is heavily used in our organization. After six years, a large number of workflows are available and regularly used to run biomedical data analysis experiments on distributed infrastructures, mostly on grids. In this paper we present our first efforts to better understand and characterise these workflows. We start with a set of considerations previously proposed in the literature (workflow dimensions and motifs), and revise these to more closely describe what we observe in our workflows. We conclude that workflow characteristics can be categorized at two levels: firstly, the features characterizing the distributed application and how to implement it as a workflow, and secondly, workflow motifs that depend on the features of the selected workflow management system. These characteristics could be useful in the future to understand a larger set of workflows and to identify functional requirements for further development workflow management systems.},
  added-at = {2017-01-12T21:51:18.000+0100},
  address = {New York, NY},
  author = {Olabarriaga, Silvia D. and Jaghoori, Mohammad Mahdi and Korkhov, Vladimir and van Schaik, Barbera and van Kampen, Antoine},
  biburl = {https://www.bibsonomy.org/bibtex/23f619f940061f643bff0af5ec8de43d9/vngudivada},
  booktitle = {Proceedings of the 8th Workshop on Workflows in Support of Large-Scale Science},
  interhash = {01756bfc5458e0a2171c532d7677a85e},
  intrahash = {3f619f940061f643bff0af5ec8de43d9},
  keywords = {DistributedComputing Workflow},
  pages = {68--76},
  publisher = {ACM},
  series = {WORKS '13},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Understanding Workflows for Distributed Computing: Nitty-gritty Details},
  url = {http://doi.acm.org/10.1145/2534248.2534255},
  year = 2013
}

@article{squire2012outline,
  abstract = {Data Science is an increasingly popular term for the deliberate, methodological study of the principles and techniques involved in the storage, management, mining, and visualization of large amounts of data, as used to solve problems in diverse domains. This paper provides a working definition of Data Science and examines the relationship between this emerging field and other, more familiar disciplines already established in the undergraduate curriculum. We then provide an operational framework (“The Six Steps”) for an introductory course in Data Science and Visualization. We provide a comprehensive description of concrete, relevant example assignments that fit cleanly into this framework. We conclude with examples of how this course can achieve secondary objectives of delivering an opportunity for emphasis on Writing and Information Literacy.},
  added-at = {2017-01-14T03:07:29.000+0100},
  author = {Squire, Megan},
  biburl = {https://www.bibsonomy.org/bibtex/27f83cd13a39a87004fa6aa1c528ba9da/vngudivada},
  interhash = {7e6ad445de8ed8df67ceda0b48b26f89},
  intrahash = {7f83cd13a39a87004fa6aa1c528ba9da},
  journal = {Issues in Information Systems},
  keywords = {CS0 DataScience ElonUniversity Visualization},
  number = 1,
  pages = {382 -- 390},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Outline and Exercises for a Novel Introductory Course in Data Science and Visualization},
  volume = 13,
  year = 2012
}

@article{guo2014refining,
  abstract = {The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.<br /><br />twitter<br />Follow us on Twitter at http://twitter.com/blogCACM<br /><br />http://cacm.acm.org/blogs/blog-cacm<br /><br />Philip Guo sees code reviews providing students "lots of pragmatic learning."},
  added-at = {2017-01-14T23:12:05.000+0100},
  address = {New York, NY},
  author = {Guo, Philip},
  biburl = {https://www.bibsonomy.org/bibtex/2e4b87ded7997e270410b0d953c9cad1f/vngudivada},
  interhash = {cc46d4e77bfa2c325064a73ce5a1f6f5},
  intrahash = {e4b87ded7997e270410b0d953c9cad1f},
  journal = {Commun. ACM},
  keywords = {CS1 Programming},
  month = sep,
  number = 9,
  pages = {10--11},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Refining Students' Coding and Reviewing Skills},
  url = {http://doi.acm.org/10.1145/2641221},
  volume = 57,
  year = 2014
}

@inproceedings{hallholt2015statisticsinfused,
  abstract = {The ability to use computational tools to collect, organize, visualize, and analyze data is a valuable skill both inside and outside of computer science. In this paper we describe the design and implementation of a statistics-infused introduction to computer science, developed in collaboration with statistics faculty, at St. Olaf College. We propose that there exists a growing demographic of 'data-centric' students who expect to write small amounts of code in the context of work in other fields, and who are eager to take a CS course adapted to their needs. This particular data-centric CS1 course has been a catalyst for collaboration between faculty in multiple fields and multiple institutions.},
  added-at = {2017-01-12T02:23:35.000+0100},
  address = {New York, NY},
  author = {Hall-Holt, Olaf A. and Sanft, Kevin R.},
  biburl = {https://www.bibsonomy.org/bibtex/2227f33f09d5daf57f0dc8da506df1bfa/vngudivada},
  booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
  interhash = {647bf678068f7474738eb8070fd5f7aa},
  intrahash = {227f33f09d5daf57f0dc8da506df1bfa},
  keywords = {CS1 DataScience Statistics},
  pages = {138--143},
  publisher = {ACM},
  series = {SIGCSE '15},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Statistics-infused Introduction to Computer Science},
  url = {http://doi.acm.org/10.1145/2676723.2677218},
  year = 2015
}

@article{uhlbien2007complexity,
  abstract = {Leadership models of the last century have been products of top-down, bureaucratic paradigms. These models are eminently effective for an economy premised on physical production but are not well-suited for a more knowledge-oriented economy. Complexity science suggests a different paradigm for leadership—one that frames leadership as a complex interactive dynamic from which adaptive outcomes (e.g., learning, innovation, and adaptability) emerge. This article draws from complexity science to develop an overarching framework for the study of Complexity Leadership Theory, a leadership paradigm that focuses on enabling the learning, creative, and adaptive capacity of complex adaptive systems (CAS) within a context of knowledge-producing organizations. This conceptual framework includes three entangled leadership roles (i.e., adaptive leadership, administrative leadership, and enabling leadership) that reflect a dynamic relationship between the bureaucratic, administrative functions of the organization and the emergent, informal dynamics of complex adaptive systems (CAS).},
  added-at = {2017-01-06T15:11:49.000+0100},
  author = {Uhl-Bien, Mary and Marion, Russ and McKelvey, Bill},
  biburl = {https://www.bibsonomy.org/bibtex/2d6084a76d97afc286fb63aa4f215b31b/vngudivada},
  doi = {http://dx.doi.org/10.1016/j.leaqua.2007.04.002},
  interhash = {2b73e3b848f8c35570fe6c17cf8c1457},
  intrahash = {d6084a76d97afc286fb63aa4f215b31b},
  journal = {The Leadership Quarterly},
  keywords = {Complexity Leadership NsfRED Theory},
  number = 4,
  pages = {298 - 318},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Complexity Leadership Theory: Shifting leadership from the industrial age to the knowledge era},
  volume = 18,
  year = 2007
}

@article{bushe2007appreciative,
  added-at = {2017-01-06T01:43:51.000+0100},
  author = {Bushe, Gervase R.},
  biburl = {https://www.bibsonomy.org/bibtex/2e7529121f65ccdd20d9ece0cb9897007/vngudivada},
  interhash = {1508de3a56774415f582b56bcd4205e4},
  intrahash = {e7529121f65ccdd20d9ece0cb9897007},
  journal = {OD Practitioner},
  keywords = {AppreciativeInquiry Leadership NsfRED},
  month = {august},
  number = 4,
  pages = {30 - 35},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Appreciative Inquiry Is Not (Just) About The Positive},
  volume = 39,
  year = 2007
}

@book{rath2007strengths,
  abstract = {Do you have the opportunity to do what you do best every day?

Chances are, you don't. All too often, our natural talents go untapped. From the cradle to the cubicle, we devote more time to fixing our shortcomings than to developing our strengths.

To help people uncover their talents, Gallup introduced the first version of its online assessment, StrengthsFinder, in 2001 which ignited a global conversation and helped millions to discover their top five talents.

In its latest national bestseller, StrengthsFinder 2.0, Gallup unveils the new and improved version of its popular assessment, language of 34 themes, and much more (see below for details). While you can read this book in one sitting, you'll use it as a reference for decades.

Loaded with hundreds of strategies for applying your strengths, this new book and accompanying website will change the way you look at yourself--and the world around you--forever.

Available exclusively in StrengthsFinder 2.0:
(using the unique access code included with each book)

* A new and upgraded edition of the StrengthsFinder assessment
* A personalized Strengths Discovery and Action-Planning Guide for applying your strengths in the next week, month, and year
* A more customized version of your top five theme report
* 50 Ideas for Action (10 strategies for building on each of your top five },
  added-at = {2017-01-05T01:33:15.000+0100},
  address = {New York},
  author = {Rath, Tom},
  biburl = {https://www.bibsonomy.org/bibtex/20a8068cc7e232d033f82fa9c54f317f7/vngudivada},
  interhash = {fdf89d8231d5d74a1c009ee927c98d99},
  intrahash = {0a8068cc7e232d033f82fa9c54f317f7},
  keywords = {Book Leadership NsfRED StrengthModel},
  publisher = {Gallup Press},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Strengths finder 2.0},
  year = 2007
}

@book{gonzalez2008digital,
  abstract = {Completely self-contained-and heavily illustrated-this introduction to basic concepts and methodologies for digital image processing is written at a level that truly is suitable for seniors and first-year graduate students in almost any technical discipline. The leading textbook in its field for more than twenty years, it continues its cutting-edge focus on contemporary developments in all mainstream areas of image processing-e.g., image fundamentals, image enhancement in the spatial and frequency domains, restoration, color image processing, wavelets, image compression, morphology, segmentation, image description, and the fundamentals of object recognition. It focuses on material that is fundamental and has a broad scope of application.},
  added-at = {2017-01-15T20:45:36.000+0100},
  address = {Upper Saddle River, N.J.},
  author = {Gonzalez, Rafael C. and Woods, Richard E.},
  biburl = {https://www.bibsonomy.org/bibtex/2bd73f6e1350f31aa5da16268e2b1e694/vngudivada},
  interhash = {74494247f343d0cedb198c4b4f0c31eb},
  intrahash = {bd73f6e1350f31aa5da16268e2b1e694},
  keywords = {Book DIP},
  publisher = {Prentice Hall},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Digital image processing},
  year = 2008
}

@inproceedings{gil2016teaching,
  abstract = {We have designed an open and modular course for data science and big data analytics using a workflow paradigm that allows students to easily experience big data through a sophisticated yet easy to use instrument that is an intelligent workflow system. A key aspect of this work is the use of semantic workflows to capture and reuse end-to-end analytic methods that experts would use to analyze big data, and the use of an intelligent workflow system to elaborate the workflow and manage its execution and resulting datasets. Through the exposure of big data analytics in a workflow framework, students will be able to get first-hand experiences with a breadth of big data topics, including multi-step data analytic and statistical methods, software reuse and composition, parallel distributed programming, high-end computing. In addition, students learn about a range of topics in AI, including semantic representations and ontologies, machine learning, natural language processing, and image analysis.},
  added-at = {2017-01-14T05:00:33.000+0100},
  author = {Gil, Yolanda},
  biburl = {https://www.bibsonomy.org/bibtex/22620edc650602844604243e79b74edbf/vngudivada},
  booktitle = {Proceedings of the Sixth Symposium on Educational Advances in Artificial Intelligence},
  interhash = {389df0d7ad1362ad757c0ce9711fc97f},
  intrahash = {2620edc650602844604243e79b74edbf},
  keywords = {BigData DataAnalytics Workflow},
  pages = {4081 -- 4088},
  series = {EAAI},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Teaching Big Data Analytics Skills with Intelligent Workflow Systems},
  year = 2016
}

@inproceedings{mcandrew2008teaching,
  abstract = {Cryptography has become an important topic in undergraduate curricula in mathematics and computer science, not just for its intrinsic interest -- about the most fun you can have with mathematics, but for its current standing as the basis for almost all computer security. From wireless networking to secure email to password protection, cryptographic methods are used to secure information, to protect users, and to protect data.

At Victoria University, cryptography has been taught as part of a mathematics and computer science degree for several years. The students all have had at least a year of tertiary mathematics, and some exposure to a computer algebra system (Maple). However, the cost of Maple, and the current licensing agreement, means that students are unable to experiment with the software away from the computer laboratories at the University. For this reason we have decided to investigate the use of open-source computer algebra systems Maxima and Axiom. Although not as full-featured and powerful as the commercial systems Maple and Mathematica, we show they are in fact admirably suited for a subject such as cryptography. In some ways Maxima and Axiom even surpass Maple and Mathematica. Student response to the introduction of these systems has been very positive.},
  added-at = {2017-01-15T18:55:17.000+0100},
  address = {New York, NY},
  author = {McAndrew, Alasdair},
  biburl = {https://www.bibsonomy.org/bibtex/2d453eb5efb2fcc8d158a53a4da8d46eb/vngudivada},
  interhash = {d14797e27493028d1a2bda331c785249},
  intrahash = {d453eb5efb2fcc8d158a53a4da8d46eb},
  journal = {SIGCSE Bull.},
  keywords = {CS1 Cryptography},
  month = mar,
  number = 1,
  pages = {325--329},
  publisher = {ACM},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Teaching Cryptography with Open-source Software},
  url = {http://doi.acm.org/10.1145/1352322.1352247},
  volume = 40,
  year = 2008
}

@inproceedings{radermacher2014investigating,
  abstract = {Graduating computer science and software engineering students do not always possess the necessary skills, abilities, or knowledge when beginning their careers in the software industry. The lack of these skills and abilities can limit the productivity of newly hired, recent graduates, or even prevent them from gaining employment. This paper presents the results of an empirical study where twenty-three managers and hiring personnel from various software companies in the United States and Europe were interviewed. Participants were asked about areas where recent graduates frequently struggled when beginning employment at their companies and which skill deficiencies might prevent a recent graduate from being hired. The results of this study indicate that recent graduates struggle with using configuration management systems (and other software tools), effectively communicating with co-workers and customers, producing unit tests for their code, and other skills or abilities. The results also indicate that a lack of project experience and problem solving abilities are the most commonly cited issues preventing students from gaining employment. This research is intended to assist educators in identifying areas where students may not measure up the expectations of industry companies and in improving the curriculum at their universities to better prepare them for their future careers.},
  added-at = {2017-01-07T20:43:43.000+0100},
  address = {New York, NY},
  author = {Radermacher, Alex and Walia, Gursimran and Knudson, Dean},
  biburl = {https://www.bibsonomy.org/bibtex/2940e56f4e4bafbb282212645e89a309e/vngudivada},
  booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
  interhash = {01bf0c41957dab58f665a428f3679d48},
  intrahash = {940e56f4e4bafbb282212645e89a309e},
  keywords = {CSEducation NsfRED},
  pages = {291--300},
  publisher = {ACM},
  series = {ICSE Companion 2014},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Investigating the Skill Gap Between Graduating Students and Industry Expectations},
  url = {http://doi.acm.org/10.1145/2591062.2591159},
  year = 2014
}

@article{zhang2016colorful,
  abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a colorization Turing test, asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32\% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
  added-at = {2017-01-16T00:52:30.000+0100},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
  biburl = {https://www.bibsonomy.org/bibtex/24e3e36a52f14de0496c400390cd8d045/vngudivada},
  interhash = {174456db51d75d667d0fdc45096c2224},
  intrahash = {4e3e36a52f14de0496c400390cd8d045},
  journal = {CoRR},
  keywords = {Colorization DIP},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Colorful Image Colorization},
  url = {http://arxiv.org/abs/1603.08511},
  volume = {abs/1603.08511},
  year = 2016
}

@book{hey2009fourth,
  abstract = {This book presents the first broad look at the rapidly emerging field of data-intensive science, with the goal of influencing the worldwide scientific and computing research communities and inspiring the next generation of scientists. Increasingly, scientific breakthroughs will be powered by advanced computing capabilities that help researchers manipulate and explore massive datasets. The speed at which any given scientific discipline advances will depend on how well its researchers collaborate with one another, and with technologists, in areas of eScience such as databases, workflow management, visualization, and cloud-computing technologies. This collection of essays expands on the vision of pioneering computer scientist Jim Gray for a new, fourth paradigm of discovery based on data-intensive science and offers insights into how it can be fully realized.},
  added-at = {2017-01-12T03:32:14.000+0100},
  address = {Redmond, Wash},
  author = {Hey, Tony and Tansley, Stewart and Tolle, Kristin},
  biburl = {https://www.bibsonomy.org/bibtex/206acf958830a60d99572d0b950e9a9a0/vngudivada},
  interhash = {296450016ca8a5f8ab16ae4d92d1fc15},
  intrahash = {06acf958830a60d99572d0b950e9a9a0},
  keywords = {Book DataScience},
  publisher = {Microsoft Research},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {The fourth paradigm: data-intensive scientific discovery},
  url = {http://research.microsoft.com/en-us/collaboration/fourthparadigm/4th_paradigm_book_complete_lr.pdf},
  year = 2009
}

@book{yau2013points,
  abstract = {The author uses examples from art, design, business, statistics, cartography, and online media, to explore concepts and ideas about illustrating data.},
  added-at = {2017-01-21T18:22:35.000+0100},
  author = {Yau, Nathan},
  biburl = {https://www.bibsonomy.org/bibtex/2e38006fcf257b3aab4d58680f5610e67/vngudivada},
  interhash = {c54891d4e2dbb1b8ede6c41a64be2ae7},
  intrahash = {e38006fcf257b3aab4d58680f5610e67},
  keywords = {Book Leadership Visualization},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Data points: visualization that means something},
  year = 2013
}

@misc{fredrickson2013updated,
  abstract = {This article presents my response to the article by Brown, Sokal, and Friedman (2013), which critically examined Losada’s conceptual and mathematical work (as presented in Losada, 1999; Losada & Heaphy, 2004; and Fredrickson & Losada; 2005) and concluded that mathematical claims for a critical tipping point positivity ratio are unfounded. In the present article, I draw recent empirical evidence together to support the continued value of computing and seeking to elevate positivity ratios. I also underscore the necessity of modeling nonlinear effects of positivity ratios and, more generally, the value of systems science approaches within affective science and positive psychology. Even when scrubbed of Losada’s now-questioned mathematical modeling, ample evidence continues to support the conclusion that, within bounds, higher positivity ratios are predictive of flourishing mental health and other beneficial outcomes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  added-at = {2017-01-05T14:56:02.000+0100},
  author = {Fredrickson, Barbara L.},
  biburl = {https://www.bibsonomy.org/bibtex/2a4f77d9ae030c7fa0aa563c64f6fa363/vngudivada},
  interhash = {7a599eeacca77ccff3abe7bb6b380751},
  intrahash = {a4f77d9ae030c7fa0aa563c64f6fa363},
  journal = {American Psychologist},
  keywords = {Leadership PositivityRatio Psychology},
  number = 9,
  pages = {814--822},
  publisher = {American Psychological Association},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Updated thinking on positivity ratios},
  volume = 68,
  year = 2013
}

@article{marbachad2010model,
  abstract = {This essay describes how the use of a concept inventory has enhanced professional development and curriculum reform efforts of a faculty teaching community. The Host Pathogen Interactions (HPI) teaching team is composed of research and teaching faculty with expertise in HPI who share the goal of improving the learning experience of students in nine linked undergraduate microbiology courses. To support evidence-based curriculum reform, we administered our HPI Concept Inventory as a pre- and post-survey to approximately 400 students each year since 2006. The resulting data include student scores as well as their open-ended explanations for distractor choices. The data have enabled us to address curriculum reform goals of 1) reconciling student learning with our expectations, 2) correlating student learning with background variables, 3) understanding student learning across institutions, 4) measuring the effect of teaching techniques on student learning, and 5) demonstrating how our courses collectively form a learning progression. The analysis of the concept inventory data has anchored and deepened the team’s discussions of student learning. Reading and discussing students’ responses revealed the gap between our understanding and the students’ understanding. We provide evidence to support the concept inventory as a tool for assessing student understanding of HPI concepts and faculty development.},
  added-at = {2017-01-06T15:24:44.000+0100},
  author = {Marbach-Ad, G. and McAdams, K. C. and Benson, S. and Briken, V. and Cathcart, L. and Chase, M. and El-Sayed, N. M. and Frauwirth, K. and Fredericksen, B. and Joseph, S. W. and Lee, V. and McIver, K. S. and Mosser, D. and Quimby, B. B. and Shields, P. and Song, W. and Stein, D. C. and Stewart, R. and Thompson, K. V. and Smith, A. C.},
  biburl = {https://www.bibsonomy.org/bibtex/2720f4da2b564eed5758087339180b4ac/vngudivada},
  interhash = {fd0465dccd8876197c653d428d157f56},
  intrahash = {720f4da2b564eed5758087339180b4ac},
  journal = {Cell Biology Education},
  keywords = {ContentThread NsfRED},
  month = dec,
  number = 4,
  pages = {408--416},
  publisher = {American Society for Cell Biology ({ASCB})},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {A Model for Using a Concept Inventory as a Tool for Students{\textquotesingle} Assessment and Faculty Professional Development},
  url = {http://dx.doi.org/10.1187/cbe.10-05-0069},
  volume = 9,
  year = 2010
}

@article{kuh2008unmasking,
  abstract = {This study examines the relationships between student engagement, college GPA, and persistence for 6,000 students attending 18 baccalaureate-granting institutions. Data sources included student-level information from the National Survey of Student Engagement, academic transcripts, merit aid, and ACT/SAT score reports. Engagement had positive, statistically significant effects on grades and persistence between the first and second year of study for students from different racial and ethnic backgrounds. Equally important, engagement had compensatory effects for historically underserved students in that they benefited more from participating in educationally purposeful activities in terms of earning higher grades and being more likely to persist.},
  added-at = {2017-01-08T19:09:24.000+0100},
  author = {Kuh, George D. and Cruce, Ty M. and Shoup, Rick and Kinzie, Jillian and Gonyea, Robert M.},
  biburl = {https://www.bibsonomy.org/bibtex/20929bc4912957ac06cb6ad5bb62ba1b8/vngudivada},
  interhash = {ae27fe62a32f18d2bac252416f661336},
  intrahash = {0929bc4912957ac06cb6ad5bb62ba1b8},
  journal = {The Journal of Higher Education},
  keywords = {NsfRED StudentEngagement StudentRetention},
  month = sep,
  number = 5,
  pages = {540 -- 563},
  timestamp = {2019-03-25T17:10:07.000+0100},
  title = {Unmasking the Effects of Student Engagement on First-Year College Grades and Persistence},
  volume = 79,
  year = 2008
}

@book{kubat2015introduction,
  abstract = {This book presents basic ideas of machine learning in a way that is easy to understand, by providing hands-on practical advice, using simple examples, and motivating students with discussions of interesting applications. The main topics include Bayesian classifiers, nearest-neighbor classifiers, linear and polynomial classifiers, decision trees, neural networks, and support vector machines. Later chapters show how to combine these simple tools by way of “boosting,” how to exploit them in more complicated domains, and how to deal with diverse advanced practical issues. One chapter is dedicated to the popular genetic algorithms.},
  added-at = {2017-03-08T13:52:03.000+0100},
  address = {New York, NY},
  author = {Kubat, Miroslav},
  biburl = {https://www.bibsonomy.org/bibtex/2799b6bd8455fa49ea083ca134e92ad35/vngudivada},
  interhash = {63f3e8fc232af21c64e1e9264c206220},
  intrahash = {799b6bd8455fa49ea083ca134e92ad35},
  keywords = {Book ML},
  publisher = {Springer},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {An introduction to machine learning},
  year = 2015
}

@book{dumont2010nature,
  abstract = {What do we know about how people learn? How do young people's motivations and emotions influence their learning? What does research show to be the benefits of group work, formative assessments, technology applications, or project-based learning and when are they most effective? How is learning affected by family background? These are among the questions addressed for the OECD by leading researchers from North America and Europe. This book brings together the lessons of research on both the nature of learning and different educational applications, and it summarises these as seven key concluding principles.

Among the contributors are Brigid Barron, Monique Boekaerts, Erik de Corte, Linda Darling-Hammond, Kurt Fischer, Andrew Furco, Richard Mayer, Lauren Resnick, Barbara Schneider, Robert Slavin, James Spillane, Elsbeth Stern and Dylan Wiliam.

The Nature of Learning: Using Research to Inspire Practice is essential reading for all those interested in knowing what research has to say about how to optimise learning in classrooms, schools and other settings. It aims, first and foremost, to inform practice and educational reform. It will be of particular interest to teachers, education leaders, teacher educators, advisors and decision makers, as well as the research community.},
  added-at = {2017-03-11T00:38:00.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/274d4a83bb9bcea649ee189378a9d8378/vngudivada},
  editor = {Dumont, Hanna and Istance, David and Benavides, Francisco},
  interhash = {397686e91f0818bc9cc3eb85ea9b4b8d},
  intrahash = {74d4a83bb9bcea649ee189378a9d8378},
  keywords = {Book Learning},
  publisher = {Organization for Economic Cooperation and Development},
  series = {Education Research and Innovation},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Nature of Learning: Using Research To Inspire Practice},
  year = 2010
}

@article{agarwal2012value,
  abstract = {Over the course of a 5-year applied research project with more than 1,400 middle school students, evidence from a number of studies revealed that retrieval practice in authentic classroom settings improves long-term learning (Agarwal et al.                2009; McDaniel et al., Journal of Educational Psychology 103:399--414, 2011; McDaniel et al.                2012; Roediger et al., Journal of Experimental Psychology: Applied 17:382--395, 2011a). Retrieval practice, or the use of quizzes and exams to engage and enhance retrieval processes, has been widely established as an effective strategy for facilitating learning in laboratory settings (e.g., Roediger et al.                2011c). In this article, we review recent findings from applied research that demonstrate that retrieval practice enhances long-term classroom learning, delayed quizzes are particularly potent for retention, quizzes benefit students' transfer to novel quiz items, and quizzes with feedback improve students' learning and metacognitive awareness. In addition to generating evidence to support retrieval-based learning, these applied research studies also enhanced the professional development of the teachers, administrators, and scientists involved in the project. In this article, it is our hope that by sharing what we have learned from a variety of perspectives, applied scientific research in K-12 classrooms will continue to be explored and generated at local, state, and national levels, improving student learning and educational decision-making.},
  added-at = {2017-03-12T01:39:21.000+0100},
  author = {Agarwal, Pooja K. and Bain, Patrice M. and Chamberlain, Roger W.},
  biburl = {https://www.bibsonomy.org/bibtex/2073b183d666f31f4dc5c6df0593862bb/vngudivada},
  interhash = {1fdb21db09cf8033a1ae6db0fb720640},
  intrahash = {073b183d666f31f4dc5c6df0593862bb},
  journal = {Educational Psychology Review},
  keywords = {Learning RetrievalPractice},
  number = 3,
  pages = {437--448},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {The Value of Applied Research: Retrieval Practice Improves Classroom Learning and Recommendations from a Teacher, a Principal, and a Scientist},
  url = {http://dx.doi.org/10.1007/s10648-012-9210-2},
  volume = 24,
  year = 2012
}

@incollection{geary2008chapter,
  abstract = {The charge of the Learning Processes Task Group is to address what is known about how children learn and understand areas of mathematics related to algebra and preparation for algebra. This summary provides brief overviews of and recommendations from the corresponding in-depth reviews provided later in the report. The reviews cover the areas of 1) General Principles of Cognition and Learning; 2) Social, Motivational, and Affective Influences on Learning; 3) What Children Bring to School; 4) Mathematical Development in Content Areas of a) Whole Number Arithmetic; b) Fractions; c) Estimation; d) Geometry; and e) Algebra; 5) Differences Among Individuals and Groups; and 6) The Brain Sciences and Mathematics Learning. For the mathematical content areas included in the reviews, the recommendations are organized around classroom practices, training of teachers and researchers, curriculum, and future research efforts.},
  added-at = {2017-03-12T00:14:17.000+0100},
  author = {Geary, David C. and Boykin, A. Wade and Embretson, Susan and Reyna, Valerie and Siegler, Robert and Berch, Daniel B. and Graban, Jennifer},
  biburl = {https://www.bibsonomy.org/bibtex/275e2b66ce7fe4de028ff0bcbc3db3d85/vngudivada},
  interhash = {9ae6e4f09ee18e75d5a03178778ee2f4},
  intrahash = {75e2b66ce7fe4de028ff0bcbc3db3d85},
  keywords = {Book Learning Mathematics},
  publisher = {U.S. Department of Education},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Chapter 4: Report of the Task Group on Learning Processes, ask Group Reports of the National Mathematics Advisory Panel},
  url = {/brokenurl# http://www2.ed.gov/about/bdscomm/list/mathpanel/report/learning-processes.pdf},
  year = 2008
}

@book{kutner2005applied,
  abstract = {Applied Linear Statistical Models", 5e, is the long established leading authoritative text and reference on statistical modeling. For students in most any discipline where statistical analysis or interpretation is used, ALSM serves as the standard work. The text includes brief introductory and review material, and then proceeds through regression and modeling for the first half, and through ANOVA and Experimental Design in the second half. All topics are presented in a precise and clear style supported with solved examples, numbered formulae, graphic illustrations, and "Notes" to provide depth and statistical accuracy and precision. Applications used within the text and the hallmark problems, exercises, and projects are drawn from virtually all disciplines and fields providing motivation for students in virtually any college. The Fifth edition provides an increased use of computing and graphical analysis throughout, without sacrificing concepts or rigor. In general, the 5e uses larger data sets in examples and exercises, and where methods can be automated within software without loss of understanding, it is so done.},
  added-at = {2017-03-11T01:16:21.000+0100},
  address = {Boston, Massachusetts},
  author = {Kutner, Michael H. and Nachtsheim, Chris and Neter, John and Li, William},
  biburl = {https://www.bibsonomy.org/bibtex/2c87188709f639a26729942b9f5989329/vngudivada},
  edition = {Fifth},
  interhash = {a0f4899d1dedb331e4f5811dd6fcfa43},
  intrahash = {c87188709f639a26729942b9f5989329},
  keywords = {Book LinearModels},
  publisher = {McGraw-Hill/ Irwin},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Applied linear statistical models},
  year = 2005
}

@book{ericsson2016secrets,
  abstract = {Have you ever wanted to learn a language or pick up an instrument, only to become too daunted by the task at hand? Expert performance guru Anders Ericsson has made a career studying chess champions, violin virtuosos, star athletes, and memory mavens. Peak condenses three decades of original research to introduce an incredibly powerful approach to learning that is fundamentally different from the way people traditionally think about acquiring a skill. Ericsson's findings have been lauded and debated, but never properly explained. So the idea of expertise still intimidates us -- we believe we need innate talent to excel, or think excelling seems prohibitively difficult. Peak belies both of these notions, proving that almost all of us have the seeds of excellence within us -- it's just a question of nurturing them by reducing expertise to a discrete series of attainable practices. Peak offers invaluable, often counterintuitive, advice on setting goals, getting feedback, identifying patterns, and motivating yourself. Whether you want to stand out at work, or help your kid achieve academic goals, Ericsson's revolutionary methods will show you how to master nearly anything.},
  added-at = {2017-03-11T21:52:02.000+0100},
  address = {New York, NY},
  author = {Ericsson, K. Anders and Pool, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/294404cb30e027966029213843bdda3ec/vngudivada},
  interhash = {a8a2b79439653d3f937aebccac89988e},
  intrahash = {94404cb30e027966029213843bdda3ec},
  keywords = {Book Learning},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Peak: secrets from the new science of expertise},
  year = 2016
}

@article{alyahya2014ontologybased,
  abstract = {With recent advancements in Semantic Web technologies, a new trend in MCQ item generation has emerged through the use of ontologies. Ontologies are knowledge representation structures that formally describe entities in a domain and their relationships, thus enabling automated inference and reasoning. Ontology-based MCQ item generation is still in its infancy, but substantial research efforts are being made in the field. However, the applicability of these models for use in an educational setting has not been thoroughly evaluated. In this paper, we present an experimental evaluation of an ontology-based MCQ item generation system known as OntoQue. The evaluation was conducted using two different domain ontologies. The findings of this study show that ontology-based MCQ generation systems produce satisfactory MCQ items to a certain extent. However, the evaluation also revealed a number of shortcomings with current ontology-based MCQ item generation systems with regard to the educational significance of an automatically constructed MCQ item, the knowledge level it addresses, and its language structure. Furthermore, for the task to be successful in producing high-quality MCQ items for learning assessments, this study suggests a novel, holistic view that incorporates learning content, learning objectives, lexical knowledge, and scenarios into a single cohesive framework.},
  added-at = {2017-02-12T04:43:11.000+0100},
  author = {Al-Yahya, Maha},
  biburl = {https://www.bibsonomy.org/bibtex/236a3dc7782e3bf72c8833fc38031af9a/vngudivada},
  interhash = {a28810cffd0773a1291306fb543bf2be},
  intrahash = {36a3dc7782e3bf72c8833fc38031af9a},
  journal = {The Scientific World Journal},
  keywords = {Ontology QuestionGeneration},
  number = {10.1155/2014/274949},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Ontology-Based Multiple Choice Question Generation},
  year = 2014
}

@article{olney2012question,
  abstract = {In this paper we present a question generation approach suitable for tutorial dialogues. The approach is based on previous psychological theories that hypothesize questions are generated from a knowledge representation modeled as a concept map. Our model semi-automatically extracts concept maps from a textbook and uses them to generate questions. The purpose of the study is to generate and evaluate pedagogically-appropriate questions at varying levels of specificity across one or more sentences. The evaluation metrics include scales from the Question Generation Shared Task and Evaluation Challenge and a new scale specific to the pedagogical nature of questions in tutoring.},
  added-at = {2017-01-30T13:34:21.000+0100},
  author = {Olney, Andrew and Graesser, Arthur and Person, Natalie},
  biburl = {https://www.bibsonomy.org/bibtex/2f0fbcd62ff93857d23272043c5dd78cd/vngudivada},
  interhash = {685b09d726af5ea18900a728cd611c2a},
  intrahash = {f0fbcd62ff93857d23272043c5dd78cd},
  journal = {Dialogue {\&} Discourse},
  keywords = {ConceptMap QuestionGeneration},
  number = 2,
  pages = {75--99},
  publisher = {Dialogue and Discourse},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Question Generation from Concept Maps},
  url = {https://doi.org/10.5087%2Fdad.2012.204},
  volume = 3,
  year = 2012
}

@article{shkotin2017graph,
  abstract = {In modern mathematics, graphs figure as one of the better-investigated class of mathematical objects. Various properties of graphs, as well as graph-processing algorithms, can be useful if graphs of a certain kind are used as denotations for CF-grammars. Furthermore, graph are well adapted to various extensions (one kind of such extensions being attributes). This paper describes a class of graphs corresponding to CF-grammars – the decision-making graphs (henceforth abbreviated as DMG).},
  added-at = {2017-02-13T01:03:21.000+0100},
  author = {Shkotin, Alex},
  biburl = {https://www.bibsonomy.org/bibtex/254da638b0fca512926d4a3b74c5429c5/vngudivada},
  interhash = {dc46747fe6c34a92d2c334d27a18438d},
  intrahash = {54da638b0fca512926d4a3b74c5429c5},
  journal = {https://arxiv.org},
  keywords = {CFG GrammarGraphs QuestionGeneration},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Graph representation of context-free grammars},
  url = {https://arxiv.org/pdf/cs/0703015.pdf},
  year = 2017
}

@article{dunlosky2013improving,
  abstract = {Some students seem to breeze through their school years, whereas others struggle, putting them at risk for getting lost in our educational system and not reaching their full potential. Parents and teachers want to help students succeed, but there is little guidance on which learning techniques are the most effective for improving educational outcomes. This leads students to implement studying strategies that are often ineffective, resulting in minimal gains in performance. What then are the best strategies to help struggling students learn?

Fortunately for students, parents, and teachers, psychological scientists have developed and evaluated the effectiveness of a wide range of learning techniques meant to enhance academic performance. In this report, Dunlosky (Kent State University), Rawson (Kent State University), Marsh (Duke University), Nathan (University of Wisconsin–Madison), and Willingham (University of Virginia) review the effectiveness of 10 commonly used learning techniques.

The authors describe each learning technique in detail and discuss the conditions under which each technique is most successful. They also describe the students (age, ability level, etc.) for whom each technique is most useful, the materials needed to utilize each technique, and the specific skills each technique promotes. To allow readers to easily identify which methods are the most effective, the authors rate the techniques as having high, medium, or low utility for improving student learning.

Which learning techniques made the grade? According to the authors, some commonly used techniques, such as underlining, rereading material, and using mnemonic devices, were found to be of surprisingly low utility. These techniques were difficult to implement properly and often resulted in inconsistent gains in student performance. Other learning techniques such as taking practice tests and spreading study sessions out over time — known as distributed practice — were found to be of high utility because they benefited students of many different ages and ability levels and enhanced performance in many different areas.

The real-world guidance provided by this report is based on psychological science, making it an especially valuable tool for students, parents, and teachers who wish to promote effective learning. Although there are many reasons why students struggle in school, these learning techniques, when used properly, should help provide meaningful gains in classroom performance, achievement test scores, and many other tasks students will encounter across their lifespan.},
  added-at = {2017-03-11T21:43:06.000+0100},
  author = {Dunlosky, John and Rawson, Katherine A. and Marsh, Elizabeth J. and Nathan, Mitchell J. and Willingham, Daniel T.},
  biburl = {https://www.bibsonomy.org/bibtex/2f7c0eb53adf4bdf316a704a3c475fe06/vngudivada},
  interhash = {a127792c475140684e9aa586cee2edc8},
  intrahash = {f7c0eb53adf4bdf316a704a3c475fe06},
  journal = {Psychological Science in the Public Interest},
  keywords = {Learning},
  month = jan,
  number = 1,
  pages = {4--58},
  publisher = {{SAGE} Publications},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Improving Students' Learning With Effective Learning Techniques},
  url = {https://doi.org/10.1177%2F1529100612453266},
  volume = 14,
  year = 2013
}

@article{boyce2016causal,
  abstract = {Rapid eye movement sleep (REMS) has been linked with spatial and emotional memory consolidation. However, establishing direct causality between neural activity during REMS and memory consolidation has proven difficult because of the transient nature of REMS and significant caveats associated with REMS deprivation techniques. In mice, we optogenetically silenced medial septum γ-aminobutyric acid–releasing (MSGABA) neurons, allowing for temporally precise attenuation of the memory-associated theta rhythm during REMS without disturbing sleeping behavior. REMS-specific optogenetic silencing of MSGABA neurons selectively during a REMS critical window after learning erased subsequent novel object place recognition and impaired fear-conditioned contextual memory. Silencing MSGABA neurons for similar durations outside REMS episodes had no effect on memory. These results demonstrate that MSGABA neuronal activity specifically during REMS is required for normal memory consolidation.},
  added-at = {2017-03-11T20:09:14.000+0100},
  author = {Boyce, Richard and Glasgow, Stephen D. and Williams, Sylvain and Adamantidis, Antoine},
  biburl = {https://www.bibsonomy.org/bibtex/204596fc9bb85d3b54427c9cd78de2f53/vngudivada},
  interhash = {514a580adc4e98373f093f0f15ea815e},
  intrahash = {04596fc9bb85d3b54427c9cd78de2f53},
  journal = {Science},
  keywords = {Learning Sleep},
  month = may,
  number = 6287,
  pages = {812 - 816},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Causal evidence for the role of {REM} sleep theta rhythm in contextual memory consolidation},
  volume = 352,
  year = 2016
}

@book{carrano2016structures,
  abstract = {Data Structures and Abstractions with Java is suitable for one- or two-semester courses in data structures (CS-2) in the departments of Computer Science, Computer Engineering, Business, and Management Information Systems. This book is also useful for programmers and software engineers interested in learning more about data structures and abstractions.

This is the most student-friendly data structures text available that introduces ADTs in individual, brief chapters — each with pedagogical tools to help students master each concept. Using the latest features of Java, this unique object-oriented presentation makes a clear distinction between specification and implementation to simplify learning, while providing maximum classroom flexibility.

Teaching and Learning Experience
This book will provide a better teaching and learning experience—for you and your students. It will help:
Aid comprehension and facilitate teaching with an approachable format and content organization: Material is organized into small segments that focus a reader’s attention and provide greater instructional flexibility.
Support learning with student-friendly pedagogy: In-text and online features help students master the material.},
  added-at = {2017-02-21T00:54:25.000+0100},
  address = {Boston, MA},
  author = {Carrano, Frank M. and Henry, Timothy and Tahiliani, Mohit},
  biburl = {https://www.bibsonomy.org/bibtex/2e4b1e83e36f2b1fbae4b188c1055b368/vngudivada},
  edition = {Fourth},
  interhash = {0eae864857d245bd7d4126bfc6e41f8f},
  intrahash = {e4b1e83e36f2b1fbae4b188c1055b368},
  keywords = {CS2 DataStructures Java},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Data structures and abstractions with Java},
  year = 2016
}

@article{dunlosky2013strengthening,
  abstract = {Before the "big test" did you use the following study strategies: highlighting, rereading, and cramming? As students many of us probably did, yet research shows that while these three strategies are commonly used, they have been ineffective in retaining information. Learning strategies have been discussed in almost every textbook on educational psychology, so many teachers likely have been introduced to at least some of them, but, given the demands of day-to-day teaching, teachers do not always have time to figure out which strategies are best to use. Dunlosky reports that in his years of research he has discovered learning strategies that have proved successful and should be part of the "Student Toolbox" to boost learning. This article presents ten learning strategies for students and then expands on their usage. They are: (1) Practice testing: self-testing or taking practice tests on to-be-learned material; (2) Distributed practice: implementing a schedule of practice that spreads out study activities over time; (3) Interleaved practice: implementing a schedule of practice that mixes different kinds of problems, or a schedule of study that mixes different kinds of material, within a single study session; (4) Elaborative interrogation: generating an explanation for why an explicitly stated fact or concept is true; (5) Self-explanation: explaining how new information is related to known information, or explaining steps taken during problem solving; (6) Rereading: restudying text material again after an initial reading; (7) Highlighting and underlining: marking potentially important portions of to-be-learned materials while reading.; (8) Summarization: writing summaries (of various lengths) of to-be-learned texts; (9) Keyword mnemonic: using keywords and mental imagery to associate verbal materials; and (10) Imagery for text: attempting to form mental images of text materials while reading or listening. Using learning strategies can increase student understanding and achievement, but are only effective if students are motivated to use them correctly. Teaching students how to guide their learning of content using effective strategies will allow them to successfully learn throughout their lifetime.},
  added-at = {2017-03-11T18:53:29.000+0100},
  author = {Dunlosky, John},
  biburl = {https://www.bibsonomy.org/bibtex/2db194c5179c4ad88bb7c7240a65dbfb3/vngudivada},
  interhash = {f54f7bc4acaed041dbe00389e4e7b673},
  intrahash = {db194c5179c4ad88bb7c7240a65dbfb3},
  journal = {American Educator},
  keywords = {Learning},
  month = {Fall},
  number = 3,
  pages = {12 - 21},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Strengthening the Student Toolbox: Study Strategies to Boost Learning},
  volume = 37,
  year = 2013
}

@article{carpenter2012using,
  abstract = {Every day, students and instructors are faced with the decision of when to study information. The timing of study, and how it affects memory retention, has been explored for many years in research on human learning. This research has shown that performance on final tests of learning is improved if multiple study sessions are separated---i.e., ``spaced'' apart---in time rather than massed in immediate succession. In this article, we review research findings of the types of learning that benefit from spaced study, demonstrations of these benefits in educational settings, and recent research on the time intervals during which spaced study should occur in order to maximize memory retention. We conclude with a list of recommendations on how spacing might be incorporated into everyday instruction.},
  added-at = {2017-03-12T01:36:29.000+0100},
  author = {Carpenter, Shana K. and Cepeda, Nicholas J. and Rohrer, Doug and Kang, Sean H. K. and Pashler, Harold},
  biburl = {https://www.bibsonomy.org/bibtex/2d08f70d9c638718f3e17aaf05bef6311/vngudivada},
  interhash = {d5b29cdb00bc2473c5f4e031d13ee8d0},
  intrahash = {d08f70d9c638718f3e17aaf05bef6311},
  journal = {Educational Psychology Review},
  keywords = {InterleavedPractice Learning},
  number = 3,
  pages = {369--378},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Using Spacing to Enhance Diverse Forms of Learning: Review of Recent Research and Implications for Instruction},
  url = {http://dx.doi.org/10.1007/s10648-012-9205-z},
  volume = 24,
  year = 2012
}

@article{klein2006making,
  abstract = {In this paper, we have laid out a theory of sensemaking that might be useful for intelligent systems applications. It's a general, empirically grounded account of sensemaking that goes significantly beyond the myths and puts forward some nonobvious, testable hypotheses about the process. When people try to make sense of events, they begin with some perspective, viewpoint, or framework - however minimal. For now, let's use a metaphor and call this a frame. We can express frames in various meaningful forms, including stories, maps, organizational diagrams, or scripts, and can use them in subsequent and parallel processes. Even though frames define what count as data, they themselves actually shape the data Furthermore, frames change as we acquire data. In other words, this is a two-way street: Frames shape and define the relevant data, and data mandate that frames change in nontrivial ways. We examine five areas of empirical findings: causal reasoning, commitment to hypotheses, feedback and learning, sense-making as a skill, and confirmation bias. In each area the Data/Frame model, and the research it's based on, doesn't align with common beliefs. For that reason, the Data/Frame model cannot be considered a depiction of commonsense views.},
  added-at = {2017-01-30T16:34:16.000+0100},
  author = {Klein and Moon and Hoffman},
  biburl = {https://www.bibsonomy.org/bibtex/2285ebb987eb081363b106d1d9c9fc25f/vngudivada},
  interhash = {38226b43ff35d7a14498af72fb92d5ef},
  intrahash = {285ebb987eb081363b106d1d9c9fc25f},
  journal = {{IEEE} Intelligent Systems},
  keywords = {SenseMaking},
  month = oct,
  number = 5,
  pages = {88--92},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Making Sense of Sensemaking 2: A Macrocognitive Model},
  url = {https://doi.org/10.1109%2Fmis.2006.100},
  volume = 21,
  year = 2006
}

@book{nilson2016teaching,
  abstract = {Teaching at Its Best is the bestselling, research-based toolbox for college instructors at any level, in any higher education setting. Packed with practical guidance, proven techniques, and expert perspectives, this book helps instructors improve student learning both face-to-face and online. This new fourth edition features five new chapters on building critical thinking into course design, creating a welcoming classroom environment, helping students learn how to learn, giving and receiving feedback, and teaching in multiple modes, along with the latest research and new questions to facilitate faculty discussion. Topics include new coverage of the flipped classroom, cutting-edge technologies, self-regulated learning, the mental processes involved in learning and memory, and more, in the accessible format and easy-to-understand style that has made this book a much-valued resource among college faculty. Good instructors are always looking for ways to improve student learning. With college classrooms becoming increasingly varied by age, ability, and experience, the need for fresh ideas and techniques has never been greater. This book provides a wealth of research-backed practices that apply across the board.},
  added-at = {2017-02-11T22:58:04.000+0100},
  address = {San Francisco, CA},
  author = {Nilson, Linda Burzotta},
  biburl = {https://www.bibsonomy.org/bibtex/29a501bcdbad687fcea814d6081faf5ba/vngudivada},
  edition = {Fourth},
  interhash = {4b3f2b7b4f5143d9c45c30def02544f7},
  intrahash = {9a501bcdbad687fcea814d6081faf5ba},
  keywords = {Book TeachingPractice},
  publisher = {Jossey-Bass},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Teaching at its best: a research-based resource for college instructors},
  year = 2016
}

@book{kulkarni2013elementary,
  abstract = {A thought-provoking look at statistical learning theory and its role in understanding human learning and inductive reasoning A joint endeavor from leading researchers in the fields of philosophy and electrical engineering, An Elementary Introduction to Statistical Learning Theory is a comprehensive and accessible primer on the rapidly evolving fields of statistical pattern recognition and statistical learning theory. Explaining these areas at a level and in a way that is not often found in other books on the topic, the authors present the basic theory behind contemporary machine learning and uniquely utilize its foundations as a framework for philosophical thinking about inductive inference. Promoting the fundamental goal of statistical learning, knowing what is achievable and what is not, this book demonstrates the value of a systematic methodology when used along with the needed techniques for evaluating the performance of a learning system. First, an introduction to machine learning is presented that includes brief discussions of applications such as image recognition, speech recognition, medical diagnostics, and statistical arbitrage. To enhance accessibility, two chapters on relevant aspects of probability theory are provided. Subsequent chapters feature coverage of topics such as the pattern recognition problem, optimal Bayes decision rule, the nearest neighbor rule, kernel rules, neural networks, support vector machines, and boosting. Appendices throughout the book explore the relationship between the discussed material and related topics from mathematics, philosophy, psychology, and statistics, drawing insightful connections between problems in these areas and statistical learning theory. All chapters conclude with a summary section, a set of practice questions, and a reference sections that supplies historical notes and additional resources for further study. An Elementary Introduction to Statistical Learning Theory is an excellent book for courses on statistical learning theory, pattern recognition, and machine learning at the upper-undergraduate and graduatelevels. It also serves as an introductory reference for researchers and practitioners in the fields of engineering, computer science, philosophy, and cognitive science that would like to further their knowledge of the topic.},
  added-at = {2017-03-11T02:23:08.000+0100},
  address = {Hoboken, N.J.},
  author = {Kulkarni, Sanjeev and Harman, Gilbert},
  biburl = {https://www.bibsonomy.org/bibtex/2dfa0a2e4eee7a7c054eb8b7d9e0290e8/vngudivada},
  interhash = {07015aad93591bd46c83e1571ac2c212},
  intrahash = {dfa0a2e4eee7a7c054eb8b7d9e0290e8},
  keywords = {Book ML StatisticalLearning},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {An elementary introduction to statistical learning theory},
  year = 2013
}

@book{manning2008introduction,
  abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
  added-at = {2017-03-05T03:50:42.000+0100},
  address = {New York, NY},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
  biburl = {https://www.bibsonomy.org/bibtex/28516d94c1f7aa1e391ddd3ace4caa23b/vngudivada},
  interhash = {b6954037b1d444f4afe4cad883b4d80c},
  intrahash = {8516d94c1f7aa1e391ddd3ace4caa23b},
  isbn = {978-0521865715},
  keywords = {Book IR},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Introduction to Information Retrieval},
  year = 2008
}

@book{carey2015learn,
  abstract = {
In the tradition of The Power of Habit and Thinking, Fast and Slow comes a practical, playful, and endlessly fascinating guide to what we really know about learning and memory today—and how we can apply it to our own lives.

From an early age, it is drilled into our heads: Restlessness, distraction, and ignorance are the enemies of success. We’re told that learning is all self-discipline, that we must confine ourselves to designated study areas, turn off the music, and maintain a strict ritual if we want to ace that test, memorize that presentation, or nail that piano recital.

But what if almost everything we were told about learning is wrong? And what if there was a way to achieve more with less effort?

In How We Learn, award-winning science reporter Benedict Carey sifts through decades of education research and landmark studies to uncover the truth about how our brains absorb and retain information. What he discovers is that, from the moment we are born, we are all learning quickly, efficiently, and automatically; but in our zeal to systematize the process we have ignored valuable, naturally enjoyable learning tools like forgetting, sleeping, and daydreaming. Is a dedicated desk in a quiet room really the best way to study? Can altering your routine improve your recall? Are there times when distraction is good? Is repetition necessary? Carey’s search for answers to these questions yields a wealth of strategies that make learning more a part of our everyday lives—and less of a chore.

By road testing many of the counterintuitive techniques described in this book, Carey shows how we can flex the neural muscles that make deep learning possible. Along the way he reveals why teachers should give final exams on the first day of class, why it is wise to interleave subjects and concepts when learning any new skill, and when it is smarter to stay up late prepping for that presentation than to rise early for one last cram session. And if this requires some suspension of disbelief, that’s because the research defies what we’ve been told, throughout our lives, about how best to learn.

The brain is not like a muscle, at least not in any straightforward sense. It is something else altogether, sensitive to mood, to timing, to circadian rhythms, as well as to location and environment. It does not take orders well, to put it mildly. If the brain is a learning machine, then it is an eccentric one. In How We Learn, Benedict Carey shows us how to exploit its quirks to our advantage.

Praise for How We Learn

This book is a revelation. I feel as if I have owned a brain for fifty-four years and only now discovered the operating manual. -- Mary Roach, bestselling author of Stiff and Gulp

A welcome rejoinder to the faddish notion that learning is all about the hours put in.  -- The New York Times Book Review

A valuable, entertaining tool for educators, students and parents. -- Shelf Awareness

How We Learn is more than a new approach to learning; it is a guide to making the most out of life. Who wouldn’t be interested in that? -- Scientific American

I know of no other source that pulls together so much of what we know about the science of memory and couples it with practical, practicable advice. -- Daniel T. Willingham, professor of psychology at the University of Virginia},
  added-at = {2017-03-10T03:49:35.000+0100},
  author = {Carey, Benedict},
  biburl = {https://www.bibsonomy.org/bibtex/2908c9a3f706e9a5ea7d2fcafb9646b2f/vngudivada},
  interhash = {dde5e79825e64edb4be6e6e5dd954246},
  intrahash = {908c9a3f706e9a5ea7d2fcafb9646b2f},
  keywords = {Book Learning},
  publisher = {Random House},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {How we learn: the surprising truth about when, where and why it happens},
  year = 2015
}

@article{carvalho2014effects,
  abstract = {Studying different concepts by frequently alternating between them (i.e., interleaving), improves discriminative contrast between different categories, while studying each concept in separate blocks emphasizes the similarities within each category. Interleaved study has been shown to improve learning of high similarity categories by increasing between-category comparison, while blocked study improves learning of low similarity categories by increasing within-category comparison.  In addition, interleaved study presents greater temporal spacing between repetitions of each category compared to blocked study, which might present long-term memory benefits. In this study we asked if the benefits of temporal spacing would interact with the benefits of sequencing for making comparisons when testing was delayed, particularly for low similarity categories. Blocked study might be predicted to promote noticing similarities across members of the same category and result in short-term benefits. However, the increase in temporal delay between repetitions inherent to interleaved study might benefit both types of categories when tested after a longer retention interval. Participants studied categories either interleaved or blocked and were tested immediately and 24 hours after study. We found an interaction between schedule of study and the type of category studied, which is consistent with the differential emphasis promoted by each sequential schedule. However, increasing the retention interval did not modulate this interaction or resulted in improved performance for interleaved study. Overall, this indicates that the benefit of interleaving is not primarily due to temporal spacing during study, but rather due to the cross-category comparisons that interleaving facilitates. We discuss the benefits of temporal spacing of repetitions in the context of sequential study and how it can be integrated with the attentional bias hypothesis proposed by Carvalho and Goldstone (2014).},
  added-at = {2017-03-12T01:19:50.000+0100},
  author = {Carvalho, Paulo F. and Goldstone, Robert L.},
  biburl = {https://www.bibsonomy.org/bibtex/2631d5077b1e33c95e0db12abf435bf57/vngudivada},
  interhash = {4707efd87011f4e78a84ab33b4fc67c0},
  intrahash = {631d5077b1e33c95e0db12abf435bf57},
  journal = {Frontiers in Psychology},
  keywords = {InterleavedPractice Learning},
  pages = 936,
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Effects of interleaved and blocked study on delayed test of category learning generalization},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2014.00936},
  volume = 5,
  year = 2014
}

@article{chali2015towards,
  abstract = {This paper is concerned with automatic generation of all possible questions from a topic of interest. Specifically, we consider that each topic is associated with a body of texts containing useful information about the topic. Then, questions are generated by exploiting the named entity information and the predicate argument structures of the sentences present in the body of texts. The importance of the generated questions is measured using Latent Dirichlet Allocation by identifying the subtopics (which are closely related to the original topic) in the given body of texts and applying the Extended String Subsequence Kernel to calculate their similarity with the questions. We also propose the use of syntactic tree kernels for the automatic judgment of the syntactic correctness of the questions. The questions are ranked by considering both their importance (in the context of the given body of texts) and syntactic correctness. To the best of our knowledge, no previous study has accomplished this task in our setting. A series of experiments demonstrate that the proposed topic-to-question generation approach can significantly outperform the state-of-the-art results.},
  added-at = {2017-01-30T13:42:36.000+0100},
  author = {Chali, Yllias and Hasan, Sadid A.},
  biburl = {https://www.bibsonomy.org/bibtex/24813c373611d531e252985bcc1734352/vngudivada},
  interhash = {6e4455f6b9007292b4b8dee235a3a719},
  intrahash = {4813c373611d531e252985bcc1734352},
  journal = {Computational Linguistics},
  keywords = {QuestionGeneration},
  month = mar,
  number = 1,
  pages = {1--20},
  publisher = {{MIT} Press Journals},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Towards Topic-to-Question Generation},
  url = {https://doi.org/10.1162%2Fcoli_a_00206},
  volume = 41,
  year = 2015
}

@article{karpicke2009metacognitive,
  abstract = {Basic research on human learning and memory has shown that practicing retrieval of information (by testing the information) has powerful effects on learning and long-term retention. Repeated testing enhances learning more than repeated reading, which often confers limited benefit beyond that gained from the initial reading of the material. Laboratory research also suggests that students lack metacognitive awareness of the mnemonic benefits of testing. The implication is that in real-world educational settings students may not engage in retrieval practise to enhance learning. To investigate students’ real-world study behaviours, we surveyed 177 college students and asked them (1) to list strategies they used when studying (an open-ended free report question) and (2) to choose whether they would reread or practise recall after studying a textbook chapter (a forced report question). The results of both questions point to the same conclusion: A majority of students repeatedly read their notes or textbook (despite the limited benefits of this strategy), but relatively few engage in self-testing or retrieval practise while studying. We propose that many students experience illusions of competence while studying and that these illusions have significant consequences for the strategies students select when they monitor and regulate their own learning. },
  added-at = {2017-03-12T00:38:33.000+0100},
  author = {Karpicke, Jeffrey D. and Butler, Andrew C. and III, Henry L. Roediger},
  biburl = {https://www.bibsonomy.org/bibtex/2d4927e03d245599d995ee508aabf41af/vngudivada},
  interhash = {094f28e189ac1b486047595a13574fdf},
  intrahash = {d4927e03d245599d995ee508aabf41af},
  journal = {Memory},
  keywords = {Learning RetrievalPractice},
  number = 4,
  pages = {471-479},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Metacognitive strategies in student learning: Do students practise retrieval when they study on their own?},
  url = {http://dx.doi.org/10.1080/09658210802647009},
  volume = 17,
  year = 2009
}

@book{mackay2016information,
  abstract = {Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography. This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks. The final part of the book describes the state of the art in error-correcting codes, including low-density parity-check codes, turbo codes, and digital fountain codes -- the twenty-first century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, David MacKay's groundbreaking book is ideal for self-learning and for undergraduate or graduate courses. Interludes on crosswords, evolution, and sex provide entertainment along the way. In sum, this is a textbook on information, communication, and coding for a new generation of students, and an unparalleled entry point into these subjects for professionals in areas as diverse as computational biology, financial engineering, and machine learning.},
  added-at = {2017-03-07T23:52:44.000+0100},
  address = {Cambridge, U.K.},
  author = {MacKay, David J. C.},
  biburl = {https://www.bibsonomy.org/bibtex/29eb4efd9edf619629dbff1747174662f/vngudivada},
  interhash = {a3797d2f21ab30d895fbc05b88f5c388},
  intrahash = {9eb4efd9edf619629dbff1747174662f},
  keywords = {Book InformationTheory ML},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Information theory, inference, and learning algorithms},
  year = 2016
}

@book{bean2013engaging,
  abstract = {Learn to design interest-provoking writing and critical thinking activities and incorporate them into your courses in a way that encourages inquiry, exploration, discussion, and debate, with Engaging Ideas, a practical nuts-and-bolts guide for teachers from any discipline. Integrating critical thinking with writing-across-the-curriculum approaches, the book shows how teachers from any discipline can incorporate these activities into their courses. This edition features new material dealing with genre and discourse community theory, quantitative/scientific literacy, blended and online learning, and other current issues.},
  added-at = {2017-03-11T13:59:28.000+0100},
  address = {San Francisco, California},
  author = {Bean, John C.},
  biburl = {https://www.bibsonomy.org/bibtex/252974d66158eaf1de1d04a191c33ad75/vngudivada},
  edition = {Second},
  interhash = {4209fbe7749c0c2f22e6c328eceba896},
  intrahash = {52974d66158eaf1de1d04a191c33ad75},
  keywords = {Book Learning StudentEngagement},
  publisher = {Jossey-Bass},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Engaging ideas: the professor's guide to integrating writing, critical thinking, and active learning in the classroom},
  year = 2013
}

@article{moussa2012consistency,
  abstract = {At rest, spontaneous brain activity measured by fMRI is summarized by a number of distinct resting state networks (RSNs) following similar temporal time courses. Such networks have been consistently identified across subjects using spatial ICA (independent component analysis). Moreover, graph theory-based network analyses have also been applied to resting-state fMRI data, identifying similar RSNs, although typically at a coarser spatial resolution. In this work, we examined resting-state fMRI networks from 194 subjects at a voxel-level resolution, and examined the consistency of RSNs across subjects using a metric called scaled inclusivity (SI), which summarizes consistency of modular partitions across networks. Our SI analyses indicated that some RSNs are robust across subjects, comparable to the corresponding RSNs identified by ICA. We also found that some commonly reported RSNs are less consistent across subjects. This is the first direct comparison of RSNs between ICAs and graph-based network analyses at a comparable resolution.},
  added-at = {2017-03-11T20:19:01.000+0100},
  author = {Moussa, Malaak N. and Steen, Matthew R. and Laurienti, Paul J. and Hayasaka, Satoru},
  biburl = {https://www.bibsonomy.org/bibtex/2d603e11215060453d2885ca0887390b2/vngudivada},
  interhash = {5ede504f51a0555ca260796e70512984},
  intrahash = {d603e11215060453d2885ca0887390b2},
  journal = {PLOS ONE},
  keywords = {Learning fMRI},
  month = {08},
  number = 8,
  pages = {1-14},
  publisher = {Public Library of Science},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Consistency of Network Modules in Resting-State {fMRI} Connectome Data},
  url = {http://dx.doi.org/10.1371%2Fjournal.pone.0044428},
  volume = 7,
  year = 2012
}

@book{lesk2014introduction,
  abstract = {Fully revised and updated, the fourth edition of Introduction to Bioinformatics shows how bioinformatics can be used as a powerful set of tools for retrieving and analyzing this biological data, and how bioinformatics can be applied to a wide range of disciplines such as molecular biology, medicine, biotechnology, forensic science, and anthropology. This new edition contains two new chapters, with significantly increased coverage of metabolic pathways, and gene expression and regulation.

Written for students without a detailed prior knowledge of programming, this book is the perfect introduction to the field of bioinformatics, providing friendly guidance and advice on how to use various methods and techniques. Additionally, frequent examples, self-test questions, problems, and exercises are incorporated throughout the text to encourage self-directed learning.},
  added-at = {2017-03-11T14:21:56.000+0100},
  address = {Cambridge, MA},
  author = {Lesk, Arthur},
  biburl = {https://www.bibsonomy.org/bibtex/2202a7302384443caac04a605b6c59ed5/vngudivada},
  edition = {Fourth},
  interhash = {bf06be0b083a45c5e7ffe31c65b424e2},
  intrahash = {202a7302384443caac04a605b6c59ed5},
  keywords = {Bioinformatics Book},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Introduction to Bioinformatics},
  year = 2014
}

@article{karpicke2008critical,
  abstract = {Learning is often considered complete when a student can produce the correct answer to a question. In our research, students in one condition learned foreign language vocabulary words in the standard paradigm of repeated study-test trials. In three other conditions, once a student had correctly produced the vocabulary item, it was repeatedly studied but dropped from further testing, repeatedly tested but dropped from further study, or dropped from both study and test. Repeated studying after learning had no effect on delayed recall, but repeated testing produced a large positive effect. In addition, students{\textquoteright} predictions of their performance were uncorrelated with actual performance. The results demonstrate the critical role of retrieval practice in consolidating learning and show that even university students seem unaware of this fact.},
  added-at = {2017-03-12T00:28:43.000+0100},
  author = {Karpicke, Jeffrey D. and Roediger, Henry L.},
  biburl = {https://www.bibsonomy.org/bibtex/2b597639cfd542f9450e4e2f028121806/vngudivada},
  doi = {10.1126/science.1152408},
  interhash = {dbf12a7552796196d528dee29d50d02e},
  intrahash = {b597639cfd542f9450e4e2f028121806},
  journal = {Science},
  keywords = {Learning},
  number = 5865,
  pages = {966--968},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {The Critical Importance of Retrieval for Learning},
  url = {http://science.sciencemag.org/content/319/5865/966},
  volume = 319,
  year = 2008
}

@article{pan2015interleaving,
  abstract = {Whereas blocking involves practicing one skill at a time before the next (for example, “skill A” before “skill B” and so on, forming the pattern “AAABBBCCC”), in interleaving one mixes, or interleaves, practice on several related skills together (forming for example the pattern “ABCABCABC”). For instance, a pianist alternates practice between scales, chords, and arpeggios.  Credit: Thinkstock
We’ve all heard the adage: practice makes perfect! In other words, acquiring skills takes time and effort. But how exactly does one go about learning a complex subject such as tennis, calculus, or even how to play the violin? An age-old answer is: practice one skill at a time. A beginning pianist might rehearse scales before chords. A young tennis player practices the forehand before the backhand. Learning researchers call this “blocking,” and because it is commonsensical and easy to schedule, blocking is dominant in schools, training programs, and other settings.
However another strategy promises improved results. Enter “interleaving,” a largely unheard-of technique that is capturing the attention of cognitive psychologists and neuroscientists. Whereas blocking involves practicing one skill at a time before the next (for example, “skill A” before “skill B” and so on, forming the pattern “AAABBBCCC”), in interleaving one mixes, or interleaves, practice on several related skills together (forming for example the pattern “ABCABCABC”). For instance, a pianist alternates practice between scales, chords, and arpeggios, while a tennis player alternates practice between forehands, backhands, and volleys.
Over the past four decades, a small but growing body of research has found that interleaving often outperforms blocking for a variety of subjects, including sports and category learning. Yet there have been almost no studies of the technique in uncontrived, real world settings—until recently. New research in schools finds that interleaving produces dramatic and long-lasting benefits for an essential skill: math. Not only does this finding have the potential to transform how math is taught, it may also change how people learn more generally.
The first signs of interleaving’s promise appeared in the domain of motor skills. One early study, published in 1986, involved training students to learn three types of badminton serves. Compared with blocking, interleaving produced better recall of each serve type and better ability to handle new situations, such as serving from the opposite side of the court. Similar results were later reported for baseball, basketball, and other sports. In 2003, one of the first studies to examine interleaving outside of sports found that using it to train medical students produced more accurate electrocardiogram diagnoses than blocking. In 2008, another widely-cited study found a similar benefit for teaching college students to recognize the painting styles of landscape artists. Even critical thinking skills benefit: in a 2011 study, students trained with the technique made more accurate assessments of complex legal scenarios.
Foreign language studies however suggest that the effectiveness of interleaving comes with an important caveat. When native English speakers used the technique to learn an entirely unfamiliar language, such as to generate English-to-Swahili translations, the results were better, the same, or worse than after blocking. These mixed results imply that learners should have some familiarity with subject materials before interleaving begins (or, the materials should be quickly or easily understood). Otherwise, as appears to be the case for foreign languages, interleaving can sometimes be more confusing than helpful.
Given interleaving’s promise, it is surprising then that few studies have investigated its utility in everyday applications. However, a new study by cognitive psychologist Doug Rohrer and colleagues at the University of South Florida, recently published in the Journal of Educational Psychology, takes a step towards addressing that gap. Rohrer and his team are the first to implement interleaving in actual classrooms. The location: middle schools in Tampa, Florida. The target skills: algebra and geometry.
The three-month study involved teaching 7th graders slope and graph problems. Weekly lessons, given by teachers, were largely unchanged from standard practice. Weekly homework worksheets, however, featured an interleaved or blocked design. When interleaved, both old and new problems of different types were mixed together. Of the nine participating classes, five used interleaving for slope problems and blocking for graph problems; the reverse occurred in the remaining four. Five days after the last lesson, each class held a review session for all students. A surprise final test occurred one day or one month later. The result? When the test was one day later, scores were 25 percent better for problems trained with interleaving; at one month later, the interleaving advantage grew to 76 percent.
These results are important for a host of reasons. First, they show that interleaving works in real-world, extended use. It is highly effective with an almost ubiquitous subject, math. The interleaving effect is long-term—lasting on the order of months—and the advantage over blocking actually increases with the passage of time (in other words, there’s less forgetting). The benefit even persists when blocked materials receive additional review. Overall, the interleaving effect can be strong, stable, and long-lasting.
Clearly interleaving does wonders for 7th grade math. Moreover, when combined with prior work showing similar benefits of the technique across a spectrum of topics (algebra, exponents, proportions, prisms, and volumes) and with students at different grade levels (elementary through college), interleaving may turn out to be among the most effective math learning techniques.
Researchers are now working to understand why interleaving yields such impressive results. One prominent explanation is that it improves the brain’s ability to tell apart, or discriminate, between concepts. With blocking, once you know what solution to use, or movement to execute, the hard part is over. With interleaving, each practice attempt is different from the last, so rote responses don’t work. Instead, your brain must continuously focus on searching for different solutions. That process can improve your ability to learn critical features of skills and concepts, which then better enables you to select and execute the correct response.
A second explanation is that interleaving strengthens memory associations. With blocking, a single strategy, temporarily held in short-term memory, is sufficient. That’s not the case with interleaving—the correct solution changes from one practice attempt to the next. As a result, your brain is continually engaged at retrieving different responses and bringing them into short-term memory. Repeating that process can reinforce neural connections between different tasks and correct responses, which enhances learning.
Both of these accounts imply that increased effort during training, either to discriminate correct responses or to strengthen them, is needed when interleaving is used. This corresponds to a potential drawback of the technique, namely that the learning process often feels more gradual and difficult at the outset. However, that added effort can generate better, longer-lasting results.
In modern society there is tremendous interest in ways to enhance learning and memory: brain training, learning apps, and so on. Interleaving has the benefit of scientific evidence in favor of its use across a range of circumstances. It also has the practical advantage of requiring no extra training, extra time, or special equipment to work. Only more careful planning is required, and possibly some extra effort at the outset.
Despite these relative advantages, interleaving remains mostly unknown and unused. Consider the example of grade school math. Out of all the math textbooks used in the U.S. today, all but one type—the Saxon series—uses blocked practice. One can only speculate on what would happen if interleaving were widely used in classrooms and in textbooks. The differences in academic achievement could be substantial.
As interleaving research progresses, we stand to learn much more about the technique: other areas where it works, or doesn’t, and what other limitations it might have. Yet that doesn’t preclude us from putting it to the test right now. For instance, are you studying statistics? Learning to play an instrument? Taking up a new sport? In all of these areas, you are faced with a series of skills or concepts to learn. The typical response would be to practice each of these, one at a time, over and over. Another option would be to mix it up. As it turns out, your brain may prefer doing exactly that.},
  added-at = {2017-03-12T01:30:25.000+0100},
  author = {Pan, Steven C.},
  biburl = {https://www.bibsonomy.org/bibtex/291de6d0b0adc9c585004ef177c3ccd8a/vngudivada},
  interhash = {b97320fe834b1646b32f7b7f74c17ca8},
  intrahash = {91de6d0b0adc9c585004ef177c3ccd8a},
  journal = {Scientific American},
  keywords = {InterleavedPractice Learning},
  number = 2,
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {The Interleaving Effect: Mixing It Up Boosts Learning},
  volume = 313,
  year = 2015
}

@book{heath2007stick,
  abstract = {Mark Twain once observed, “A lie can get halfway around the world before the truth can even get its boots on.” His observation rings true: Urban legends, conspiracy theories, and bogus public-health scares circulate effortlessly. Meanwhile, people with important ideas–business people, teachers, politicians, journalists, and others–struggle to make their ideas “stick.”

Why do some ideas thrive while others die? And how do we improve the chances of worthy ideas? In Made to Stick, accomplished educators and idea collectors Chip and Dan Heath tackle head-on these vexing questions. Inside, the brothers Heath reveal the anatomy of ideas that stick and explain ways to make ideas stickier, such as applying the “human scale principle,” using the “Velcro Theory of Memory,” and creating “curiosity gaps.”

In this indispensable guide, we discover that sticky messages of all kinds–from the infamous “kidney theft ring” hoax to a coach’s lessons on sportsmanship to a vision for a new product at Sony–draw their power from the same six traits.

Made to Stick is a book that will transform the way you communicate ideas. It’s a fast-paced tour of success stories (and failures)–the Nobel Prize-winning scientist who drank a glass of bacteria to prove a point about stomach ulcers; the charities who make use of “the Mother Teresa Effect”; the elementary-school teacher whose simulation actually prevented racial prejudice. Provocative, eye-opening, and often surprisingly funny, Made to Stick shows us the vital principles of winning ideas–and tells us how we can apply these rules to making our own messages stick.

Brothers Chip and Dan Heath in their New York Time’s best-selling book, “Made to Stick,” explore the stickiness of an idea. Those of us who spend time in the start-up world marvel at why one idea gains traction and other, seemingly better ideas, fall to wayside. The Heath brothers provide insights on this phenomenon and provide help for those bent on creating ideas that are “sticky.”

“Sticky” ideas are understandable, memorable, and effective in changing thought or behavior. The six underlying SUCCESs principles for making things “stick” are:

• Simplicity – Simple=core+compact. Find and share your core idea; make it simple and profound. “It’s the economy, stupid” (Clinton campaign, 1992) is a great example. The inverted pyramid approach which is used in journalism is a good tool to get your headline.
• Unexpectedness - We need to violate people’s expectations to get them to pay attention. Break existing patterns to get people’s attention. Southwest flight attendants use humor (there are two doors on either side if you need to jump!) to hold attention when giving the pre-flight safety announcement. Humans adapt incredibly quickly to patterns. Consistent sensory stimulation makes us tune out.
• Concreteness – You must help people understand and remember. Don’t use abstractions. Make your core idea concrete. Use common knowledge to make your idea stick. Our greatest villain is the Curse of Knowledge or when we assume everyone knows what we know or shares our unique perspective. We have to see it from the “others” point of view. We forget what other people do not know and slip into “abstractspeak.” Boeing’s criteria for a new plane was not “the best passenger plane in the world” but one that can seat 131 passengers and land on Runway 2-22 at LaGuardia. No ambiguity here.
• Credibility – Help people believe by making sure your idea carries its own credentials. Pass the “Sinatra Test.” Examples offered include “Where’s the Beef?” and Reagan’s “Are you better off today?” Both were credible and resonated as they were based on common shared knowledge.
• Emotional– Make people care by using the power of association, appealing to self-interest, or identity. “People donate to Rokia more than a wide swath of Africa”; “Honoring the Game” versus the use of the word ‘sportsmanship’; “I’m in charge of morale” as stated by a US military cook in Iraq. We must make people feel something to get them to care. We are wired to feel things, not abstractions.
• Stories – Stories get people to act on our ideas. Stories act as a kind of mental flight simulator, preparing us to respond more quickly and effectively. Stories are told and retold because they contain wisdom. The Healths provide what they view are the three basic story plots – the Challenge Plot, The Connection Plot, and the Creativity Plot. Stories can almost single-handedly defeat “The Curse of Knowledge.” I have been involved in a ministry for people in career-transition for over fifteen years. We consistently advise those in-transitions to create stories to highlight their skills and experience when interviewing. It is well understood that interviewers will mostly remember your comportment and more importantly, your stories.

A chapter is devoted to each principle with the authors providing context for clarity and understanding, examples, and tools to guide the development of a “sticky” idea.

The Curse of Knowledge is what escapes most when trying to pitch an idea. It is the natural psychological tendency that consistently gets in the way of our ability to successfully create “sticky ideas” using these principles. Once we know something, we find it hard to imagine what it was like not to know. This knowledge has “cursed” us and makes it difficult to share our knowledge with others. It is because we cannot readily re-create our listeners state of mind. When a CEO discusses “unlocking shareholder value,” there is a tune playing in his head that the employees can’t hear. On the other hand, President John F. Kennedy knew that opaque, abstract missions don’t captivate and inspire people so he concretely challenged the country with “landing on the moon by the end of the decade.”

Throughout the book, the authors present “Idea Clinics” which illustrate how an idea can be made stickier. Example: ”Do smokers really need to understand the workings of the lungs in order to appreciate the dangers of smoking?”

The book itself is “sticky’ filled with stories of normal people facing normal problems who did an amazing thing simply by applying these principles, even if they were not aware that they were doing this. They distinguish themselves by crafting ideas that made a difference.

Do your ideas gain traction and “Stick” or are they cast aside for less important ideas? “Made to Stick” was written for you.},
  added-at = {2017-03-10T23:59:00.000+0100},
  author = {Heath, Chip and Heath, Dan},
  biburl = {https://www.bibsonomy.org/bibtex/28081c194718406357fa6595d204cde6e/vngudivada},
  interhash = {ddd1ed42c154498456b15712b0a3bf19},
  intrahash = {8081c194718406357fa6595d204cde6e},
  keywords = {Book Leadership},
  publisher = {Random House},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Made to Stick: Why Some Ideas Survive and Others Die},
  year = 2007
}

@inproceedings{rus2011question,
  abstract = {The First Shared Task Evaluation Challenge on Question Generation took place in 2010 as part of the 3rd workshop on Question Generation. The campaign included two tasks: Question Generation from Sentences and Question Generation from Paragraphs. This status report briefly summarizes the motivation, tasks and results. Lessons learned relevant to future QG-STECs are also offered.},
  added-at = {2017-02-13T00:20:18.000+0100},
  address = {Stroudsburg, PA},
  author = {Rus, Vasile and Piwek, Paul and Stoyanchev, Svetlana and Wyse, Brendan and Lintean, Mihai and Moldovan, Cristian},
  biburl = {https://www.bibsonomy.org/bibtex/25da5df9ab17abbd76694c4f53aa2ec3e/vngudivada},
  booktitle = {Proceedings of the $13^{th}$ European Workshop on Natural Language Generation},
  interhash = {fbb12a1d74a54c35c94e438f81125c1a},
  intrahash = {5da5df9ab17abbd76694c4f53aa2ec3e},
  keywords = {EvaluationChallenge QuestionGeneration},
  pages = {318--320},
  publisher = {Association for Computational Linguistics},
  series = {ENLG '11},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Question Generation Shared Task and Evaluation Challenge: Status Report},
  year = 2011
}

@article{fan2012automatic,
  abstract = {Access to a large amount of knowledge is critical for success at answering open-domain questions for DeepQA systems such as IBM Watson™. Formal representation of knowledge has the advantage of being easy to reason with, but acquisition of structured knowledge in open domains from unstructured data is often difficult and expensive. Our central hypothesis is that shallow syntactic knowledge and its implied semantics can be easily acquired and can be used in many areas of a question-answering system. We take a two-stage approach to extract the syntactic knowledge and implied semantics. First, shallow knowledge from large collections of documents is automatically extracted. Second, additional semantics are inferred from aggregate statistics of the automatically extracted shallow knowledge. In this paper, we describe in detail what kind of shallow knowledge is extracted, how it is automatically done from a large corpus, and how additional semantics are inferred from aggregate statistics. We also briefly discuss the various ways extracted knowledge is used throughout the IBM DeepQA system.},
  added-at = {2017-01-30T16:17:35.000+0100},
  author = {Fan, J. and Kalyanpur, A. and Gondek, D. C. and Ferrucci, D. A.},
  biburl = {https://www.bibsonomy.org/bibtex/23a1db065c8f0cc80008576e8d7ea0d58/vngudivada},
  interhash = {48ac522100ca9e0703f1285a8c5f2dfb},
  intrahash = {3a1db065c8f0cc80008576e8d7ea0d58},
  journal = {{IBM} Journal of Research and Development},
  keywords = {KnowledgeExtraction QuestionGeneration},
  month = may,
  number = {3.4},
  pages = {5:1--5:10},
  publisher = {{IBM}},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Automatic knowledge extraction from documents},
  url = {https://doi.org/10.1147%2Fjrd.2012.2186519},
  volume = 56,
  year = 2012
}

@book{lang2016small,
  abstract = {Research into how we learn has opened the door for utilizing cognitive theory to facilitate better student learning. But that's easier said than done. Many books about cognitive theory introduce radical but impractical theories, failing to make the connection to the classroom. In Small Teaching, James Lang presents a strategy for improving student learning with a series of modest but powerful changes that make a big difference--many of which can be put into practice in a single class period. These strategies are designed to bridge the chasm between primary research and the classroom environment in a way that can be implemented by any faculty in any discipline, and even integrated into pre-existing teaching techniques. Learn, for example: How does one become good at retrieving knowledge from memory? How does making predictions now help us learn in the future? How do instructors instill fixed or growth mindsets in their students? Each chapter introduces a basic concept in cognitive theory, explains when and how it should be employed, and provides firm examples of how the intervention has been or could be used in a variety of disciplines. Small teaching techniques include brief classroom or online learning activities, one-time interventions, and small modifications in course design or communication with students.
Cognitive psychologists, neuroscientists, and biologists all have produced a revealing body of research over the past several decades on how human beings learn, but often translating these findings into the classroom is overwhelming for busy instructors. Small Teaching bridges the gap between research and practice by providing a fully developed strategy for making deliberate, structured, and incremental steps towards tuning into how your students are hardwired to learn.

Developed by a global authority on teaching and learning, the practice of "small teaching" enables every type of educator in all disciplines to energize and boost student understanding by introducing small activities that require minimal preparation and grading. The models inside are specifically designed to be used as both one-time experiences to innovate a course session or unit plan as well as a menu of options that can be combined into an entirely new teaching approach. Each chapter gives examples of how a particular learning phenomenon appears in everyday life, supporting research and findings, up to five small-teaching models, guidance for customizing your own models, and quick-reference features when you need inspiration fifteen minutes before class. Small teaching techniques include brief classroom or online learning activities, one-time interventions, and small modifications in course design or communication with students that let you:

Capture or recapture the students' attention, provide quick opportunities for student engagement, and introduce or wrap up new learning
Deepen student understanding of material and expand their ability to analyze and improve their own learning
Give students the tools, techniques, and principles to effectively practice a range of cognitive skills
Small Teaching gives you all the know-how and how-to for making big improvements to your learning environment.},
  added-at = {2017-02-11T21:42:26.000+0100},
  address = {San Francisco, CA},
  author = {Lang, James M.},
  biburl = {https://www.bibsonomy.org/bibtex/2f2260618a1fbba9239946589bd295737/vngudivada},
  edition = {First},
  interhash = {8ec6cca005055b13bf03a9a8d624a609},
  intrahash = {f2260618a1fbba9239946589bd295737},
  keywords = {Book Learning TeachingPractice},
  publisher = {Jossey-Bass},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Small teaching: everyday lessons from the science of learning},
  year = 2016
}

@inproceedings{mishra2008generating,
  abstract = {Tools for generating test queries for databases do not explicitly take into account the actual data in the database. As a consequence, such tools cannot guarantee suitable coverage of test cases commonly required for database testing. In this paper, we investigate the problem of generating queries that satisfy cardinality constraints on intermediate subexpressions when executed on a given test database. Such queries are required to test the performance of a database system under different operating conditions. We formally analyze this problem, quantify its difficulty and follow up this analysis with a description of a practical algorithm which utilizes sampling and space pruning techniques to quickly generate test queries that have desired properties. We present the results of an experimental evaluation of our approach as implemented in an open source data manager, demonstrating the utility of our proposal.},
  added-at = {2017-02-12T22:51:47.000+0100},
  address = {New York, NY, USA},
  author = {Mishra, Chaitanya and Koudas, Nick and Zuzarte, Calisto},
  biburl = {https://www.bibsonomy.org/bibtex/2dff21b943ac9c44ef43c339f5530bfdb/vngudivada},
  booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
  interhash = {7e4f8759554578d7867256510bb635b7},
  intrahash = {dff21b943ac9c44ef43c339f5530bfdb},
  keywords = {DBMSTesting QueryGeneration SQL},
  pages = {499--510},
  publisher = {ACM},
  series = {SIGMOD '08},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Generating Targeted Queries for Database Testing},
  url = {http://doi.acm.org/10.1145/1376616.1376668},
  year = 2008
}

@article{roediger2012inexpensive,
  abstract = {The need to improve the educational system has never been greater. People in congress and business argue for expensive technological applications to improve education despite a lack of empirical evidence for their efficacy. We argue that one inexpensive avenue for improving education has been largely ignored. Cognitive and educational psychologists have identified strategies that greatly improve learning and retention of information, and yet these techniques are not generally applied in education nor taught in education schools. In fact, teachers often use instructional practices known to be wrong (i.e., massing rather than interleaving examples to explain a topic). We identify three general principles that are inexpensive to implement and have been shown in both laboratory and field experiments to improve learning: (1) distribution (spacing and interleaving) of practice in learning facts and skills; (2) retrieval practice (via self testing) for durable learning; and (3) explanatory questioning (elaborative interrogation and self-explanation) as a study strategy. We describe each technique, provide supporting evidence, and discuss classroom applications. Each principle can be applied to most subject matters from kindergarten to higher education. Applying findings from cognitive psychology to classroom instruction is no panacea for educational problems, but it represents one helpful and inexpensive strategy.

Many advocate the use of expensive high tech tools to promote student learning, but these have not been empirically validated. ⿺ Three principles from cognitive psychology (spacing, testing, explanatory questioning) effectively enhance long-term learning. ⿺ These principles can be incorporated into classroom instruction and study routines at little or no financial cost.},
  added-at = {2017-03-12T01:02:22.000+0100},
  author = {Roediger, Henry L. and Pyc, Mary A.},
  biburl = {https://www.bibsonomy.org/bibtex/284d68bce97ec0f04673f5a686e0c5ec2/vngudivada},
  interhash = {27dfb8867adc5b80284483c6de0df4b2},
  intrahash = {84d68bce97ec0f04673f5a686e0c5ec2},
  journal = {Journal of Applied Research in Memory and Cognition},
  keywords = {CognitivePsychology Learning Memory},
  number = 4,
  pages = {242 - 248},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Inexpensive techniques to improve education: Applying cognitive psychology to enhance educational practice},
  url = {http://www.sciencedirect.com/science/article/pii/S2211368112000915},
  volume = 1,
  year = 2012
}

@inproceedings{brown2005automatic,
  abstract = {In the REAP system, users are automatically provided with texts to read targeted to their individual reading levels. To find appropriate texts, the user's vocabulary knowledge must be assessed. We describe an approach to automatically generating questions for vocabulary assessment. Traditionally, these assessments have been hand-written. Using data from WordNet, we generate 6 types of vocabulary questions. They can have several forms, including wordbank and multiple-choice. We present experimental results that suggest that these automatically-generated questions give a measure of vocabulary skill that correlates well with subject performance on independently developed human-written questions. In addition, strong correlations with standardized vocabulary tests point to the validity of our approach to automatic assessment of word knowledge.},
  added-at = {2017-02-12T05:47:16.000+0100},
  address = {Stroudsburg, PA, USA},
  author = {Brown, Jonathan C. and Frishkoff, Gwen A. and Eskenazi, Maxine},
  biburl = {https://www.bibsonomy.org/bibtex/2ab566e42cf13cdcfa31caa410756640e/vngudivada},
  booktitle = {Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing},
  interhash = {2e8f57cae042d8d53e89c22fca99e654},
  intrahash = {ab566e42cf13cdcfa31caa410756640e},
  keywords = {QuestionGeneration VocabularyAssessment},
  pages = {819--826},
  publisher = {Association for Computational Linguistics},
  series = {HLT '05},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Automatic Question Generation for Vocabulary Assessment},
  url = {http://dx.doi.org/10.3115/1220575.1220678},
  year = 2005
}

@book{swain2015study,
  abstract = {
“The only real education is self-education. The best that the teacher can do for the student is to show him what he can do for himself and how he can do it. "If little labor, little are our gains; Man's fortunes are according to his pains.”

CONTENTS

PAGE PREFACE INTRODUCTION I. THE PROPER MENTAL ATTITUDE (a) Distinction between reading and understanding (b) Distinction between facts, opinions, and logical conclusions (c) Importance of the questioning habit (d) Inquiring into methods of ascertaining facts (e) Studying evidence of reliability of a writer (f) Importance of caution (g) Importance of the scientific attitude of mind (h) Intellectual modesty (i) Wisdom rather than knowledge the aim II. STUDYING UNDERSTANDINGLY (a) Importance of definite ideas (1) Use of the dictionary (2) Practice in definition (3) Importance of the study of logic (b) Stating a thing in different ways (c) Stating a thing negatively as well as positively (d) Observation of necessary qualifying words or phrases (e) Reflection, illustration, and application (f) Keeping the mind active (g) Study of causes of differences of opinion (h) Discrimination of mere assertion from proof III. SYSTEM (a) Importance of grasping the fundamental idea (b) Preliminary arrangement of ideas (c) Classification and arrangement IV. MENTAL INITIATIVE (a) Interest in subject of study essential (b) Formulation of problem essential (c) Independent work essential (d) Drawing conclusions independent of author (e) Independence in arriving at conclusions (f) Generalizing (g) Going beyond the book (h) Visualizing results V. HABITS OF WORK (a) Selection of book (b) Proper number of subjects to be studied at once (c) Haste undesirable (d) Taking studies seriously (e) Judicious skipping (f) Systematic program of work (g) Cultivation of concentration (h) Applying what is learned (i) Avoidance of indifference (j) Thorough knowledge of a few books (k) List of references should be made (l) Frequent reviews desirable (m) Regular times for recreative study (n) Physical exercise essential SUGGESTIONS TO TEACHERS IMPORTANCE OF REFUSING TO BE DISCOURAGED, AND OF SEEKING THE WORK ONE CAN DO BEST REFERENCES

“Many an earnest student, after repeated failures, assumes a sort of hopeless, discouraged attitude of mind, which naturally leads him into the habit of trying to learn his lessons by memorizing in the hope of being able to pass, if only by scraping through, and into other bad habits which have been referred to in the foregoing pages. Such an attitude of mind should be resolutely opposed, and the teacher, even when severely correcting a student, should encourage him to see the possibilities that are within his reach if he will exercise his will and put forth his utmost powers in a proper manner. Success in the work of the world depends much more upon will than upon brains; but all faculties, whether mental or moral, can be cultivated and developed to an almost unlimited extent. A study of the biographies of men who have succeeded should be urged upon the student, and such a study will show how often success has been attained only after repeated failures. It is scarcely too much to say to a student that he can attain anything he desires, if he desires it with sufficient intensity; that is to say, if he possesses sufficient will power, and if he will train himself to direct his efforts properly.”

This book has a philosophical view about learning. It takes many principles about how we develop our understanding in any subject. As a student, I could say that it provides essential insights as well as advises.

There is a world of difference between being a good student (getting good grades) and being a good learner (actually retaining information and being a more intelligent person). This book provides insight into the difference, and gives great instructions on how to really study and learn mindfulness when presented with information. The copyright on this is from 1917. It is 95 years old, and still 100% applicable to students today.


},
  added-at = {2017-03-10T13:11:50.000+0100},
  author = {Swain, George},
  biburl = {https://www.bibsonomy.org/bibtex/24d565e1e23fe6c5f52745811e0f96eab/vngudivada},
  interhash = {2d4c89d1a9092084996a77fc62fcddf7},
  intrahash = {4d565e1e23fe6c5f52745811e0f96eab},
  keywords = {Book HowToStudy Learning},
  note = {ISBN: 978-1505871685},
  publisher = {CreateSpace Independent Publishing Platform},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {How To Study: The System of Successful Learning},
  year = 2015
}

@article{taylor2010effects,
  abstract = {Previous research shows that interleaving rather than blocking practice of different skills (e.g. abcbcacab instead of aaabbbccc) usually improves subsequent test performance. Yet interleaving, but not blocking, ensures that practice of any particular skill is distributed, or spaced, because any two opportunities to practice the same task are not consecutive. Hence, because spaced practice typically improves test performance, the previously observed test benefits of interleaving may be due to spacing rather than interleaving per se. In the experiment reported herein, children practiced four kinds of mathematics problems in an order that was interleaved or blocked, and the degree of spacing was fixed. The interleaving of practice impaired practice session performance yet doubled scores on a test given one day later. An analysis of the errors suggested that interleaving boosted test scores by improving participants' ability to pair each problem with the appropriate procedure. Copyright © 2009 John Wiley & Sons, Ltd.},
  added-at = {2017-03-12T01:17:05.000+0100},
  author = {Taylor, Kelli and Rohrer, Doug},
  biburl = {https://www.bibsonomy.org/bibtex/2b965754f259f0a5a60229de7139c94da/vngudivada},
  interhash = {a5d8cbb80b418cb8c6326b4978f73706},
  intrahash = {b965754f259f0a5a60229de7139c94da},
  journal = {Applied Cognitive Psychology},
  keywords = {InterleavedPractice Learning},
  number = 6,
  pages = {837--848},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {The effects of interleaved practice},
  url = {http://dx.doi.org/10.1002/acp.1598},
  volume = 24,
  year = 2010
}

@inproceedings{jin2016selecting,
  abstract = {We address the problem of generating a set of questions from a plain text document. Traditionally, question generation (QG) systems apply syntactical transformation on individual sentences to generate open domain questions. We hypothesize that a QG system informed by domain knowledge can ask more important questions. To this end, we propose two lightly-supervised methods to select salient target concepts for QG based on domain knowledge collected from a corpus. One method selects important semantic roles with bootstrapping and the other selects important semantic relations with Open Information Extraction (OpenIE). We demonstrate the effectiveness of the two proposed methods on heterogeneous corpora in the business domain. Both methods lead to systems that ask more important and more grammatical questions. This work exploits domain knowledge in QG task and provides a promising paradigm to generate domain-specific questions.},
  added-at = {2017-01-30T13:36:10.000+0100},
  author = {Jin, Yiping and Le, Phu},
  biburl = {https://www.bibsonomy.org/bibtex/2f10c912cc3778cc7094177d8e810e105/vngudivada},
  booktitle = {Proceedings of the 9th International Natural Language Generation conference},
  interhash = {ca61b70134e63ee55870eeb65b7acf73},
  intrahash = {f10c912cc3778cc7094177d8e810e105},
  keywords = {QuestionGeneration},
  publisher = {Association for Computational Linguistics ({ACL})},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Selecting Domain-Specific Concepts for Question Generation With  Lightly-Supervised Methods
		          },
  url = {https://doi.org/10.18653%2Fv1%2Fw16-6623},
  year = 2016
}

@article{guida2012chunks,
  abstract = {Our review of research on PET and fMRI neuroimaging of experts and expertise acquisition reveals two apparently discordant patterns in working-memory-related tasks. When experts are involved, studies show activations in brain regions typically activated during long-term memory tasks that are not observed with novices, a result that is compatible with functional brain reorganization. By contrast, when involving novices and training programs, studies show a decrease in brain regions typically activated during working memory tasks, with no functional reorganization. We suggest that the latter result is a consequence of practice periods that do not allow important structures to be completely acquired: knowledge structures (i.e., Ericsson and Kintsch's retrieval structures; Gobet and Simon's templates) and in a lesser way, chunks. These structures allow individuals to improve performance on working-memory tasks, by enabling them to use part of long-term memory as working memory, causing a cerebral functional reorganization. Our hypothesis is that the two brain activation patterns observed in the literature are not discordant, but involve the same process of expertise acquisition in two stages: from decreased activation to brain functional reorganization. The dynamic of these two physiological stages depend on the two above-mentioned psychological constructs: chunks and knowledge structures.},
  added-at = {2017-03-12T00:17:30.000+0100},
  author = {Guida, Alessandro and Gobet, Fernand and Tardieu, Hubert and Nicolas, Serge},
  biburl = {https://www.bibsonomy.org/bibtex/2d62a8464890ab3405eecfa851f8095d9/vngudivada},
  interhash = {6ecdeb177a245d9fb74dd8a5ec2fbab6},
  intrahash = {d62a8464890ab3405eecfa851f8095d9},
  journal = {Brain and Cognition},
  keywords = {Brain Cognition Learning},
  note = {DOI: 10.1016/j.bandc.2012.01.010},
  number = 3,
  pages = {221 - 244},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {How chunks, long-term working memory and templates offer a cognitive explanation for neuroimaging data on expertise acquisition: A two-stage framework},
  volume = 79,
  year = 2012
}

@book{harrington2012machine,
  abstract = {Machine Learning in Action is unique book that blends the foundational theories of machine learning with the practical realities of building tools for everyday data analysis. You'll use the flexible Python programming language to build programs that implement algorithms for data classification, forecasting, recommendations, and higher-level features like summarization and simplification.},
  added-at = {2017-02-24T03:54:46.000+0100},
  address = {Shelter Island, N.Y.},
  author = {Harrington, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/22338428e97cdc7ecd040c9140b0b4cc7/vngudivada},
  interhash = {cbf6492b69e6ffabdb858ae1296fc814},
  intrahash = {2338428e97cdc7ecd040c9140b0b4cc7},
  keywords = {Book MachineLearning},
  publisher = {Manning Publications Co.},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Machine learning in action},
  year = 2012
}

@book{heath2010switch,
  abstract = {Why is it so hard to make lasting changes in our companies, in our communities, and in our own lives?

The primary obstacle is a conflict that’s built into our brains, say Chip and Dan Heath, authors of the critically acclaimed bestseller Made to Stick. Psychologists have discovered that our minds are ruled by two different systems—the rational mind and the emotional mind—that compete for control. The rational mind wants a great beach body; the emotional mind wants that Oreo cookie. The rational mind wants to change something at work; the emotional mind loves the comfort of the existing routine. This tension can doom a change effort—but if it is overcome, change can come quickly.

In Switch, the Heaths show how everyday people—employees and managers, parents and nurses—have united both minds and, as a result, achieved dramatic results:
●      The lowly medical interns who managed to defeat an entrenched, decades-old medical practice that was endangering patients.
●      The home-organizing guru who developed a simple technique for overcoming the dread of housekeeping.
●      The manager who transformed a lackadaisical customer-support team into service zealots by removing a standard tool of customer service

In a compelling, story-driven narrative, the Heaths bring together decades of counterintuitive research in psychology, sociology, and other fields to shed new light on how we can effect transformative change. Switch shows that successful changes follow a pattern, a pattern you can use to make the changes that matter to you, },
  added-at = {2017-03-10T13:36:14.000+0100},
  author = {Heath, Chip and Heath, Dan},
  biburl = {https://www.bibsonomy.org/bibtex/28fb9a8f44fcafd7d8fd1962ac9b8d1e1/vngudivada},
  interhash = {93d662829b58830b1f449328f05f113f},
  intrahash = {8fb9a8f44fcafd7d8fd1962ac9b8d1e1},
  keywords = {Book Change Leadership},
  publisher = {Crown Business},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Switch: How to Change Things When Change Is Hard},
  year = 2010
}

@misc{rawson2011optimizing,
  abstract = {The literature on testing effects is vast but supports surprisingly few prescriptive conclusions for how to schedule practice to achieve both durable and efficient learning. Key limitations are that few studies have examined the effects of initial learning criterion or the effects of relearning, and no prior research has examined the combined effects of these 2 factors. Across 3 experiments, 533 students learned conceptual material via retrieval practice with restudy. Items were practiced until they were correctly recalled from 1 to 4 times during an initial learning session and were then practiced again to 1 correct recall in 1–5 subsequent relearning sessions (across experiments, more than 100,000 short-answer recall responses were collected and hand-scored). Durability was measured by cued recall and rate of relearning 1–4 months after practice, and efficiency was measured by total practice trials across sessions. A consistent qualitative pattern emerged: The effects of initial learning criterion and relearning were subadditive, such that the effects of initial learning criterion were strong prior to relearning but then diminished as relearning increased. Relearning had pronounced effects on long-term retention with a relatively minimal cost in terms of additional practice trials. On the basis of the overall patterns of durability and efficiency, our prescriptive conclusion for students is to practice recalling concepts to an initial criterion of 3 correct recalls and then to relearn them 3 times at widely spaced intervals. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  added-at = {2017-03-12T01:47:32.000+0100},
  author = {Rawson, Katherine A. and Dunlosky, John},
  biburl = {https://www.bibsonomy.org/bibtex/22dd5e33c3213d527e9b143f95ed2005d/vngudivada},
  interhash = {4c194d4e831ea7953fd315abd4957098},
  intrahash = {2dd5e33c3213d527e9b143f95ed2005d},
  journal = {Journal of Experimental Psychology: General},
  keywords = {Learning RetrievalPractice},
  number = 3,
  pages = {283--302},
  publisher = {American Psychological Association},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Optimizing schedules of retrieval practice for durable and efficient learning: How much is enough?},
  volume = 140,
  year = 2011
}

@book{bransford2004people,
  abstract = {First released in the Spring of 1999, How People Learn has been expanded to show how the theories and insights from the original book can translate into actions and practice, now making a real connection between classroom activities and learning behavior. This edition includes far-reaching suggestions for research that could increase the impact that classroom teaching has on actual learning.

Like the original edition, this book offers exciting new research about the mind and the brain that provides answers to a number of compelling questions. When do infants begin to learn? How do experts learn and how is this different from non-experts? What can teachers and schools do-with curricula, classroom settings, and teaching methods--to help children learn most effectively? New evidence from many branches of science has significantly added to our understanding of what it means to know, from the neural processes that occur during learning to the influence of culture on what people see and absorb.

How People Learn examines these findings and their implications for what we teach, how we teach it, and how we assess what our children learn. The book uses exemplary teaching to illustrate how approaches based on what we now know result in in-depth learning. This new knowledge calls into question concepts and practices firmly entrenched in our current education system.

Topics include:

How learning actually changes the physical structure of the brain.
How existing knowledge affects what people notice and how they learn.
What the thought processes of experts tell us about how to teach.
The amazing learning potential of infants.
The relationship of classroom learning and everyday settings of community and workplace.
Learning needs and opportunities for teachers.
A realistic look at the role of technology in education.},
  added-at = {2017-03-11T00:48:06.000+0100},
  address = {Washington, D.C.},
  biburl = {https://www.bibsonomy.org/bibtex/241d6869967380d8bf2e0cd4fd7002b70/vngudivada},
  editor = {Bransford, John D. and Brown, Ann L. and Cocking, Rodney R.},
  interhash = {57b75ae59d5be23fcb7f3dd3b160ace3},
  intrahash = {41d6869967380d8bf2e0cd4fd7002b70},
  keywords = {Book Learning},
  publisher = {National Research Council, National Academy Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {How people learn brain, mind, experience, and school},
  year = 2004
}

@article{karpicke2011response,
  abstract = {Mintzes et al. comment on our study in which we showed that retrieval practice enhances meaningful learning more than elaborative studying with concept mapping. Here, we consider and rebut claims that are based on mischaracterizations of our paper and speculations rather than evidence. We emphasize that randomized, controlled studies in both laboratory and classroom settings are essential to identifying effective strategies that promote meaningful learning.},
  added-at = {2017-03-12T00:46:05.000+0100},
  author = {Karpicke, Jeffrey D. and Blunt, Janell R.},
  biburl = {https://www.bibsonomy.org/bibtex/22da94a9ebd5bda3e132e2dc890a4734d/vngudivada},
  interhash = {1ce0a7cbb2d0da4ee7ec389913271787},
  intrahash = {2da94a9ebd5bda3e132e2dc890a4734d},
  journal = {Science},
  keywords = {Learning RetrievalPractice},
  number = 6055,
  pages = {453--453},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Response to Comment on {\textquotedblleft}Retrieval Practice Produces More Learning than Elaborative Studying with Concept Mapping{\textquotedblright}},
  url = {http://science.sciencemag.org/content/334/6055/453.4},
  volume = 334,
  year = 2011
}

@book{barber2015bayesian,
  abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
  added-at = {2017-03-07T23:31:53.000+0100},
  address = {Cambridge, United Kingdom},
  author = {Barber, David},
  biburl = {https://www.bibsonomy.org/bibtex/251b28f0be067692665b390c19a0eab38/vngudivada},
  interhash = {36c18a0888a17745bce742093ab43513},
  intrahash = {51b28f0be067692665b390c19a0eab38},
  isbn = {978-0521518147},
  keywords = {BayesianLearning Book ML MachineLearning},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Bayesian reasoning and machine learning},
  year = 2015
}

@article{andrewshanna2012brains,
  abstract = {During the many idle moments that comprise daily life, the human brain increases its activity across a set of midline and lateral cortical brain regions known as the “default network.” Despite the robustness with which the brain defaults to this pattern of activity, surprisingly little is known about the network’s precise anatomical organization and adaptive functions. To provide insight into these questions, this article synthesizes recent literature from structural and functional imaging with a growing behavioral literature on mind wandering. Results characterize the default network as a set of interacting hubs and subsystems that play an important role in internal mentation – the introspective and adaptive mental activities in which humans spontaneously and deliberately engage in everyday.},
  added-at = {2017-03-11T20:25:11.000+0100},
  author = {Andrews-Hanna, Jessica R.},
  biburl = {https://www.bibsonomy.org/bibtex/2d4c7feb7fa6b15249fb168d10a050b4c/vngudivada},
  interhash = {64a22d1be2c5faaf0706c834ea36d245},
  intrahash = {d4c7feb7fa6b15249fb168d10a050b4c},
  journal = {The Neuroscientist},
  keywords = {Learning},
  month = jun,
  number = 3,
  pages = {251--270},
  publisher = {{SAGE} Publications},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {The Brain's Default Network and Its Adaptive Role in Internal Mentation},
  url = {https://doi.org/10.1177%2F1073858411403316},
  volume = 18,
  year = 2012
}

@article{fattoh2014semantic,
  abstract = {This research proposes an automatic question generation model for evaluating the understanding of semantic attributes in a sentence. The Semantic Role Labeling and Named Entity Recognition are used as a preprocessing step to convert the input sentence into a semantic pattern. The Artificial Immune System is used to build a classifier that will be able to classify the patterns according to the question type in the training phase. The question types considered here are the set of WH-questions like who, when, where, why, and how. A pattern matching phase is applied for selecting the best matching question pattern for the test sentence. The proposed model is tested against a set of sentences obtained from many sources such as the TREC 2007 dataset for question answering, Wikipedia articles, and English book of grade II preparatory. The experimental results of the proposed model are promising in determining the question type with classification accuracy reaching 95\%, and 87\% in generating the new question patterns.},
  added-at = {2017-01-30T16:42:15.000+0100},
  author = {Fattoh, Ibrahim E. and Aboutabl, Amal E. and Haggag, Mohamed H.},
  biburl = {https://www.bibsonomy.org/bibtex/2b848ace755960a11a6a12e203b16d8a9/vngudivada},
  interhash = {9bb7b3eca84030eca6f7214460a24703},
  intrahash = {b848ace755960a11a6a12e203b16d8a9},
  journal = {I.J. Modern Education and Computer Science},
  keywords = {QuestionGeneration},
  number = 1,
  pages = {1 - 8},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Semantic Question Generation Using Artificial Immunity},
  volume = 1,
  year = 2014
}

@book{sedgewick2015introduction,
  abstract = {Today, anyone in a scientific or technical discipline needs programming skills. Python is an ideal first programming language, and Introduction to Programming in Python is the best guide to learning it.

Princeton University’s Robert Sedgewick, Kevin Wayne, and Robert Dondero have crafted an accessible, interdisciplinary introduction to programming in Python that emphasizes important and engaging applications, not toy problems. The authors supply the tools needed for students to learn that programming is a natural, satisfying, and creative experience.

This example-driven guide focuses on Python’s most useful features and brings programming to life for every student in the sciences, engineering, and computer science.

Coverage includes
Basic elements of programming: variables, assignment statements, built-in data types, conditionals, loops, arrays, and I/O, including graphics and sound
Functions, modules, and libraries: organizing programs into components that can be independently debugged, maintained, and reused
Object-oriented programming and data abstraction: objects, modularity, encapsulation, and more
Algorithms and data structures: sort/search algorithms, stacks, queues, and symbol tables
Examples from applied math, physics, chemistry, biology, and computer science—all compatible with Python 2 and 3
Drawing on their extensive classroom experience, the authors provide Q&As, exercises, and opportunities for creative practice throughout. An extensive amount of supplementary information is available at introcs.cs.princeton.edu/python. With source code, I/O libraries, solutions to selected exercises, and much more, this companion website empowers people to use their own computers to teach and learn the material.},
  added-at = {2017-02-20T22:00:16.000+0100},
  address = {Reading, MA},
  author = {Sedgewick, Robert and Wayne, Kevin and Dondero, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/27fd2adafa1d08e6711fc78c8633d94cd/vngudivada},
  interhash = {1c43835adcc75f0316bf69065c4f0a8d},
  intrahash = {7fd2adafa1d08e6711fc78c8633d94cd},
  keywords = {Book Python},
  publisher = {Addison-Wesley},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {An introduction to programming in Python: an interdisciplinary approach},
  year = 2015
}

@book{dayan2014theoretical,
  abstract = {Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.

The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.},
  added-at = {2017-02-20T19:29:56.000+0100},
  address = {Cambridge, MA},
  author = {Dayan, Peter and Abbott, Laurence},
  biburl = {https://www.bibsonomy.org/bibtex/247e4b7c80d10c836bc0374d10be798c1/vngudivada},
  interhash = {30c8fc19046771d016710b7bfac3597a},
  intrahash = {47e4b7c80d10c836bc0374d10be798c1},
  keywords = {Book Neuroscience},
  publisher = {MIT Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  year = 2014
}

@book{alpaydin2010introduction,
  abstract = {The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. The second edition of Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. In order to present a unified treatment of machine learning problems and solutions, it discusses many methods from different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The text covers such topics as supervised learning, Bayesian decision theory, parametric methods, multivariate methods, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, and reinforcement learning. New to the second edition are chapters on kernel machines, graphical models, and Bayesian estimation; expanded coverage of statistical tests in a chapter on design and analysis of machine learning experiments; case studies available on the Web (with downloadable results for instructors); and many additional exercises. All chapters have been revised and updated. Introduction to Machine Learning can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods.},
  added-at = {2017-03-08T13:50:32.000+0100},
  address = {Cambridge, MA},
  author = {Alpaydin, Ethem},
  biburl = {https://www.bibsonomy.org/bibtex/2e0efebffe45c697969eab3f985fe2619/vngudivada},
  interhash = {8b5e1ffc834b8807c49dcbad7972f1fa},
  intrahash = {e0efebffe45c697969eab3f985fe2619},
  keywords = {Book ML},
  publisher = {MIT Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Introduction to machine learning},
  year = 2010
}

@inproceedings{binnig2007qagen,
  abstract = {Today, a common methodology for testing a database management system (DBMS) is to generate a set of test databases and then execute queries on top of them. However, for DBMS testing, it would be a big advantage if we can control the input and/or the output (e.g., the cardinality) of each individual operator of a test query for a particular test case. Unfortunately, current database generators generate databases independent of queries. As a result, it is hard to guarantee that executing the test query on the generated test databases can obtain the desired (intermediate) query results that match the test case. In this paper, we propose a novel way for DBMS testing. Instead of first generating a test database and then seeing how well it matches a particular test case (or otherwise use a trial-and-error approach to generate another test database), we propose to generate a query-aware database for each test case. To that end, we designed a query-aware test database generator called QAGen. In addition to the database schema and the set of basic constraints defined on the base tables, QAGen takes the query and the set of constraints defined on the query as input, and generates a query-aware test database as output. The generated database guarantees that the test query can get the desired (intermediate) query results as defined in the test case. This approach of testing facilitates a wide range of DBMS testing tasks such as testing of memory managers and testing the cardinality estimation components of query optimizers.},
  added-at = {2017-02-12T05:17:12.000+0100},
  address = {New York, NY},
  author = {Binnig, Carsten and Kossmann, Donald and Lo, Eric and \"{O}zsu, M. Tamer},
  biburl = {https://www.bibsonomy.org/bibtex/2874820a130e0b83340a5afcae12ad352/vngudivada},
  booktitle = {Proceedings of the 2007 ACM SIGMOD international conference on Management of data},
  interhash = {bb7668a63c1238becc862bda5dd8eaef},
  intrahash = {874820a130e0b83340a5afcae12ad352},
  keywords = {QuestionGeneration SQL},
  pages = {341--352},
  publisher = {ACM},
  series = {SIGMOD '07},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {{QAGen}: generating query-aware test databases},
  year = 2007
}

@book{christiansen2017creating,
  abstract = {Language is a hallmark of the human species; the flexibility and unbounded expressivity of our linguistic abilities is unique in the biological world. Morten Christiansen and Nick Chater argue that to understand this astonishing phenomenon, we must consider how language is created: moment by moment, in the generation and understanding of individual utterances; year by year, as new language learners acquire language skills; and generation by generation, as languages change, split, and fuse through the processes of cultural evolution.},
  added-at = {2017-02-18T15:25:02.000+0100},
  address = {Cambridge, MA},
  author = {Christiansen, Morten H. and Chater, Nick},
  biburl = {https://www.bibsonomy.org/bibtex/204a8454a09b128d9f5dd8c530813e54c/vngudivada},
  interhash = {f71beebb29944058ca856a02ea07a30a},
  intrahash = {04a8454a09b128d9f5dd8c530813e54c},
  keywords = {Book Linguistics},
  publisher = {The MIT Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Creating language: integrating evolution, acquisition, and processing},
  url = {http://dx.doi.org/10.7551/mitpress/9780262034319.001.0001},
  year = 2017
}

@book{valcourt2017systematic,
  abstract = {A brilliant young scientist introduces us to the fascinating field that is changing our understanding of how the body works and the way we can approach healing.

SYSTEMATIC is the first book to introduce general readers to systems biology, which is improving medical treatments and our understanding of living things. In traditional bottom-up biology, a biologist might spend years studying how a single protein works, but systems biology studies how networks of those proteins work together--how they promote health and how to remedy the situation when the system isn't functioning properly.

Breakthroughs in systems biology became possible only when powerful computer technology enabled researchers to process massive amounts of data to study complete systems, and has led to progress in the study of gene regulation and inheritance, cancer drugs personalized to an individual's genetically unique tumor, insights into how the brain works, and the discovery that the bacteria and other microbes that live in the gut may drive malnutrition and obesity. Systems biology is allowing us to understand more complex phenomena than ever before.

In accessible prose, SYSTEMATIC sheds light not only on how systems within the body work, but also on how research is yielding new kinds of remedies that enhance and harness the body's own defenses.},
  added-at = {2017-03-11T14:16:26.000+0100},
  author = {Valcourt, James R.},
  biburl = {https://www.bibsonomy.org/bibtex/2973e9302073095dbbfbf01dbb59cb9f2/vngudivada},
  interhash = {7c2032aaa434cc908960acec72a6a579},
  intrahash = {973e9302073095dbbfbf01dbb59cb9f2},
  keywords = {Book SystemsBiology},
  publisher = {Bloomsbury Sigma},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Systematic: how systems biology is transforming modern medicine},
  year = 2017
}

@book{mcguire2015teaching,
  abstract = {"It shouldn’t be surprising that a volume intent on teaching students how to learn is just as intent on teaching the reader how to do just that, but it is still refreshing to read a book that lays out its goals, sticks to the promises it makes, and even creates its own study guide based on how much time the reader has to give to the text. Well-structured and clear, Saundra Yancy McGuire’s Teach Students How to Learn is as thoughtful about itself as it is about the content it presents. McGuire has composed this book to reflect her own response to and engagement with a pressing problem in higher education: namely, that many students, even those who qualify for admission at prestigious institutions, arrive without ever having been taught to learn by anything but rote memorization. Faced with college’s demands of skills higher in Bloom’s Taxonomy, they find themselves struggling and even failing.

With this book McGuire gives teachers the tools they need to move their students past the high school model of retention until regurgitation, helping them instead to internalize a more nuanced, flexible understanding of learning. To convey this understanding, McGuire focuses on student mindset, encouraging educators to bring in everything from neurobiological models to fellow student success stories in order to help learners see that they are not stuck being 'bad' at something – that change is not only possible, but already well within reach.

Most of all, McGuire is a fun writer. Personal and plainspoken, her style makes the pages fly by. (Any worries that this book might drown the reader in jargon should be alleviated by the appearance of the words ‘metacognition, schmetacognition’. I would recommend this book in particular to educators working with students from underserved communities, as giving students access to these techniques will help ensure their success far beyond the boundaries of a single classroom.” (Reflective Teaching (Wabash Center))

"I just wanted to write you a quick note to talk about how much I enjoyed your book Teach Students How to Learn. I work as the Associate Director for Teaching and Learning at the Faculty Center at my university. Every year I get to put on a Summer Teaching Institute for faculty. The theme that came out of much of the work we have been doing about High-Impact Practices and what we want to accomplish at the university centered on life-long learning and nurturing autonomy and agency in students. I wanted to make that the theme of the institute this year and in looking for materials I came across your book. I loved it and it is the book we are going to cover this summer. I think it will provide a wonderful road map for us as we try move past how best to teach information to our students into helping our students become better learners." (Matthew C. Atherton, Ph.D. California State University San Marcos)

“Dr. McGuire’s book, Teach Students How to Learn: Strategies You Can Incorporate into Any Course to Improve Student Metacognition, Study Skills, and Motivation, is a must read for faculty, staff, students, and top administrators. Students are coming to college not knowing what to expect or how to handle the level of preparedness that is expected of them. The strategies in this book are not difficult to implement or to include in the instruction of early core classes or a freshman seminar class. The best or the least prepared students can learn from Dr. McGuire’s strategies.” (NCLCA Newsletter)

"For those interested in helping students develop strong metacognitive skills, Dr. Saundra McGuire’s book, Teach Students How to Learn: Strategies You Can Incorporate Into Any Course to Improve Student Metacognition, Study Skills, and Motivation, is concise, practical, and much less overwhelming than trying to figure out what to do on your own. It is both a consolidation of the research surrounding metacognition, mindset, and motivation and a how-to guide for putting that research into practice. (Improve with Metacognition)

“If you are already convinced – or are at least willing to consider the possibility – that your students could learn more deeply and achieve more success than they are at present, this book is for you. If you are frustrated by students who seem unmotivated and disengaged, this book is for you. If you find it challenging to teach underprepared students, this book is for you. And if you care about educational equity and fairness, this book is for you.

The not-so-familiar good news is that these same students can both survive and thrive in higher education. The message from relevant research is quite clear: What students do in college matters more than who they are or which institution they attend. What these underprepared students need most to do is to learn how to learn.

In this book, Saundra McGuire provides specific, practical, research-based strategies to teach students how to learn, focusing on the three key M’s – mindset, motivation, and metacognition.

The book offers a broad range of strategies for teachers and for students, along with a wealth of examples, illustrations, and resources.” (Thomas A. Angelo, Clinical Professor of Educational Innovation & Research, The Division of Practice Advancement and Clinical Education and Director of Educator Development, Eshelman School of Pharmacy)

"This book is a wonderful resource for college faculty. It provides us with practical, yet powerful learning strategies and metacognition techniques that can be easily incorporated into our courses, and which in turn, will improve student learning. Dr. McGuire shares both research and her personal experiences, as well as her expertise in teaching all kinds of diverse students with tremendous success. This book is a welcome addition for the post secondary teaching and learning field and should be read and utilized by all." (Kathleen F. Gabriel, Associate Professor, School of Education)

"Teachers need to learn as much as their students. In a masterly and spirited exposition, spangled with wit and exhortation, rife with pragmatic strategies, Saundra McGuire teaches teachers how to awake in their students the powers dormant in them. Be aware, and you will learn!" (Roald Hoffmann, 1981 Nobel Laureate in Chemistry)

"An electrifying book! McGuire demonstrates how learning strategies can improve learning―and then charges faculty to teach them, complete with the slides for doing so in your class. . . A must read―and must do―for every teacher who struggles with students who don’t learn as much as they could or should!" (Tara Gray, Ph.D., Director)

“Dr. McGuire's specific strategies serve me as paradigms I can adapt for my literature courses. Many of the specific exercises McGuire uses to illustrate metacognition quickly convinced my students that cognitive functions such as pattern recognition effectively guide the close reading of a text while taking time to overview a text and place it in context helps more advanced students take on the challenges of literary theory. The strategies outlined here take away the mystery, not the magic, of writing about literature.” (Helen Whall, Professor of English and Director of Comprehensive Academic Advising)

“Based on solid scientific theory and real classroom case studies, Dr. McGuire’s workshop on Metacognition provides the participants with sound pedagogical advice and an impressive array of ready-to-use, result oriented teaching techniques for a 21st century classroom. With a metacognitive approach to teaching and learning, everything comes together.” (Irina Ivliyeva, Associate Professor of Russian)

“I believe The Study Cycle handout was particularly useful because it provided a helpful step-by-step approach for students to learn the material in a class more effectively. Moreover, it is a good way to place the accountability for student success where it belongs―on the shoulders of the students (with professors providing guidance and support for their learning).” (Larry Gragg, Curators’ Teaching Professor and Chair, Department of History and Political Science)},
  added-at = {2017-03-10T03:22:28.000+0100},
  author = {McGuire, Saundra Yancy},
  biburl = {https://www.bibsonomy.org/bibtex/29b8cf81bb39777aab7c99857a1592a0b/vngudivada},
  interhash = {7df51e85f71cd04421012bb4f0923653},
  intrahash = {9b8cf81bb39777aab7c99857a1592a0b},
  keywords = {Book Learning},
  publisher = {Stylus Publishing},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Teaching Students How to Learn: Strategies You Can Incorporate Into Any Course to Improve Student Metacognition, Study Skills, and Motivation},
  year = 2015
}

@article{alyahya2014ontologybased,
  abstract = {With recent advancements in Semantic Web technologies, a new trend in MCQ item generation has emerged through the use of ontologies. Ontologies are knowledge representation structures that formally describe entities in a domain and their relationships, thus enabling automated inference and reasoning. Ontology-based MCQ item generation is still in its infancy, but substantial research efforts are being made in the field. However, the applicability of these models for use in an educational setting has not been thoroughly evaluated. In this paper, we present an experimental evaluation of an ontology-based MCQ item generation system known as OntoQue. The evaluation was conducted using two different domain ontologies. The findings of this study show that ontology-based MCQ generation systems produce satisfactory MCQ items to a certain extent. However, the evaluation also revealed a number of shortcomings with current ontology-based MCQ item generation systems with regard to the educational significance of an automatically constructed MCQ item, the knowledge level it addresses, and its language structure. Furthermore, for the task to be successful in producing high-quality MCQ items for learning assessments, this study suggests a novel, holistic view that incorporates learning content, learning objectives, lexical knowledge, and scenarios into a single cohesive framework.},
  added-at = {2017-03-09T03:29:19.000+0100},
  author = {Al-Yahya, Maha},
  biburl = {https://www.bibsonomy.org/bibtex/2f743e9f6637f51e70e0ce5eeecf904bb/vngudivada},
  interhash = {a28810cffd0773a1291306fb543bf2be},
  intrahash = {f743e9f6637f51e70e0ce5eeecf904bb},
  journal = {The Scientific World Journal},
  keywords = {QuestionGeneration},
  pages = {1-9},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Ontology-Based Multiple Choice Question Generation},
  url = {http://dx.doi.org/10.1155/2014/274949},
  volume = 2014,
  year = 2014
}

@article{brusilovsky2010learning,
  abstract = {Rich, interactive eLearning tools receive a lot of attention nowadays from both practitioners and researchers. However, broader dissemination of these tools is hindered by the technical difficulties of their integration into existing platforms. This article explores the technical and conceptual problems of using several interactive educational tools in the context of a single course. It presents an integrated Exploratorium for database courses, an experimental platform, which provides personalized access to several types of interactive learning activities. Several classroom studies of the Exploratorium have demonstrated its value in both the integration of several tools and the provision of personalized access.},
  added-at = {2017-02-12T06:02:18.000+0100},
  address = {New York, NY},
  author = {Brusilovsky, Peter and Sosnovsky, Sergey and Yudelson, Michael V. and Lee, Danielle H. and Zadorozhny, Vladimir and Zhou, Xin},
  biburl = {https://www.bibsonomy.org/bibtex/2e8b9e80f70d45d92feaa0d08096f7547/vngudivada},
  interhash = {2c25c126e146f13a1d8640e8e15efa72},
  intrahash = {e8b9e80f70d45d92feaa0d08096f7547},
  journal = {Trans. Comput. Educ.},
  keywords = {QuestionGeneration SQL},
  month = jan,
  number = 4,
  pages = {19:1--19:15},
  publisher = {ACM},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Learning SQL Programming with Interactive Tools: From Integration to Personalization},
  url = {http://doi.acm.org/10.1145.1656255.1656257},
  volume = 9,
  year = 2010
}

@article{rohrer2014benefit,
  abstract = {Most mathematics assignments consist of a group of problems requiring the same strategy. For example, a lesson on the quadratic formula is typically followed by a block of problems requiring students to use that formula, which means that students know the appropriate strategy before they read each problem. In an alternative approach, different kinds of problems appear in an interleaved order, which requires students to choose the strategy on the basis of the problem itself. In the classroom-based experiment reported here, grade 7 students (n = 140) received blocked or interleaved practice over a nine-week period, followed two weeks later by an unannounced test. The mean test scores were greater for material learned by interleaved practice rather than by blocked practice (72 {\%} vs. 38 {\%}, d = 1.05). This interleaving effect was observed even though the different kinds of problems were superficially dissimilar from each other, whereas previous interleaved mathematics studies had required students to learn nearly identical kinds of problems. We conclude that interleaving improves mathematics learning not only by improving discrimination between different kinds of problems, but also by strengthening the association between each kind of problem and its corresponding strategy.},
  added-at = {2017-03-12T01:21:52.000+0100},
  author = {Rohrer, Doug and Dedrick, Robert F. and Burgess, Kaleena},
  biburl = {https://www.bibsonomy.org/bibtex/2d0b66e27f6c91dff81ffe5c2a5509b79/vngudivada},
  interhash = {16cb1521f10f38cefbae44af3769147f},
  intrahash = {d0b66e27f6c91dff81ffe5c2a5509b79},
  journal = {Psychonomic Bulletin {\&} Review},
  keywords = {InterleavedPractice Learning Mathematics},
  number = 5,
  pages = {1323--1330},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {The benefit of interleaved mathematics practice is not limited to superficially similar kinds of problems},
  url = {http://dx.doi.org/10.3758/s13423-014-0588-3},
  volume = 21,
  year = 2014
}

@book{sedgewick2007introduction,
  abstract = {By emphasizing the application of computer programming not only in success stories in the software industry but also in familiar scenarios in physical and biological science, engineering, and applied mathematics, Introduction to Programming in Java takes an interdisciplinary approach to teaching programming with the Java™ programming language.

Elements of Programming: Your First Program; Built-in Types of Data; Conditionals and Loops; Arrays; Input and Output. Functions and Modules: Static Methods; Libraries and Clients; Recursion. Object-Oriented Programming: Data Types; Creating Data Types; Designing Data Types. Algorithms and Data Structures: Performance; Sorting and Searching; Stacks and Queues; Symbol Tables.},
  added-at = {2017-02-21T00:48:56.000+0100},
  address = {Reading, MA},
  author = {Sedgewick, Robert and Wayne, Kevin},
  biburl = {https://www.bibsonomy.org/bibtex/2bcdd1b85af73eea3ce166736764afd14/vngudivada},
  interhash = {7302a549941461784327be5781f9a4a5},
  intrahash = {bcdd1b85af73eea3ce166736764afd14},
  keywords = {Book CS1 Java},
  publisher = {Addison-Wesley},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Introduction to programming: an interdisciplinary approach},
  year = 2007
}

@article{kornell2010spacing,
  abstract = {We compared the effects of spaced versus massed practice on young and older adults' ability to learn visually complex paintings. We expected a spacing advantage when 1 painting per artist was studied repeatedly and tested (repetition) but perhaps a massing advantage, especially for older adults, when multiple different paintings by each artist were studied and tested (induction). We were surprised to find that spacing facilitated both inductive and repetition learning by both young and older adults, even though the participants rated massing superior to spacing for inductive learning. Thus, challenging learners of any age appears to have unintuitive benefits for both memory and induction. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  added-at = {2017-03-12T00:50:56.000+0100},
  author = {Kornell, Nate and Castel, Alan D. and Eich, Teal S. and Bjork, Robert A.},
  biburl = {https://www.bibsonomy.org/bibtex/2e46761e62eb8f3db73e2466e404723d7/vngudivada},
  interhash = {a477aa1103d0022b8a95e82a0718b094},
  intrahash = {e46761e62eb8f3db73e2466e404723d7},
  journal = {Psychology and Aging},
  keywords = {Learning Spacing},
  number = 2,
  pages = {498--503},
  publisher = {American Psychological Association},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Spacing as the friend of both memory and induction in young and older adults},
  url = {/brokenurl#DOI: 10.1037/a0017807},
  volume = 25,
  year = 2010
}

@book{gladwell2013outliers,
  abstract = {In this stunning new book, Malcolm Gladwell takes us on an intellectual journey through the world of "outliers"--the best and the brightest, the most famous and the most successful. He asks the question: what makes high-achievers different?

His answer is that we pay too much attention to what successful people are like, and too little attention to where they are from: that is, their culture, their family, their generation, and the idiosyncratic experiences of their upbringing. Along the way he explains the secrets of software billionaires, what it takes to be a great soccer player, why Asians are good at math, and what made the Beatles the greatest rock band.

Brilliant and entertaining, Outliers is a landmark work that will simultaneously delight and illuminate.},
  added-at = {2017-03-10T04:06:57.000+0100},
  address = {New York, NY},
  author = {Gladwell, Malcolm},
  biburl = {https://www.bibsonomy.org/bibtex/289a064b44ba3a3d79f93de808bbb8358/vngudivada},
  interhash = {3b207029f408760187066db9b851014b},
  intrahash = {89a064b44ba3a3d79f93de808bbb8358},
  keywords = {Book Learning Success},
  publisher = {Little, Brown and Company},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Outliers: the story of success},
  year = 2013
}

@article{karpicke2012retrievalbased,
  abstract = {Retrieval is the key process for understanding learning and for promoting learning, yet retrieval is not often granted the central role it deserves. Learning is typically identified with the encoding or construction of knowledge, and retrieval is considered merely the assessment of learning that occurred in a prior experience. The retrieval-based learning perspective outlined here is grounded in the fact that all expressions of knowledge involve retrieval and depend on the retrieval cues available in a given context. Further, every time a person retrieves knowledge, that knowledge is changed, because retrieving knowledge improves one’s ability to retrieve it again in the future. Practicing retrieval does not merely produce rote, transient learning; it produces meaningful, long-term learning. Yet retrieval practice is a tool many students lack metacognitive awareness of and do not use as often as they should. Active retrieval is an effective but undervalued strategy for promoting meaningful learning.},
  added-at = {2017-03-12T00:20:22.000+0100},
  author = {Karpicke, Jeffrey D.},
  biburl = {https://www.bibsonomy.org/bibtex/2b1250df049711af35e2dffdfa4cc0af7/vngudivada},
  interhash = {db0f17ca4854a696c8aa51e5cdfd4dcf},
  intrahash = {b1250df049711af35e2dffdfa4cc0af7},
  journal = {Current Directions in Psychological Science},
  keywords = {Learning},
  month = jun,
  number = 3,
  pages = {157--163},
  publisher = {{SAGE} Publications},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Retrieval-Based Learning},
  url = {https://doi.org/10.1177%2F0963721412443552},
  volume = 21,
  year = 2012
}

@book{stodden2014implementing,
  abstract = {In computational science, reproducibility requires that researchers make code and data available to others so that the data can be analyzed in a similar manner as in the original publication. Code must be available to be distributed, data must be accessible in a readable format, and a platform must be available for widely distributing the data and code. In addition, both data and code need to be licensed permissively enough so that others can reproduce the work without a substantial legal burden.

Implementing Reproducible Research covers many of the elements necessary for conducting and distributing reproducible research. It explains how to accurately reproduce a scientific result.

Divided into three parts, the book discusses the tools, practices, and dissemination platforms for ensuring reproducibility in computational science. It describes:

Computational tools, such as Sweave, knitr, VisTrails, Sumatra, CDE, and the Declaratron system
Open source practices, good programming practices, trends in open science, and the role of cloud computing in reproducible research
Software and methodological platforms, including open source software packages, RunMyCode platform, and open access journals
Each part presents contributions from leaders who have developed software and other products that have advanced the field. Supplementary material is available at www.ImplementingRR.org.},
  added-at = {2017-03-05T18:22:11.000+0100},
  author = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
  biburl = {https://www.bibsonomy.org/bibtex/26e5e0645ca72d249508ceabdd5fb5ac5/vngudivada},
  interhash = {cc2deccc810f9588a577e8506327ab11},
  intrahash = {6e5e0645ca72d249508ceabdd5fb5ac5},
  keywords = {Book ReproducibleResearch},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Implementing reproducible research},
  year = 2014
}

@book{ericsson2009development,
  abstract = {Professionals such as medical doctors, aeroplane pilots, lawyers, and technical specialists find that some of their peers have reached high levels of achievement that are difficult to measure objectively. In order to understand to what extent it is possible to learn from these expert performers for the purpose of helping others improve their performance, we first need to reproduce and measure this performance. This book is designed to provide the first comprehensive overview of research on the acquisition and training of professional performance as measured by objective methods rather than by subjective ratings by supervisors. In this collection of articles, the world's foremost experts discuss methods for assessing the experts' knowledge and review our knowledge on how we can measure professional performance and design training environments that permit beginning and experienced professionals to develop and maintain their high levels of performance, using examples from a wide range of professional domains.},
  added-at = {2017-03-11T21:44:44.000+0100},
  address = {New York, NY},
  author = {Ericsson, K. Anders},
  biburl = {https://www.bibsonomy.org/bibtex/2857d08a42fc51c014e5ecb6d90692c96/vngudivada},
  interhash = {cc9616d0a2ca3dc07391b804ccbb71f5},
  intrahash = {857d08a42fc51c014e5ecb6d90692c96},
  keywords = {Book Learning},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Development of professional expertise: toward measurement of expert performance and design of optimal learning environments},
  year = 2009
}

@article{brown1989situated,
  abstract = {Many teaching practices implicitly assume that conceptual knowledge can be abstracted from the situations in which it is learned and used. This article argues that this assumption inevitably limits the effectiveness of such practices. Drawing on recent research into cognition as it is manifest in everyday activity, the authors argue that knowledge is situated, being in part a product of the activity, context, and culture in which it is developed and used. They discuss how this view of knowledge affects our understanding of learning, and they note that conventional schooling too often ignores the influence of school culture on what is learned in school. As an alternative to conventional practices, they propose cognitive apprenticeship (Collins, Brown, & Newman, in press), which honors the situated nature of knowledge. They examine two examples of mathematics instruction that exhibit certain key features of this approach to teaching.},
  added-at = {2017-03-11T21:39:38.000+0100},
  author = {Brown, J. S. and Collins, A. and Duguid, P.},
  biburl = {https://www.bibsonomy.org/bibtex/2f29b96a0c68ae1f415c3c7b517193fb5/vngudivada},
  interhash = {e99c2debb6f529dae6f2683f32df2688},
  intrahash = {f29b96a0c68ae1f415c3c7b517193fb5},
  journal = {Educational Researcher},
  keywords = {Learning},
  month = jan,
  number = 1,
  pages = {32--42},
  publisher = {American Educational Research Association ({AERA})},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Situated Cognition and the Culture of Learning},
  url = {https://doi.org/10.3102%2F0013189x018001032},
  volume = 18,
  year = 1989
}

@article{roediger2011critical,
  abstract = {Learning is usually thought to occur during episodes of studying, whereas retrieval of information on testing simply serves to assess what was learned. We review research that contradicts this traditional view by demonstrating that retrieval practice is actually a powerful mnemonic enhancer, often producing large gains in long-term retention relative to repeated studying. Retrieval practice is often effective even without feedback (i.e. giving the correct answer), but feedback enhances the benefits of testing. In addition, retrieval practice promotes the acquisition of knowledge that can be flexibly retrieved and transferred to different contexts. The power of retrieval practice in consolidating memories has important implications for both the study of memory and its application to educational practice.},
  added-at = {2017-03-12T01:04:45.000+0100},
  author = {Roediger, Henry L. and Butler, Andrew C.},
  biburl = {https://www.bibsonomy.org/bibtex/293d7a12619710f38e204cb61ab7df131/vngudivada},
  interhash = {6c67fe4de60b6aa30ad0a8da4b83f97f},
  intrahash = {93d7a12619710f38e204cb61ab7df131},
  journal = {Trends in Cognitive Sciences},
  keywords = {Learning RetrievalPractice},
  number = 1,
  pages = {20 - 27},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {The critical role of retrieval practice in long-term retention},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661310002081},
  volume = 15,
  year = 2011
}

@book{fink2013creating,
  abstract = {In this thoroughly updated edition of L. Dee Fink's bestselling classic, he discusses new research on how people learn, active learning, and the effectiveness of his popular model adds more examples from online teaching; and further focuses on the impact of student engagement on student learning. The book explores the changes in higher education nationally and internationally since the publication of the previous edition, includes additional procedures for integrating one's course, and adds strategies for dealing with student resistance to innovative teaching. This edition continues to provide conceptual and procedural tools that are invaluable for all teachers when designing instruction. It shows how to use a taxonomy of significant learning and systematically combine the best research-based practices for learning-centered teaching with a teaching strategy in a way that results in powerful learning experiences for students. Acquiring a deeper understanding of the design process will empower teachers to creatively design courses that will result in significant learning for students},
  added-at = {2017-02-11T20:17:10.000+0100},
  address = {San Francisco, CA},
  author = {Fink, L. Dee},
  biburl = {https://www.bibsonomy.org/bibtex/2c890c87bf624639c99ccad9e9d692bee/vngudivada},
  edition = {Second},
  interhash = {45835a5bc58e8fa5271f856519cffc2f},
  intrahash = {c890c87bf624639c99ccad9e9d692bee},
  keywords = {Book CourseDesign Learning},
  publisher = {Jossey-Bass},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Creating significant learning experiences: an integrated approach to designing college courses},
  year = 2013
}

@article{dervin1998sensemaking,
  abstract = {The Sense-making approach to studying and understanding users and designing systems to serve their needs is reviewed. The approach, developed to focus on user sense making and sense unmaking in the fields of communication and library and information science, is reviewed in terms of its implications for knowledge management. Primary emphasis is placed on moving conceptualizations of users, information and reality from the noun-based knowledge-as-map frameworks of the past to verb-based frameworks emphasizing diversity, complexity and sense-making potentials. Knowledge management is described as a field on the precipice of chaos, reaching for a means of emphasizing diversity, complexity and people over centrality, simplicity and technology. Sense making, as an approach, is described as a methodology disciplining the cacophony of diversity and complexity without homogenizing it. Knowledge is reconceptualized from noun to verb.},
  added-at = {2017-01-30T16:16:35.000+0100},
  author = {Dervin, Brenda},
  biburl = {https://www.bibsonomy.org/bibtex/26101d974655b4444784cc7253dc0d011/vngudivada},
  interhash = {2086397d7cd8b062e29d3b888f67cb01},
  intrahash = {6101d974655b4444784cc7253dc0d011},
  journal = {Journal of Knowledge Management},
  keywords = {KnowledgeSeeking SenseMaking},
  number = 2,
  pages = {36 - 46},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Sense‐making theory and practice: an overview of user interests in knowledge seeking and use},
  url = {http://dx.doi.org/10.1108/13673279810249369    },
  volume = 2,
  year = 1998
}

@book{xie2016dynamic,
  abstract = {Suitable for both beginners and advanced users, Dynamic Documents with R and knitr, Second Edition makes writing statistical reports easier by integrating computing directly with reporting. Reports range from homework, projects, exams, books, blogs, and web pages to virtually any documents related to statistical graphics, computing, and data analysis. The book covers basic applications for beginners while guiding power users in understanding the extensibility of the knitr package,},
  added-at = {2017-03-05T17:56:23.000+0100},
  address = {Boca Raton, Florida},
  author = {Xie, Yihui},
  biburl = {https://www.bibsonomy.org/bibtex/27da25ebde1dfd90244a12e563befb19b/vngudivada},
  interhash = {e164f9bc77b9e0229a6b60e937c61196},
  intrahash = {7da25ebde1dfd90244a12e563befb19b},
  keywords = {Book R knitr},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Dynamic documents with R and knitr},
  year = 2016
}

@article{liu2012gasks,
  abstract = {Many electronic feedback systems have been proposed for writing support. However, most of these systems only aim at supporting writing to communicate instead of writing to learn, as in the case of literature review writing. Trigger questions are potentially forms of support for writing to learn, but current automatic question generation approaches focus on factual question generation for reading comprehension or vocabulary assessment. This article presents a novel Automatic Question Generation (AQG) system, called G-Asks, which generates specific trigger questions as a form of support for students' learning through writing. We conducted a large-scale case study, including 24 human supervisors and 33 research students, in an Engineering Research Method course and compared questions generated by G-Asks with human generated questions. The results indicate that G-Asks can generate questions as useful as human supervisors ('useful' is one of five question quality measures) while significantly outperforming Human Peer and Generic Questions in most quality measures after filtering out questions with grammatical and semantic errors. Furthermore, we identified the most frequent question types, derived from the human supervisors' questions and discussed how the human supervisors generate such questions from the source text.},
  added-at = {2017-01-30T13:27:19.000+0100},
  author = {Liu, Ming and Calvo, Rafael and Rus, Vasile},
  biburl = {https://www.bibsonomy.org/bibtex/2df7858e6256793da227921eaad99972e/vngudivada},
  interhash = {d49143a392326f12fcd7f37669992440},
  intrahash = {df7858e6256793da227921eaad99972e},
  journal = {Dialogue {\&} Discourse},
  keywords = {QuestionGeneration},
  number = 2,
  pages = {101--124},
  publisher = {Dialogue and Discourse},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {G-Asks: An Intelligent Automatic Question Generation System for Academic Writing Support},
  url = {https://doi.org/10.5087%2Fdad.2012.205},
  volume = 3,
  year = 2012
}

@book{alder2001introduction,
  added-at = {2017-03-11T14:31:34.000+0100},
  author = {Alder, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2f65ccf0598de8c3068764fb9a8b168e8/vngudivada},
  interhash = {2d7edfb37db46e0a6d242685315a3a44},
  intrahash = {f65ccf0598de8c3068764fb9a8b168e8},
  keywords = {Book MathematicalModeling},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {An Introduction to Mathematical Modeling},
  url = {http://mtm.ufsc.br/~daniel/matap/IntMatMod.pdf},
  year = 2001
}

@book{meerschaert2013mathematical,
  abstract = {Meerschaert's new edition strengthens his position as the survey text of choice for mathematical modeling courses, adding ample instructor support and leveraging on-line delivery for solutions manuals and software ancillaries. From genetic engineering to hurricane prediction, mathematical models guide much of the decision-making in our society, and if the assumptions and methods underlying the modeling are flawed, the outcome can be disastrously poor, as recent events have proved. Since mathematical modeling is a rapidly growing specialty with applications in so many scientific and technical disciplines, there is a need for mathematically rigorous treatments of the subject, and particularly for texts that expose students to a range of possible approaches."--Publisher's website},
  added-at = {2017-03-11T14:29:45.000+0100},
  address = {Amsterdam, Netherlands},
  author = {Meerschaert, Mark M.},
  biburl = {https://www.bibsonomy.org/bibtex/2bf9ee618a438a7d857ab18cb4c088659/vngudivada},
  edition = {Fourth},
  interhash = {5955c5a85793366d211e4c56bfd39753},
  intrahash = {bf9ee618a438a7d857ab18cb4c088659},
  keywords = {Book MathematicalModeling},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Mathematical modeling},
  year = 2013
}

@article{ritter2007cognitive,
  abstract = {For 25 years, we have been working to build cognitive models of mathematics, which have become a basis for middle- and high-school curricula. We discuss the theoretical background of this approach and evidence that the resulting curricula are more effective than other approaches to instruction. We also discuss how embedding a well specified theory in our instructional software allows us to dynamically evaluate the effectiveness of our instruction at a more detailed level than was previously possible. The current widespread use of the software is allowing us to test hypotheses across large numbers of students. We believe that this will lead to new approaches both to understanding mathematical cognition and to improving instruction.},
  added-at = {2017-03-08T16:52:43.000+0100},
  author = {Ritter, Steven and Anderson, John R. and Koedinger, Kenneth R. and Corbett, Albert},
  biburl = {https://www.bibsonomy.org/bibtex/2c2c1407d527f9b9c75c1eddc65e9a098/vngudivada},
  interhash = {d4f90c20f749296997d39be61b9cb6c4},
  intrahash = {c2c1407d527f9b9c75c1eddc65e9a098},
  journal = {Psychonomic Bulletin {\&} Review},
  keywords = {CognitiveComputing CognitiveTutor},
  number = 2,
  pages = {249--255},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Cognitive Tutor: Applied research in mathematics education},
  url = {http://dx.doi.org/10.3758/BF03194060},
  volume = 14,
  year = 2007
}

@book{gagne1992principles,
  abstract = {Describes a rationally consistent basis for instructional design based in cognitive psychology and information-processing theory. Prepares teachers to design and develop a course, unit, and module of instruction; outlines the nine stages of instructional design procedure; and integrates current research and practice in the movement toward performance systems technology.},
  added-at = {2017-03-10T04:11:12.000+0100},
  author = {Gagne, Robert M.},
  biburl = {https://www.bibsonomy.org/bibtex/26277a5131657118b6bd05fdff9a09377/vngudivada},
  edition = {Fourth},
  interhash = {ae638b52f7ed88ddd8dfdeae758b3aa9},
  intrahash = {6277a5131657118b6bd05fdff9a09377},
  keywords = {Book InstructionalDesign},
  publisher = {Wadsworth Publishing},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Principles of Instructional Design },
  year = 1992
}

@article{siddiqi2010improving,
  abstract = {Automated short-answer marking cannot “guarantee” 100 percent agreement between the marks generated by a software system and marks produced separately by a human. This problem has prevented automated marking systems from being used in high-stake short-answer marking. This paper describes how an automated short-answer marking system, called IndusMarker, can be effectively used to improve teaching and learning in an Object-Oriented Programming course taught at a university. The system is designed for factual answers where there is a clear criterion for answers being right or wrong. The system is based on structure matching, i.e., matching a prespecified structure, developed via a purpose-built structure editor, with the content of the student's answer text.},
  added-at = {2017-02-13T00:34:57.000+0100},
  author = {Siddiqi, Raheel and Harrison, Christopher J and Siddiqi, Rosheena},
  biburl = {https://www.bibsonomy.org/bibtex/2c91884fc168e071d754dcc70293d8786/vngudivada},
  interhash = {6f9e8d0e409968afc269415436710aaa},
  intrahash = {c91884fc168e071d754dcc70293d8786},
  journal = {{IEEE} Transactions on Learning Technologies},
  keywords = {QuestionGeneration},
  month = jul,
  number = 3,
  pages = {237--249},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Improving Teaching and Learning through Automated Short-Answer Marking},
  volume = 3,
  year = 2010
}

@book{geary2007origin,
  abstract = {Darwin considered an understanding of the evolution of the human mind and brain to be of major importance to the evolutionary sciences. This groundbreaking book sets out a comprehensive, integrated theory of why and how the human mind has developed to function as it does. Geary proposes that human motivational, affective, behavioral, and cognitive systems have evolved to process social and ecological information (e.g., facial expressions) that covaried with survival or reproductive options during human evolution. Further, he argues that the ultimate focus of all of these systems is to support our attempts to gain access to and control of resources - more specifically, the social (e.g., mates), biological (e.g., food), and physical (e.g., territory) resources that supported successful survival and reproduction over time. In this view, Darwin's conceptualization of natural selection as a struggle for existence becomes, for us, a struggle with other human beings for control of the available resources. This struggle provides a means of integrating modular brain and cognitive systems such as language with those brain and cognitive systems that support general intelligence. findings in cognitive science and neuroscience as well as primatology, anthropology, and sociology. The book also explores a number of issues that are of interest in modern society, including how general intelligence relates to academic achievement, occupational status, and income. Readers will find this book a thought-provoking read and an impetus for new theories of mind.},
  added-at = {2017-03-12T00:08:23.000+0100},
  address = {Washington, D.C.},
  author = {Geary, David C.},
  biburl = {https://www.bibsonomy.org/bibtex/242e2ebae389745135139c3f7368b4fce/vngudivada},
  interhash = {88cb27bd02ad6734f520f6b67c82a3be},
  intrahash = {42e2ebae389745135139c3f7368b4fce},
  keywords = {Book Mind},
  publisher = {American Psychological Association},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {The origin of mind: evolution of brain, cognition, and general intelligence},
  year = 2007
}

@book{blitzstein2015introduction,
  abstract = {Developed from celebrated Harvard statistics lectures, Introduction to Probability provides essential language and tools for understanding statistics, randomness, and uncertainty. The book explores a wide variety of applications and examples, ranging from coincidences and paradoxes to Google PageRank and Markov chain Monte Carlo (MCMC). Additional application areas explored include genetics, medicine, computer science, and information theory. The print book version includes a code that provides free access to an eBook version. The authors present the material in an accessible style and motivate concepts using real-world examples. Throughout, they use stories to uncover connections between the fundamental distributions in statistics and conditioning to reduce complicated problems to manageable pieces. The book includes many intuitive explanations, diagrams, and practice problems. Each chapter ends with a section showing how to perform relevant simulations and calculations in R, a free statistical software environment},
  added-at = {2017-02-03T13:02:17.000+0100},
  author = {Blitzstein, Joseph K. and Hwang, Jessica},
  biburl = {https://www.bibsonomy.org/bibtex/29570f2a19c1028cdd61c2f55c754ff88/vngudivada},
  interhash = {a792570ebc59e6d76437c96de5a7f56a},
  intrahash = {9570f2a19c1028cdd61c2f55c754ff88},
  keywords = {Book ParExcellence Probability Statistics},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Introduction to probability},
  year = 2015
}

@article{bruce2010informed,
  abstract = {The idea of informed learning, applicable in academic, workplace and community settings, has been derived largely from a program of phenomenographic research in the field of information literacy, which has illuminated the experience of using information to learn. Informed learning is about simultaneous attention to information use and learning, where both information and learning are considered to be relational; and is built upon a series of key concepts such as second-order perspective, simultaneity, awareness, and relationality. Informed learning also relies heavily on reflection as a strategy for bringing about learning. As a pedagogical construct, informed learning supports inclusive curriculum design and implementation. Aspects of the informed learning research agenda are currently being explored at the Queensland University of Technology (QUT).},
  added-at = {2017-01-30T16:08:11.000+0100},
  author = {Bruce, Christine and Hughes, Hilary},
  biburl = {https://www.bibsonomy.org/bibtex/2917cca5918e2047f7f3722cd0b89fc3d/vngudivada},
  interhash = {4efb672a3c459397020392df7efc96b5},
  intrahash = {917cca5918e2047f7f3722cd0b89fc3d},
  journal = {Library \& Information Science Research},
  keywords = {Learning QuestionGeneration},
  number = 4,
  pages = {544 - 549},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Informed learning: A pedagogical construct attending simultaneously to information use and learning},
  volume = 32,
  year = 2010
}

@article{paul2015researchers,
  abstract = {Too often school assessments heighten anxiety and hinder learning. New research shows how to reverse the trend.

Since the enactment of No Child Left Behind in 2002, parents' and teachers' opposition to the law's mandate to test “every child, every year” in grades three through eight has been intensifying.
Critics charge that the high-stakes assessments inflict anxiety on students and teachers, turning classrooms into test-preparation factories instead of laboratories of meaningful learning.
Research in cognitive science and psychology shows that testing, done right, can be an effective way to learn. Taking tests can produce better recall of facts and a deeper understanding than an education devoid of exams.
Tests being developed to assess how well students have met the Common Core State Standards show promise as evaluations of deep learning.
In schools across the U.S., multiple-choice questions such as this one provoke anxiety, even dread. Their appearance means it is testing time, and tests are big, important, excruciatingly unpleasant events.
But not at Columbia Middle School in Illinois, in the classroom of eighth grade history teacher Patrice Bain. Bain has lively blue eyes, a quick smile, and spiky platinum hair that looks punkish and pixieish at the same time. After displaying the question on a smartboard, she pauses as her students enter their responses on numbered devices known as clickers.
“Okay, has everyone put in their answers?” she asks. “Number 19, we're waiting on you!” Hurriedly, 19 punches in a selection, and together Bain and her students look over the class's responses, now displayed at the bottom of the smartboard screen. “Most of you got it—John Glenn—very nice.” She chuckles and shakes her head at the answer three of her students have submitted. “Oh, my darlings,” says Bain in playful reproach. “Khrushchev was not an astronaut!”
Bain moves on to the next question, briskly repeating the process of asking, answering and explaining as she and her students work through the decade of the 1960s.
When every student gives the correct answer, the class members raise their hands and wiggle their fingers in unison, an exuberant gesture they call “spirit fingers.” This is the case with the Bay of Pigs question: every student nails it.
“All right!” Bain enthuses. “That's our fifth spirit fingers today!”
The banter in Bain's classroom is a world away from the tense standoffs at public schools around the country. Since the enactment of No Child Left Behind in 2002, parents' and teachers' opposition to the law's mandate to test “every child, every year” in grades three through eight has been intensifying. A growing number of parents are withdrawing their children from the annual state tests; the epicenter of the “opt-out” movement may be New York State, where as many as 90 percent of students in some districts reportedly refused to take the year-end examination last spring. Critics of U.S. schools' heavy emphasis on testing charge that the high-stakes assessments inflict anxiety on students and teachers, turning classrooms into test-preparation factories instead of laboratories of genuine, meaningful learning.
In the always polarizing debate over how American students should be educated, testing has become the most controversial issue of all. Yet a crucial piece has been largely missing from the discussion so far. Research in cognitive science and psychology shows that testing, done right, can be an exceptionally effective way to learn. Taking tests, as well as engaging in well-designed activities before and after tests, can produce better recall of facts—and deeper and more complex understanding—than an education without exams. But a testing regime that actively supports learning, in addition to simply assessing, would look very different from the way American schools “do” testing today.
What Bain is doing in her classroom is called retrieval practice. The practice has a well-established base of empirical support in the academic literature, going back almost 100 years—but Bain, unaware of this research, worked out something very similar on her own over the course of a 21-year career in the classroom.
“I've been told I'm a wonderful teacher, which is nice to hear, but at the same time I feel the need to tell people: ‘No, it's not me—it's the method,’” says Bain in an interview after her class has ended. “I felt my way into this approach, and I've seen it work such wonders that I want to get up on a mountaintop and shout so everyone can hear me: ‘You should be doing this, too!’ But it's been hard to persuade other teachers to try it.”
Then, eight years ago, she met Mark McDaniel through a mutual acquaintance. McDaniel is a psychology professor at Washington University in St. Louis, a half an hour's drive from Bain's school. McDaniel had started to describe to Bain his research on retrieval practice when she broke in with an exclamation. “Patrice said, ‘I do that in my classroom! It works!’” McDaniel recalls. He went on to explain to Bain that what he and his colleagues refer to as retrieval practice is, essentially, testing. “We used to call it ‘the testing effect’ until we got smart and realized that no teacher or parent would want to touch a technique that had the word ‘test’ in it,” McDaniel notes now.
Retrieval practice does not use testing as a tool of assessment. Rather it treats tests as occasions for learning, which makes sense only once we recognize that we have misunderstood the nature of testing. We think of tests as a kind of dipstick that we insert into a student's head, an indicator that tells us how high the level of knowledge has risen in there—when in fact, every time a student calls up knowledge from memory, that memory changes. Its mental representation becomes stronger, more stable and more accessible.
Why would this be? It makes sense considering that we could not possibly remember everything we encounter, says Jeffrey Karpicke, a professor of cognitive psychology at Purdue University. Given that our memory is necessarily selective, the usefulness of a fact or idea—as demonstrated by how often we have had reason to recall it—makes a sound basis for selection. “Our minds are sensitive to the likelihood that we'll need knowledge at a future time, and if we retrieve a piece of information now, there's a good chance we'll need it again,” Karpicke explains. “The process of retrieving a memory alters that memory in anticipation of demands we may encounter in the future.”
Studies employing functional magnetic resonance imaging of the brain are beginning to reveal the neural mechanisms behind the testing effect. In the handful of studies that have been conducted so far, scientists have found that calling up information from memory, as compared with simply restudying it, produces higher levels of activity in particular areas of the brain. These brain regions are associated with the so-called consolidation, or stabilization, of memories and with the generation of cues that make memories readily accessible later on. Across several studies, researchers have demonstrated that the more active these regions are during an initial learning session, the more successful is study participants' recall weeks or months later.
According to Karpicke, retrieving is the principal way learning happens. “Recalling information we've already stored in memory is a more powerful learning event than storing that information in the first place,” he says. “Retrieval is ultimately the process that makes new memories stick.” Not only does retrieval practice help students remember the specific information they retrieved, it also improves retention for related information that was not directly tested. Researchers theorize that while sifting through our mind for the particular piece of information we are trying to recollect, we call up associated memories and in so doing strengthen them as well. Retrieval practice also helps to prevent students from confusing the material they are currently learning with material they learned previously and even appears to prepare students' minds to absorb the material still more thoroughly when they encounter it again after testing (a phenomenon researchers call “test-potentiated learning”).
Hundreds of studies have demonstrated that retrieval practice is better at improving retention than just about any other method learners could use. To cite one example: in a study published in 2008 by Karpicke and his mentor, Henry Roediger III of Washington University, the authors reported that students who quizzed themselves on vocabulary terms remembered 80 percent of the words later on, whereas students who studied the words by repeatedly reading them over remembered only about a third of the words. Retrieval practice is especially powerful compared with students' most favored study strategies: highlighting and rereading their notes and textbooks, practices that a recent review found to be among the least effective.
And testing does not merely enhance the recall of isolated facts. The process of pulling up information from memory also fosters what researchers call deep learning. Students engaging in deep learning are able to draw inferences from, and make connections among, the facts they know and are able to apply their knowledge in varied contexts (a process learning scientists refer to as transfer). In an article published in 2011 in the journal Science, Karpicke and his Purdue colleague Janell Blunt explicitly compared retrieval practice with a study technique known as concept mapping. An activity favored by many teachers as a way to promote deep learning, concept mapping asks students to draw a diagram that depicts the body of knowledge they are learning, with the relations among concepts represented by links among nodes, like roads linking cities on a map.
In their study, Karpicke and Blunt directed groups of undergraduate volunteers—200 in all—to read a passage taken from a science textbook. One group was then asked to create a concept map while referring to the text; another group was asked to recall, from memory, as much information as they could from the text they had just read. On a test given to all the students a week later, the retrieval-practice group was better able to recall the concepts presented in the text than the concept-mapping group. More striking, the former group was also better able to draw inferences and make connections among multiple concepts contained in the text. Overall, Karpicke and Blunt concluded, retrieval practice was about 50 percent more effective at promoting both factual and deep learning.
Transfer—the ability to take knowledge learned in one context and apply it to another—is the ultimate goal of deep learning. In an article published in 2010 University of Texas at Austin psychologist Andrew Butler demonstrated that retrieval practice promotes transfer better than the conventional approach of studying by rereading. In Butler's experiment, students engaged either in rereading or in retrieval practice after reading a text that pertained to one “knowledge domain”—in this case, bats' use of sound waves to find their way around. A week later the students were asked to transfer what they had learned about bats to a second knowledge domain: the navigational use of sound waves by submarines. Students who had quizzed themselves on the original text about bats were better able to transfer their bat learning to submarines.
Robust though such findings are, they were until recently almost exclusively made in the laboratory, with college students as subjects. McDaniel had long wanted to apply retrieval practice in real-world schools, but gaining access to K–12 classrooms was a challenge. With Bain's help, McDaniel and two of his Washington University colleagues, Roediger and Kathleen McDermott, set up a randomized controlled trial at Columbia Middle School that ultimately involved nine teachers and more than 1,400 students. During the course of the experiment, sixth, seventh and eighth graders learned about science and social studies in one of two ways: 1) material was presented once, then teachers reviewed it with students three times; 2) material was presented once, and students were quizzed on it three times (using clickers like the ones in Bain's current classroom).
When the results of students' regular unit tests were calculated, the difference between the two approaches was clear: students earned an average grade of C+ on material that had been reviewed and A− on material that had been quizzed. On a follow-up test administered eight months later, students still remembered the information they had been quizzed on much better than the information they had reviewed.
“I had always thought of tests as a way to assess—not as a way to learn—so initially I was skeptical,” says Andria Matzenbacher, a former teacher at Columbia who now works as an instructional designer. “But I was blown away by the difference retrieval practice made in the students' performance.” Bain, for one, was not surprised. “I knew that this method works, but it was good to see it proven scientifically,” she says. McDaniel, Roediger and McDermott eventually extended the study to nearby Columbia High School, where quizzing generated similarly impressive results. In an effort to make retrieval practice a common strategy in classrooms across the country, the Washington University team (with the help of research associate Pooja K. Agarwal, now at Harvard University) developed a manual for teachers, How to Use Retrieval Practice to Improve Learning.
Even with the weight of evidence behind them, however, advocates of retrieval practice must still contend with a reflexively negative reaction to testing among many teachers and parents. They also encounter a more thoughtful objection, which goes something like this: American students are tested so much already—far more often than students in other countries, such as Finland and Singapore, which regularly place well ahead of the U.S. in international evaluations. If testing is such a great way to learn, why aren't our students doing better?
Marsha Lovett has a ready answer to that question. Lovett, director of the Eberly Center for Teaching Excellence and Educational Innovation at Carnegie Mellon University, is an expert on “metacognition”—the capacity to think about our own learning, to be aware of what we know and do not know, and to use that awareness to effectively manage the learning process.
Yes, Lovett says, American students take a lot of tests. It is what happens afterward—or more precisely, what does not happen—that causes these tests to fail to function as learning opportunities. Students often receive little information about what they got right and what they got wrong. “That kind of item-by-item feedback is essential to learning, and we're throwing that learning opportunity away,” she says. In addition, students are rarely prompted to reflect in a big-picture way on their preparation for, and performance on, the test. “Often students just glance at the grade and then stuff the test away somewhere and never look at it again,” Lovett says. “Again, that's a really important learning opportunity that we're letting go to waste.”
A few years ago Lovett came up with a way to get students to engage in reflection after a test. She calls it an “exam wrapper.” When the instructor hands back a graded test to a student, along with it comes a piece of paper literally wrapped around the test itself. On this paper is a list of questions: a short exercise that students are expected to complete and hand in. The wrapper that Lovett designed for a math exam includes such questions as:
Based on the estimates above, what will you do differently in preparing for the next test? For example, will you change your study habits or try to sharpen specific skills? Please be specific. Also, what can we do to help?
The idea, Lovett says, is to get students thinking about what they did not know or did not understand, why they failed to grasp this information and how they could prepare more effectively in advance of the next test. Lovett has been promoting the use of exam wrappers to the Carnegie Mellon faculty for several years now, and a number of professors, especially in the sciences, have incorporated the technique into their courses. They hand out exam wrappers with graded exams, collect the wrappers once they are completed, and—cleverest of all—they hand back the wrappers at the time when students are preparing for the next test.
Does this practice make a difference? In 2013 Lovett published a study of exam wrappers as a chapter in the edited volume Using Reflection and Metacognition to Improve Student Learning. It reported that the metacognitive skills of students in classes that used exam wrappers increased more across the semester than those of students in courses that did not employ exam wrappers. In addition, an end-of-semester survey found that among students who were given exam wrappers, more than half cited specific changes they had made in their approach to learning and studying as a result of filling out the wrapper.
The practice of using exam wrappers is beginning to spread to other universities and to K–12 schools. Lorie Xikes teaches at Riverdale High School in Fort Myers, Fla., and has used exam wrappers in her AP Biology class. When she hands back graded tests, the exam wrapper includes such questions as:
Based on your responses to the questions above, name at least three things you will do differently in preparing for the next test. BE SPECIFIC.
“Students usually just want to know their grade, and that's it,” Xikes says. “Having them fill out the exam wrapper makes them stop and think about how they go about getting ready for a test and whether their approach is working for them or not.”
In addition to distributing exam wrappers, Xikes also devotes class time to going over the graded exam, question by question—feedback that helps students develop the crucial capacity of “metacognitive monitoring,” that is, keeping tabs on what they know and what they still need to learn. Research on retrieval practice shows that testing can identify specific gaps in students' knowledge, as well as puncture the general overconfidence to which students are susceptible—but only if prompt feedback is provided as a corrective.
Over time, repeated exposure to this testing-feedback loop can motivate students to develop the ability to monitor their own mental processes. Affluent students who receive a top-notch education may acquire this skill as a matter of course, but this capacity is often lacking among low-income students who attend struggling schools—holding out the hopeful possibility that retrieval practice could actually begin to close achievement gaps between the advantaged and the underprivileged.
This is just what James Pennebaker and Samuel Gosling, professors at the University of Texas at Austin, found when they instituted daily quizzes in the large psychology course they teach together. The quizzes were given online, using software that informed students whether they had responded correctly to a question immediately after they submitted an answer. The grades earned by the 901 students in the course featuring daily quizzes were, on average, about half a letter grade higher than those earned by a comparison group of 935 of Pennebaker and Gosling's previous students, who had experienced a more traditionally designed course covering the same material.
Astonishingly, students who took the daily quizzes in their psychology class also performed better in their other courses, during the semester they were enrolled in Pennebaker and Gosling's class and in the semesters that followed—suggesting that the frequent tests accompanied by feedback worked to improve their general skills of self-regulation. Most exciting to the professors, the daily quizzes led to a 50 percent reduction in the achievement gap, as measured by grades, among students of different social classes. “Repeated testing is a powerful practice that directly enhances learning and thinking skills, and it can be especially helpful to students who start off with a weaker academic background,” Gosling says.
Gosling and Pennebaker, who (along with U.T. graduate student Jason Ferrell) published their findings on the effects of daily quizzes in 2013 in the journal PLOS ONE, credited the “rapid, targeted, and structured feedback” that students received with boosting the effectiveness of repeated testing. And therein lies a dilemma for American public school students, who take an average of 10 standardized tests a year in grades three through eight, according to a recent study conducted by the Center for American Progress. Unlike the instructor-written tests given by the teachers and professors profiled here, standardized tests are usually sold to schools by commercial publishing companies. Scores on these tests often arrive weeks or even months after the test is taken. And to maintain the security of test items—and to use the items again on future tests—testing firms do not offer item-by-item feedback, only a rather uninformative numerical score.
There is yet another feature of standardized state tests that prevents them from being used more effectively as occasions for learning. The questions they ask are overwhelmingly of a superficial nature—which leads, almost inevitably, to superficial learning.
If the state tests currently in use in U.S. were themselves assessed on the difficulty and depth of the questions they ask, almost all of them would flunk. That is the conclusion reached by Kun Yuan and Vi-Nhuan Le, both then behavioral scientists at RAND Corporation, a nonprofit think tank. In a report published in 2012 Yuan and Le evaluated the mathematics and English language arts tests offered by 17 states, rating each question on the tests on the cognitive challenge it poses to the test taker. The researchers used a tool called Webb's Depth of Knowledge—created by Norman Webb, a senior scientist at the Wisconsin Center for Education Research—which identifies four levels of mental rigor, from DOK1 (simple recall), to DOK2 (application of skills and concepts), through DOK3 (reasoning and inference), and DOK4 (extended planning and investigation).
Most questions on the state tests Yuan and Le examined were at level DOK1 or DOK2. The authors used level DOK4 as their benchmark for questions that measure deeper learning, and by this standard the tests are failing utterly. Only 1 to 6 percent of students were assessed on deeper learning in reading through state tests, Yuan and Le report; 2 to 3 percent were assessed on deeper learning in writing; and 0 percent were assessed on deeper learning in mathematics. “What tests measure matters because what's on the tests tends to drive instruction,” observes Linda Darling-Hammond, emeritus professor at the Stanford Graduate School of Education and a national authority on learning and assessment. That is especially true, she notes, when rewards and punishments are attached to the outcomes of the tests, as is the case under the No Child Left Behind law and states' own “accountability” measures.
According to Darling-Hammond, the provisions of No Child Left Behind effectively forced states to employ inexpensive, multiple-choice tests that could be scored by machine—and it is all but impossible, she contends, for such tests to measure deep learning. But other kinds of tests could do so. Darling-Hammond wrote, with her Stanford colleague Frank Adamson, the 2014 book Beyond the Bubble Test, which describes a very different vision of assessment: tests that pose open-ended questions (the answers to which are evaluated by teachers, not machines); that call on students to develop and defend an argument; and that ask test takers to conduct a scientific experiment or construct a research report.
In the 1990s Darling-Hammond points out, some American states had begun to administer such tests; that effort ended with the passage of No Child Left Behind. She acknowledges that the movement toward more sophisticated tests also stalled because of concerns about logistics and cost. Still, assessing students in this way is not a pie-in-the-sky fantasy: Other nations, such as England and Australia, are doing so already. “Their students are performing the work of real scientists and historians, while our students are filling in bubbles,” Darling-Hammond says. “It's pitiful.”
She does see some cause for optimism: A new generation of tests are being developed in the U.S. to assess how well students have met the Common Core State Standards, the set of academic benchmarks in literacy and math that have been adopted by 43 states. Two of these tests—Smarter Balanced and Partnership for Assessment of Readiness for College and Careers (PARCC)—show promise as tests of deep learning, says Darling-Hammond, pointing to a recent evaluation conducted by Joan Herman and Robert Linn, researchers at U.C.L.A.'s National Center for Research on Evaluation, Standards, and Student Testing (CRESST). Herman notes that both tests intend to emphasize questions at and above level 2 on Webb's Depth of Knowledge, with at least a third of a student's total possible score coming from questions at DOK3 and DOK4. “PARCC and Smarter Balanced may not go as far as we would have liked,” Herman conceded in a blog post last year, but “they are likely to produce a big step forward.”},
  added-at = {2017-03-12T01:33:13.000+0100},
  author = {Paul, Annie Murphy},
  biburl = {https://www.bibsonomy.org/bibtex/2f0aaece0ba9243ecea689b844bd9f777/vngudivada},
  interhash = {f5568fe60422c5b4023d8ede62689b02},
  intrahash = {f0aaece0ba9243ecea689b844bd9f777},
  journal = {Scientific American},
  keywords = {Learning},
  number = 2,
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Researchers Find That Frequent Tests Can Boost Learning},
  volume = 313,
  year = 2015
}

@techreport{coburn2013researchpractice,
  abstract = {In this paper commissioned by the William T. Grant Foundation, Cynthia Coburn, Ph.D.; William Penuel, Ph.D.; and Kimberly Geil, Ph.D. survey the current landscape of partnerships involving school districts to better understand different types of collaborations, the challenges they face, and their strategies for success. Through this paper, they develop sharp insights about this promising field.},
  added-at = {2017-02-04T01:54:08.000+0100},
  author = {Coburn, Cynthia E. and Penuel, William R. and Geil, Kimberly E.},
  biburl = {https://www.bibsonomy.org/bibtex/207816481467faca492bb706029309ae4/vngudivada},
  interhash = {9da5641653ae378909a8debea510e125},
  intrahash = {07816481467faca492bb706029309ae4},
  keywords = {NSF RPP},
  month = jan,
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Research-Practice Partnerships: A Strategy for Leveraging Research for Educational Improvement in School Districts},
  year = 2013
}

@book{he2011mathematics,
  abstract = {"Mathematics of Bioinformatics: Theory, Methods, and Applications provides a comprehensive format for connecting and integrating information derived from mathematical methods and applying it to the understanding of biological sequences, structures, and networks. Each chapter is divided into a number of sections based on the bioinformatics topics and related mathematical theory and methods. Each topic of the section is comprised of the following three parts: an introduction to the biological problems in bioinformatics; a presentation of relevant topics of mathematical theory and methods to the bioinformatics problems introduced in the first part; an integrative overview that draws the connections and interfaces between bioinformatics problems/issues and mathematical theory/methods/applications"--Résumé de l'éditeur.},
  added-at = {2017-03-11T14:25:03.000+0100},
  address = {Hoboken, NJ},
  author = {He, Matthew and Petukhov, S. V.},
  biburl = {https://www.bibsonomy.org/bibtex/2cbfc0bb37cb6307fc46745a2718090b8/vngudivada},
  interhash = {dfbc53fb71b029c83e6a29e6b822b35f},
  intrahash = {cbfc0bb37cb6307fc46745a2718090b8},
  keywords = {Bioinformatics Book},
  publisher = {John Wiley \& Sons},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Mathematics of bioinformatics: theory, practice, and applications},
  year = 2011
}

@article{lyons2012hurts,
  abstract = {Math can be difficult, and for those with high levels of mathematics-anxiety (HMAs), math is associated with tension, apprehension, and fear. But what underlies the feelings of dread effected by math anxiety? Are HMAs’ feelings about math merely psychological epiphenomena, or is their anxiety grounded in simulation of a concrete, visceral sensation – such as pain – about which they have every right to feel anxious? We show that, when anticipating an upcoming math-task, the higher one’s math anxiety, the more one increases activity in regions associated with visceral threat detection, and often the experience of pain itself (bilateral dorso-posterior insula). Interestingly, this relation was not seen during math performance, suggesting that it is not that math itself hurts; rather, the anticipation of math is painful. Our data suggest that pain network activation underlies the intuition that simply anticipating a dreaded event can feel painful. These results may also provide a potential neural mechanism to explain why HMAs tend to avoid math and math-related situations, which in turn can bias HMAs away from taking math classes or even entire math-related career paths.},
  added-at = {2017-03-11T20:31:06.000+0100},
  author = {Lyons, Ian M. and Beilock, Sian L.},
  biburl = {https://www.bibsonomy.org/bibtex/2707e03a9e565f64bd29f6dd36fb09495/vngudivada},
  interhash = {08eed717d60b7e68822cb7cd2db87eaf},
  intrahash = {707e03a9e565f64bd29f6dd36fb09495},
  journal = {PLOS ONE},
  keywords = {Learning Mathematics},
  month = {10},
  number = 10,
  pages = {1-6},
  publisher = {Public Library of Science},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {When Math Hurts: Math Anxiety Predicts Pain Network Activation in Anticipation of Doing Math},
  url = {http://dx.doi.org/10.1371%2Fjournal.pone.0048076},
  volume = 7,
  year = 2012
}

@book{vanderlans2006introduction,
  abstract = {The Classic SQL Tutorial: Fully Updated for Today’s Standards and Today’s Top Databases
For twenty years, van der Lans’ Introduction to SQL has been the definitive SQL tutorial for database professionals everywhere, regardless of experience or platform. Now van der Lans has systematically updated this classic guide to reflect the latest SQL standards and the newest versions of today’s leading RDBMSs: Oracle, Microsoft SQL Server, DB2, and MySQL.

Using case study examples and hands-on exercises, van der Lans illuminates every key SQL concept, technique, and statement. Drawing on decades of experience as an SQL standards team member and enterprise consultant, he reveals exactly why SQL works as it does–and how to get the most out of it. You’ll gain powerful insight into everything from basic queries to stored procedures, transactions to data security. Whether you’re a programmer or DBA, a student or veteran, this book will take you from “apprentice” to true SQL master.
  Writing queries and updating data: all you need to know about SELECT
  Working with joins, functions, and subqueries
  Creating database objects: tables, indexes, views, and more
  Specifying keys and other integrity constraints
  Using indexes to improve efficiency
  Enforcing security via passwords and privileges
  Building stored procedures and triggers
  Developing with embedded SQL and ODBC
  Working with transactions, including rollbacks, savepoints, isolation levels, and more
  Optimizing performance by reformulating SQL statements
  Using object-relational features: subtables, references, sets, and user-defined data types
  Reference section: SQL statement definitions and SQL function lists},
  added-at = {2017-02-13T13:10:38.000+0100},
  address = {Boston, MA},
  author = {van der Lans, Rick F.},
  biburl = {https://www.bibsonomy.org/bibtex/25f9cb29b1823d8f9bc151fe183b9f5f3/vngudivada},
  edition = {Fourth},
  interhash = {8285f327bad0593c3cf710c2aad53d9b},
  intrahash = {5f9cb29b1823d8f9bc151fe183b9f5f3},
  keywords = {Book RDBMS SQL},
  publisher = {Addison-Wesley},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Introduction to SQL: Mastering the Relational Database Language},
  year = 2006
}

@article{roediger2006power,
  abstract = {A powerful way of improving one's memory for material is to be tested on that material. Tests enhance later retention more than additional study of the material, even when tests are given without feedback. This surprising phenomenon is called the testing effect, and although it has been studied by cognitive psychologists sporadically over the years, today there is a renewed effort to learn why testing is effective and to apply testing in educational settings. In this article, we selectively review laboratory studies that reveal the power of testing in improving retention and then turn to studies that demonstrate the basic effects in educational settings. We also consider the related concepts of dynamic testing and formative assessment as other means of using tests to improve learning. Finally, we consider some negative consequences of testing that may occur in certain circumstances, though these negative effects are often small and do not cancel out the large positive effects of testing. Frequent testing in the classroom may boost educational achievement at all levels of education.},
  added-at = {2017-03-12T01:08:36.000+0100},
  author = {Roediger, Henry L. and Karpicke, Jeffrey D.},
  biburl = {https://www.bibsonomy.org/bibtex/2eef620595a090cb96dbb06a9858d65d8/vngudivada},
  interhash = {116f47e600f59740e9201d5921bb70c2},
  intrahash = {eef620595a090cb96dbb06a9858d65d8},
  journal = {Perspectives on Psychological Science},
  keywords = {Learning MemoryTesting},
  month = sep,
  number = 3,
  pages = {181--210},
  publisher = {{SAGE} Publications},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {The Power of Testing Memory: Basic Research and Implications for Educational Practice},
  url = {https://doi.org/10.1111%2Fj.1745-6916.2006.00012.x},
  volume = 1,
  year = 2006
}

@article{karpicke2012retrievalbased,
  abstract = {Learning is often identified with the acquisition, encoding, or construction of new knowledge, while retrieval is often considered only a means of assessing knowledge, not a process that contributes to learning. Here, we make the case that retrieval is the key process for understanding and for promoting learning. We provide an overview of recent research showing that active retrieval enhances learning, and we highlight ways researchers have sought to extend research on active retrieval to meaningful learning---the learning of complex educational materials as assessed on measures of inference making and knowledge application. However, many students lack metacognitive awareness of the benefits of practicing active retrieval. We describe two approaches to addressing this problem: classroom quizzing and a computer-based learning program that guides students to practice retrieval. Retrieval processes must be considered in any analysis of learning, and incorporating retrieval into educational activities represents a powerful way to enhance learning.},
  added-at = {2017-03-12T00:24:12.000+0100},
  author = {Karpicke, Jeffrey D. and Grimaldi, Phillip J.},
  biburl = {https://www.bibsonomy.org/bibtex/2b1643f3d9ffb9f5bed100ede1d8ae3b6/vngudivada},
  interhash = {a8a5fa66b1c8dc8e1f980c8b389e67c7},
  intrahash = {b1643f3d9ffb9f5bed100ede1d8ae3b6},
  journal = {Educational Psychology Review},
  keywords = {Learning},
  number = 3,
  pages = {401--418},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Retrieval-Based Learning: A Perspective for Enhancing Meaningful Learning},
  url = {http://dx.doi.org/10.1007/s10648-012-9202-2},
  volume = 24,
  year = 2012
}

@book{lieven2009crosslinguistic,
  abstract = {This volume covers state-of-the-art research in the field of crosslinguistic approaches to the psychology of language. The forty chapters cover a wide range of topics that represent the many research interests of a pioneer, Dan Isaac Slobin, who has been a major intellectual and creative force in the field of child language development, linguistics, and psycholinguistics for the past four decades. Slobin has insisted on a rigorous, crosslinguistic approach in his attempt to identify universal developmental patterns in language learning, to explore the effects of particular types of languages on psycholinguistic processes, to determine the extent to which universals of language and language behavior are determined by modality (vocal/auditory vs. manual/visual) and, finally, to investigate the relation between linguistic and cognitive processes. In this volume, researchers take up the challenge of the differences between languages to forward research in four major areas with which Slobin has been concerned throughout his career: language learning in crosslinguistic perspective (spoken and sign languages); the integration of language specific factors in narrative skill; theoretical issues in typology, language development and language change; and, the relationship between language and cognition. All chapters are written by leading researchers currently working in these fields, who are Slobin's colleagues, collaborators or former students in linguistics, psychology, anthropology, and cognitive science. Each section starts with an introductory chapter that connects the themes of the chapters and reviews Slobin's contribution in the context of past research trends and future directions. The whole volume focuses squarely on the central argument: universals of human language and of its development are embodied and revealed in its diverse manifestations and utilization. "Crosslinguistic Approaches to the Study of Language" is a key resource for those interested in the range of differences between languages and how this impacts on learning, cognition and language change, and a tribute to Dan Slobin's momentous contribution to the field.},
  added-at = {2017-03-05T04:12:07.000+0100},
  author = {Lieven, Elena and Guo, Jiansheng and Budwig, Nancy and Ervin-Tripp, Susan and Nakamura, Keiko and Ozcaliskan, Seyda},
  biburl = {https://www.bibsonomy.org/bibtex/218b8d5d5a398de96e08fa548d428c4a8/vngudivada},
  interhash = {59dab8e852edf4fef6333fb5f4fe7855},
  intrahash = {18b8d5d5a398de96e08fa548d428c4a8},
  keywords = {Book Crosslinguistics},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Crosslinguistic Approaches to the Study of Language: Research in the Tradition of Dan Isaac Slobin},
  url = {http://www.myilibrary.com?id=199420},
  year = 2009
}

@book{oakley2014numbers,
  abstract = {Whether you are a student struggling to fulfill a math or science requirement, or you are embarking on a career change that requires a new skill set, A Mind for Numbers offers the tools you need to get a better grasp of that intimidating material. Engineering professor Barbara Oakley knows firsthand how it feels to struggle with math. She flunked her way through high school math and science courses, before enlisting in the army immediately after graduation. When she saw how her lack of mathematical and technical savvy severely limited her options—both to rise in the military and to explore other careers—she returned to school with a newfound determination to re-tool her brain to master the very subjects that had given her so much trouble throughout her entire life.

In A Mind for Numbers, Dr. Oakley lets us in on the secrets to learning effectively—secrets that even dedicated and successful students wish they’d known earlier. Contrary to popular belief, math requires creative, as well as analytical, thinking. Most people think that there’s only one way to do a problem, when in actuality, there are often a number of different solutions—you just need the creativity to see them. For example, there are more than three hundred different known proofs of the Pythagorean Theorem. In short, studying a problem in a laser-focused way until you reach a solution is not an effective way to learn. Rather, it involves taking the time to step away from a problem and allow the more relaxed and creative part of the brain to take over. The learning strategies in this book apply not only to math and science, but to any subject in which we struggle. We all have what it takes to excel in areas that don't seem to come naturally to us at first, and learning them does not have to be as painful as we might think!},
  added-at = {2017-03-10T03:41:22.000+0100},
  address = {New York, NY},
  author = {Oakley, Barbara A.},
  biburl = {https://www.bibsonomy.org/bibtex/29c406406e54d9ff22a6b813628bb40be/vngudivada},
  interhash = {c6b01815e287e32795b55ac82d78d97c},
  intrahash = {9c406406e54d9ff22a6b813628bb40be},
  keywords = {Book Learning},
  publisher = {Jeremy P. Tarcher/Penguin},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {A mind for numbers: how to excel at math and science},
  year = 2014
}

@book{silberman2006teaching,
  abstract = {It’s not what you tell your students that counts but rather what they take away from the classroom! Teaching Actively is the long-awaited follow-up to Mel Silberman’s best-selling book Active Learning:101 Strategies to Teach Any Subject.  Where Active Learning is the ultimate book of “recipes” for improving classroom teaching, Teaching Actively shows teachers how to become exceptional chefs.

Teaching Actively is a comprehensive plan teachers can follow rather than a collection of ideas from which to pick and chose.  The book outlines an eight-step plan for any teacher who has students with basic skills in “reading, writing, and arithmetic” and the ability to collaborate with fellow classmates. It is applicable for all grades and focuses and contains practical, ready-to-use ideas for improving a student’s involvement in his or her education. The book demonstrates how to facilitate games, practice exercises, role-plays, and other experiential activities to enhance the learning.

From engaging students to presenting ideas, Teaching Actively is an approach to teaching that any teacher can utilize to improve the learning in his or her classroom. Silberman offers practical suggestions on involving students in their own learning and addresses specific ways to encourage peer-learning and incorporate technology into the lesson.},
  added-at = {2017-03-11T14:05:44.000+0100},
  address = {Boston, Massachusetts},
  author = {Silberman, Mel},
  biburl = {https://www.bibsonomy.org/bibtex/29b83bc70fedc11d17784d4ae93a92053/vngudivada},
  interhash = {4915d17f3d64cfb870953ae297f4a311},
  intrahash = {9b83bc70fedc11d17784d4ae93a92053},
  keywords = {Book Learning StudentEngagement},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Teaching actively: eight steps and 32 strategies to spark learning in any classroom},
  year = 2006
}

@inproceedings{chen2006automatic,
  abstract = {This paper introduces a method for the semi-automatic generation of grammar test items by applying Natural Language Processing (NLP) techniques. Based on manually-designed patterns, sentences gathered from the Web are transformed into tests on grammaticality. The method involves representing test writing knowledge as test patterns, acquiring authentic sentences on the Web, and applying generation strategies to transform sentences into items. At runtime, sentences are converted into two types of TOEFL-style question: multiple-choice and error detection. We also describe a prototype system FAST (Free Assessment of Structural Tests). Evaluation on a set of generated questions indicates that the proposed method performs satisfactory quality. Our methodology provides a promising approach and offers significant potential for computer assisted language learning and assessment.},
  added-at = {2017-02-12T06:26:41.000+0100},
  address = {Stroudsburg, PA},
  author = {Chen, Chia-Yin and Liou, Hsien-Chin and Chang, Jason S.},
  biburl = {https://www.bibsonomy.org/bibtex/293a01570c3bcfc63dc164d7eeeba2c12/vngudivada},
  booktitle = {Proceedings of the COLING/ACL on Interactive Presentation Sessions},
  interhash = {bd7d878b81235f8ca116c9019b340894},
  intrahash = {93a01570c3bcfc63dc164d7eeeba2c12},
  keywords = {Grammar QuestionGeneration},
  pages = {1--4},
  publisher = {Association for Computational Linguistics},
  series = {COLING-ACL '06},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {{FAST}: An Automatic Generation System for Grammar Tests},
  url = {http://dx.doi.org/10.3115/1225403.1225404},
  year = 2006
}

@inproceedings{abdulkhalek2010automated,
  abstract = {We present a novel approach for generating syntactically and semantically correct SQL queries as inputs for testing relational databases. We leverage the SAT-based Alloy tool-set to reduce the problem of generating valid SQL queries into a SAT problem. Our approach translates SQL query constraints into Alloy models, which enable it to generate valid queries that cannot be automatically generated using conventional grammar-based generators. Given a database schema, our new approach combined with our previous work on ADUSA, automatically generates (1) syntactically and semantically valid SQL queries for testing, (2) input data to populate test databases, and (3) expected result of executing the given query on the generated data. Experimental results show that not only can we automatically generate valid queries which detect bugs in database engines, but also we are able to combine this work with our previous work on ADUSA to automatically generate input queries and tables as well as expected query execution outputs to enable automated testing of database engines.},
  added-at = {2017-02-12T22:44:57.000+0100},
  address = {New York, NY},
  author = {Abdul Khalek, Shadi and Khurshid, Sarfraz},
  biburl = {https://www.bibsonomy.org/bibtex/247dd12fa7e914eaa6ef3fbb0114088d9/vngudivada},
  booktitle = {Proceedings of the IEEE/ACM International Conference on Automated Software Engineering},
  interhash = {cd579f11817398baf58161ac33398116},
  intrahash = {47dd12fa7e914eaa6ef3fbb0114088d9},
  keywords = {QuestionGeneration SQL},
  pages = {329--332},
  publisher = {ACM},
  series = {ASE '10},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Automated SQL Query Generation for Systematic Testing of Database Engines},
  url = {http://doi.acm.org/10.1145/1858996.1859063},
  year = 2010
}

@book{dirksen2016design,
  abstract = {Products, technologies, and workplaces change so quickly today that everyone is continually learning. Many of us are also teaching, even when it's not in our job descriptions. Whether it's giving a presentation, writing documentation, or creating a website or blog, we need and want to share our knowledge with other people. But if you've ever fallen asleep over a boring textbook, or fast-forwarded through a tedious e-learning exercise, you know that creating a great learning experience is harder than it seems.

In Design For How People Learn, Second Edition, you'll discover how to use the key principles behind learning, memory, and attention to create materials that enable your audience to both gain and retain the knowledge and skills you're sharing. Updated to cover new insights and research into how we learn and remember, this new edition includes new techniques for using social media for learning as well as two brand new chapters on designing for habit and best practices for evaluating learning, such as how and when to use tests. Using accessible visual metaphors and concrete methods and examples, Design For How People Learn, Second Edition will teach you how to leverage the fundamental concepts of instructional design both to improve your own learning and to engage your audience.},
  added-at = {2017-03-10T03:56:59.000+0100},
  address = {Berkeley, California},
  author = {Dirksen, Julie},
  biburl = {https://www.bibsonomy.org/bibtex/2d4a50bd9f97499501fdeff850aa6d186/vngudivada},
  edition = {Second},
  interhash = {d3a832e733917e90ac42b9afa21823ce},
  intrahash = {d4a50bd9f97499501fdeff850aa6d186},
  keywords = {Book Learning},
  publisher = {New Riders},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Design for how people learn},
  year = 2016
}

@book{weisberg2013applied,
  abstract = {Praise for the Third Edition

"...this is an excellent book which could easily be used as a course text..."
—International Statistical Institute

The Fourth Edition of Applied Linear Regression provides a thorough update of the basic theory and methodology of linear regression modeling. Demonstrating the practical applications of linear regression analysis techniques, the Fourth Edition uses interesting, real-world exercises and examples.

Stressing central concepts such as model building, understanding parameters, assessing fit and reliability, and drawing conclusions, the new edition illustrates how to develop estimation, confidence, and testing procedures primarily through the use of least squares regression. While maintaining the accessible appeal of each previous edition, Applied Linear Regression, Fourth Edition features:

Graphical methods stressed in the initial exploratory phase, analysis phase, and summarization phase of an analysis
In-depth coverage of parameter estimates in both simple and complex models, transformations, and regression diagnostics
Newly added material on topics including testing, ANOVA, and variance assumptions
Updated methodology, such as bootstrapping, cross-validation binomial and Poisson regression, and modern model selection methods
Applied Linear Regression, Fourth Edition is an excellent textbook for upper-undergraduate and graduate-level students, as well as an appropriate reference guide for practitioners and applied statisticians in engineering, business administration, economics, and the social sciences.},
  added-at = {2017-03-10T20:57:23.000+0100},
  address = {New York, NY},
  author = {Weisberg, Sanford},
  biburl = {https://www.bibsonomy.org/bibtex/28fc499d689d46cdf3aea8eb46d1fa00b/vngudivada},
  edition = {Fourth},
  interhash = {5ce307846ba47979b27927761c03b94f},
  intrahash = {8fc499d689d46cdf3aea8eb46d1fa00b},
  keywords = {Book LinearRegression},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Applied Linear Regression},
  year = 2013
}

@book{byrne2017learning,
  abstract = {Learning and Memory: A Comprehensive Reference, second edition is the authoritative resource for scientists and students interested in all facets of learning and memory. No other reference work covers so wide a territory and in so much depth.

The study of learning (the process of acquiring new information) and memory (retention of that information for future use) has been and continues to be the focus of intense scientific research because our memories and plans for the future consolidate who we are, while disruption of these processes dramatically interferes with our daily lives.

The second edition of Learning and Memory: A Comprehensive Reference includes updated chapters reflecting the state of the art of research in this area. Coverage of sleep and memory has been significantly expanded while neuromodulators in memory processing, neurogenesis and epigenetics are also covered in greater detail. New chapters have been included to reflect the massive increase in research into working memory and the educational relevance of memory research.

The most comprehensive and authoritative resource available on the study of learning and memory and its mechanisms. Chapters in each section offer detailed and complex coverage of the topics under discussion, providing an authoritative introduction  and enabling users to quickly understand complex subjects
The work incorporates the expertise of over 150 outstanding investigators in the field, providing a ‘one-stop’ resource of reputable information from world-leading scholars with easy cross-referencing of related articles to promote understanding and further research
Thematic structuring of the work encourages easy searchability of topics. Suggested further reading for each chapter helps readers to continue their research in more depth and a glossary of key terms is a helpful guide for users unfamiliar with neuroscience terminology},
  added-at = {2017-03-12T00:56:07.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/24c78de95eafeded80aeba7891b6acb58/vngudivada},
  edition = {Second},
  editor = {Byrne, John H.},
  interhash = {f5951dfad46626559263e640d07d8bf4},
  intrahash = {4c78de95eafeded80aeba7891b6acb58},
  keywords = {Learning Memory},
  publisher = {Academic Press},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Learning and Memory: A Comprehensive Reference},
  url = {http://dx.doi.org/10.1016/B978-012370509-9.00145-5},
  year = 2017
}

@inbook{le2014automatic,
  abstract = {Recently, researchers from multiple disciplines have been showing their common interest in automatic question generation for educational purposes. In this paper, we review the state of the art of approaches to developing educational applications of question generation. We conclude that although a great variety of techniques on automatic question generation exists, just a small amount of educational systems exploiting question generation has been developed and deployed in real classroom settings. We also propose research directions for deploying the question technology in computer-supported educational systems.},
  added-at = {2017-01-30T04:38:00.000+0100},
  address = {New York, NY},
  author = {Le, Nguyen-Thinh and Kojiri, Tomoko and Pinkwart, Niels},
  biburl = {https://www.bibsonomy.org/bibtex/2ccfac5add3b5017cab293bca4e46e716/vngudivada},
  booktitle = {Advanced Computational Methods for Knowledge Engineering: Proceedings of the 2nd International Conference on Computer Science, Applied Mathematics and Applications (ICCSAMA 2014)},
  editor = {van Do, Tien and Thi, Hoai An Le and Nguyen, Ngoc Thanh},
  interhash = {31186a1e5d5c04a33162e1d4d9743cad},
  intrahash = {ccfac5add3b5017cab293bca4e46e716},
  keywords = {QuestionGeneration},
  pages = {325--338},
  publisher = {Springer International Publishing},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Automatic Question Generation for Educational Applications -- The State of Art},
  url = {http://dx.doi.org/10.1007/978-3-319-06569-4_24},
  year = 2014
}

@article{chater2010language,
  abstract = {Recent research suggests that language evolution is a process of cultural change, in which linguistic structures are shaped through repeated cycles of learning and use by domain-general mechanisms. This paper draws out the implications of this viewpoint for understanding the problem of language acquisition, which is cast in a new, and much more tractable, form. In essence, the child faces a problem of induction, where the objective is to coordinate with others (C-induction), rather than to model the structure of the natural world (N-induction). We argue that, of the two, C-induction is dramatically easier. More broadly, we argue that understanding the acquisition of any cultural form, whether linguistic or otherwise, during development, requires considering the corresponding question of how that cultural form arose through processes of cultural evolution. This perspective helps resolve the “logical” problem of language acquisition and has far-reaching implications for evolutionary psychology.},
  added-at = {2017-03-09T17:30:38.000+0100},
  author = {Chater, Nick and Christiansen, Morten H.},
  biburl = {https://www.bibsonomy.org/bibtex/2dfb957ff02c7af3256ffd8c99116ce30/vngudivada},
  interhash = {a883df46b7b15ca7f8735455d62d1a51},
  intrahash = {dfb957ff02c7af3256ffd8c99116ce30},
  journal = {Cognitive Science},
  keywords = {LanguageAcquisition LanguageEvolution NLP},
  number = 7,
  pages = {1131--1157},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Language Acquisition Meets Language Evolution},
  url = {http://dx.doi.org/10.1111/j.1551-6709.2009.01049.x},
  volume = 34,
  year = 2010
}

@book{weimer2013learnercentered,
  abstract = {In this new edition of the classic work, one of the nation's most highly regarded authorities on effective college teaching offers a comprehensive introduction to the topic of learner-centered teaching in the college and university classroom, including the most up-to-date examples of practice in action from a variety of disciplines, an entirely new chapter on the research support for learner-centered approaches, and a more in-depth discussion of how students' developmental issues impact the effectiveness of learner-centered teaching. Learner-Centered Teaching shows how to tie teaching and curriculum to the process and objectives of learning rather than to the content delivery alone.},
  added-at = {2017-02-11T22:38:15.000+0100},
  address = {San Francisco, CA},
  author = {Weimer, Maryellen},
  biburl = {https://www.bibsonomy.org/bibtex/2d73198375e08ffd859a97700ea96d004/vngudivada},
  edition = {Second},
  interhash = {e767a35e4f443ad128244d43e11663d0},
  intrahash = {d73198375e08ffd859a97700ea96d004},
  keywords = {Book Learning TeachingPractice},
  publisher = {Jossey-Bass},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Learner-centered teaching: five key changes to practice},
  year = 2013
}

@phdthesis{heilman2011automatic,
  abstract = {Texts with potential educational value are becoming available through the Internet (e.g., Wikipedia, news services). However, using these new texts in classrooms introduces many challenges, one of which is that they usually lack practice exercises and assessments. Here, we address part of this challenge by automating the creation of a specific type of assessment item.

 Specifically, we focus on automatically generating factual WH questions. Our goal is to create an automated system that can take as input a text and produce as output questions for assessing a reader's knowledge of the information in the text. The questions could then be presented to a teacher, who could select and revise the ones that he or she judges to be useful.


After introducing the problem, we describe some of the computational and linguistic challenges presented by factual question generation. We then present an implemented system that leverages existing natural language processing techniques to address some of these challenges. The system uses a combination of manually encoded transformation rules and a statistical question ranker trained on a tailored dataset of labeled system output.


We present experiments that evaluate individual components of the system as well as the system as a whole. We found, among other things, that the question ranker roughly doubled the acceptability rate of top-ranked questions.


In a user study, we tested whether K-12 teachers could efficiently create factual questions by selecting and revising suggestions from the system. Offering automatic suggestions reduced the time and effort spent by participants, though it also affected the types of questions that were created.

This research supports the idea that natural language processing can help teachers efficiently create instructional content. It provides solutions to some of the major challenges in question generation and an analysis and better understanding of those that remain.},
  added-at = {2017-01-30T17:10:09.000+0100},
  address = {Pittsburgh, Pensylvania},
  author = {Heilman, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2f28d655f4648197bf37b96222a0ceeed/vngudivada},
  interhash = {f59a1b7ad6aa392f27cdd24be5f505b0},
  intrahash = {f28d655f4648197bf37b96222a0ceeed},
  keywords = {QuestionGeneration},
  school = {Carnegie Mellon University},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Automatic Factual Question Generation from Text},
  type = {Ph.D. Thesis},
  url = {http://www.cs.cmu.edu/~ark/mheilman/questions/papers/heilman-question-generation-dissertation.pdf},
  year = 2011
}

@book{wainwright2008graphical,
  abstract = {The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms -- among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations -- can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.},
  added-at = {2017-03-08T03:01:32.000+0100},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  biburl = {https://www.bibsonomy.org/bibtex/22c51c1724f9b80946b4a0e12ab22eaf8/vngudivada},
  interhash = {2d7afa0e81253b2d2b86968141dba8a7},
  intrahash = {2c51c1724f9b80946b4a0e12ab22eaf8},
  journal = {Foundations and Trends in Machine Learning},
  keywords = {Book GraphicalModel PGM},
  number = {1–2},
  pages = {1 -- 305},
  series = {Foundations and Trends in Machine Learning},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Graphical Models, Exponential Families, and Variational Inference},
  url = {http://dx.doi.org/10.1561/2200000001},
  volume = 1,
  year = 2008
}

@book{lutz2013learning,
  abstract = {Describes the features of the Python programming language, covering such topics as types and operations, statements and syntax, functions, modules, classes and OOP, and exceptions and tools. - Publisher},
  added-at = {2017-02-20T22:05:34.000+0100},
  address = {Sebastopol, California},
  author = {Lutz, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/2a4baf4cad6c7f9af2f1048f9af7c4bf7/vngudivada},
  edition = {Fifth},
  interhash = {ba74a73bacdda726264b3fb60cc2449a},
  intrahash = {a4baf4cad6c7f9af2f1048f9af7c4bf7},
  keywords = {Book Python},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Learning Python},
  year = 2013
}

@article{faragher2012understanding,
  abstract = {This article provides a simple and intuitive derivation of the Kalman filter, with the aim of teaching this useful tool to students from disciplines that do not require a strong mathematical background. The most complicated level of mathematics required to understand this derivation is the ability to multiply two Gaussian functions together and reduce the result to a compact form.},
  added-at = {2017-03-07T18:28:42.000+0100},
  author = {Faragher, Ramsey},
  biburl = {https://www.bibsonomy.org/bibtex/22ef6affe4b2b766e5355b27793634bba/vngudivada},
  interhash = {1bed197eec866ef1579a5c769e63a175},
  intrahash = {2ef6affe4b2b766e5355b27793634bba},
  journal = {{IEEE} Signal Processing Magazine},
  keywords = {KalmanFiter NLP},
  month = sep,
  number = 5,
  pages = {128--132},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation [Lecture Notes]},
  url = {https://doi.org/10.1109%2Fmsp.2012.2203621},
  volume = 29,
  year = 2012
}

@article{rohrer2007increasing,
  abstract = {Because people forget much of what they learn, students could benefit from learning strategies that yield long-lasting knowledge. Yet surprisingly little is known about how long-term retention is most efficiently achieved. Here we examine how retention is affected by two variables: the duration of a study session and the temporal distribution of study time across multiple sessions. Our results suggest that a single session devoted to the study of some material should continue long enough to ensure that mastery is achieved but that immediate further study of the same material is an inefficient use of time. Our data also show that the benefit of distributing a fixed amount of study time across two study sessions—the spacing effect—depends jointly on the interval between study sessions and the interval between study and test. We discuss the practical implications of both findings, especially in regard to mathematics learning.},
  added-at = {2017-03-12T01:11:43.000+0100},
  author = {Rohrer, Doug and Pashler, Harold},
  biburl = {https://www.bibsonomy.org/bibtex/2c6579bbd6a920d739876c97289cd6a3d/vngudivada},
  interhash = {4c2e0542555bb0370a2c845f0333268a},
  intrahash = {c6579bbd6a920d739876c97289cd6a3d},
  journal = {Current Directions in Psychological Science},
  keywords = {Learning Retention},
  month = aug,
  number = 4,
  pages = {183--186},
  publisher = {{SAGE} Publications},
  timestamp = {2019-03-25T17:09:44.000+0100},
  title = {Increasing Retention Without Increasing Study Time},
  url = {https://doi.org/10.1111%2Fj.1467-8721.2007.00500.x},
  volume = 16,
  year = 2007
}

@article{rohrer2010recent,
  abstract = {There has been a recent upsurge of interest in exploring how choices of methods and timing of instruction affect the rate and persistence of learning. The authors review three lines of experimentation—all conducted using educationally relevant materials and time intervals—that call into question important aspects of common instructional practices. First, research reveals that testing, although typically used merely as an assessment device, directly potentiates learning and does so more effectively than other modes of study. Second, recent analysis of the temporal dynamics of learning show that learning is most durable when study time is distributed over much greater periods of time than is customary in educational settings. Third, the interleaving of different types of practice problems (which is quite rare in math and science texts) markedly improves learning. The authors conclude by discussing the frequently observed dissociation between people’s perceptions of which learning procedures are most effective and which procedures actually promote durable learning.},
  added-at = {2017-03-12T01:51:02.000+0100},
  author = {Rohrer, D. and Pashler, H.},
  biburl = {https://www.bibsonomy.org/bibtex/2fd33f59cae637fca31b2d381bc8a4a53/vngudivada},
  interhash = {8b0cd0e3678994b5d8d51537961925ab},
  intrahash = {fd33f59cae637fca31b2d381bc8a4a53},
  journal = {Educational Researcher},
  keywords = {Learning},
  month = jun,
  number = 5,
  pages = {406--412},
  publisher = {American Educational Research Association ({AERA})},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Recent Research on Human Learning Challenges Conventional Instructional Strategies},
  url = {https://doi.org/10.3102%2F0013189x10374770},
  volume = 39,
  year = 2010
}

@article{hinsen2017roles,
  abstract = {Many of us write code regularly as part of our scientific activity, perhaps even as a full-time job. But even though we write--and use--more and more code, we rarely think about the roles that this code will have in our research, in our publications, and ultimately in the scientific record. In this article, the author outlines some frequent roles of code in computational science. These roles aren't exclusive; in fact, it's common for a piece of code to have several roles, at the same time or as an evolution over time. Thinking about these roles, ideally before starting to write the code, is a good habit to develop.},
  added-at = {2017-05-08T00:07:39.000+0200},
  address = {Piscataway, NJ},
  author = {Hinsen, Konrad},
  biburl = {https://www.bibsonomy.org/bibtex/2005227e831a96c0894c23df992498947/vngudivada},
  interhash = {3c8beb3be2b3585468268a7eb6ceef7f},
  intrahash = {005227e831a96c0894c23df992498947},
  journal = {Computing in Science \& Engineering},
  keywords = {ComputationalScience R},
  month = jan,
  number = 1,
  pages = {78--82},
  publisher = {IEEE Educational Activities Department},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {The Roles of Code in Computational Science},
  volume = 19,
  year = 2017
}

@article{erickson2017beginning,
  abstract = {Understanding a Bayesian perspective demands comfort with conditional probability and with probabilities that appear to change as we acquire additional information. This paper suggests a simple context in conditional probability that helps develop the understanding students would need for a successful introduction to Bayesian reasoning.},
  added-at = {2017-03-18T00:11:49.000+0100},
  author = {Erickson, Tim},
  biburl = {https://www.bibsonomy.org/bibtex/2c0c219d7fd9d0ee80ca259bb78551408/vngudivada},
  interhash = {8c43b7b7437c4c65fad4effdf547c45d},
  intrahash = {c0c219d7fd9d0ee80ca259bb78551408},
  journal = {Teaching Statistics},
  keywords = {Learning StatisticsEducation},
  number = 1,
  pages = {30--35},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Beginning Bayes},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/test.12121/full},
  volume = 39,
  year = 2017
}

@book{kruschke2015doing,
  abstract = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition provides an accessible approach for conducting Bayesian data analysis, as material is explained clearly with concrete examples. Included are step-by-step instructions on how to carry out Bayesian data analyses in the popular and free software R and WinBugs, as well as new programs in JAGS and Stan. The new programs are designed to be much easier to use than the scripts in the first edition. In particular, there are now compact high-level scripts that make it easy to run the programs on your own data sets.

The book is divided into three parts and begins with the basics: models, probability, Bayes’ rule, and the R programming language. The discussion then moves to the fundamentals applied to inferring a binomial probability, before concluding with chapters on the generalized linear model. Topics include metric-predicted variable on one or two groups; metric-predicted variable with one metric predictor; metric-predicted variable with multiple metric predictors; metric-predicted variable with one nominal predictor; and metric-predicted variable with multiple nominal predictors. The exercises found in the text have explicit purposes and guidelines for accomplishment.

This book is intended for first-year graduate students or advanced undergraduates in statistics, data analysis, psychology, cognitive science, social sciences, clinical sciences, and consumer sciences in business.

Accessible, including the basics of essential concepts of probability and random sampling
Examples with R programming language and JAGS software
Comprehensive coverage of all scenarios addressed by non-Bayesian textbooks: t-tests, analysis of variance (ANOVA) and comparisons in ANOVA, multiple regression, and chi-square (contingency table analysis)
Coverage of experiment planning
R and JAGS computer programming code on website
Exercises have explicit purposes and guidelines for accomplishment
Provides step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs},
  added-at = {2017-04-30T17:45:49.000+0200},
  address = {Amsterdam, The Netherlands},
  author = {Kruschke, John K.},
  biburl = {https://www.bibsonomy.org/bibtex/254c699c64714c8b16744e848513d51a4/vngudivada},
  edition = {Second},
  interhash = {b3892d44ea886f71e967eacbc3b06cf8},
  intrahash = {54c699c64714c8b16744e848513d51a4},
  isbn = {978-0124058880},
  keywords = {BayesianMethods Book ParExcellence},
  publisher = {Academic Press},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Doing Bayesian data analysis: a tutorial with R, JAGS, and Stan},
  year = 2015
}

@inproceedings{gaedke2005modeling,
  abstract = {As the Web is increasingly used as a platform for heterogeneous applications, we are faced with new requirements to authentication, authorization and identity management. Modern architectures have to control access not only to single, isolated systems, but to whole business-spanning federations of applications and services. This task is complicated by the diversity of today's specifications concerning e.g. privacy, system integrity and distribution in the web. As an approach to such problems, in this paper, we introduce a solution catalogue of reusable building blocks for Identity and Access Management (IAM). The concepts of these blocks have been realized in a configurable system that supports IAM solutions for Web-based applications.},
  added-at = {2017-05-20T20:59:26.000+0200},
  address = {New York, NY},
  author = {Gaedke, Martin and Meinecke, Johannes and Nussbaumer, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/23ec184d01c0c20b30f359090208f730b/vngudivada},
  booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
  doi = {10.1145/1062745.1062916},
  interhash = {acf38c95cf9aaac396076b8a778de8ff},
  intrahash = {3ec184d01c0c20b30f359090208f730b},
  keywords = {FederatedIdentityManagement Security},
  location = {Chiba, Japan},
  pages = {1156--1157},
  publisher = {ACM},
  series = {WWW '05},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {A Modeling Approach to Federated Identity and Access Management},
  url = {http://doi.acm.org/10.1145/1062745.1062916},
  year = 2005
}

@article{fawcett2017storm,
  abstract = {This article describes a hands-on activity that has been used with students aged 12–18 years to promote the study of Statistics. We believe there is evidence to suggest an increase in student enthusiasm for Statistics at school, within the Mathematics curriculum, but also within other subjects such as Geography. We also believe that the use of such activities has resulted in some students giving more serious thought to studying Statistics at University. The activity described here is supported with a web-based application to allow younger or less experienced students to engage with the material.},
  added-at = {2017-03-18T02:41:01.000+0100},
  author = {Fawcett, Lee and Newman, Keith},
  biburl = {https://www.bibsonomy.org/bibtex/29dc65a3a3f28c94e93b116bc0965ae72/vngudivada},
  interhash = {2fb230ba6c7006a97a1cc2875c661f95},
  intrahash = {9dc65a3a3f28c94e93b116bc0965ae72},
  journal = {Teaching Statistics},
  keywords = {Learning StatisticalLearning},
  number = 1,
  pages = {2--13},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {The storm of the century! Promoting student enthusiasm for applied statistics},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/test.12115/full},
  volume = 39,
  year = 2017
}

@article{bainbridge2015using,
  abstract = {In this global information age, schools that teach public affairs and administration must meet the needs of students. Increasingly, this means providing students information in online classrooms to help them reach their highest potential. The acts of teaching and learning online generate data, but to date, that information has remained largely untapped for assessing student performance.

Using data generated by students in an online Master of Public Administration program, drawn from the Marist College Open Academic Analytics Initiative, we identify and analyze characteristics and behaviors that best provide early indication of a student being academically at risk, paying particular attention to the usage of online tools. We find that fairly simple learning analytics models achieve high levels of sensitivity (over 80\% of at-risk students identified) with relatively low false positive rates (13.5\%). Results will be used to test interventions for improving student performance in real time.},
  added-at = {2017-04-09T23:55:38.000+0200},
  author = {Bainbridge, J. and Melitski, J. and Zahradnik, A. and Lauria, E. and Jayaprakash, S. and Baron, J.},
  biburl = {https://www.bibsonomy.org/bibtex/204cd48b134fd69e606fc41df2e85c928/vngudivada},
  interhash = {91a85e1567c15616ee2416cc1a9a451a},
  intrahash = {04cd48b134fd69e606fc41df2e85c928},
  journal = {Journal of Public Affairs Education},
  keywords = {EDM LearningAnalytics},
  number = 2,
  pages = {247 -- 262},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Using learning analytics to predict at-risk students in online graduate public affairs and administration education},
  volume = 21,
  year = 2015
}

@article{woltman2017promoting,
  abstract = {Road injury is an immediately relevant topic for 9–19 year olds. Current availability of Open Data makes it increasingly possible to find locally relevant data. Statistical lessons developed from these data can mutually reinforce life lessons about minimizing risk on the road. Devon County Council demonstrate how a wide array of statistical thinking can be developed in this context.},
  added-at = {2017-03-18T14:30:51.000+0100},
  author = {Woltman, Marie},
  biburl = {https://www.bibsonomy.org/bibtex/2f47358dfa62b6a514d51f8c6be794b64/vngudivada},
  interhash = {53989774acc9f8ae5707b23548e2f854},
  intrahash = {f47358dfa62b6a514d51f8c6be794b64},
  journal = {Teaching Statistics},
  keywords = {Learning StatisticalLearning},
  number = 1,
  pages = {26--29},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Promoting statistical thinking in schools with road injury data},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/test.12117/full},
  volume = 39,
  year = 2017
}

@inbook{camenisch2007federated,
  abstract = {The more real business and interaction with public authorities is performed in digital form, the more important the handling of identities over open networks becomes. The rise in identity theft as a result of the misuse of global but unprotected identifiers like credit card numbers is one strong indicator of this. Setting up individual passwords between a person and every organization he or she interacts with also offers very limited security in practice. Federated identity management addresses this critical issue. Classic proposals like Kerberos and PKIs never gained wide acceptance because of two problems: actual deployment to end users and privacy. We describe modern approaches that solve these problems. The first approach is browser-based protocols, where the user only needs a standard browser without special settings. We discuss the specific protocol types and security challenges of this protocol class, as well as what level of privacy can and cannot be achieved within this class. The second approach, private credentials, solves the problems that none of the prior solutions could solve, but requires the user to install some local software. Private credentials allow the user to reveal only the minimum information necessary to conduct transactions. In particular, it enables unlinkable transactions even for certified attributes. We sketch the cryptographic solutions and describe how optional properties such as revocability can be achieved, in particular in the idemix system.},
  added-at = {2017-05-20T20:44:35.000+0200},
  address = {Berlin, Germany},
  author = {Camenisch, Jan and Pfitzmann, Birgit},
  biburl = {https://www.bibsonomy.org/bibtex/248cbdcbff8b85b8c4e990927773cadd2/vngudivada},
  booktitle = {Security, Privacy, and Trust in Modern Data Management},
  doi = {10.1007/978-3-540-69861-6_15},
  editor = {Petkovi{\'{c}}, Milan and Jonker, Willem},
  interhash = {7d5bef1aa7d12ea40b0e6d8e75a493cc},
  intrahash = {48cbdcbff8b85b8c4e990927773cadd2},
  keywords = {FederatedIdentityManagement Security},
  pages = {213--238},
  publisher = {Springer},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Federated Identity Management},
  url = {http://dx.doi.org/10.1007/978-3-540-69861-6_15},
  year = 2007
}

@article{fincher2015doing,
  abstract = {Research on the cognitive, educational, and policy dimensions of teaching computing is critical to achieving "computer literacy."},
  added-at = {2017-03-26T05:29:41.000+0200},
  address = {New York, NY, USA},
  author = {Fincher, Sally},
  biburl = {https://www.bibsonomy.org/bibtex/25d015adb4ddc206a32840533775ac6d9/vngudivada},
  interhash = {4b2f7d5a0b46e7456103a833e7afce31},
  intrahash = {5d015adb4ddc206a32840533775ac6d9},
  journal = {Commun. ACM},
  keywords = {ComputingEducation},
  month = apr,
  number = 5,
  pages = {24--26},
  publisher = {ACM},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {What Are We Doing when We Teach Computing in Schools?},
  url = {http://doi.acm.org/10.1145/2742693},
  volume = 58,
  year = 2015
}

@book{haykin2009neural,
  abstract = {Neural Networks and Learning Machines, Third Edition is renowned for its thoroughness and readability. This well-organized and completely up-to-date text remains the most comprehensive treatment of neural networks from an engineering perspective. This is ideal for professional engineers and research scientists.

Matlab codes used for the computer experiments in the text are available for download at: http://www.pearsonhighered.com/haykin/

Refocused, revised and renamed to reflect the duality of neural networks and learning machines, this edition recognizes that the subject matter is richer when these topics are studied together. Ideas drawn from neural networks and machine learning are hybridized to perform improved learning tasks beyond the capability of either independently.},
  added-at = {2017-03-18T17:31:57.000+0100},
  address = {Upper Saddle River, NJ},
  author = {Haykin, Simon S.},
  biburl = {https://www.bibsonomy.org/bibtex/2e5015812328aaeccd73d8b03a7e36831/vngudivada},
  edition = {Third},
  interhash = {4cef19efafc52ae42607f9832a205214},
  intrahash = {e5015812328aaeccd73d8b03a7e36831},
  keywords = {Book Learning NeuralNetwork},
  publisher = {Pearson Education},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Neural networks and learning machines},
  year = 2009
}

@book{aggarwal2014clustering,
  abstract = {Features

Presents core methods for data clustering, including probabilistic, density- and grid-based, and spectral clustering
Explores various problems and scenarios pertaining to multimedia, text, biological, categorical, network, streams, and uncertain data
Offers in-depth insight on the clustering process, including different ways to cluster the same data set
Includes an extensive bibliography at the end of each chapter
Summary

Research on the problem of clustering tends to be fragmented across the pattern recognition, database, data mining, and machine learning communities. Addressing this problem in a unified way, Data Clustering: Algorithms and Applications provides complete coverage of the entire area of clustering, from basic methods to more refined and complex data clustering approaches. It pays special attention to recent issues in graphs, social networks, and other domains.

The book focuses on three primary aspects of data clustering:

Methods, describing key techniques commonly used for clustering, such as feature selection, agglomerative clustering, partitional clustering, density-based clustering, probabilistic clustering, grid-based clustering, spectral clustering, and nonnegative matrix factorization
Domains, covering methods used for different domains of data, such as categorical data, text data, multimedia data, graph data, biological data, stream data, uncertain data, time series clustering, high-dimensional clustering, and big data
Variations and Insights, discussing important variations of the clustering process, such as semisupervised clustering, interactive clustering, multiview clustering, cluster ensembles, and cluster validation
In this book, top researchers from around the world explore the characteristics of clustering problems in a variety of application areas. They also explain how to glean detailed insight from the clustering process—including how to verify the quality of the underlying clusters—through supervision, human intervention, or the automated generation of alternative clusters.},
  added-at = {2017-05-15T04:16:31.000+0200},
  address = {Boca Raton, Florida},
  author = {Aggarwal, Charu C. and Reddy, Chandan K. (1980-) and Press, CRC and Francis, Taylor &},
  biburl = {https://www.bibsonomy.org/bibtex/2ad01b0fd30c13a739f30f7dfa92f5857/vngudivada},
  interhash = {9e444f7e562e36b9853e788caf439fe6},
  intrahash = {ad01b0fd30c13a739f30f7dfa92f5857},
  keywords = {sys:relevantfor:ecu-cc-research Book Clustering},
  publisher = {CRC Press/Taylor \& Francis Group},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Data clustering: algorithms and applications},
  url = {http://haralick.org/ML/data_clustering.pdf},
  year = 2014
}

@techreport{stiles2012understanding,
  abstract = {This guide provides an introduction to the major risk categories faced by a higher education institution considering investments in time, energy, and money in analytics work. Under the right circumstances, decision making can be enhanced by the tools and techniques of analytics; large data sets, analytics engines, and new data visualization techniques have considerable potential to enhance both student learning and institutional business intelligence. However, careful consideration must be given to the risks of such investments for those in institutional leadership roles as well as the risks associated with data and information governance, compliance, and quality.},
  added-at = {2017-04-10T00:17:47.000+0200},
  author = {Stiles, Randall J.},
  biburl = {https://www.bibsonomy.org/bibtex/200ebf2bbfc64b0e6edbedef6b450c550/vngudivada},
  institution = {EDUCAUSE},
  interhash = {e0abb9541d1d18032793577818c98ff0},
  intrahash = {00ebf2bbfc64b0e6edbedef6b450c550},
  keywords = {DataAnalytics EDM Privacy},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Understanding and Managing the Risks of Analytics in Higher Education: A Guide},
  url = {http://net.educause.edu/ir/library/pdf/EPUB1201.pdf},
  year = 2012
}

@inbook{baker2014educational,
  abstract = {In recent years, two communities have grown around a joint interest on how big data can be exploited to benefit education and the science of learning: Educational Data Mining and Learning Analytics. This article discusses the relationship between these two communities, and the key methods and approaches of educational data mining. The article discusses how these methods emerged in the early days of research in this area, which methods have seen particular interest in the EDM and learning analytics communities, and how this has changed as the field matures and has moved to making significant contributions to both educational research and practice.},
  added-at = {2017-03-24T02:06:24.000+0100},
  address = {New York, NY},
  author = {Baker, Ryan Shaun and Inventado, Paul Salvador},
  biburl = {https://www.bibsonomy.org/bibtex/213890db91a758a631c691f2e5a544025/vngudivada},
  booktitle = {Learning Analytics: From Research to Practice},
  editor = {Larusson, Johann Ari and White, Brandon},
  interhash = {c87fc17f5900f62768af9f5ab2076340},
  intrahash = {13890db91a758a631c691f2e5a544025},
  keywords = {LearningAnalytics},
  pages = {61--75},
  publisher = {Springer},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Educational Data Mining and Learning Analytics},
  url = {http://dx.doi.org/10.1007/978-1-4614-3305-7_4},
  year = 2014
}

@article{chen2017strategic,
  abstract = { Many educational policies provide learners with more resources (e.g., new learning activities, study materials, or technologies), but less often do they address whether students are using these resources effectively. We hypothesized that making students more self-reflective about how they should approach their learning with the resources available to them would improve their class performance. We designed a novel Strategic Resource Use intervention that students could self-administer online and tested its effects in two cohorts of a college-level introductory statistics class. Before each exam, students randomly assigned to the treatment condition strategized about which academic resources they would use for studying, why each resource would be useful, and how they would use their resources. Students randomly assigned to the treatment condition reported being more self-reflective about their learning throughout the class, used their resources more effectively, and outperformed students in the control condition by an average of one third of a letter grade in the class.},
  added-at = {2017-05-19T13:13:34.000+0200},
  author = {Chen, Patricia and Chavez, Omar and Ong, Desmond C. and Gunderson, Brenda},
  biburl = {https://www.bibsonomy.org/bibtex/296d3130569fe92a817b6ef304b28cfa7/vngudivada},
  doi = {10.1177/0956797617696456},
  interhash = {4f142d75ce9407bb12cbcd538dd8067d},
  intrahash = {96d3130569fe92a817b6ef304b28cfa7},
  journal = {Psychological Science},
  keywords = {Learning MetaCognition},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Strategic Resource Use for Learning: A Self-Administered Intervention That Guides Self-Reflection on Effective Resource Use Enhances Academic Performance},
  url = {http://dx.doi.org/10.1177/0956797617696456},
  year = 2017
}

@article{kurt2017seconds,
  abstract = {This paper moves away from reminiscent mechanical repetition and drills, which were in vogue when teaching vocabulary before the rise of technology. With the support of technology, innovative methodologies that are more effective and enjoyable can be implemented into vocabulary teaching. In this particular context, there seems to be a lack of technology integration in vocabulary teaching because of teachers being untrained and/or not provided with the necessary technology. The aim of this study was to foster vocabulary development through the implementation of Vine vocabulary videos in English vocabulary learning. An embedded mixed methods design was employed to collect necessary data for analysis. The results of the post-test revealed that the practice of Vine vocabulary videos was effective and improved participants' vocabulary. The content analysis of the semi-structured interviews carried out with participants indicated that they had enjoyed the whole process and found it very motivating and effective. This study claims that adopting smartphones into a vocabulary course will enable English as a foreign language learners to expand and consolidate their vocabulary learning outside the classroom, engage them in a collaborative learning environment, practice and use the language being learnt and share their knowledge and experiences with their peers.

Practitioner Notes
What is already known about this topic:


* Mobile technologies are widely used to improve English language learners' vocabulary.

* Vine is a well-known application used by people to expose their videos in 6 s.

* Motivating English language learners to improve their vocabulary has always been challenging.

* Adopting smartphones into a vocabulary course will enable EFL learners to expand and consolidate their learning outside the classroom and share their knowledge and experiences with their peers.

What this paper adds:


* Learner-prepared VVVs have been used in a course to improve English language learners' vocabulary in a joyous way.

* Through VVVs, learners developed their creative and imaginary skills.

* VVVs were discovered to be an effective tool for subconscious collaborative vocabulary learning.

* VVVs motivate, encourage and therefore induce language learners to allocate more time to the educational applications of mobile and internet technologies.

* While preparing VVVs, students improved their collaborative and language skills.

Implications for practice and/or policy:


* More guidance on how to make better VVVs should be given to students.

* Learners should be involved in producing course materials through VVVs.

* Learners should be exposed to more visual sample representations of target vocabulary.

* Preparing VVVs should be part of every English language syllabus because they are verified to improve the language skills of learners.

},
  added-at = {2017-03-18T02:21:56.000+0100},
  author = {Kurt, M. and Bensen, H.},
  biburl = {https://www.bibsonomy.org/bibtex/208a84f2b03c5c461eb94360956052d4d/vngudivada},
  interhash = {765c312013aa3fa8fb311bc68c87a93f},
  intrahash = {08a84f2b03c5c461eb94360956052d4d},
  journal = {Journal of Computer Assisted Learning},
  keywords = {EFL Learning},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Six seconds to visualize the word: improving EFL learners' vocabulary through VVVs},
  url = {http://dx.doi.org/10.1111/jcal.12182},
  year = 2017
}

@article{wells2017introducing,
  abstract = {This exercise introduces students to the application of statistics and its investigative methods in political science. It helps students gain a better understanding and a greater appreciation of statistics through a real world application.},
  added-at = {2017-03-18T00:25:04.000+0100},
  author = {Wells, Dominic D. and Nemire, Nathan A.},
  biburl = {https://www.bibsonomy.org/bibtex/22390ef945b8525aa019952512fe747ca/vngudivada},
  interhash = {c9d3704518efe2a5aa5f1db7d0cdcea4},
  intrahash = {2390ef945b8525aa019952512fe747ca},
  journal = {Teaching Statistics},
  keywords = {Learning StatisticsEducation},
  pages = {in press},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Introducing students to the application of statistics and investigative methods in political science},
  year = 2017
}

@book{gelman2014bayesian,
  abstract = {Now in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied approach to analysis using up-to-date Bayesian methods. The authors―all leaders in the statistics community―introduce basic concepts from a data-analytic perspective before presenting advanced methods. Throughout the text, numerous worked examples drawn from real applications and research emphasize the use of Bayesian inference in practice.

New to the Third Edition

Four new chapters on nonparametric modeling
Coverage of weakly informative priors and boundary-avoiding priors
Updated discussion of cross-validation and predictive information criteria
Improved convergence monitoring and effective sample size calculations for iterative simulation
Presentations of Hamiltonian Monte Carlo, variational Bayes, and expectation propagation
New and revised software code
The book can be used in three different ways. For undergraduate students, it introduces Bayesian inference starting from first principles. For graduate students, the text presents effective current approaches to Bayesian modeling and computation in statistics and related fields. For researchers, it provides an assortment of Bayesian methods in applied statistics. Additional materials, including data sets used in the examples, solutions to selected exercises, and software instructions, are available on the book’s web page.},
  added-at = {2017-05-17T21:20:12.000+0200},
  address = {Boca Raton, FL},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal Steven and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  biburl = {https://www.bibsonomy.org/bibtex/2c5217d9e10f2133d30ba5aa7045ed146/vngudivada},
  edition = {3rd edition},
  interhash = {d9a0e8b398102e1121c2931b73f50168},
  intrahash = {c5217d9e10f2133d30ba5aa7045ed146},
  isbn = {978-1439840955},
  keywords = {BayesianMethods Book},
  publisher = {Chapman and Hall/CRC},
  refid = {986525949},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Bayesian data analysis},
  year = 2014
}

@book{booth2016craft,
  abstract = {With more than three-quarters of a million copies sold since its first publication, The Craft of Research has helped generations of researchers at every level—from first-year undergraduates to advanced graduate students to research reporters in business and government—learn how to conduct effective and meaningful research. Conceived by seasoned researchers and educators Wayne C. Booth, Gregory G. Colomb, and Joseph M. Williams, this fundamental work explains how to find and evaluate sources, anticipate and respond to reader reservations, and integrate these pieces into an argument that stands up to reader critique.

The fourth edition has been thoroughly but respectfully revised by Joseph Bizup and William T. FitzGerald. It retains the original five-part structure, as well as the sound advice of earlier editions, but reflects the way research and writing are taught and practiced today. Its chapters on finding and engaging sources now incorporate recent developments in library and Internet research, emphasizing new techniques made possible by online databases and search engines. Bizup and FitzGerald provide fresh examples and standardized terminology to clarify concepts like argument, warrant, and problem.

Following the same guiding principle as earlier editions—that the skills of doing and reporting research are not just for elite students but for everyone—this new edition retains the accessible voice and direct approach that have made The Craft of Research a leader in the field of research reference. With updated examples and information on evaluation and using contemporary sources, this beloved classic is ready for the next generation of researchers.},
  added-at = {2017-05-26T14:32:27.000+0200},
  author = {Booth, Wayne C. and Colomb, Gregory G. and Williams, Joseph M. and Bizup, Joseph and FitzGerald, William T.},
  biburl = {https://www.bibsonomy.org/bibtex/2f44ddd1a21e575dadb5ee19a42b6f638/vngudivada},
  interhash = {2579ea589e9aa69fe7b06029b5626833},
  intrahash = {f44ddd1a21e575dadb5ee19a42b6f638},
  keywords = {Book Research},
  refid = {951115619},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {The Craft of Research},
  year = 2016
}

@article{bebermeier2016practicing,
  abstract = {This article outlines the execution of a workshop in which students were encouraged to actively review the course contents on descriptive statistics by creating exercises for their fellow students. In a first-year statistics course in psychology, 39 out of 155 students participated in the workshop. In a subsequent evaluation, the workshop was assessed as useful and appropriate to encourage students to practice statistics and to become prepared for the exam.},
  added-at = {2017-03-17T23:53:54.000+0100},
  author = {Bebermeier, Sarah and Reiss, Katharina},
  biburl = {https://www.bibsonomy.org/bibtex/27cdbf4efac897f087035893e172e9b44/vngudivada},
  interhash = {482b59587134eeab971d9df651f674d0},
  intrahash = {7cdbf4efac897f087035893e172e9b44},
  journal = {Teaching Statistics},
  keywords = {Statistics StatisticsEducation},
  number = 2,
  pages = {40--44},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Practicing statistics by creating exercises for fellow students},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/test.12089/full},
  volume = 38,
  year = 2016
}

@article{brainard1998sixyear,
  abstract = {In 1991, the Women in Engineering (WIE) Initiative at the University of Washington was funded by the Alfred P. Sloan Foundation to conduct a longitudinal study of undergraduate women pursuing degrees in science or engineering. Cohorts of approximately 100 students have been added to the study each year, for a current total of 672 participants. The objectives are: (a) to determine an accurate measure of retention by tracking individual students through their science and engineering academic careers; (b) to examine factors affecting retention of women in science and engineering; and (c) to evaluate the effectiveness of WIE's programs targeted at increasing enrollment and retention of women in science and engineering. These programs include interventions primarily during the freshman and sophomore years, which are critical attrition points. The results of this study are reported annually to the Dean of Engineering and related departments for consideration in policy formulation. Annual results of the study have shown consistent patterns of persistence factors and barriers for these high-achieving women; most notably a significant drop in academic self-confidence during their freshman year in college. In addition, individual tracking of these women has shown a retention that is much higher than the estimated national average for engineering and science students.},
  added-at = {2017-05-11T22:15:55.000+0200},
  author = {Brainard, Suzanne G. and Carlin, Linda},
  biburl = {https://www.bibsonomy.org/bibtex/20e3838349396c305dc78bfcd97031570/vngudivada},
  interhash = {3daf7779d803ed2442c0fc76b5259a3e},
  intrahash = {0e3838349396c305dc78bfcd97031570},
  journal = {Journal of Engineering Education},
  keywords = {Retention},
  number = 4,
  pages = {369--375},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {A Six-Year Longitudinal Study of Undergraduate Women in Engineering and Science*},
  url = {http://dx.doi.org/10.1002/j.2168-9830.1998.tb00367.x},
  volume = 87,
  year = 1998
}

@inproceedings{kotsiantis2007supervised,
  abstract = {The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.},
  added-at = {2017-05-15T04:34:58.000+0200},
  address = {Amsterdam, The Netherlands},
  author = {Kotsiantis, S. B.},
  biburl = {https://www.bibsonomy.org/bibtex/216519c6d3a3e47fbfb461de69608133c/vngudivada},
  booktitle = {Proceedings of the 2007 Conference on Emerging Artificial Intelligence Applications in Computer Engineering: Real Word AI Systems with Applications in eHealth, HCI, Information Retrieval and Pervasive Technologies},
  interhash = {923bad3021a70430ed017dd8d3967ac4},
  intrahash = {16519c6d3a3e47fbfb461de69608133c},
  keywords = {Classification ML},
  pages = {3--24},
  publisher = {IOS Press},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Supervised Machine Learning: A Review of Classification Techniques},
  year = 2007
}

@book{christensen2011plane,
  abstract = {This textbook provides a wide-ranging introduction to the use and theory of linear models for analyzing data. The author's emphasis is on providing a unified treatment of linear models, including analysis of variance models and regression models, based on projections, orthogonality, and other vector space ideas. Every chapter comes with numerous exercises and examples that make it ideal for a graduate-level course. All of the standard topics are covered in depth: ANOVA, estimation including Bayesian estimation, hypothesis testing, multiple comparisons, regression analysis, and experimental design models. In addition, the book covers topics that are not usually treated at this level, but which are important in their own right: balanced incomplete block designs, testing for lack of fit, testing for independence, models with singular covariance matrices, variance component estimation, best linear and best linear unbiased prediction, collinearity, and variable selection. This new edition includes a more extensive discussion of best prediction and associated ideas of R2, as well as new sections on inner products and perpendicular projections for more general spaces and Milliken and Graybill’s generalization of Tukey’s one degree of freedom for nonadditivity test.},
  added-at = {2017-05-15T05:22:09.000+0200},
  address = {New York, NY},
  author = {Christensen, Ronald},
  biburl = {https://www.bibsonomy.org/bibtex/2cdd02c6abcd4f220a687a4e79c66e9b2/vngudivada},
  interhash = {a6ac2e220532e14bdbe91e9d079ecd96},
  intrahash = {cdd02c6abcd4f220a687a4e79c66e9b2},
  keywords = {Book ML},
  publisher = {Springer},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Plane answers to complex questions the theory of linear models},
  year = 2011
}

@article{wing2006computational,
  abstract = {It represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.},
  added-at = {2017-03-26T04:17:18.000+0200},
  address = {New York, NY, USA},
  author = {Wing, Jeannette M.},
  biburl = {https://www.bibsonomy.org/bibtex/2414da226e840c7c1e296fa56cdc95bb3/vngudivada},
  interhash = {f3dccbeb01cd42f3db20b71734707c2c},
  intrahash = {414da226e840c7c1e296fa56cdc95bb3},
  journal = {Commun. ACM},
  keywords = {ComputationalThinking},
  month = mar,
  number = 3,
  pages = {33--35},
  publisher = {ACM},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Computational Thinking},
  url = {http://doi.acm.org/10.1145/1118178.1118215},
  volume = 49,
  year = 2006
}

@inproceedings{seevers2006improving,
  abstract = {Retention of lower division students is a continuing concern in academia. In response to these concerns, a program was initiated in the Boise State University College of Engineering to improve lower division retention via research and internships. Inclusion of lower division students in both university research and industry internships is contrary to prevailing perceptions of student capabilities. However, lower division engineering students generally possess numerous basic skills that enable them to work in an engineering environment where they can gain experience and confidence. Phase One of the Retention through Research and Internships Program was a pilot program in which seven first year engineering students were placed in research laboratories with faculty mentors within the College of Engineering during the 2004-05 academic year. At the beginning of the Fall 2005 semester, 100% of the participating students remained in the program. In addition, interviews of the students revealed that many believed that the research laboratory experience and environment increased their confidence and motivation, and was pivotal in their decision to remain in engineering. As a result of the successful Phase One pilot program, a Phase Two program has been initiated, in which first and second year engineering students are being placed in industry internships during the academic year 2005-06.},
  added-at = {2017-05-11T23:19:05.000+0200},
  address = {Chicago, Illinois},
  author = {Seevers, Melinda and Pyke, Pat and Knowlton, William and Schrader, Cheryl and Gardner, John},
  biburl = {https://www.bibsonomy.org/bibtex/28b2245f7ea454cf5acdc64f216cc4c29/vngudivada},
  booktitle = {2006 Annual Conference \& Exposition},
  interhash = {ac85f944bdda7212839d8c585f33ee65},
  intrahash = {8b2245f7ea454cf5acdc64f216cc4c29},
  keywords = {Retention},
  month = {June},
  pages = {11.734.1 - 11.734.10},
  publisher = {ASEE Conferences},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Improving Engineering Undergraduate Retention via Research and Internships},
  url = {https://peer.asee.org/1394},
  year = 2006
}

@article{theamericansocietyforengineeringeducationasee2012practices,
  added-at = {2017-05-11T23:27:50.000+0200},
  author = {{The American Society for Engineering Education (ASEE)}},
  biburl = {https://www.bibsonomy.org/bibtex/2811ddb13fafdb28f0cc404cd93799937/vngudivada},
  interhash = {fb88934837d140bdb3d7080ddea045f4},
  intrahash = {811ddb13fafdb28f0cc404cd93799937},
  keywords = {ASEE Retention},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Best Practices and Strategies for Retaining Engineering, Engineering Technology and Computing Students},
  url = {https://www.asee.org/retention-project/best-practices-and-strategies/ASEE-Student-Retention-Project.pdf},
  year = 2012
}

@book{stone2013bayes,
  abstract = {Discovered by an 18th century mathematician and preacher, Bayes' rule is a cornerstone of modern probability theory. In this richly illustrated book, a range of accessible examples is used to show how Bayes' rule is actually a natural consequence of common sense reasoning. Bayes' rule is then derived using intuitive graphical representations of probability, and Bayesian analysis is applied to parameter estimation. As an aid to understanding, online computer code (in MatLab, Python and R) reproduces key numerical results and diagrams. The tutorial style of writing, combined with a comprehensive glossary, makes this an ideal primer for novices who wish to become familiar with the basic principles of Bayesian analysis.},
  added-at = {2017-05-21T05:27:59.000+0200},
  author = {Stone, James},
  biburl = {https://www.bibsonomy.org/bibtex/211acae2c47acab9d95a23917252b3865/vngudivada},
  interhash = {63d1f3caf6e0eb00e824ec6942d9bb75},
  intrahash = {11acae2c47acab9d95a23917252b3865},
  keywords = {BayesianMethods Book},
  publisher = {Sebtel Press},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Bayes' Rule: A Tutorial Introduction to Bayesian Analysis },
  year = 2013
}

@article{paulson2007developers,
  abstract = {Software developers are always looking for ways to boost their effectiveness and productivity and perform complex jobs more quickly and easily, particularly as projects have become increasingly large and complex. Programmers want to shed unneeded complexity and outdated methodologies and move to approaches that focus on making programming simpler and faster. With this in mind, many developers are increasingly using dynamic languages such as JavaScript, Perl, Python, and Ruby. Although software experts disagree on the exact definition, a dynamic language basically enables programs that can change their code and logical structures at runtime, adding variable types, module names, classes, and functions as they are running. These languages frequently are interpreted and generally check typing at runtime},
  added-at = {2017-05-08T00:15:45.000+0200},
  author = {Paulson, L. D.},
  biburl = {https://www.bibsonomy.org/bibtex/2e3bc45c625e15d7cfc7760a15f4ca692/vngudivada},
  interhash = {e124feee49515f289a3aa7451a6fd864},
  intrahash = {e3bc45c625e15d7cfc7760a15f4ca692},
  journal = {Computer},
  keywords = {DynamicProgrammingLanguage R},
  month = feb,
  number = 2,
  pages = {12-15},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Developers shift to dynamic programming languages},
  volume = 40,
  year = 2007
}

@book{petkovic2007security,
  abstract = {Privacy and Security Issues in a Digital World.- Privacy in the Law.- Ethical Aspects of Information Security and Privacy.- Data and System Security.- Authorization and Access Control.- Role-Based Access Control.- XML Security.- Database Security.- Trust Management.- Trusted Platforms.- Strong Authentication with Physical Unclonable Functions.- Privacy Enhancing.- Privacy-Preserving Data Mining.- Statistical Database Security.- Different Search Strategies on Encrypted Data Compared.- Client-Server Trade-Offs in Secure Computation.- Federated Identity Management.- Accountable Anonymous Communication.- Digital Asset Protection.- An Introduction to Digital Rights Management Systems.- Copy Protection Systems.- Forensic Watermarking in Digital Rights Management.- Person-Based and Domain-Based Digital Rights Management.- Digital Rights Management Interoperability.- DRM for Protecting Personal Content.- Enhancing Privacy for Digital Rights Management.- Selected Topics on Privacy and Security in Ambient Intelligence.- The Persuasiveness of Ambient Intelligence.- Privacy Policies.- Security and Privacy on the Semantic Web.- Private Person Authentication in an Ambient World.- RFID and Privacy.- Malicious Software in Ubiquitous Computing.},
  added-at = {2017-05-20T21:42:29.000+0200},
  address = {Berlin, Germany},
  biburl = {https://www.bibsonomy.org/bibtex/2b216335bbf186206c19949e89f02782f/vngudivada},
  editor = {Petkovic, Milan and Jonker, Willem},
  interhash = {cd509352bb5eb078dd70e439cb942582},
  intrahash = {b216335bbf186206c19949e89f02782f},
  keywords = {DBMS Privacy Security Trust},
  publisher = {Springer},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Security, Privacy and Trust in Modern Data Management },
  year = 2007
}

@book{nagarajan2013bayesian,
  abstract = {Bayesian Networks in R with Applications in Systems Biology is unique as it introduces the reader to the essential concepts in Bayesian network modeling and inference in conjunction with examples in the open-source statistical environment R. The level of sophistication is also gradually increased across the chapters with exercises and solutions for enhanced understanding for hands-on experimentation of the theory and concepts. The application focuses on systems biology with emphasis on modeling pathways and signaling mechanisms from high-throughput molecular data. Bayesian networks have proven to be especially useful abstractions in this regard. Their usefulness is especially exemplified by their ability to discover new associations in addition to validating known ones across the molecules of interest. It is also expected that the prevalence of publicly available high-throughput biological data sets may encourage the audience to explore investigating novel paradigms using the approaches presented in the book.},
  added-at = {2017-05-18T00:34:20.000+0200},
  address = {New York, NY},
  author = {Nagarajan, Radhakrishnan and Scutari, Marco},
  biburl = {https://www.bibsonomy.org/bibtex/2dbfe9d5cd34299c6dbd928b9663bfc37/vngudivada},
  doi = {10.1007/978-1-4614-6446-4},
  interhash = {b6f6b45d42f4f97d54267591be056ca0},
  intrahash = {dbfe9d5cd34299c6dbd928b9663bfc37},
  keywords = {BayesianMethods Book},
  publisher = {Springer},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Bayesian Networks in {R} with Applications in Systems Biology},
  year = 2013
}

@techreport{wadler2014research,
  abstract = {This talk surveys advice from experts, including Richard Hamming, William Strunk, E. B. White, Donald Knuth, and others, on how to conduct your research and communicate your results.},
  added-at = {2017-05-17T21:02:34.000+0200},
  author = {Wadler, Philip},
  biburl = {https://www.bibsonomy.org/bibtex/233cd0bb850b90e37e97b4efaefd56d54/vngudivada},
  interhash = {e8f4ee6ef8c35f9569898505a49197f7},
  intrahash = {33cd0bb850b90e37e97b4efaefd56d54},
  keywords = {Hamming Research Wadler},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {You and Your Research \& The Elements of Style},
  year = 2014
}

@article{gerds2016kaplanmeier,
  abstract = {Survival is difficult to estimate when observation periods of individuals differ in length. Students imagine sailing the Titanic and then recording whether they "live" or "die". A clever algorithm is performed which results in the Kaplan-Meier estimate of survival.},
  added-at = {2017-03-17T23:59:49.000+0100},
  author = {Gerds, Thomas A.},
  biburl = {https://www.bibsonomy.org/bibtex/2f11dc5bc81eca010398c3c0d6a7dc5fc/vngudivada},
  interhash = {780c22e9da177b35f9cf0b3df37400c7},
  intrahash = {f11dc5bc81eca010398c3c0d6a7dc5fc},
  journal = {Teaching Statistics},
  keywords = {Learning StatisticsEducation},
  number = 2,
  pages = {45--49},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {The Kaplan–Meier theatre},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/test.12095/full},
  volume = 38,
  year = 2016
}

@book{stone2015information,
  abstract = {Originally developed by Claude Shannon in the 1940s, information theory laid the foundations for the digital revolution, and is now an essential tool in telecommunications, genetics, linguistics, brain sciences, and deep space communication. In this richly illustrated book, accessible examples are used to introduce information theory in terms of everyday games like ‘20 questions’ before more advanced topics are explored. Online MatLab and Python computer programs provide hands-on experience of information theory in action, and PowerPoint slides give support for teaching. Written in an informal style, with a comprehensive glossary and tutorial appendices, this text is an ideal primer for novices who wish to learn the essential principles and applications of information theory.},
  added-at = {2017-05-21T05:30:00.000+0200},
  author = {Stone, James},
  biburl = {https://www.bibsonomy.org/bibtex/29167dbed60efd12e5eb3f8884e95d23e/vngudivada},
  interhash = {b270818d8adb698833190999f966a519},
  intrahash = {9167dbed60efd12e5eb3f8884e95d23e},
  keywords = {Book InformationTheory},
  publisher = {Sebtel Press},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Information Theory: A Tutorial Introduction},
  year = 2015
}

@article{inda2011memory,
  abstract = {An established memory can be made transiently labile if retrieved or reactivated. Over time, it becomes again resistant to disruption and this process that renders the memory stable is termed reconsolidation. The reasons why a memory becomes labile after retrieval and reconsolidates still remains debated. Here, using inhibitory avoidance (IA) learning in rats, we provide evidence that retrievals of a young memory, which are accompanied by its reconsolidation, result in memory strengthening and contribute to its overall consolidation. This function associated to reconsolidation is temporally limited. With the passage of time, the stored memory undergoes important changes, as revealed by the behavioral outcomes of its retrieval. Over time, without explicit retrievals, memory first strengthens and becomes refractory to both retrieval-dependent interference and strengthening. At later times, the same retrievals that lead to reconsolidation of a young memory extinguish an older memory. We conclude that the storage of information is very dynamic and that its temporal evolution regulates behavioral outcomes. These results are important for potential clinical applications.},
  added-at = {2017-03-12T20:34:22.000+0100},
  author = {Inda, Maria Carmen and Muravieva, Elizaveta V. and Alberini, Cristina M.},
  biburl = {https://www.bibsonomy.org/bibtex/26a1dc39beb5ae7b4ce7ebdd9db80c32f/vngudivada},
  interhash = {29fa59aef9b15ac71f716212d0be4232},
  intrahash = {6a1dc39beb5ae7b4ce7ebdd9db80c32f},
  journal = {Journal of Neuroscience},
  keywords = {Learning RetrievalPractice},
  month = feb,
  number = 5,
  pages = {1635 --1643},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Memory retrieval and the passage of time: from reconsolidation and strengthening to extinction},
  volume = 31,
  year = 2011
}

@inproceedings{ahn2005managing,
  abstract = {We have witnessed that the Internet is now a prime vehicle for business, community, and personal interactions. The notion of identity is the important component of this vehicle. Identity management has been recently considered to be a viable solution for simplifying user management across enterprise applications. The network identity of each user is the global set of personal credentials and preferences constituting the various accounts. The prevalence of business alliances or coalitions necessitates the further evolution of identity management, named federated identity management (FIM). The main motivation of FIM is to facilitate the federation of identities among business partners emphasizing on ease of user management. In this paper, we investigate privacy issues in FIM, especially focusing on Liberty Alliance approach. We attempt to identify practical business scenarios that help us understand privacy issues in FIM. Also, we propose systematic mechanisms to specify privacy preferences in FIM.},
  added-at = {2017-05-20T20:55:52.000+0200},
  address = {New York, NY},
  author = {Ahn, Gail-Joon and Lam, John},
  biburl = {https://www.bibsonomy.org/bibtex/2b6987612188b6b7cf9d0e42f6f27f962/vngudivada},
  booktitle = {Proceedings of the 2005 Workshop on Digital Identity Management},
  doi = {10.1145/1102486.1102492},
  interhash = {f449ce23550d88430e1054079388fca3},
  intrahash = {b6987612188b6b7cf9d0e42f6f27f962},
  keywords = {FederatedIdentityManagement Security},
  pages = {28--36},
  publisher = {ACM},
  series = {DIM '05},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Managing Privacy Preferences for Federated Identity Management},
  url = {http://doi.acm.org/10.1145/1102486.1102492},
  year = 2005
}

@book{rasmussen2008gaussian,
  abstract = {Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.

The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.

The book was awarded the 2009 DeGroot Prize of the International Society for Bayesian Analysis.},
  added-at = {2017-06-02T03:47:36.000+0200},
  address = {Cambridge, MA},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  biburl = {https://www.bibsonomy.org/bibtex/23de177738c33251313401a31d9f09d42/vngudivada},
  interhash = {56cdfcb6a315fffa51fa5986d74e859f},
  intrahash = {3de177738c33251313401a31d9f09d42},
  keywords = {BayesianMethods Book},
  publisher = {MIT Press},
  refid = {552376743},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Gaussian processes for machine learning},
  year = 2008
}

@inproceedings{barker2014results,
  abstract = {The recent upsurge in enrollments in computing means that student attrition has a substantial opportunity cost. Admitting a student who leaves both reduces graduation yield and prevents another equally qualified student from enrolling. Professors cannot change the background of students, but they can control many aspects of student experience in the computing major. This paper presents the results of a study to understand strongest predictors of retention in undergraduate computing based on a large-scale survey administered in 14 U.S. institutions. Although some factors have more influence for certain demographic groups, findings from this data set suggest that some teaching practices have more power for predicting retention in computing including: relevant and meaningful assignments, examples, and curriculum; faculty interaction with students; student collaboration on programming assignments; and for male students, pace and workload expectations relative to existing experience. Other interactions such as those that a student has with teaching assistants or peers in extracurricular activities seem to have less value for predicting retention. Faculty would be wise to protect their enrollment investments by inspecting course themes, assignments, and examples for student interest and ensuring that students have many opportunities to interact with faculty both in and outside of class.},
  added-at = {2017-05-11T21:33:49.000+0200},
  author = {Barker, Lecia and Hovey, Christopher Lynnly and Thompson, Leisa D.},
  biburl = {https://www.bibsonomy.org/bibtex/209bec24542553a20d689ce812be367b8/vngudivada},
  booktitle = {2014 {IEEE} Frontiers in Education Conference ({FIE}) Proceedings},
  interhash = {232788e29f676bda01e4bba719ce004c},
  intrahash = {09bec24542553a20d689ce812be367b8},
  keywords = {Retention},
  month = oct,
  publisher = {{IEEE}},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Results of a large-scale, multi-institutional study of undergraduate retention in computing},
  url = {https://doi.org/10.1109%2Ffie.2014.7044267},
  year = 2014
}

@book{willingham2010students,
  abstract = {Easy-to-apply, scientifically-based approaches for engaging students in the classroom
Cognitive scientist Dan Willingham focuses his acclaimed research on the biological and cognitive basis of learning. His book will help teachers improve their practice by explaining how they and their students think and learn. It reveals-the importance of story, emotion, memory, context, and routine in building knowledge and creating lasting learning experiences.

Nine, easy-to-understand principles with clear applications for the classroom
Includes surprising findings, such as that intelligence is malleable, and that you cannot develop "thinking skills" without facts
How an understanding of the brain's workings can help teachers hone their teaching skills.},
  added-at = {2017-04-05T03:38:02.000+0200},
  address = {San Francisco, CA},
  author = {Willingham, Daniel T.},
  biburl = {https://www.bibsonomy.org/bibtex/2d45b7e232fe7499bc38278c326a43d8f/vngudivada},
  interhash = {6a2f241b845d6f28b3ccccab589452b8},
  intrahash = {d45b7e232fe7499bc38278c326a43d8f},
  keywords = {Book CognitiveScience Learning},
  publisher = {Jossey-Bass},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Why don't students like school?: a cognitive scientist answers questions about how the mind works and what it means for the classroom},
  year = 2010
}

@inproceedings{shin2004ensuring,
  abstract = {Surveys and polling data confirm that the Internet is now a prime vehicle for business, community, and personal interactions. The notion of identity is the important component of this vehicle. When users interact with services on the Internet, they often tailor the services in some way for their personal use. For example, a user may establish an account with a username and password and/or set some preferences for what information the user wants displayed and how the user wants it displayed. The network identity of each user is the overall global set of these attributes constituting the various accounts. In this paper, we investigate two well-known federated identity management (FIM) solutions, Microsoft Passport and Liberty Alliance, attempting to identify information assurance (IA) requirements in FIM. In particular, we focus on principal IA requirements for Web services (WS) which plays an integral role in enriching identity management through federation.},
  added-at = {2017-05-20T21:26:20.000+0200},
  author = {Shin, Dongwan and Ahn, Gail-Joon and Shenoy, Prasad},
  biburl = {https://www.bibsonomy.org/bibtex/277e32ba510656f2f630f1fa784b30f53/vngudivada},
  booktitle = {{IEEE} International Conference on Performance, Computing, and Communications},
  doi = {10.1109/pccc.2004.1395193},
  interhash = {4892a9e96facbbfa507afd32f6ab01fd},
  intrahash = {77e32ba510656f2f630f1fa784b30f53},
  keywords = {FederatedIdentityManagement},
  publisher = {{IEEE}},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Ensuring information assurance in federated identity management},
  year = 2004
}

@misc{murray2017markov,
  added-at = {2017-05-17T21:27:47.000+0200},
  author = {Murray, Iain},
  biburl = {https://www.bibsonomy.org/bibtex/2dd72ba72a3db2e2a7a75f871380db77c/vngudivada},
  interhash = {1b2184e13f4643585e5728c75c03b6b0},
  intrahash = {dd72ba72a3db2e2a7a75f871380db77c},
  keywords = {MCMC BayesianMethods},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Markov Chain Monte Carlo},
  year = 2017
}

@article{daniel2015analytics,
  abstract = {Institutions of higher education are operating in an increasingly complex and competitive environment. This paper identifies contemporary challenges facing institutions of higher education worldwide and explores the potential of Big Data in addressing these challenges. The paper then outlines a number of opportunities and challenges associated with the implementation of Big Data in the context of higher education. The paper concludes by outlining future directions relating to the development and implementation of an institutional project on Big Data.},
  added-at = {2017-04-09T23:59:33.000+0200},
  author = {Daniel, Ben},
  biburl = {https://www.bibsonomy.org/bibtex/2bd651fe177df73f01b4d776111d7ac87/vngudivada},
  interhash = {82067895f99bce35e0368465890cc304},
  intrahash = {bd651fe177df73f01b4d776111d7ac87},
  journal = {British Journal of Educational Technology},
  keywords = {BigData DataAnalytics LearningAnalytics},
  number = 5,
  pages = {904--920},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Big Data and analytics in higher education: Opportunities and challenges},
  url = {http://dx.doi.org/10.1111/bjet.12230},
  volume = 46,
  year = 2015
}

@misc{zumbrunn2011encouraging,
  abstract = {Self-regulated learning (SLR) is recognized as an important predictor of student academic motivation and achievement. This process requires students to independently plan, monitor, and assess their learning. However, few students naturally do this well. This paper provides a review of the literature including: the definition of SRL; an explanation of the relationship between SRL and motivation in the classroom; specific SRL strategies for student use; approaches for encouraging student SRL; and a discussion of some of the challenges educators might encounter while teaching students to be self-regulated, life-long learners.},
  added-at = {2017-05-19T13:16:40.000+0200},
  author = {Zumbrunn, Sharon and Tadlock, Joseph and Roberts, Elizabeth Danielle},
  biburl = {https://www.bibsonomy.org/bibtex/29e3f26a1dc33381f0f22201c5b878a7a/vngudivada},
  interhash = {af4926dd8d376dd084d55156e31745c1},
  intrahash = {9e3f26a1dc33381f0f22201c5b878a7a},
  keywords = {Learning MetaCognition SelfRegulatedLearning},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Encouraging Self-Regulated Learning in the Classroom: A Review of the Literature},
  year = 2011
}

@article{peckham2007increasing,
  abstract = {To address the alarming decrease in students in Rhode Island computer science programs and the under-representation of women and minorities, we have devised a program to introduce students to research in computer graphics, art and new media. This program integrates good mentoring practice and pedagogy, including problem-based learning. Special attention is paid to creating a cohort of students who come together every week to learn about the research process, and ethical and societal issues related to it. Each student takes a small project from the proposal stage, through design and implementation, to publication and presentation. We report on the first two years of the program.},
  added-at = {2017-05-11T23:34:56.000+0200},
  address = {New York, NY},
  author = {Peckham, Joan and Stephenson, Peter and Herv{\'e}, Jean-Yves and Hutt, Ron and Encarna\c{c}\ {a}o, Miguel},
  biburl = {https://www.bibsonomy.org/bibtex/2629fc0fc9108a2b195461576b488257f/vngudivada},
  interhash = {3657ebcae53d8d944444e5d25091c5d7},
  intrahash = {629fc0fc9108a2b195461576b488257f},
  journal = {SIGCSE Bull.},
  keywords = {Retention},
  month = mar,
  number = 1,
  pages = {124--128},
  publisher = {ACM},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Increasing Student Retention in Computer Science Through Research Programs for Undergraduates},
  url = {http://doi.acm.org/10.1145/1227504.1227354},
  volume = 39,
  year = 2007
}

@book{newport2006become,
  abstract = {Looking to jumpstart your GPA? Most college students believe that straight A’s can be achieved only through cramming and painful all-nighters at the library. But Cal Newport knows that real straight-A students don’t study harder—they study smarter. A breakthrough approach to acing academic assignments, from quizzes and exams to essays and papers, How to Become a Straight-A Student reveals for the first time the proven study secrets of real straight-A students across the country and weaves them into a simple, practical system that anyone can master. You will learn how to:

Streamline and maximize your study time
Conquer procrastination
Absorb the material quickly and effectively
Know which reading assignments are critical—and which are not
Target the paper topics that wow professors
Provide A+ answers on exams
Write stellar prose without the agony

A strategic blueprint for success that promises more free time, more fun, and top-tier results, How to Become a Straight-A Student is the only study guide written by students for students—with the insider knowledge and real-world methods to help you master the college system and rise to the top of the class.},
  added-at = {2017-03-20T15:44:20.000+0100},
  author = {Newport, Cal},
  biburl = {https://www.bibsonomy.org/bibtex/21f3161e48aad78157d7f9fc45d2ded2d/vngudivada},
  interhash = {9fdd4ba582e1603081187c8389448171},
  intrahash = {1f3161e48aad78157d7f9fc45d2ded2d},
  keywords = {Learning},
  publisher = {Three Rivers Press},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {How to Become a Straight-A Student: The Unconventional Strategies Real College Students Use to Score High While Studying Less},
  year = 2006
}

@book{polya2014solve,
  abstract = {A perennial bestseller by eminent mathematician G. Polya, How to Solve It will show anyone in any field how to think straight. In lucid and appealing prose, Polya reveals how the mathematical method of demonstrating a proof or finding an unknown can be of help in attacking any problem that can be "reasoned" out--from building a bridge to winning a game of anagrams. Generations of readers have relished Polya's deft--indeed, brilliant--instructions on stripping away irrelevancies and going straight to the heart of the problem.},
  added-at = {2017-03-13T02:26:53.000+0100},
  address = {Princeton New Jersey},
  author = {Polya, George},
  biburl = {https://www.bibsonomy.org/bibtex/20313e0c8630b178fc697b11250f9d8ab/vngudivada},
  interhash = {ca2801f1641e7237e6097a26188f0f68},
  intrahash = {0313e0c8630b178fc697b11250f9d8ab},
  keywords = {Book Learning Mathematics},
  publisher = {Princeton University Press},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {How to solve it: a new aspect of mathematical method},
  year = 2014
}

@article{raelin2014gendered,
  abstract = {Background
Longstanding data have established that women earn about 20% of undergraduate degrees in engineering; they also have lower academic self-efficacy in the science, technology, engineering, and math fields than do men. This study examines these findings through a longitudinal design that explores whether cooperative education (co-op) can improve the retention of women (and men) in their undergraduate studies.

Purpose
This study examines the effect on retention of demographic characteristics, cooperative education, contextual support, and three dimensions of self-efficacy – work, career, and academic – and their change over time. It incorporates longitudinal measures and a data check at the end of the students' fifth year.

Design/Method
Respondents filled out 20-minute surveys, approximately one year apart, during three separate time periods. The study introduced and validated a number of new scales. The data for each time period were submitted to successive analyses.

Results
The findings verified the study's pathways model. Academic achievement and academic self-efficacy, as well as contextual support in the case for women, in all time periods were critical to retention. Work self-efficacy, developed by students between their second and fourth years, was also an important factor in retention, although it was strongly tied to the students' participation in co-op programs. Higher retention was associated with an increased number of co-ops completed by students.

Conclusion
Relationships between work self-efficacy and co-op participation and between academic self-efficacy and academic achievement play a critical role in retention for both male and female students.
},
  added-at = {2017-05-11T22:10:35.000+0200},
  author = {Raelin, Joseph A. and Bailey, Margaret B. and Hamann, Jerry and Pendleton, Leslie K. and Reisberg, Rachelle and Whitman, David L.},
  biburl = {https://www.bibsonomy.org/bibtex/2c9d27464017519af59413718f920ae4a/vngudivada},
  interhash = {ab9820ecfcb4ae0c797f22e5aa226372},
  intrahash = {c9d27464017519af59413718f920ae4a},
  journal = {Journal of Engineering Education},
  keywords = {Retention},
  number = 4,
  pages = {599--624},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {The Gendered Effect of Cooperative Education, Contextual Support, and Self-Efficacy on Undergraduate Retention},
  url = {http://dx.doi.org/10.1002/jee.20060},
  volume = 103,
  year = 2014
}

@article{scullin2010remembering,
  abstract = {Remembering to execute deferred goals (prospective memory) is a ubiquitous memory challenge, and one that is often not successfully accomplished. Could sleeping after goal encoding promote later execution? We evaluated this possibility by instructing participants to execute a prospective memory goal after a short delay (20 min), a 12-hr wake delay, or a 12-hr sleep delay. Goal execution declined after the 12-hr wake delay relative to the short delay. In contrast, goal execution was relatively preserved after the 12-hr sleep delay relative to the short delay. The sleep-enhanced goal execution was not accompanied by a decline in performance of an ongoing task in which the prospective memory goal was embedded, which suggests that the effect was not a consequence of attentional resources being reallocated from the ongoing task to the prospective memory goal. Our results suggest that consolidation processes active during sleep increase the probability that a goal will be spontaneously retrieved and executed.},
  added-at = {2017-03-12T01:53:52.000+0100},
  author = {Scullin, Michael K. and McDaniel, Mark A.},
  biburl = {https://www.bibsonomy.org/bibtex/2a6ac0dc64d332a2b19a47d42a545fa76/vngudivada},
  interhash = {5216d4e2312b8e0e57835a4581dcfc8e},
  intrahash = {a6ac0dc64d332a2b19a47d42a545fa76},
  journal = {Psychological Science},
  keywords = {Learning},
  month = jul,
  number = 7,
  pages = {1028--1035},
  publisher = {{SAGE} Publications},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Remembering to Execute a Goal: Sleep on It!},
  url = {https://doi.org/10.1177%2F0956797610373373},
  volume = 21,
  year = 2010
}

@article{papamitsiou2014learning,
  abstract = {This paper aims to provide the reader with a comprehensive background for understanding current knowledge on Learning Analytics (LA) and Educational Data Mining (EDM) and its impact on adaptive learning. It constitutes an overview of empirical evidence behind key objectives of the potential adoption of LA/EDM in generic educational strategic planning. We examined the literature on experimental case studies conducted in the domain during the past six years (2008 - 2013). Search terms identified 209 mature pieces of research work, but inclusion criteria limited the key studies to 40. We analyzed the research questions, methodology and findings of these published papers and categorized them accordingly. We used non-statistical methods to evaluate and interpret findings of the collected studies. The results have highlighted four distinct major directions of the LA/EDM empirical research. We discuss on the emerged added value of LA/EDM research and highlight the significance of further implications. Finally, we set our thoughts on possible uncharted key questions to investigate both from pedagogical and technical considerations.},
  added-at = {2017-04-09T23:36:15.000+0200},
  author = {Papamitsiou, Zacharoula and Economides, Anastasios A.},
  biburl = {https://www.bibsonomy.org/bibtex/24208c84ce9d6b531a8a40a3a571bbf8b/vngudivada},
  interhash = {69576d599e0b9ab00d8f73c0478bd33e},
  intrahash = {4208c84ce9d6b531a8a40a3a571bbf8b},
  journal = {Educational Technology \& Society},
  keywords = {EDM LearningAnalytics},
  number = 4,
  pages = {49 -- 64},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Learning Analytics and Educational Data Mining in Practice: A Systematic Literature Review of Empirical Evidence},
  url = {http://ifets.info/journals/17_4/4.pdf},
  volume = 74,
  year = 2014
}

@article{gregerman1998undergraduate,
  abstract = {This article evaluates the impact of a program promoting student-faculty research partnerships on college student retention. The program, built on the premise that successful retention efforts integrate students into the core academic mission of the university, targets first-year and sophomore undergraduates. Findings of a participant-control group design show that the research partnerships are most effective in promoting the retention of students at greater risk for college attritionÑAfrican American students and students with low GPAs.},
  added-at = {2017-05-11T22:00:58.000+0200},
  author = {Gregerman, S. R. and Lerner, J. S. and Hippel, W. and Jonides, J. and Nagda, B. A.},
  biburl = {https://www.bibsonomy.org/bibtex/2cbfcfef4b72ac9664e9ccc5693b29ac0/vngudivada},
  interhash = {93eadfaa901facee8adec0fbb66b470b},
  intrahash = {cbfcfef4b72ac9664e9ccc5693b29ac0},
  journal = {The Review of Higher Education},
  keywords = {Retention},
  note = {Project MUSE, doi:10.1353/rhe.1998.0016},
  number = 1,
  pages = {55--72},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Undergraduate Student-Faculty Research Partnerships Affect Student Retention},
  volume = 22,
  year = 1998
}

@article{soleimani2014parsimonious,
  abstract = {We propose a parsimonious topic model for text corpora. In related models such as Latent Dirichlet Allocation (LDA), all words are modeled topic-specifically, even though many words occur with similar frequencies across different topics. Our modeling determines salient words for each topic, which have topic-specific probabilities, with the rest explained by a universal shared model. Further, in LDA all topics are in principle present in every document. By contrast, our model gives sparse topic representation, determining the (small) subset of relevant topics for each document. We derive a Bayesian Information Criterion (BIC), balancing model complexity and goodness of fit. Here, interestingly, we identify an effective sample size and corresponding penalty specific to each parameter type in our model. We minimize BIC to jointly determine our entire model—the topic-specific words, document-specific topics, all model parameter values, and the total number of topics—in a wholly unsupervised fashion. Results on three text corpora and an image dataset show that our model achieves higher test set likelihood and better agreement with ground-truth class labels, compared to LDA and to a model designed to incorporate sparsity.},
  added-at = {2017-05-08T00:28:37.000+0200},
  address = {Los Alamitos, CA},
  author = {Soleimani, Hossein and Miller, David J.},
  biburl = {https://www.bibsonomy.org/bibtex/24c783cdaa4af141cfed24ec104db5c5d/vngudivada},
  interhash = {f1bdd4d32564306989f541b53b6bf4b8},
  intrahash = {4c783cdaa4af141cfed24ec104db5c5d},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {TopicModel},
  number = 3,
  pages = {824 - 837},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Parsimonious Topic Models with Salient Word Discovery},
  volume = 27,
  year = 2014
}

@article{maler2008identity,
  abstract = {Federated identity management lets users dynamically distribute identity information across security domains, increasing the portability of their digital identities. It also raises new architectural challenges and significant security and privacy issues.},
  added-at = {2017-05-20T20:49:02.000+0200},
  author = {Maler, Eve and Reed, Drummond},
  biburl = {https://www.bibsonomy.org/bibtex/21ce27f889e808bca1e35ed29ea9c6aea/vngudivada},
  doi = {10.1109/msp.2008.50},
  interhash = {763521ac6c1feccf13722a70a7d48af0},
  intrahash = {1ce27f889e808bca1e35ed29ea9c6aea},
  journal = {{IEEE} Security {\&} Privacy Magazine},
  keywords = {FederatedIdentityManagement Security},
  month = mar,
  number = 2,
  pages = {16--23},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {The Venn of Identity: Options and Issues in Federated Identity Management},
  url = {https://doi.org/10.1109%2Fmsp.2008.50},
  volume = 6,
  year = 2008
}

@article{reichert1997taking,
  abstract = {In this paper we identify the engineering schools that either graduate large classes of African Americans or that retain relatively high percentages of African American students in engineering; point out that modest improvements in student retention would significantly affect the total number of bachelors degrees earned annually by African Americans in engineering; examine the measures implemented by some schools that are particularly effective at retaining African Americans in engineering; and present our views on the future of diversity efforts in engineering. The message we wish to emphasize is that at the very least colleges and universities should endeavor to retain those under represented minority undergraduates who have decided to pursue an engineering education. Frequently used abbreviations EWC — Engineering Workforce Commission, HBCU — historically black college or university, MEP — minority engineering program, MES — minority engineering society, MRR — minority retention rate, NACME — National Action Council for Minorities in Engineering, TWI — traditionally white institution},
  added-at = {2017-05-11T22:05:24.000+0200},
  author = {Reichert, Monty and Absher, Martha},
  biburl = {https://www.bibsonomy.org/bibtex/20bf49ea0e0b0724edeccdf3472e8eb43/vngudivada},
  interhash = {b65f9e94accde25bdf396c57b049664c},
  intrahash = {0bf49ea0e0b0724edeccdf3472e8eb43},
  journal = {Journal of Engineering Education},
  keywords = {Retention},
  number = 3,
  pages = {241--253},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Taking Another Look at Educating African American Engineers: The Importance of Undergraduate Retention},
  url = {http://dx.doi.org/10.1002/j.2168-9830.1997.tb00291.x},
  volume = 86,
  year = 1997
}

@incollection{lovett2013exams,
  added-at = {2017-03-20T13:59:54.000+0100},
  address = {Sterling, VA},
  author = {Lovett, M. C.},
  biburl = {https://www.bibsonomy.org/bibtex/239c0298136f66d94bf0eab215b6a9034/vngudivada},
  booktitle = {Using reflection and metacognition to improve student learning},
  editor = {Kaplan, M. and Silver, N. and Lavaque-Manty, D. and Meizlish, D.},
  interhash = {efb0f3c8f18f6806ae76d1652f417b21},
  intrahash = {39c0298136f66d94bf0eab215b6a9034},
  keywords = {ExamWrapper Learning MetaCognition},
  publisher = {Stylus Publishers},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Make exams worth more than grades: Using exam wrappers to promote metacognition},
  url = {http://www.learningwrappers.org},
  year = 2013
}

@article{macfadyen2010mining,
  abstract = {Earlier studies have suggested that higher education institutions could harness the predictive power of Learning Management System (LMS) data to develop reporting tools that identify at-risk students and allow for more timely pedagogical interventions. This paper confirms and extends this proposition by providing data from an international research project investigating which student online activities accurately predict academic achievement. Analysis of LMS tracking data from a Blackboard Vista-supported course identified 15 variables demonstrating a significant simple correlation with student final grade. Regression modelling generated a best-fit predictive model for this course which incorporates key variables such as total number of discussion messages posted, total number of mail messages sent, and total number of assessments completed and which explains more than 30% of the variation in student final grade. Logistic modelling demonstrated the predictive power of this model, which correctly identified 81% of students who achieved a failing grade. Moreover, network analysis of course discussion forums afforded insight into the development of the student learning community by identifying disconnected students, patterns of student-to-student communication, and instructor positioning within the network. This study affirms that pedagogically meaningful information can be extracted from LMS-generated student tracking data, and discusses how these findings are informing the development of a customizable dashboard-like reporting tool for educators that will extract and visualize real-time data on student engagement and likelihood of success.},
  added-at = {2017-04-10T00:03:34.000+0200},
  author = {Macfadyen, Leah P. and Dawson, Shane},
  biburl = {https://www.bibsonomy.org/bibtex/2c05109caed1bd0c1c628c1e2fcfcd60b/vngudivada},
  interhash = {3d6caec9e621c4874911176d3399709e},
  intrahash = {c05109caed1bd0c1c628c1e2fcfcd60b},
  journal = {Computers & Education},
  keywords = {EDM LearningAnalytics},
  number = 2,
  pages = {588 - 599},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Mining LMS data to develop an “early warning system” for educators: A proof of concept},
  url = {http://www.sciencedirect.com/science/article/pii/S0360131509002486},
  volume = 54,
  year = 2010
}

@article{logan2012metacognition,
  abstract = {Although memory performance benefits from the spacing of information at encoding, judgments of learning (JOLs) are often not sensitive to the benefits of spacing. The present research examines how practice, feedback, and instruction influence JOLs for spaced and massed items. In Experiment 1, in which JOLs were made after the presentation of each item and participants were given multiple study-test cycles, JOLs were strongly influenced by the repetition of the items, but there was little difference in JOLs for massed versus spaced items. A similar effect was shown in Experiments 2 and 3, in which participants scored their own recall performance and were given feedback, although participants did learn to assign higher JOLs to spaced items with task experience. In Experiment 4, after participants were given direct instruction about the benefits of spacing, they showed a greater difference for JOLs of spaced vs massed items, but their JOLs still underestimated their recall for spaced items. Although spacing effects are very robust and have important implications for memory and education, people often underestimate the benefits of spaced repetition when learning, possibly due to the reliance on processing fluency during study and attending to repetition, and not taking into account the beneficial aspects of study schedule.},
  added-at = {2017-03-12T20:29:48.000+0100},
  author = {Logan, Jessica M. and Castel, Alan D. and Haber, Sara and Viehman, Emily J.},
  biburl = {https://www.bibsonomy.org/bibtex/2bffd64bbff92a35802a1dc786b8ccb85/vngudivada},
  interhash = {0b50ed433188a55af6d2357feae49f71},
  intrahash = {bffd64bbff92a35802a1dc786b8ccb85},
  journal = {Metacognition and Learning},
  keywords = {InterleavedPractice Metacognition},
  number = 3,
  pages = {175--195},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Metacognition and the spacing effect: the role of repetition, feedback, and instruction on judgments of learning for massed and spaced rehearsal},
  url = {http://dx.doi.org/10.1007/s11409-012-9090-3},
  volume = 7,
  year = 2012
}

@article{konrad2016promote,
  abstract = {Flexibility in applying existing knowledge to similar cues is a corner stone of memory development in infants. Here, we examine the effect of sleep on the flexibility of memory retrieval using a deferred imitation paradigm. Forty-eight 12-month-old infants were randomly assigned to either a nap or a no-nap demonstration condition (scheduled around their natural daytime sleep schedule) or to a baseline control condition. In the demonstration conditions, infants watched an experimenter perform three target actions on a hand puppet. Immediately afterwards, infants were allowed to practice the target actions three times. In a test session 4-hr later, infants were given the opportunity to reproduce the actions with a novel hand puppet differing in color from the puppet used during the demonstration session. Only infants in the nap-condition performed significantly more target actions than infants in the baseline control condition. Furthermore, they were faster to carry out the first target action than infants in the no-nap condition. We conclude that sleep had a facilitative effect on infants’ flexibility of memory retrieval.
},
  added-at = {2017-03-12T02:00:09.000+0100},
  author = {Konrad, Carolin and Seehagen, Sabine and Schneider, Silvia and Herbert, Jane S.},
  biburl = {https://www.bibsonomy.org/bibtex/215b417b3522dbd2de23395dbe630a117/vngudivada},
  interhash = {844d34ebc9342def8a406bc528a5fe90},
  intrahash = {15b417b3522dbd2de23395dbe630a117},
  journal = {Developmental Psychobiology},
  keywords = {Learning},
  number = 7,
  pages = {866--874},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Naps promote flexible memory retrieval in 12-month-old infants},
  url = {http://dx.doi.org/10.1002/dev.21431},
  volume = 58,
  year = 2016
}

@inbook{chadwick2009federated,
  abstract = {This paper addresses the topic of federated identity management. It discusses in detail the following topics: what is digital identity, what is identity management, what is federated identity management, Kim Cameron’s 7 Laws of Identity, how can we protect the user’s privacy in a federated environment, levels of assurance, some past and present federated identity management systems, and some current research in FIM.},
  added-at = {2017-05-20T20:31:51.000+0200},
  address = {Berlin, Germany},
  author = {Chadwick, David W.},
  biburl = {https://www.bibsonomy.org/bibtex/254ee913d6a15928f946a0a9eced06d30/vngudivada},
  booktitle = {Foundations of Security Analysis and Design V: FOSAD 2007/2008/2009 Tutorial Lectures},
  doi = {10.1007/978-3-642-03829-7_3},
  editor = {Aldini, Alessandro and Barthe, Gilles and Gorrieri, Roberto},
  interhash = {e91b2b5319eb123cd708cdfa16a9c57a},
  intrahash = {54ee913d6a15928f946a0a9eced06d30},
  isbn = {978-3-642-03829-7},
  keywords = {Book FederatedIdentityManagement Security},
  pages = {96--120},
  publisher = {Springer},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Federated Identity Management},
  url = {http://dx.doi.org/10.1007/978-3-642-03829-7_3},
  year = 2009
}

@book{chowdhury2017analytics,
  abstract = {Data Analytics for Intelligent Transportation Systems provides in-depth coverage of data-enabled methods for analyzing intelligent transportation systems that includes detailed coverage of the tools needed to implement these methods using big data analytics and other computing techniques. The book examines the major characteristics of connected transportation systems, along with the fundamental concepts of how to analyze the data they produce.

It explores collecting, archiving, processing, and distributing the data, designing data infrastructures, data management and delivery systems, and the required hardware and software technologies. Users will learn how to design effective data visualizations, tactics on the planning process, and how to evaluate alternative data analytics for different connected transportation applications, along with key safety and environmental applications for both commercial and passenger vehicles, data privacy and security issues, and the role of social media data in traffic planning.

Includes case studies in each chapter that illustrate the application of concepts covered
Presents extensive coverage of existing and forthcoming intelligent transportation systems and data analytics technologies
Contains contributors from both leading academic and commercial researchers
Explains how to design effective data visualizations, tactics on the planning process, and how to evaluate alternative data analytics for different connected transportation applications},
  added-at = {2017-04-06T13:05:00.000+0200},
  address = {New York, NY},
  biburl = {https://www.bibsonomy.org/bibtex/23e166806bc90c4e0eb276b3c4d26a14e/vngudivada},
  editor = {Chowdhury, Mashrur and Apon, Amy and Dey, Kakan},
  interhash = {ee475e735d1c74dd897fb52fa9ad0516},
  intrahash = {3e166806bc90c4e0eb276b3c4d26a14e},
  keywords = {DataAnalytics IntelligentTransportationSystem MachineLearning},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Data analytics for intelligent transportation systems},
  year = 2017
}

@book{holmes2008innovations,
  abstract = {Bayesian networks currently provide one of the most rapidly growing areas of research in computer science and statistics. In compiling this volume we have brought together contributions from some of the most prestigious researchers in this field. Each of the twelve chapters is self-contained.

Both theoreticians and application scientists/engineers in the broad area of artificial intelligence will find this volume valuable. It also provides a useful sourcebook for Graduate students since it shows the direction of current research.},
  added-at = {2017-06-02T00:40:25.000+0200},
  address = {Berlin, Germany},
  biburl = {https://www.bibsonomy.org/bibtex/2c0254120f09b466048376d6535653304/vngudivada},
  doi = {10.1007/978-3-540-85066-3},
  editor = {Holmes, Dawn E. and Jain, Lakhmi C.},
  interhash = {cd539168e9b11cf0ff282f09f82f7e47},
  intrahash = {c0254120f09b466048376d6535653304},
  isbn = {978-3-540-85065-6},
  keywords = {BayesianMethods ML},
  publisher = {Springer},
  series = {Studies in Computational Intelligence},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Innovations in Bayesian Networks: Theory and Applications},
  volume = 156,
  year = 2008
}

@book{meyer2010threshold,
  abstract = {Over the last decade the notion of 'threshold concepts' has proved influential around the world as a powerful means of exploring and discussing the key points of transformation that students experience in their higher education courses and the 'troublesome knowledge' that these often present. Threshold concepts provoke in the learner a state of 'liminality' in which transformation takes place, requiring the integration of new understanding and the letting go of previous learning stances. Insights gained by learners as they cross thresholds can be exhilarating but might also be unsettling, requiring an uncomfortable shift in identity, or, paradoxically, a sense of loss. The liminal space can be a suspended state of partial understanding, or 'stuck place', in which understanding approximates to a kind of 'mimicry'. "Threshold Concepts and Transformational Learning" substantially increases the empirical evidence for threshold concepts across a large number of disciplinary contexts and from the higher education sectors of many countries. This new volume develops further theoretical perspectives and provides fresh pedagogical directions. It will be of interest to teachers, practitioners and managers in all disciplines as well as to educational researchers.},
  added-at = {2017-03-18T14:11:08.000+0100},
  address = {Boston, Massachusetts},
  author = {Meyer, Jan H.F and Land, Ray and Baillie, Caroline},
  biburl = {https://www.bibsonomy.org/bibtex/2fe1d7addef15a501e048765810ce6aad/vngudivada},
  interhash = {442d9aba146e04c60ce028a283747eac},
  intrahash = {fe1d7addef15a501e048765810ce6aad},
  keywords = {Book Learning TransformationalLearning},
  publisher = {Sense Publishers},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Threshold concepts and transformational learning},
  year = 2010
}

@article{criminisi2012decision,
  abstract = {This review presents a unified, efficient model of random decision forests which can be applied to a number of machine learning, computer vision, and medical image analysis tasks.

Our model extends existing forest-based techniques as it unifies classification, regression, density estimation, manifold learning, semisupervised learning, and active learning under the same decision forest framework. This gives us the opportunity to write and optimize the core implementation only once, with application to many diverse tasks.
The proposed model may be used both in a discriminative or generative way and may be applied to discrete or continuous, labeled or unlabeled data.


The main contributions of this review are: (1) Proposing a unified, probabilistic and efficient model for a variety of learning tasks; (2) Demonstrating margin-maximizing properties of classification forests; (3) Discussing probabilistic regression forests in comparison with other nonlinear regression algorithms; (4) Introducing density forests for estimating probability density functions; (5) Proposing an efficient algorithm for sampling from a density forest; (6) Introducing manifold forests for nonlinear dimensionality reduction; (7) Proposing new algorithms for transductive learning and active learning. Finally, we discuss how alternatives such as random ferns and extremely randomized trees stem from our more general forest model.


This document is directed at both students who wish to learn the basics of decision forests, as well as researchers interested in the new contributions. It presents both fundamental and novel concepts in a structured way, with many illustrative examples and real-world applications. Thorough comparisons with state-of-the-art algorithms such as support vector machines, boosting and Gaussian processes are presented and relative advantages and disadvantages discussed. The many synthetic examples and existing commercial applications demonstrate the validity of the proposed model and its flexibility.},
  added-at = {2017-06-03T15:36:41.000+0200},
  author = {Criminisi, Antonio and Shotton, Jamie and Konukoglu, Ender},
  biburl = {https://www.bibsonomy.org/bibtex/22762cf22c783351a4507981acb99deb5/vngudivada},
  doi = {10.1561/0600000035},
  interhash = {d0c051a22aeb5fa6131b07bd58f40d2b},
  intrahash = {2762cf22c783351a4507981acb99deb5},
  issn = {1572-2740},
  journal = {Foundations and Trends® in Computer Graphics and Vision},
  keywords = {DecisionTree ML Probability},
  number = {2–3},
  pages = {81-227},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Decision Forests: A Unified Framework for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning},
  url = {http://dx.doi.org/10.1561/0600000035},
  volume = 7,
  year = 2012
}

@article{domingos2012useful,
  abstract = {Tapping into the 'folk knowledge' needed to advance machine learning applications. Machine learning algorithms can figure out how to perform important tasks by generalizing from examples. This is often feasible and cost-effective where manual programming is not. As more data becomes available, more ambitious problems can be tackled. Machine learning is widely used in computer science and other fields. However, developing successful machine learning applications requires a substantial amount of 'black art' that is difficult to find in textbooks. This article summarizes 12 key lessons that machine learning researchers and practitioners have learned. These include pitfalls to avoid, important issues to focus on, and answers to common questions.},
  added-at = {2017-05-12T17:41:51.000+0200},
  address = {New York, NY},
  author = {Domingos, Pedro},
  biburl = {https://www.bibsonomy.org/bibtex/27c848538188ee0d620423fe9dd352e6c/vngudivada},
  interhash = {28f49d94d3029e886460cde63094e482},
  intrahash = {7c848538188ee0d620423fe9dd352e6c},
  journal = {Commun. ACM},
  keywords = {ML},
  month = oct,
  number = 10,
  pages = {78--87},
  publisher = {ACM},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {A Few Useful Things to Know About Machine Learning},
  url = {http://doi.acm.org/10.1145/2347736.2347755},
  volume = 55,
  year = 2012
}

@inproceedings{sabourin2015student,
  abstract = {While the field of educational data mining (EDM) has generated many innovations for improving educational software and student learning, the mining of student data has recently come under a great deal of scrutiny. Many stakeholder groups, including public officials, media outlets, and parents, have voiced concern over the privacy of student data and their efforts have garnered national attention. The momentum behind and scrutiny of student privacy has made it increasingly difficult for EDM applications to transition from academia to industry. Based on experience as academic researchers transitioning into industry, we present three primary areas of concern related to student privacy in practice: policy, corporate social responsibility, and public opinion. Our discussion will describe the key challenges faced within these categories, strategies for overcoming them, and ways in which the academic EDM community can support the adoption of innovative technologies in large-scale production.},
  added-at = {2017-04-10T00:09:40.000+0200},
  author = {Sabourin, Jennifer and Kosturko, Lucy and FitzGerald, Clare and McQuiggan, Scott},
  biburl = {https://www.bibsonomy.org/bibtex/2d7e8fc09b023ca5de0b24a87c46ca1b3/vngudivada},
  booktitle = {Proceedings of the International Conference on Educational Data Mining (EDM)},
  interhash = {3719d000eef02bbcd7ea7101092bd852},
  intrahash = {d7e8fc09b023ca5de0b24a87c46ca1b3},
  keywords = {EDM Privacy},
  publisher = {International Educational Data Mining Society},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Student Privacy and Educational Data Mining: Perspectives from Industry},
  year = 2015
}

@inproceedings{volkovs2014continuous,
  abstract = {In declarative data cleaning, data semantics are encoded as constraints and errors arise when the data violates the constraints. Various forms of statistical and logical inference can be used to reason about and repair inconsistencies (errors) in data. Recently, unified approaches that repair both errors in data and errors in semantics (the constraints) have been proposed. However, both data-only approaches and unified approaches are by and large static in that they apply cleaning to a single snapshot of the data and constraints. We introduce a continuous data cleaning framework that can be applied to dynamic data and constraint environments. Our approach permits both the data and its semantics to evolve and suggests repairs based on the accumulated evidence to date. Importantly, our approach uses not only the data and constraints as evidence, but also considers the past repairs chosen and applied by a user (user repair preferences). We introduce a repair classifier that predicts the type of repair needed to resolve an inconsistency, and that learns from past user repair preferences to recommend more accurate repairs in the future. Our evaluation shows that our techniques achieve high prediction accuracy and generate high quality repairs. Of independent interest, our work makes use of a set of data statistics that are shown to be sensitive to predicting particular repair types.
Note: As originally published there are errors in the document. The authors note the following corrections to Section V "Classification Framework:" For the group 6 statistics, the definition of V_X should be V_X = {\Pi_X(t) | t \in I \ F } ; For the group 7 statistics, the definition of S_X should be S_X = {\Pi_X(t) | t \notin I \ F }; and For the group 8 statistics, the definition of V_A should be V_A = {\Pi_A(t) | t \in I \ F, \Pi_X(t) = \Pi_X(p) }.},
  added-at = {2017-04-02T01:34:11.000+0200},
  author = {Volkovs, Maksims and Chiang, Fei and Szlichta, Jaroslaw and Miller, Renee J.},
  biburl = {https://www.bibsonomy.org/bibtex/2ee68ea609c1f0f5c8d551a58a281286c/vngudivada},
  booktitle = {2014 {IEEE} 30th International Conference on Data Engineering},
  interhash = {c956c25a0cb8d03b3fe67a5fda4a7e73},
  intrahash = {ee68ea609c1f0f5c8d551a58a281286c},
  keywords = {DataCleaning},
  month = mar,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Continuous data cleaning},
  url = {https://doi.org/10.1109%2Ficde.2014.6816655},
  year = 2014
}

@book{scutari2014bayesian,
  abstract = {Features

Describes theoretical and practical aspects of Bayesian networks
Illustrates all modeling steps with R code
Analyzes real-world examples so readers understand current applications of Bayesian networks
Covers basic graph and probability theory in the appendices
Includes solutions to exercises at the back of the book
Provides supporting material online, including the first author’s bnlearn R package
Summary

Understand the Foundations of Bayesian Networks—Core Properties and Definitions Explained

Bayesian Networks: With Examples in R introduces Bayesian networks using a hands-on approach. Simple yet meaningful examples in R illustrate each step of the modeling process. The examples start from the simplest notions and gradually increase in complexity. The authors also distinguish the probabilistic models from their estimation with data sets.

The first three chapters explain the whole process of Bayesian network modeling, from structure learning to parameter learning to inference. These chapters cover discrete Bayesian, Gaussian Bayesian, and hybrid networks, including arbitrary random variables.

The book then gives a concise but rigorous treatment of the fundamentals of Bayesian networks and offers an introduction to causal Bayesian networks. It also presents an overview of R and other software packages appropriate for Bayesian networks. The final chapter evaluates two real-world examples: a landmark causal protein signaling network paper and graphical modeling approaches for predicting the composition of different body parts.

Suitable for graduate students and non-statisticians, this text provides an introductory overview of Bayesian networks. It gives readers a clear, practical understanding of the general approach and steps involved.},
  added-at = {2017-05-18T00:32:39.000+0200},
  address = {Boca Raton, Florida},
  author = {Scutari, Marco and Denis, Jean-Baptiste},
  biburl = {https://www.bibsonomy.org/bibtex/291ca8243c5e46c6c18b11e4f824a4369/vngudivada},
  interhash = {5a53005104122ac6f2492a2c82f7679a},
  intrahash = {91ca8243c5e46c6c18b11e4f824a4369},
  keywords = {BayesianMethods Book},
  publisher = {Chapman and Hall},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Bayesian Networks with Examples in {R}},
  year = 2014
}

@techreport{pashler2007organizing,
  added-at = {2017-03-19T22:40:46.000+0100},
  address = {Washington, D.C.},
  author = {Pashler, Harold and Bain, Patrice M. and Bottge, Brian A. and Graesser, Arthur and Koedinger, Kenneth and McDaniel, Mark and Metcalfe, Janet},
  biburl = {https://www.bibsonomy.org/bibtex/2fb1998a0a3619291e0c6783f9a69883a/vngudivada},
  institution = {National Center for Education Research, Institute of Education Sciences, U.S. Department of Education},
  interhash = {c66eebf64d0b6efd5b35b1d567dda8c2},
  intrahash = {fb1998a0a3619291e0c6783f9a69883a},
  keywords = {Learning},
  number = {NCER 2007-2004},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Organizing Instruction and Study to Improve Student Learning},
  url = {http://files.eric.ed.gov/fulltext/ED498555.pdf},
  year = 2007
}

@book{mirkin2012clustering,
  abstract = {Features

Provides detailed coverage of selected clustering techniques, notably K-means, divisive clustering, network clustering, spectral clustering, additive clustering, and consensus clustering
Emphasizes the application of the methods through detailed case studies
Offers MATLAB® code for the examples on the book’s website
Illustrates methods by computation on specially selected small real-world datasets
Gives an extensive advice on computational interpretation of clusters, including use of hierarchical ontologies
Summary

Often considered more of an art than a science, books on clustering have been dominated by learning through example with techniques chosen almost through trial and error. Even the two most popular, and most related, clustering methods—K-Means for partitioning and Ward's method for hierarchical clustering—have lacked the theoretical underpinning required to establish a firm relationship between the two methods and relevant interpretation aids. Other approaches, such as spectral clustering or consensus clustering, are considered absolutely unrelated to each other or to the two above mentioned methods.

Clustering: A Data Recovery Approach, Second Edition presents a unified modeling approach for the most popular clustering methods: the K-Means and hierarchical techniques, especially for divisive clustering. It significantly expands coverage of the mathematics of data recovery, and includes a new chapter covering more recent popular network clustering approaches—spectral, modularity and uniform, additive, and consensus—treated within the same data recovery approach. Another added chapter covers cluster validation and interpretation, including recent developments for ontology-driven interpretation of clusters. Altogether, the insertions added a hundred pages to the book, even in spite of the fact that fragments unrelated to the main topics were removed.

Illustrated using a set of small real-world datasets and more than a hundred examples, the book is oriented towards students, practitioners, and theoreticians of cluster analysis. Covering topics that are beyond the scope of most texts, the author’s explanations of data recovery methods, theory-based advice, pre- and post-processing issues and his clear, practical instructions for real-world data mining make this book ideally suited for teaching, self-study, and professional reference.},
  added-at = {2017-05-15T04:21:42.000+0200},
  address = {Boca Raton, FL},
  author = {Mirkin, Boris Grigorievitch},
  biburl = {https://www.bibsonomy.org/bibtex/21a32a1d0fb301d185a016a099f0f3d7d/vngudivada},
  interhash = {f76e2046ea257f88682f722e197e6cca},
  intrahash = {1a32a1d0fb301d185a016a099f0f3d7d},
  keywords = {Book Clustering},
  publisher = {Chapman \& Hall/CRC},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Clustering for data mining a data recovery approach},
  url = {http://www.dcs.bbk.ac.uk/~mirkin/papers/n.pdf},
  year = 2012
}

@article{yadav2016computational,
  abstract = {The recent focus on computational thinking as a key 21st century skill for all students has led to a number of curriculum initiatives to embed it in K-12 classrooms. In this paper, we discuss the key computational thinking constructs, including algorithms, abstraction, and automation. We further discuss how these ideas are related to current educational reforms, such as Common Core and Next Generation Science Standards and provide specific means that would allow teachers to embed these ideas in their K-12 classrooms, including recommendations for instructional technologists and professional development experts for infusing computational thinking into other subjects. In conclusion, we suggest that computational thinking ideas outlined in this paper are key to moving students from merely being technology-literate to using computational tools to solve problems.},
  added-at = {2017-03-26T21:07:02.000+0200},
  author = {Yadav, Aman and Hong, Hai and Stephenson, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/2d0220e22cce9c8fbb1f23248798659f5/vngudivada},
  interhash = {3a003e10c72ae4cde230d4d14e0676d2},
  intrahash = {d0220e22cce9c8fbb1f23248798659f5},
  journal = {TechTrends},
  keywords = {ComputationalThinking},
  number = 6,
  pages = {565--568},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Computational Thinking for All: Pedagogical Approaches to Embedding 21st Century Problem Solving in K-12 Classrooms},
  url = {http://dx.doi.org/10.1007/s11528-016-0087-7},
  volume = 60,
  year = 2016
}

@article{weiland2017importance,
  abstract = {Context is at the core of any statistical investigation, yet many statistics tasks barely require students to go beyond superficial consideration of the contexts the tasks are situated in. In this article, I discuss a framework for evaluating the level of interaction with context a task requires of students and how to modify tasks to increase the levels of interaction required.},
  added-at = {2017-03-18T14:34:13.000+0100},
  author = {Weiland, Travis},
  biburl = {https://www.bibsonomy.org/bibtex/2a5bbeafd499976e6fb214dbfecd7a25d/vngudivada},
  interhash = {e85f8a16700b9558024f4976b9cc715a},
  intrahash = {a5bbeafd499976e6fb214dbfecd7a25d},
  journal = {Teaching Statistics},
  keywords = {Learning StatisticalLearning},
  number = 1,
  pages = {20--25},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {The importance of context in task selection},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/test.12116/full},
  volume = 39,
  year = 2017
}

@article{shokouhi2011federated,
  abstract = {Federated search (federated information retrieval or distributed information retrieval) is a technique for searching multiple text collections simultaneously. Queries are submitted to a subset of collections that are most likely to return relevant answers. The results returned by selected collections are integrated and merged into a single list. Federated search is preferred over centralized search alternatives in many environments. For example, commercial search engines such as Google cannot easily index uncrawlable hidden web collections while federated search systems can search the contents of hidden web collections without crawling. In enterprise environments, where each organization maintains an independent search engine, federated search techniques can provide parallel search over multiple collections.

There are three major challenges in federated search. For each query, a subset of collections that are most likely to return relevant documents are selected. This creates the collection selection problem. To be able to select suitable collections, federated search systems need to acquire some knowledge about the contents of each collection, creating the collection representation problem. The results returned from the selected collections are merged before the final presentation to the user. This final step is the result merging problem.

The goal of this work, is to provide a comprehensive summary of the previous research on the federated search challenges described above.

Web search has significantly evolved in recent years. For many years, web search engines such as Google and Yahoo! were only providing search service over text documents. Aggregated search was one of the first steps to go beyond text search, and was the beginning of a new era for information seeking and retrieval. These days, web search engines support aggregated search over a number of verticals, and blend different types of documents (e.g. images, videos) in their search results. Moreover, web search engines have started to crawl and search the hidden web. Federated search (federated information retrieval or distributed information retrieval) has played a key role in providing the technology for aggregated search and crawling the hidden web. The application of federated search is not limited to the web search engines. There are many scenarios such as digital libraries in which information is distributed across different sources/servers. Peer-to-peer networks and personalized search are two examples in which federated search has been successfully used for searching multiple independent collections. Federated Search provides a comprehensive summary of the research done to date, looks at some of the challenges still to be faced, and suggests some directions for future research on this important and current topic.},
  added-at = {2017-06-03T19:33:59.000+0200},
  author = {Shokouhi, Milad and Si, Luo},
  biburl = {https://www.bibsonomy.org/bibtex/279a85f88c166f809b3268f03d3ee4cb5/vngudivada},
  doi = {10.1561/1500000010},
  interhash = {6eb30a34603b7bce1bb97140a913e980},
  intrahash = {79a85f88c166f809b3268f03d3ee4cb5},
  issn = {1554-0669},
  journal = {Foundations and Trends in Information Retrieval},
  keywords = {FederatedSearch IR},
  number = 1,
  pages = {1-102},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Federated Search},
  url = {http://dx.doi.org/10.1561/1500000010},
  volume = 5,
  year = 2011
}

@article{keresztes2013testing,
  abstract = {The testing effect refers to the phenomenon that repeated retrieval of memories promotes better long-term retention than repeated study. To investigate the neural correlates of the testing effect, we used event-related functional magnetic resonance imaging methods while participants performed a cued recall task. Prior to the neuroimaging experiment, participants learned Swahili-German word pairs, then half of the word pairs were repeatedly studied, whereas the other half were repeatedly tested. For half of the participants, the neuroimaging experiment was performed immediately after the learning phase; a 1-week retention interval was inserted for the other half of the participants. We found that a large network of areas identified in a separate 2-back functional localizer scan were active during the final recall of the word pair associations. Importantly, the learning strategy (retest or restudy) of the word pairs determined the manner in which the retention interval affected the activations within this network. Recall of previously restudied memories was accompanied by reduced activation within this network at long retention intervals, but no reduction was observed for previously retested memories. We suggest that retrieval promotes learning via stabilizing cue-related activation patterns in a network of areas usually associated with cognitive and attentional control functions.},
  added-at = {2017-03-12T05:01:39.000+0100},
  author = {Keresztes, A and Kaiser, D and Kovacs, G and Racsmany, M},
  biburl = {https://www.bibsonomy.org/bibtex/27b39fbfdb71a57e62727618608910bba/vngudivada},
  interhash = {8ad288bac25dd42808b9c8d4d1933ef4},
  intrahash = {7b39fbfdb71a57e62727618608910bba},
  journal = {Cereb Cortex},
  keywords = {Learning Testing},
  month = nov,
  number = 11,
  pages = {3025 -- 35},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Testing promotes long-term learning via stabilizing activation patterns in a large network of brain areas},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/23796945},
  volume = 24,
  year = 2013
}

@incollection{pepper2009topic,
  abstract = {Topic Maps is an international standard technology for describing knowledge structures and using them to improve the findability of information. It is based on a formal model that subsumes those of traditional finding aids such as indexes, glossaries, and thesauri, and extends them to cater for the additional complexities of digital information. Topic Maps is increasingly used in enterprise information integration, knowledge management, e-learning, and digital libraries, and as the foundation for Web-based information delivery solutions. This entry provides a comprehensive treatment of the core concepts, as well as describing the background and current status of the standard and its relationship to traditional knowledge organization techniques.},
  added-at = {2017-04-29T01:32:01.000+0200},
  author = {Pepper, Steve},
  biburl = {https://www.bibsonomy.org/bibtex/286e68e6c55db4978ae88ec237d1062af/vngudivada},
  booktitle = {Encyclopedia of Library and Information Sciences, Third Edition},
  interhash = {24f5420fcfa4e187b553090848ff473e},
  intrahash = {86e68e6c55db4978ae88ec237d1062af},
  keywords = {TopicMap},
  month = dec,
  pages = {5247--5259},
  publisher = {{CRC} Press},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Topic Maps},
  url = {https://doi.org/10.1081%2Fe-elis3-120044331},
  year = 2009
}

@article{ding2017machine,
  abstract = {Big data validation and system verification are crucial for ensuring the quality of big data applications. However, a rigorous technique for such tasks is yet to emerge. During the past decade, we have developed a big data system called CMA for investigating the classification of biological cells based on cell morphology that is captured in diffraction images. CMA includes a group of scientific software tools, machine learning algorithms, and a large scale cell image repository. We have also developed a framework for rigorous validation of the massive scale image data and verification of both the software systems and machine learning algorithms. Different machine learning algorithms integrated with image processing techniques were used to automate the selection and validation of the massive scale image data in CMA. An experiment based technique guided by a feature selection algorithm was introduced in the framework to select optimal machine learning features. An iterative metamorphic testing approach is applied for testing the scientific software. Due to the non-testable characteristic of the scientific software, a machine learning approach is introduced for developing test oracles iteratively to ensure the adequacy of the test coverage criteria. Performance of the machine learning algorithms is evaluated with the stratified N-fold cross validation and confusion matrix. We describe the design of the proposed framework with CMA as the case study. The effectiveness of the framework is demonstrated through verifying and validating the data set, software systems and algorithms in CMA.},
  added-at = {2017-04-07T03:52:31.000+0200},
  author = {Ding, Junhua and Hu, Xin-Hua and Gudivada, Venkat},
  biburl = {https://www.bibsonomy.org/bibtex/24764597061d6b1b43e08d6b5d547b790/vngudivada},
  interhash = {530d74ee294cd7cd8d8ee272c0660b21},
  intrahash = {4764597061d6b1b43e08d6b5d547b790},
  journal = {{IEEE} Transactions on Big Data},
  keywords = {BigData DataQuality ImageData},
  pages = {1--1},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {A Machine Learning Based Framework for Verification and Validation of Massive Scale Image Data},
  url = {https://doi.org/10.1109%2Ftbdata.2017.2680460},
  year = 2017
}

@article{groth2016overcoming,
  abstract = {Students can struggle to understand and use terms that describe probabilities. Such struggles lead to difficulties comprehending classroom conversations. In this article, we describe some specific misunderstandings a group of students (ages 11–12) held in regard to vocabulary such as certain, likely and unlikely. We discuss our efforts to help the students use such terms appropriately. In particular, we show how engaging students in a game requiring the use of the terms helped them begin to develop their vocabulary more fully. We also explain how a probability ladder visual organizer helped students begin to organize their thinking about the meanings of terms relative to one another.},
  added-at = {2017-03-18T02:32:43.000+0100},
  author = {Groth, Randall E. and Butler, Jaime and Nelson, Delmar},
  biburl = {https://www.bibsonomy.org/bibtex/25c3b285289b4d760f0506756a5f4f6ba/vngudivada},
  interhash = {bf59599e26e1235c74c83593754bc59b},
  intrahash = {5c3b285289b4d760f0506756a5f4f6ba},
  journal = {Teaching Statistics},
  keywords = {Learning StatisticalLearning},
  number = 3,
  pages = {102--107},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Overcoming challenges in learning probability vocabulary},
  url = {http://dx.doi.org/10.1111/test.12109},
  volume = 38,
  year = 2016
}

@inproceedings{boldi2005,
  added-at = {2017-05-04T00:46:40.000+0200},
  author = {Boldi, Paolo and Vigna, Sebastiano},
  biburl = {https://www.bibsonomy.org/bibtex/21c22f8ac746b6629eb5dd0f7a375427a/vngudivada},
  booktitle = {The Fourteenth Text REtrieval Conference (TREC 2005) Proceedings},
  editor = {Voorhees, Ellen M. and Buckland, Lori P.},
  interhash = {289a4a5a6b81b36e4565c743c2b5d2d0},
  intrahash = {1c22f8ac746b6629eb5dd0f7a375427a},
  keywords = {MG4J SearchEngine},
  number = {SP 500-266},
  publisher = {NIST},
  series = {Special Publications},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {{M}{G}4{J} at {T}{R}{E}{C} 2005},
  year = 2005
}

@article{chen2000stationary,
  abstract = {This paper presents a variant of the popular beer game. We call the new game the stationary beer game, which models the material and information flows in a production-distribution channel serving a stationary market where the customer demands in different periods are independent and identically distributed. Different players, who all know the demand distribution, manage the different stages of the channel. Summarizing the initial experience with the stationary beer game, the paper provides compelling reasons why this game is an effective teaching tool.},
  added-at = {2017-03-18T14:01:19.000+0100},
  author = {Chen, Fangruo and Samroengraja, Rungson},
  biburl = {https://www.bibsonomy.org/bibtex/2e3c9880ba7c32d9c57adb3dea217d2e9/vngudivada},
  interhash = {71c477b18a371a8cd6295e7a6098c363},
  intrahash = {e3c9880ba7c32d9c57adb3dea217d2e9},
  journal = {Production and Operations Management},
  keywords = {Learning StatisticalLearning},
  number = 1,
  pages = {19--30},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {The Stationary Beer Game},
  url = {http://dx.doi.org/10.1111/j.1937-5956.2000.tb00320.x},
  volume = 9,
  year = 2000
}

@book{strang2016introduction,
  abstract = {Gilbert Strang's textbooks have changed the entire approach to learning linear algebra -- away from abstract vector spaces to specific examples of the four fundamental subspaces: the column space and nullspace of A and A'.
This new fifth edition has become more than a textbook for the basic linear algebra course. That is its first purpose and always will be. The new chapters about applications of the SVD, probability and statistics, and Principal Component Analysis in finance and genetics, make it also a textbook for a second course, plus a resource at work. Linear algebra has become central in modern applied mathematics. This book supports the value of understanding linear algebra.

Introduction to Linear Algebra, Fifth Edition includes challenge problems to complement the review problems that have been highly praised in previous editions. The basic course is followed by eight applications: differential equations in engineering, graphs and networks, statistics, Fourier methods and the FFT, linear programming, computer graphics, cryptography, Principal Component Analysis, and singular values.

Audience: Thousands of teachers in colleges and universities and now high schools are using this book, which truly explains this crucial subject. This text is for readers everywhere, with support from the websites and video lectures. Every chapter begins with a summary for efficient review.

Contents: Chap. 1: Introduction to Vectors; Chap. 2: Solving Linear Equations; Chap. 3: Vector Spaces and Subspaces; Chap. 4: Orthogonality; Chap. 5: Determinants; Chap. 6: Eigenvalues and Eigenvectors; Chap. 7: Singular Value Decomposition; Chap. 8: Linear Transformations; Chap. 9: Complex Vectors and Matrices; Chap. 10: Applications; Chap. 11: Numerical Linear Algebra; Chap. 12: Linear Algebra in Probability and Statistics; Matrix Factorizations; Index; Six Great Theorems.},
  added-at = {2017-06-02T13:59:01.000+0200},
  address = {Wellesley, MA},
  author = {Strang, Gilbert},
  biburl = {https://www.bibsonomy.org/bibtex/286d68d2f3d89890ed83ca9434eaa406f/vngudivada},
  interhash = {bb56ccb908d8281de1ce384ad88f4210},
  intrahash = {86d68d2f3d89890ed83ca9434eaa406f},
  keywords = {Book LinearAlgebra},
  publisher = {Wellesley-Cambridge Press},
  refid = {971435014},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Introduction to linear algebra},
  url = {http://math.mit.edu/~gs/linearalgebra/},
  year = 2016
}

@book{rojas2000neural,
  abstract = {Neural networks are a computing paradigm that is finding increasing attention among computer scientists. In this book, theoretical laws and models previously scattered in the literature are brought together into a general theory of artificial neural nets. Always with a view to biology and starting with the simplest nets, it is shown how the properties of models change when more general computing elements and net topologies are introduced. Each chapter contains examples, numerous illustrations, and a bibliography. The book is aimed at readers who seek an overview of the field or who wish to deepen their knowledge. It is suitable as a basis for university courses in neurocomputing.},
  added-at = {2017-03-18T17:10:58.000+0100},
  address = {New York, NY},
  author = {Rojas, Raúl},
  biburl = {https://www.bibsonomy.org/bibtex/2ed6d6971f3cf9bfb76e657c55e6a0107/vngudivada},
  interhash = {f3de651d8eabc8032d079486a3683327},
  intrahash = {ed6d6971f3cf9bfb76e657c55e6a0107},
  keywords = {Book NeuralNetwork},
  publisher = {Springer},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Neural networks: a systematic introduction},
  url = {https://page.mi.fu-berlin.de/rojas/neural/},
  year = 2000
}

@article{sarawagi2008information,
  abstract = {The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem.

This review is a survey of information extraction research of over two decades from these diverse communities. We create a taxonomy of the field along various dimensions derived from the nature of the extraction task, the techniques used for extraction, the variety of input resources exploited, and the type of output produced. We elaborate on rule-based and statistical methods for entity and relationship extraction. In each case we highlight the different kinds of models for capturing the diversity of clues driving the recognition process and the algorithms for training and efficiently deploying the models. We survey techniques for optimizing the various steps in an information extraction pipeline, adapting to dynamic data, integrating with existing entities and handling uncertainty in the extraction process.},
  added-at = {2017-05-07T20:38:41.000+0200},
  author = {Sarawagi, Sunita},
  biburl = {https://www.bibsonomy.org/bibtex/273e27c94c877e7b6f39b204e9eb42080/vngudivada},
  interhash = {afc767b7f9a7fef896a0672c1e8ff241},
  intrahash = {73e27c94c877e7b6f39b204e9eb42080},
  journal = {Foundations and Trends\textregistered in Databases},
  keywords = {IE InformationExtraction NLP},
  number = 3,
  pages = {261 -- 377},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Information Extraction},
  url = {http://dx.doi.org/10.1561/1900000003},
  volume = 1,
  year = 2008
}

@article{blei2012probabilistic,
  abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
  added-at = {2017-04-29T01:40:07.000+0200},
  address = {New York, NY},
  author = {Blei, David M.},
  biburl = {https://www.bibsonomy.org/bibtex/213ee50ad26ce2e026ca142b337f1ace7/vngudivada},
  interhash = {18e051101afab02daab5de5f6cf1643e},
  intrahash = {13ee50ad26ce2e026ca142b337f1ace7},
  journal = {Commun. ACM},
  keywords = {TopicModel},
  month = apr,
  number = 4,
  pages = {77--84},
  publisher = {ACM},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Probabilistic Topic Models},
  volume = 55,
  year = 2012
}

@inproceedings{zhai2012flexible,
  abstract = {Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for exploring document collections. Because of the increasing prevalence of large datasets, there is a need to improve the scalability of inference for LDA. In this paper, we introduce a novel and flexible large scale topic modeling package in MapReduce (Mr. LDA). As opposed to other techniques which use Gibbs sampling, our proposed framework uses variational inference, which easily fits into a distributed environment. More importantly, this variational implementation, unlike highly tuned and specialized implementations based on Gibbs sampling, is easily extensible. We demonstrate two extensions of the models possible with this scalable framework: informed priors to guide topic discovery and extracting topics from a multilingual corpus. We compare the scalability of Mr. LDA against Mahout, an existing large scale topic modeling package. Mr. LDA out-performs Mahout both in execution speed and held-out likelihood.},
  added-at = {2017-05-16T23:22:50.000+0200},
  address = {New York, NY},
  author = {Zhai, Ke and Boyd-Graber, Jordan and Asadi, Nima and Alkhouja, Mohamad L.},
  biburl = {https://www.bibsonomy.org/bibtex/26362c9cec4bda6c5dd9bee21170b6528/vngudivada},
  booktitle = {Proceedings of the 21st International Conference on World Wide Web},
  interhash = {8ae1446057c1c96ec48ff53258bac964},
  intrahash = {6362c9cec4bda6c5dd9bee21170b6528},
  keywords = {LDA MapReduce TopicModel},
  pages = {879--888},
  publisher = {ACM},
  series = {WWW '12},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Mr. LDA: A Flexible Large Scale Topic Modeling Package Using Variational Inference in MapReduce},
  url = {http://doi.acm.org/10.1145/2187836.2187955},
  year = 2012
}

@article{scutari2010learning,
  abstract = {bnlearn is an R package which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables. Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the snow package to improve their performance via parallel computing. Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use. Advanced plotting options are provided by the Rgraphviz package.},
  added-at = {2017-05-18T00:35:58.000+0200},
  author = {Scutari, Marco},
  biburl = {https://www.bibsonomy.org/bibtex/2229995b6ebb969375abcb81b6fe45965/vngudivada},
  doi = {10.18637/jss.v035.i03},
  interhash = {ee1a164155cab591ec9371045e35a762},
  intrahash = {229995b6ebb969375abcb81b6fe45965},
  journal = {Journal of Statistical Software},
  keywords = {BayesianMethods Book},
  number = 3,
  pages = {1--22},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Learning Bayesian Networks with the bnlearn {R} Package},
  volume = 35,
  year = 2010
}

@inbook{morandat2012evaluating,
  abstract = {R is a dynamic language for statistical computing that combines lazy functional features and object-oriented programming. This rather unlikely linguistic cocktail would probably never have been prepared by computer scientists, yet the language has become surprisingly popular. With millions of lines of R code available in repositories, we have an opportunity to evaluate the fundamental choices underlying the R language design. Using a combination of static and dynamic program analysis we assess the success of different language features.},
  added-at = {2017-05-04T00:01:29.000+0200},
  address = {Berlin, Heidelberg},
  author = {Morandat, Flor{\'e}al and Hill, Brandon and Osvald, Leo and Vitek, Jan},
  biburl = {https://www.bibsonomy.org/bibtex/295551e9b273257f71bf54160b44c1b77/vngudivada},
  booktitle = {ECOOP 2012 -- Object-Oriented Programming: 26th European Conference, Beijing, China, June 11-16, 2012. Proceedings},
  editor = {Noble, James},
  interhash = {8da2130ab6b831e3b4b51c020cad0c29},
  intrahash = {95551e9b273257f71bf54160b44c1b77},
  keywords = {ProgrammingLanguage R},
  pages = {104--131},
  publisher = {Springer Berlin Heidelberg},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Evaluating the Design of the R Language},
  url = {http://dx.doi.org/10.1007/978-3-642-31057-7_6},
  year = 2012
}

@book{christensen2011bayesian,
  abstract = {Features

Covers a large number of statistical models
Emphasizes the elicitation of reasonable prior information
Explores numerical approximations via simulation
Uses WinBUGS and R for computational problems
Reviews basic concepts of matrix algebra and probability
Includes numerous exercises and real-world examples throughout
Provides data, programming code, and other materials at www.stat.unm.edu/~fletcher
Summary

Emphasizing the use of WinBUGS and R to analyze real data, Bayesian Ideas and Data Analysis: An Introduction for Scientists and Statisticians presents statistical tools to address scientific questions. It highlights foundational issues in statistics, the importance of making accurate predictions, and the need for scientists and statisticians to collaborate in analyzing data. The WinBUGS code provided offers a convenient platform to model and analyze a wide range of data.

The first five chapters of the book contain core material that spans basic Bayesian ideas, calculations, and inference, including modeling one and two sample data from traditional sampling models. The text then covers Monte Carlo methods, such as Markov chain Monte Carlo (MCMC) simulation. After discussing linear structures in regression, it presents binomial regression, normal regression, analysis of variance, and Poisson regression, before extending these methods to handle correlated data. The authors also examine survival analysis and binary diagnostic testing. A complementary chapter on diagnostic testing for continuous outcomes is available on the book’s website. The last chapter on nonparametric inference explores density estimation and flexible regression modeling of mean functions.

The appropriate statistical analysis of data involves a collaborative effort between scientists and statisticians. Exemplifying this approach, Bayesian Ideas and Data Analysis focuses on the necessary tools and concepts for modeling and analyzing scientific data.

Data sets and codes are provided on a supplemental website.},
  added-at = {2017-05-17T23:14:31.000+0200},
  address = {Boca Raton, Florida},
  author = {Christensen, Ronald and Johnson, Wesley O. and Branscum, Adam and Hanson, Timothy E.},
  biburl = {https://www.bibsonomy.org/bibtex/205872033c3a9a7c4f2760aebb0f8aaba/vngudivada},
  description = {Book Webpage at Publisher Site:  http://www.ics.uci.edu/~wjohnson/BIDA/BIDABook.html},
  interhash = {02b4e13112f58b974fc8ba0332da26d6},
  intrahash = {05872033c3a9a7c4f2760aebb0f8aaba},
  keywords = {Book BayesianMethods},
  publisher = {Chapman \& Hall/CRC},
  refid = {814148386},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Bayesian ideas and data analysis: an introduction for scientists and statisticians},
  url = {http://bacbuc.hd.free.fr/WebDAV/data/Bouquins/Christensen%20-%20Bayesian%20Ideas%20and%20Data%20Analysis%20-%202011.pdf},
  year = 2011
}

@book{moser2012students,
  abstract = {This easy-to-read guide provides a concise introduction to the engineering background of modern communication systems, from mobile phones to data compression and storage. Background mathematics and specific engineering techniques are kept to a minimum so that only a basic knowledge of high-school mathematics is needed to understand the material covered. The authors begin with many practical applications in coding, including the repetition code, the Hamming code and the Huffman code. They then explain the corresponding information theory, from entropy and mutual information to channel capacity and the information transmission theorem. Finally, they provide insights into the connections between coding theory and other fields. Many worked examples are given throughout the book, using practical applications to illustrate theoretical definitions. Exercises are also included, enabling readers to double-check what they have learned and gain glimpses into more advanced topics, making this perfect for anyone who needs a quick introduction to the subject.},
  added-at = {2017-05-21T06:06:47.000+0200},
  address = {Boston, Massachusetts},
  author = {Moser, Stefan M. and Chen, Po-Ning},
  biburl = {https://www.bibsonomy.org/bibtex/2ce03f72c4000167a1de580f256f2a1b7/vngudivada},
  interhash = {09f0ab704ffc97af429d33667405536d},
  intrahash = {ce03f72c4000167a1de580f256f2a1b7},
  keywords = {Book InformationTheory},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {A Student's Guide to Coding and Information Theory},
  year = 2012
}

@article{sanderson2010collection,
  abstract = {Use of test collections and evaluation measures to assess the effectiveness of information retrieval systems has its origins in work dating back to the early 1950s. Across the nearly 60 years since that work started, use of test collections is a de facto standard of evaluation. This monograph surveys the research conducted and explains the methods and measures devised for evaluation of retrieval systems, including a detailed look at the use of statistical significance testing in retrieval experimentation. This monograph reviews more recent examinations of the validity of the test collection approach and evaluation measures as well as outlining trends in current research exploiting query logs and live labs. At its core, the modern-day test collection is little different from the structures that the pioneering researchers in the 1950s and 1960s conceived of. This tutorial and review shows that despite its age, this long-standing evaluation method is still a highly valued tool for retrieval research.

Use of test collections and evaluation measures to assess the effectiveness of information retrieval systems has its origins in work dating back to the early 1950s. Across the nearly 60 years since that work started, use of test collections is a de facto standard of evaluation. Test Collection Based Evaluation of Information Retrieval Systems surveys the research conducted and explains the methods and measures devised for evaluation of retrieval systems, including a detailed look at the use of statistical significance testing in retrieval experimentation. Test Collection Based Evaluation of Information Retrieval Systems reviews more recent examinations of the validity of the test collection approach and evaluation measures as well as outlining trends in current research exploiting query logs and live labs. At its core, the modern day test collection is little different from the structures that the pioneering researchers in the 1950s and '60s conceived of. Test Collection Based Evaluation of Information Retrieval Systems shows that despite its age, this long-standing evaluation method is still a highly valued tool for retrieval research.},
  added-at = {2017-06-03T19:31:06.000+0200},
  author = {Sanderson, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/22522138295e70bb73a6cbe41a812b1f3/vngudivada},
  doi = {10.1561/1500000009},
  interhash = {db02003a3b3a47c5e3df48b8e244ad57},
  intrahash = {2522138295e70bb73a6cbe41a812b1f3},
  journal = {Foundations and Trends{\textregistered} in Information Retrieval},
  keywords = {Evaluation IR},
  number = 4,
  pages = {247--375},
  publisher = {Now Publishers},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Test Collection Based Evaluation of Information Retrieval Systems},
  url = {https://doi.org/10.1561%2F1500000009},
  volume = 4,
  year = 2010
}

@inproceedings{madsen2005federated,
  abstract = {Federated identity management is sometimes criticized as exacerbating the problem of online identity theft, based as it is on the idea of connecting together previously separate islands of identity information. This paper explores this conjecture, and argues that, while such linkages do undeniably increase the potential scope of a successful theft of identity information, this risk is more than offset by the much greater value federated identity, in combination with strong authentication, offers in preventing such theft in the first place.},
  acmid = {1102500},
  added-at = {2017-05-20T21:29:07.000+0200},
  address = {New York, NY},
  author = {Madsen, Paul and Koga, Yuzo and Takahashi, Kenji},
  biburl = {https://www.bibsonomy.org/bibtex/21f1286b43720e23fa2e3352e7b38d0d0/vngudivada},
  booktitle = {Proceedings of the 2005 Workshop on Digital Identity Management},
  doi = {10.1145/1102486.1102500},
  interhash = {62bfb83a2a783c7effad4f7afe4217f0},
  intrahash = {1f1286b43720e23fa2e3352e7b38d0d0},
  keywords = {FederatedIdentityManagement Security},
  pages = {77--83},
  publisher = {ACM},
  series = {DIM '05},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Federated Identity Management for Protecting Users from ID Theft},
  url = {http://doi.acm.org/10.1145/1102486.1102500},
  year = 2005
}

@article{denobrega2017introducing,
  abstract = {A strategy to facilitate understanding of spatial randomness is described, using student activities developed in sequence: looking at spatial patterns, simulating approximate spatial randomness using a grid of equally-likely squares, using binomial probabilities for approximations and predictions and then comparing with given Poisson probabilities. Key questions are discussed with students on concepts needed to understand the approximate models and to generate predictions based on the approximations. The sequence is structured to be interactive to encourage student's interest and curiosity.},
  added-at = {2017-03-18T14:27:02.000+0100},
  author = {De Nóbrega, José Renato},
  biburl = {https://www.bibsonomy.org/bibtex/2644021bec45371c8c0dc523e8d82d119/vngudivada},
  interhash = {e8746584bcfe48c4aab2359f2a9421ee},
  intrahash = {644021bec45371c8c0dc523e8d82d119},
  journal = {Teaching Statistics},
  keywords = {Learning StatisticalLearning},
  number = 1,
  pages = {16--19},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Introducing perception and modelling of spatial randomness in classroom},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/test.12113/full},
  volume = 39,
  year = 2017
}

@inbook{heckerman2008tutorial,
  abstract = {A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with Bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.},
  added-at = {2017-06-02T00:33:44.000+0200},
  address = {Berlin, Heidelberg},
  author = {Heckerman, David},
  biburl = {https://www.bibsonomy.org/bibtex/20deafb60305c7123920789825af6f5da/vngudivada},
  booktitle = {Innovations in Bayesian Networks: Theory and Applications},
  doi = {10.1007/978-3-540-85066-3_3},
  editor = {Holmes, Dawn E. and Jain, Lakhmi C.},
  interhash = {e15c077964b9e7d79b843b82f3dcab8c},
  intrahash = {0deafb60305c7123920789825af6f5da},
  isbn = {978-3-540-85066-3},
  keywords = {BayesianMethods ML},
  pages = {33--82},
  publisher = {Springer},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {A Tutorial on Learning with Bayesian Networks},
  url = {http://dx.doi.org/10.1007/978-3-540-85066-3_3},
  year = 2008
}

@article{shim2005federated,
  abstract = {Most confidential, valuable resources on the World Wide Web are protected by some form of authentication technology. To access these resources, either via the public Internet or private intranets, users must verify their digital identity. This can range from a simple user-name-password combination to biometric data such as fingerprints to physical objects like hardware tokens and smart cards. Federated identity management would enable individuals to interact with various service providers or Web sites with trust relationships by signing in just once.},
  added-at = {2017-05-20T20:37:59.000+0200},
  author = {Shim, S.S.Y. and Bhalla, Geetanjali and Pendyala, Vishnu},
  biburl = {https://www.bibsonomy.org/bibtex/221d5d52f934ee41bcb984b62541dae76/vngudivada},
  doi = {10.1109/mc.2005.408},
  interhash = {36dc5c9899d0aad35dee9b1d707c0775},
  intrahash = {21d5d52f934ee41bcb984b62541dae76},
  journal = {Computer},
  keywords = {FederatedIdentityManagement Security},
  month = dec,
  number = 12,
  pages = {120--122},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Federated identity management},
  url = {https://doi.org/10.1109%2Fmc.2005.408},
  volume = 38,
  year = 2005
}

@inproceedings{wing2017embracing,
  abstract = {Sources of uncertainty abound. Noisy sensor data. Machine learning methods. Hardware and software failures. The physical world. Human behavior. In the past, computer science handled uncertainty by abstracting it away or avoiding it. In the future, instead, computer science needs to embrace uncertainty as a first-class entity. How do we represent uncertainty in our computational models? Probabilities. Thus, we need to make sure that every computer science student learns probability and statistics. Data science, where data drives discovery and decision-making in all fields of study, underscores the importance of having a command of probability and statistics. At the heart of data science is data analytics whose methods such as machine learning rely on probabilistic and statistical reasoning. And since data serve as the currency of any data analytics workflow, explicit representation of probability distributions can help us calculate the degrees of uncertainty throughout a flow. Programming and software engineering courses will need to elevate the status of such data flows to that given to algorithms, data structures, and modular design. In this talk I will discuss the implications of embracing uncertainty on undergraduate computer science curricula.},
  added-at = {2017-03-26T04:24:37.000+0200},
  address = {New York, NY},
  author = {Wing, Jeannette},
  biburl = {https://www.bibsonomy.org/bibtex/2f10de42a3b1042205b3a497f5edfb501/vngudivada},
  booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
  interhash = {2e141d39dea05e2c4e4ff33bcae3361c},
  intrahash = {f10de42a3b1042205b3a497f5edfb501},
  keywords = {Probability Statistics Uncertainty},
  pages = {7--7},
  publisher = {ACM},
  series = {SIGCSE '17},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Embracing Uncertainty},
  url = {http://doi.acm.org/10.1145/3017680.3025045},
  year = 2017
}

@article{macgillivray2017countering,
  added-at = {2017-03-18T00:30:51.000+0100},
  author = {MacGillivray, Helen and Hewson, Paul},
  biburl = {https://www.bibsonomy.org/bibtex/2d3e6ea3a57ec1f345df2d037fc4adf51/vngudivada},
  interhash = {7ea812f8cd01a98722f198788eddd9f9},
  intrahash = {d3e6ea3a57ec1f345df2d037fc4adf51},
  journal = {Teaching Statistics},
  keywords = {Learning StatisticsEducation},
  number = 1,
  pages = {36--38},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Countering default mode in Teaching Statistics},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/test.12122/full},
  volume = 39,
  year = 2017
}

@article{scutari2017bayesian,
  abstract = {It is well known in the literature that the problem of learning the structure of Bayesian networks is very hard to tackle: Its computational complexity is super-exponential in the number of nodes in the worst case and polynomial in most real-world scenarios.Efficient implementations of score-based structure learning benefit from past and current research in optimization theory, which can be adapted to the task by using the network score as the objective function to maximize. This is not true for approaches based on conditional independence tests, called constraint-based learning algorithms. The only optimization in widespread use, backtracking, leverages the symmetries implied by the definitions of neighborhood and Markov blanket. In this paper we illustrate how backtracking is implemented in recent versions of the bnlearn R package, and how it degrades the stability of Bayesian network structure learning for little gain in terms of speed. As an alternative, we describe a software architecture and framework that can be used to parallelize constraint-based structure learning algorithms (also implemented in bnlearn) and we demonstrate its performance using four reference networks and two real-world data sets from genetics and systems biology. We show that on modern multi-core or multiprocessor hardware parallel implementations are preferable over backtracking, which was developed when single-processor machines were the norm.},
  added-at = {2017-05-18T00:34:53.000+0200},
  author = {Scutari, Marco},
  biburl = {https://www.bibsonomy.org/bibtex/21b253c768aaf858b5a3f1cf0dea54bd6/vngudivada},
  doi = {10.18637/jss.v077.i02},
  interhash = {c766cd13f2abdf795a5b60fac624e9da},
  intrahash = {1b253c768aaf858b5a3f1cf0dea54bd6},
  journal = {Journal of Statistical Software},
  keywords = {BayesianMethods Book},
  number = 2,
  pages = {1--20},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Bayesian Network Constraint-Based Structure Learning Algorithms: Parallel and Optimized Implementations in the {bnlearn} {R} Package},
  volume = 77,
  year = 2017
}

@inproceedings{ide2012model,
  abstract = {This paper describes a comprehensive standard for resource description developed within ISO TC37 SC4). The standard is instantiated in a system of XML headers that accompany data and annotation documents represented using the the Linguistic Annotation Framework's Graph Annotation Format (GrAF) (Ide and Suderman, 2007; Ide and Suderman, Submitted). It provides mechanisms for describing the organization of the resource, documenting the conventions used in the resource, associating data and annotation documents, and defining and selecting defined portions of the resource and its annotations. It has been designed to accommodate the use of XML technologies for processing, including XPath, XSLT, and, by virtue of the system's linkage strategy, RDF/OWL, and to accommodate linkage to web-based ontologies and data category registries such as the OLiA ontologies (Chiarcos, 2012) and ISOCat (Marc Kemps-Snijders and Wright, 2008).},
  acmid = {2392757},
  added-at = {2017-06-02T19:02:27.000+0200},
  address = {Stroudsburg, PA, USA},
  author = {Ide, Nancy and Suderman, Keith},
  biburl = {https://www.bibsonomy.org/bibtex/268f1f07c44d5fe78307b96d77b54b51d/vngudivada},
  booktitle = {Proceedings of the Sixth Linguistic Annotation Workshop},
  interhash = {7055eedd8bc8a0987808aec6b2f79194},
  intrahash = {68f1f07c44d5fe78307b96d77b54b51d},
  keywords = {LingusticResource},
  location = {Jeju, Republic of Korea},
  numpages = {10},
  pages = {57--66},
  publisher = {Association for Computational Linguistics},
  series = {LAW VI '12},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {A Model for Linguistic Resource Description},
  url = {http://dl.acm.org/citation.cfm?id=2392747.2392757},
  year = 2012
}

@article{vandaele2016students,
  abstract = {Although constructivist theories have shown learning is accelerated by involvement and meaningful lecturer–student and student–student interaction, these ingredients are mostly absent from large attendance lectures. A number of studies have already focused on more active ways of learning in large lecture classrooms, most often by using student response systems or “clickers”. This field study wishes to extend the current knowledge base by providing an overview of how students and lecturers experience technology in large enrolment courses. An intervention introducing meaningful use of mobile technology in large attendance lectures was therefore set-up and different aspects were evaluated: interaction and involvement, pleasantness and need for future implementation of an intervention. Participants were 185 bachelor students of Applied Psychology and three lecturers. A mixed method design was used, combining an online questionnaire consisting of multiple choice questions using a 5-point Likert response scale and open ended questions, with focus group interviews. Focus groups with both students and lecturers provided additional data. Results showed that students experience increased involvement and interaction, that they found the didactical use pleasant and that they were convinced of the need for future use of mobile technology in daily education practice. Focus group interviews with students confirmed these findings under the condition that the used technology was integrated functionally in the lecture. The involved lecturers reported on positive effects and showed themselves to be favorable toward using handheld, mobile technology in large attendance lectures to boost interaction and involvement, even though they admitted to feeling unease about surrendering a level of control over the pedagogic setting.},
  added-at = {2017-03-17T23:55:08.000+0100},
  author = {Van Daele, Tom and Frijns, Carolien and Lievens, Jeroen},
  biburl = {https://www.bibsonomy.org/bibtex/25072b63ddac16050c445880fbd97f7a7/vngudivada},
  interhash = {9577e28dc6d7de025b2ee1f3457fa036},
  intrahash = {5072b63ddac16050c445880fbd97f7a7},
  journal = {British Journal of Educational Technology},
  keywords = {HandheldTechnology Learning},
  pages = {n/a--n/a},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {How do students and lecturers experience the interactive use of handheld technology in large enrolment courses?},
  url = {/brokenurl#DOI: 10.1111/bjet.12500},
  year = 2016
}

@article{zhai2008statistical,
  abstract = {Statistical language models have recently been successfully applied to many information retrieval problems. A great deal of recent work has shown that statistical language models not only lead to superior empirical performance, but also facilitate parameter tuning and open up possibilities for modeling nontraditional retrieval problems. In general, statistical language models provide a principled way of modeling various kinds of retrieval problems. The purpose of this survey is to systematically and critically review the existing work in applying statistical language models to information retrieval, summarize their contributions, and point out outstanding challenges.

Statistical Language Models for Information Retrieval systematically and critically reviews the existing work in applying statistical language models to information retrieval, summarizes their contributions, and points out outstanding challenges. Statistical language models have recently been successfully applied to many information retrieval problems. A great deal of recent work has shown that statistical language models not only lead to superior empirical performance, but also facilitate parameter tuning and open up possibilities for modeling non-traditional retrieval problems. In general, statistical language models provide a principled way of modeling various kinds of retrieval problems. Statistical Language Models for Information Retrieval reviews the development of this language modeling approach. It surveys a wide range of retrieval models based on language modeling and attempts to make connections between this new family of models and traditional retrieval models. It summarizes the progress made so far in these models and point out remaining challenges to be solved to further increase their impact. Statistical Language Models for Information Retrieval is written for readers who already have some basic knowledge about information retrieval. Some knowledge of probability and statistics such as the maximum likelihood estimator is helpful, but not a prerequisite to understanding the high-level discussion.},
  added-at = {2017-06-03T19:23:02.000+0200},
  author = {Zhai, ChengXiang},
  biburl = {https://www.bibsonomy.org/bibtex/27608c9e3bd19eab863597844583fafdc/vngudivada},
  doi = {10.1561/1500000008},
  interhash = {ca770afde8a4b6310624493752ca33ff},
  intrahash = {7608c9e3bd19eab863597844583fafdc},
  issn = {1554-0669},
  journal = {Foundations and Trends\textregistered in Information Retrieval},
  keywords = {IR LanguageModel Survey},
  number = 3,
  pages = {137 -- 213},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Statistical Language Models for Information Retrieval A Critical Review},
  volume = 2,
  year = 2008
}

@article{jung2004information,
  abstract = {Text data present in images and video contain useful information for automatic annotation, indexing, and structuring of images. Extraction of this information involves detection, localization, tracking, extraction, enhancement, and recognition of the text from a given image. However, variations of text due to differences in size, style, orientation, and alignment, as well as low image contrast and complex background make the problem of automatic text extraction extremely challenging. While comprehensive surveys of related problems such as face detection, document analysis, and image \& video indexing can be found, the problem of text information extraction is not well surveyed. A large number of techniques have been proposed to address this problem, and the purpose of this paper is to classify and review these algorithms, discuss benchmark data and performance evaluation, and to point out promising directions for future research.},
  added-at = {2017-05-07T21:44:10.000+0200},
  author = {Jung, Keechul and Kim, Kwang In and Jain, Anil K.},
  biburl = {https://www.bibsonomy.org/bibtex/2892702da30e850e0edc5382bd6494248/vngudivada},
  interhash = {d6f412367528b2ee4900f7196fb69400},
  intrahash = {892702da30e850e0edc5382bd6494248},
  journal = {Pattern Recognition },
  keywords = {IE OCR},
  month = jan,
  number = 5,
  pages = {977 - 997},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Text information extraction in images and video: a survey },
  url = {https://doi.org/10.1016/j.patcog.2003.10.012},
  volume = 37,
  year = 2004
}

@techreport{bienkowski2012enhancing,
  abstract = {In data mining and data analytics, tools and techniques once confined to research laboratories are being adopted by forward-looking industries to generate business intelligence for improving decision making. Higher education institutions are beginning to use analytics for improving the services they provide and for increasing student grades and retention. The U.S. Department of Education's National Education Technology Plan, as one part of its model for 21st-century learning powered by technology, envisions ways of using data from online learning systems to improve instruction. With analytics and data mining experiments in education starting to proliferate, sorting out fact from fiction and identifying research possibilitiesand practical applications are not easy. This issue brief is intended to help policymakers and administrators understand how analytics and data mining have been-and can be-applied for educational improvement. At present, educational data mining tends to focus on developing new tools for discovering patterns in data. These patterns are generally about the microconcepts involved in learning: one-digit multiplication, subtraction with carries, and so on. Learning analytics-at least as it is currently contrasted with data mining-focuses on applying tools and techniques at larger scales, such as in courses and at schools and postsecondary institutions. But both disciplines work with patterns and prediction: If we can discern the pattern in the data and make sense of what is happening, we can predict what should come next and take the appropriate action. Educational data mining and learning analytics are used to research and build models in several areas that can influence online learning systems. One area is user modeling, which encompasses what a learner knows, what a learner's behavior and motivation are, what the user experience is like, and how satisfied users are with online learning. At the simplest level, analytics can detect when a student in an online course is going astray and nudge him or her on to a course correction. At the most complex, they hold promise of detecting boredom from patterns of key clicks and redirecting the student's attention. Because these data are gathered in real time, there is a real possibility of continuous improvement via multiple feedback loops that operate at different time scales-immediate to the student for the next problem, daily to the teacher for the next day's teaching, monthly to the principal for judging progress, and annually to the district and state administrators for overall school improvement. The same kinds of data that inform user or learner models can be used to profile users. Profiling as used here means grouping similar users into categories using salient characteristics. These categories then can be used to offer experiences to groups of users or to make recommendations to the users and adaptations to how a system performs. User modeling and profiling are suggestive of real-time adaptations. In contrast, some applications of data mining and analytics are for more experimental purposes. Domain modeling is largely experimental with the goal of understanding how to present a topic and at what level of detail. The study of learning components and instructional principles also uses experimentation to understand what is effective at promoting learning. These examples suggest that the actions from data mining and analytics are always automatic, but that is less often the case. Visual data analyticsclosely involve humans to help make sense of data, from initial pattern detection and model building to sophisticated data dashboards that present data in a way that humans can act upon. K-12 schools and school districts are starting to adopt such institution-level analyses for detecting areas for instructional improvement, setting policies, and measuring results. Making visible students' learning and assessment activities opens up the possibility for students to develop skills in monitoring their own learning and to see directly how their effort improves their success. Teachers gain views into students' performance that help them adapt their teaching or initiate tutoring, tailored assignments, and the like. Robust applications of educational data mining and learning analytics techniques come with costs and challenges. Information technology (IT) departments will understand the costs associated with collecting and storing logged data, while algorithm developers will recognize the computational costs these techniques still require. Another technical challenge is that educational data systems are not interoperable, so bringing together administrative data and classroom-level data remains a challenge. Yet combining these data can give algorithms better predictive power. Combining data about student performance-online tracking, standardized tests, teachergenerated tests-to form one simplified picture of what a student knows can be difficult and must meet acceptable standards for validity. It also requires careful attention to student and teacher privacy and the ethical obligations associated with knowing and acting on student data. Educational data mining and learning analytics have the potential to make visible data that have heretofore gone unseen, unnoticed, and therefore unactionable. To help further the fields and gain value from their practical applications, the recommendations are that educators and administrators: • Develop a culture of using data for making instructional decisions. • Involve IT departments in planning for data collection and use. • Be smart data consumers who ask critical questions about commercial offerings and create demand for the most useful features and uses. • Start with focused areas where data will help, show success, and then expand to new areas. • Communicate with students and parents about where data come from and how the data are used. • Help align state policies with technical requirements for online learning systems.Researchers and software developers are encouraged to: • Conduct research on usability and effectiveness of data displays. • Help instructors be more effective in the classroom with more realtime and data-based decision support tools, including recommendation services. • Continue to research methods for using identified student information where it will help most, anonymizing data when required, and understanding how to align data across different systems. • Understand how to repurpose predictive models developed in one context to another. A final recommendation is to create and continue strong collaboration across research, commercial, and educational sectors. Commercial companies operate on fast development cycles and can produce data useful for research. Districts and schools want properly vetted learning environments. Effective partnerships can help these organizations codesign the best tools.},
  added-at = {2017-04-09T23:47:20.000+0200},
  author = {Bienkowski, Marie and Feng, Mingyu and Means, Barbara},
  biburl = {https://www.bibsonomy.org/bibtex/251cf6e61e3724aff4f7a8492fa609c96/vngudivada},
  institution = {Office of Educational Technology, U.S. Department of Education},
  interhash = {cde6e3f315968a3cc008e2794377f5e3},
  intrahash = {51cf6e61e3724aff4f7a8492fa609c96},
  keywords = {EDM LearningAnalytics},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Enhancing Teaching and Learning Through Educational Data Mining and Learning Analytics: An Issue Brief},
  year = 2012
}

@techreport{agarwal2013retrieval,
  added-at = {2017-03-19T21:20:23.000+0100},
  author = {Agarwal, Pooja K. and Henry L. Roediger, III and McDaniel, Mark A. and McDermott, Kathleen B.},
  biburl = {https://www.bibsonomy.org/bibtex/2a75fa96552dd78e5d32d6d45ea40230a/vngudivada},
  institution = {Washington University in St. Louis},
  interhash = {babdf51efab67f73c847bf61710fb9aa},
  intrahash = {a75fa96552dd78e5d32d6d45ea40230a},
  keywords = {Learning RetrievalPractice},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {How to Use Retrieval Practice to Improve Learning},
  year = 2013
}

@article{levine2011teaching,
  abstract = {Introductory business statistics students often receive little guidance on how to apply the methods they learn to further business objectives they may one day face. And those students may fail to see the continuity among the topics taught in an introductory course if they learn those methods outside a context that provides a unifying framework. The DCOVA problem-solving framework that presents discrete steps to define, collect, organize, visualize, and analyze data addresses these concerns while helping to enhance the perceived value of taking statistics courses.},
  added-at = {2017-03-18T02:24:38.000+0100},
  author = {Levine, David M. and Stephan, David F.},
  biburl = {https://www.bibsonomy.org/bibtex/2bde87d19854551dd3af194f7e3ea15a9/vngudivada},
  interhash = {a00317f1a4437b02a0ac04314799e5a2},
  intrahash = {bde87d19854551dd3af194f7e3ea15a9},
  journal = {Decision Sciences Journal of Innovative Education},
  keywords = {Learning StatisticsEducation},
  number = 3,
  pages = {395--400},
  publisher = {Blackwell Publishing Inc},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Teaching Introductory Business Statistics Using the DCOVA Framework},
  url = {http://dx.doi.org/10.1111/j.1540-4609.2011.00316.x},
  volume = 9,
  year = 2011
}

@article{murnieks2016drawn,
  abstract = {Extant research affirms that angel investors seek passionate entrepreneurs but questions surround whether there is value in passion itself, or if it is instead used as a marker for other important characteristics like tenacity and inspirational leadership. Employing both a qualitative and quantitative study, we find that angels value passion in addition to tenacity, as well as both together, when evaluating entrepreneurs for investment. We also find that the entrepreneurial experience of angels positively moderates the value provided by passion and tenacity.},
  added-at = {2017-07-23T23:37:12.000+0200},
  author = {Murnieks, Charles Y. and Cardon, Melissa S. and Sudek, Richard and White, T. Daniel and Brooks, Wade T.},
  biburl = {https://www.bibsonomy.org/bibtex/2e2fca280a844e90afa8a2c8bd379a2a4/vngudivada},
  doi = {10.1016/j.jbusvent.2016.05.002},
  interhash = {657ef4d39604edb2553d206847a788c6},
  intrahash = {e2fca280a844e90afa8a2c8bd379a2a4},
  journal = {Journal of Business Venturing},
  keywords = {Inspiration Passion STEM},
  month = jul,
  number = 4,
  pages = {468--484},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Drawn to the fire: The role of passion, tenacity and inspirational leadership in angel investing},
  url = {https://doi.org/10.1016%2Fj.jbusvent.2016.05.002},
  volume = 31,
  year = 2016
}

@book{hoyt2014health,
  abstract = {The lead editor and author Robert Hoyt MD FACP is a practicing internal medicine physician who has taught Health Informatics for the past decade and conducted clinical research. He believed that Health Informatics textbooks should be exciting and very current so he opted to put together a strong team of contributors and use publish-on-demand publishing to be sure each edition is published rapidly. He would not have been able to do this without the help of his physician wife Ann Yoshihashi MD.

Health Informatics (HI) focuses on the application of information technology (IT) in healthcare to improve individual and population health, education and research. The goal of the textbook is to stimulate and educate healthcare and IT professionals and students about the key topics in this rapidly changing field. This extensively updated sixth edition reflects the changes in technology, policies and innovations that have occurred recently. Topics include HI overview, electronic health records, healthcare data analytics, health information exchange, architecture of information systems, evidence based medicine, consumer health informatics, HI ethics, quality improvement strategies and more. The 22 chapters feature learning objectives, case studies, recommended reading, future trends, key points, conclusions and over 1800 references. It is available as a paperback, eBook (PDF) and Kindle. Free Online Resources are available on the textbook companion website informaticseducation.org and an instructor's manual and slide libraries are available for instructors.},
  added-at = {2017-06-26T18:46:04.000+0200},
  address = {Online},
  biburl = {https://www.bibsonomy.org/bibtex/2a08fc00b4fe3a3d993e4301591f02c6b/vngudivada},
  edition = {Sixth},
  editor = {Hoyt, Robert E. and Yoshihashi, Ann K.},
  interhash = {6b903b8fe8bbec1e35447ecd4ddb5fda},
  intrahash = {a08fc00b4fe3a3d993e4301591f02c6b},
  isbn = {978-1304791108},
  keywords = {Book Healthcare},
  publisher = {lulu.com},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Health Informatics: Practical Guide for Healthcare and Information Technology Professionals},
  year = 2014
}

@article{berk2014inspiring,
  abstract = {The most effective ways to promote learning and inspire careers related to science, technology, engineering, and mathematics (STEM) remain elusive. To address this gap, we reviewed the literature and designed and implemented a high-fidelity, medical simulation-based Harvard Medical School MEDscience course, which was integrated into high school science classes through collaboration between medical school and K{\textendash}12 faculty. The design was based largely on the literature on concepts and mechanisms of self-efficacy. A structured telephone survey was conducted with 30 program alumni from the inaugural school who were no longer in high school. Near-term effects, enduring effects, contextual considerations, and diffusion and dissemination were queried. Students reported high incoming attitudes toward STEM education and careers, and these attitudes showed before versus after gains (P \&lt; .05). Students in this modest sample overwhelmingly attributed elevated and enduring levels of impact on their interest and confidence in pursuing a science or healthcare-related career to the program. Additionally, 63\% subsequently took additional science or health courses, 73\% participated in a job or educational experience that was science related during high school, and 97\% went on to college. Four of every five program graduates cited a health-related college major, and 83\% offered their strongest recommendation of the program to others. Further study and evaluation of simulation-based experiences that capitalize on informal, naturalistic learning and promote self-efficacy are warranted.},
  added-at = {2017-07-23T20:25:34.000+0200},
  author = {Berk, Louis J. and Muret-Wagstaff, Sharon L. and Goyal, Riya and Joyal, Julie A. and Gordon, James A. and Faux, Russell and Oriol, Nancy E.},
  biburl = {https://www.bibsonomy.org/bibtex/25826c0c386e1cd304fc9b570a2b683b2/vngudivada},
  doi = {10.1152/advan.00143.2013},
  interhash = {f1d8ce59b05f5c23d1eb16fdcfa59a03},
  intrahash = {5826c0c386e1cd304fc9b570a2b683b2},
  issn = {1043-4046},
  journal = {Advances in Physiology Education},
  keywords = {Healthcare STEM Simulation},
  number = 3,
  pages = {210--215},
  publisher = {American Physiological Society},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Inspiring careers in STEM and healthcare fields through medical simulation embedded in high school science education},
  volume = 38,
  year = 2014
}

@article{lecun2015learning,
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  added-at = {2017-06-03T21:12:17.000+0200},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/228b287c654f3bb95e864f749abd5cf5a/vngudivada},
  interhash = {6e8511bc64ba3e808ebf330db96a4ea5},
  intrahash = {28b287c654f3bb95e864f749abd5cf5a},
  journal = {Nature},
  keywords = {DeepLearning Survey Tutorial},
  month = may,
  number = 7553,
  pages = {436--444},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Deep learning},
  url = {http://dx.doi.org/10.1038/nature14539},
  volume = 521,
  year = 2015
}

@article{stets2017science,
  abstract = {The initiative to increase the number of students in STEM disciplines and train them for a science-related job is a current national focus. Using longitudinal panel data from a national study that followed underrepresented college students in STEM fields, we investigate the neglected role that social psychological processes play in influencing science activity among the young. We study the impact of identity processes related to being a science student on entering a science occupation. More broadly, we examine whether an identity formulated in one institutional setting (education) has effects that persist to another institutional setting (the economy). We find that the science identity positively impacts the likelihood of entering a science occupation. It also serves as a mediator for other factors that are related to educational success. This provides insight into how an identity can guide behavior to move persons into structural positions across institutional domains.},
  added-at = {2017-07-24T01:18:45.000+0200},
  author = {Stets, Jan E. and Brenner, Philip S. and Burke, Peter J. and Serpe, Richard T.},
  biburl = {https://www.bibsonomy.org/bibtex/21057abb56e79ac93bd4d57fae5bffa7a/vngudivada},
  doi = {10.1016/j.ssresearch.2016.10.016},
  interhash = {31da31f600f0886ac82a2ba30b48f4f0},
  intrahash = {1057abb56e79ac93bd4d57fae5bffa7a},
  journal = {Social Science Research},
  keywords = {STEM ScienceIdentity},
  month = may,
  pages = {1--14},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {The science identity and entering a science occupation},
  url = {https://doi.org/10.1016%2Fj.ssresearch.2016.10.016},
  volume = 64,
  year = 2017
}

@article{balog2012expertise,
  abstract = {People have looked for experts since before the advent of computers. With advances in information retrieval technology and the large-scale availability of digital traces of knowledge-related activities, computer systems that can fully automate the process of locating expertise have become a reality. The past decade has witnessed tremendous interest, and a wealth of results, in expertise retrieval as an emerging subdiscipline in information retrieval. This survey highlights advances in models and algorithms relevant to this field. We draw connections among methods proposed in the literature and summarize them in five groups of basic approaches. These serve as the building blocks for more advanced models that arise when we consider a range of content-based factors that may impact the strength of association between a topic and a person. We also discuss practical aspects of building an expert search system and present applications of the technology in other domains, such as blog distillation and entity retrieval. The limitations of current approaches are also pointed out. We end our survey with a set of conjectures on what the future may hold for expertise retrieval research.},
  added-at = {2017-06-03T21:19:46.000+0200},
  author = {Balog, Krisztian and Fang, Yi and de Rijke, Maarten and Serdyukov, Pavel and Si, Luo},
  biburl = {https://www.bibsonomy.org/bibtex/2637ef0b52405f3067371c83b95913896/vngudivada},
  doi = {10.1561/1500000024},
  interhash = {3e5606ad82b612fc456f09b6629ec127},
  intrahash = {637ef0b52405f3067371c83b95913896},
  journal = {Foundations and Trends in Information Retrieval},
  keywords = {ExpertiseRetrieval IR},
  number = {2–3},
  pages = {127-256},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Expertise Retrieval},
  url = {http://dx.doi.org/10.1561/1500000024},
  volume = 6,
  year = 2012
}

@article{deng2014learning,
  abstract = {This book is aimed to provide an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria: 1) expertise or knowledge of the authors; 2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and 3) the application areas that have the potential to be impacted significantly by deep learning and that have gained concentrated research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.

In Chapter 1, we provide the background of deep learning, as intrinsically connected to the use of multiple layers of nonlinear transformations to derive features from the sensory signals such as speech and visual images. In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. In Chapter 2, a brief historical account of deep learning is presented. In particular, selected chronological development of speech recognition is used to illustrate the recent impact of deep learning that has become a dominant technology in speech recognition industry within only a few years since the start of a collaboration between academic and industrial researchers in applying deep learning to speech recognition. In Chapter 3, a three-way classification scheme for a large body of work in deep learning is developed. We classify a growing number of deep learning techniques into unsupervised, supervised, and hybrid categories, and present qualitative descriptions and a literature survey for each category. From Chapter 4 to Chapter 6, we discuss in detail three popular deep networks and related learning methods, one in each category. Chapter 4 is devoted to deep autoencoders as a prominent example of the unsupervised deep learning techniques. Chapter 5 gives a major example in the hybrid deep network category, which is the discriminative feed-forward neural network for supervised learning with many layers initialized using layer-by-layer generative, unsupervised pre-training. In Chapter 6, deep stacking networks and several of the variants are discussed in detail, which exemplify the discriminative or supervised deep learning techniques in the three-way categorization scheme.

In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing and of applied artificial intelligence. In Chapter 7, we review the applications of deep learning to speech and audio processing, with emphasis on speech recognition organized according to several prominent themes. In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing. Chapter 9 is devoted to selected applications of deep learning to information retrieval including Web search. In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. Selected applications of deep learning to multi-modal processing and multi-task learning are reviewed in Chapter 11. Finally, an epilogue is given in Chapter 12 to summarize what we presented in earlier chapters and to discuss future challenges and directions.This book is aimed to provide an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria: 1) expertise or knowledge of the authors; 2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and 3) the application areas that have the potential to be impacted significantly by deep learning and that have gained concentrated research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.

In Chapter 1, we provide the background of deep learning, as intrinsically connected to the use of multiple layers of nonlinear transformations to derive features from the sensory signals such as speech and visual images. In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. In Chapter 2, a brief historical account of deep learning is presented. In particular, selected chronological development of speech recognition is used to illustrate the recent impact of deep learning that has become a dominant technology in speech recognition industry within only a few years since the start of a collaboration between academic and industrial researchers in applying deep learning to speech recognition. In Chapter 3, a three-way classification scheme for a large body of work in deep learning is developed. We classify a growing number of deep learning techniques into unsupervised, supervised, and hybrid categories, and present qualitative descriptions and a literature survey for each category. From Chapter 4 to Chapter 6, we discuss in detail three popular deep networks and related learning methods, one in each category. Chapter 4 is devoted to deep autoencoders as a prominent example of the unsupervised deep learning techniques. Chapter 5 gives a major example in the hybrid deep network category, which is the discriminative feed-forward neural network for supervised learning with many layers initialized using layer-by-layer generative, unsupervised pre-training. In Chapter 6, deep stacking networks and several of the variants are discussed in detail, which exemplify the discriminative or supervised deep learning techniques in the three-way categorization scheme.

In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing and of applied artificial intelligence. In Chapter 7, we review the applications of deep learning to speech and audio processing, with emphasis on speech recognition organized according to several prominent themes. In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing. Chapter 9 is devoted to selected applications of deep learning to information retrieval including Web search. In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. Selected applications of deep learning to multi-modal processing and multi-task learning are reviewed in Chapter 11. Finally, an epilogue is given in Chapter 12 to summarize what we presented in earlier chapters and to discuss future challenges and directions.},
  added-at = {2017-06-03T21:09:17.000+0200},
  author = {Deng, Li and Yu, Dong},
  biburl = {https://www.bibsonomy.org/bibtex/211c5b9e640c434080e19270e7a6b345c/vngudivada},
  doi = {10.1561/2000000039},
  interhash = {8c4ded35a5ad2bc7f0f90df6262fc1d1},
  intrahash = {11c5b9e640c434080e19270e7a6b345c},
  issn = {1932-8346},
  journal = {Foundations and Trends® in Signal Processing},
  keywords = {DeepLearning},
  number = {3–4},
  pages = {197-387},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Deep Learning: Methods and Applications},
  url = {http://dx.doi.org/10.1561/2000000039},
  volume = 7,
  year = 2014
}

@book{topol2016patient,
  abstract = {A trip to the doctor is almost a guarantee of misery. You'll make an appointment months in advance. You'll probably wait for several hours until you hear "the doctor will see you now"-but only for fifteen minutes! Then you'll wait even longer for lab tests, the results of which you'll likely never see, unless they indicate further (and more invasive) tests, most of which will probably prove unnecessary (much like physicals themselves). And your bill will be astronomical.

In The Patient Will See You Now, Eric Topol, one of the nation's top physicians, shows why medicine does not have to be that way. Instead, you could use your smartphone to get rapid test results from one drop of blood, monitor your vital signs both day and night, and use an artificially intelligent algorithm to receive a diagnosis without having to see a doctor, all at a small fraction of the cost imposed by our modern healthcare system.

The change is powered by what Topol calls medicine's "Gutenberg moment." Much as the printing press took learning out of the hands of a priestly class, the mobile internet is doing the same for medicine, giving us unprecedented control over our healthcare. With smartphones in hand, we are no longer beholden to an impersonal and paternalistic system in which "doctor knows best." Medicine has been digitized, Topol argues; now it will be democratized. Computers will replace physicians for many diagnostic tasks, citizen science will give rise to citizen medicine, and enormous data sets will give us new means to attack conditions that have long been incurable. Massive, open, online medicine, where diagnostics are done by Facebook-like comparisons of medical profiles, will enable real-time, real-world research on massive populations. There's no doubt the path forward will be complicated: the medical establishment will resist these changes, and digitized medicine inevitably raises serious issues surrounding privacy. Nevertheless, the result-better, cheaper, and more human health care-will be worth it.

Provocative and engrossing, The Patient Will See You Now is essential reading for anyone who thinks they deserve better health care. That is, for all of us.},
  added-at = {2017-06-26T18:37:04.000+0200},
  address = {New York, N.Y},
  author = {Topol, Eric J.},
  biburl = {https://www.bibsonomy.org/bibtex/20989e6ff1c24606b0c8a76af55865d5f/vngudivada},
  interhash = {c2f2a9de7ac603ec6dfd070b0d61be74},
  intrahash = {0989e6ff1c24606b0c8a76af55865d5f},
  isbn = {978-0465054749},
  keywords = {Book Healthcare},
  publisher = {Basic Books},
  refid = {980900095},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {The patient will see you now the future of medicine is in your hands},
  year = 2016
}

@article{miller2011learning,
  abstract = {The power of a web-based forensic science game to teach content and motivate STEM careers was tested among secondary students. More than 700 secondary school students were exposed to one of the three web-based forensic cases for approximately 60 min. Gain scores from pre-test to a delayed post-test indicated significant gains in content knowledge. In addition, the game’s usability ratings were a strong predictor of learning. A positive relationship between role-play experience and science career motivation was observed, which suggests a role for authentic virtual experiences in inspiring students to consider STEM careers.},
  added-at = {2017-07-23T23:19:09.000+0200},
  author = {Miller, Leslie M. and Chang, Ching-I. and Wang, Shu and Beier, Margaret E. and Klisch, Yvonne},
  biburl = {https://www.bibsonomy.org/bibtex/20de151c3a095b3567fc0534923889498/vngudivada},
  doi = {10.1016/j.compedu.2011.01.016},
  interhash = {a5c4e12fc50fd13270724a5d44cf511a},
  intrahash = {0de151c3a095b3567fc0534923889498},
  journal = {Computers {\&} Education},
  keywords = {HighSchool Pedagogy STEM},
  month = aug,
  number = 1,
  pages = {1425--1433},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Learning and motivational impacts of a multimedia science game},
  url = {https://doi.org/10.1016%2Fj.compedu.2011.01.016},
  volume = 57,
  year = 2011
}

@book{stirzaker1999probability,
  abstract = {This is a simple and concise introduction to probability theory. Self-contained and readily accessible, it is written in an informal tutorial style with concepts and techniques defined and developed as necessary. After an elementary discussion of chance, the central and crucial rules and ideas of probability including independence and conditioning are set out. Examples, demonstrations, and exercises are used throughout to explore the ways in which probability is motivated by, and applied to, real life problems in science, medicine, gaming and other subjects of interest. This book is suitable for students taking introductory courses in probability and will provide a solid foundation for more advanced courses in probability and statistics. It would also be a valuable reference to those needing a working knowledge of probability theory and will appeal to anyone interested in this endlessly fascinating and entertaining subject.},
  added-at = {2017-06-09T19:17:21.000+0200},
  author = {Stirzaker, David},
  biburl = {https://www.bibsonomy.org/bibtex/2e7496f58809714ae57f7965726826627/vngudivada},
  interhash = {e5bb06309d3b05400f1e866489d9b862},
  intrahash = {e7496f58809714ae57f7965726826627},
  keywords = {Book Probability},
  publisher = {Cambridge university Press},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Probability and Random Variables: A Beginner's Guide},
  year = 1999
}

@article{hossain2012motivate,
  abstract = {STEM (science, technology, engineering and mathematics) has been a powerful engine of prosperity in the US since World War II. Currently, American students' performances and enthusiasm in STEM education are inadequate for the US to maintain its leadership in STEM professions unless the government takes more actions to motivate a new generation of US students towards STEM careers. Despite of coherent actions taken by the government and various institutions, the US cannot ensure the production of a sufficient number of experts in STEM fields to meet its national and global needs. The current situation is that the US is largely dependent on the foreign-born STEM workforce. This paper starts with a deeper look at the participation rate of American students in STEM careers and the basis of career choices by the US students. The discussion is driven by barriers and misconceptions about STEM education. It concludes with recommendations for how to motivate more US students to pursue STEM careers. },
  added-at = {2017-08-20T18:59:15.000+0200},
  author = {Hossain, Md. Mokter and G. Robinson, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2f623e1e2d50b54f414d269f3b7376ae5/vngudivada},
  interhash = {fe9f44d64099207069238d0b978cf461},
  intrahash = {f623e1e2d50b54f414d269f3b7376ae5},
  journal = { US-China Education Review},
  keywords = {sys:relevantfor:ecu-cc-research Motivation STEM},
  pages = {442 - 451},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {How to Motivate US Students to Pursue STEM (Science, Technology, Engineering and Mathematics) Careers},
  url = {https://eric.ed.gov/?id=ED533548},
  volume = {A 4},
  year = 2012
}

@book{knaflic2015storytelling,
  abstract = {Storytelling with Data teaches you the fundamentals of data visualization and how to communicate effectively with data. You'll discover the power of storytelling and the way to make data a pivotal point in your story. The lessons in this illuminative text are grounded in theory, but made accessible through numerous real-world examples—ready for immediate application to your next graph or presentation.

Storytelling is not an inherent skill, especially when it comes to data visualization, and the tools at our disposal don't make it any easier. This book demonstrates how to go beyond conventional tools to reach the root of your data, and how to use your data to create an engaging, informative, compelling story. Specifically, you'll learn how to:

Understand the importance of context and audience
Determine the appropriate type of graph for your situation
Recognize and eliminate the clutter clouding your information
Direct your audience's attention to the most important parts of your data
Think like a designer and utilize concepts of design in data visualization
Leverage the power of storytelling to help your message resonate with your audience
Together, the lessons in this book will help you turn your data into high impact visual stories that stick with your audience. Rid your world of ineffective graphs, one exploding 3D pie chart at a time. There is a story in your data—Storytelling with Data will give you the skills and power to tell it!},
  added-at = {2017-06-26T18:51:44.000+0200},
  address = {New York, NY},
  author = {Knaflic, Cole Nussbaumer},
  biburl = {https://www.bibsonomy.org/bibtex/217890859db1ef27e2ad8500c29a1b23c/vngudivada},
  interhash = {81fe61523f9eb55ad717f864265e4362},
  intrahash = {17890859db1ef27e2ad8500c29a1b23c},
  isbn = {978-1119002253},
  keywords = {Book Healthcare},
  publisher = {John Wiley},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Storytelling with Data: A Data Visualization Guide for Business Professionals},
  year = 2015
}

@book{baron2014probability,
  abstract = {Student-Friendly Coverage of Probability, Statistical Methods, Simulation, and Modeling Tools
Incorporating feedback from instructors and researchers who used the previous edition, Probability and Statistics for Computer Scientists, Second Edition helps students understand general methods of stochastic modeling, simulation, and data analysis; make optimal decisions under uncertainty; model and evaluate computer systems and networks; and prepare for advanced probability-based courses. Written in a lively style with simple language, this classroom-tested book can now be used in both one- and two-semester courses.

New to the Second Edition

Axiomatic introduction of probability
Expanded coverage of statistical inference, including standard errors of estimates and their estimation, inference about variances, chi-square tests for independence and goodness of fit, nonparametric statistics, and bootstrap
More exercises at the end of each chapter
Additional MATLAB® codes, particularly new commands of the Statistics Toolbox
In-Depth yet Accessible Treatment of Computer Science-Related Topics
Starting with the fundamentals of probability, the text takes students through topics heavily featured in modern computer science, computer engineering, software engineering, and associated fields, such as computer simulations, Monte Carlo methods, stochastic processes, Markov chains, queuing theory, statistical inference, and regression. It also meets the requirements of the Accreditation Board for Engineering and Technology (ABET).

Encourages Practical Implementation of Skills
Using simple MATLAB commands (easily translatable to other computer languages), the book provides short programs for implementing the methods of probability and statistics as well as for visualizing randomness, the behavior of random variables and stochastic processes, convergence results, and Monte Carlo simulations. Preliminary knowledge of MATLAB is not required. Along with numerous computer science applications and worked examples, the text presents interesting facts and paradoxical statements. Each chapter concludes with a short summary and many exercises.},
  added-at = {2017-06-09T19:23:44.000+0200},
  address = {Boca Raton, Florida},
  author = {Baron, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2c6f6dc74b8aff062b21b6f7e0a9f7f31/vngudivada},
  interhash = {a7b09a50f5c1769fb4c35278354bcbeb},
  intrahash = {c6f6dc74b8aff062b21b6f7e0a9f7f31},
  isbn = {9781439875919 143987591X},
  keywords = {Book Probability Statistics},
  publisher = {CRC Press },
  refid = {890379507},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Probability and Statistics for Computer Scientists, Second Edition},
  year = 2014
}

@article{bott2014creativity,
  abstract = {Studies suggest that individuals with greater creative potential have enhanced executive function. Here we tested the hypothesis that a creativity training intervention would increase both low and high-level executive functions. Fifteen participants completed a 5-week creative capacity building program (CCBP) and 15 participants completed a control intervention consisting of a parallel 5-week language capacity building training program (LCBP). Goal-directed attention and processing speed were measured with the Delis–Kaplan Executive Function System (D-KEFS) color–word interference test. Results revealed higher scores post-training associated with CCBP compared to LCBP on the primary D-KEFS measure of combined completion time for color-naming and word-reading conditions, and the primary contrast measure of combined completion time for color-naming and word-reading compared to completion time for inhibition switching. Relative to LCBP, CCBP leads to improvement performance on measures reflecting lower-level executive functions (goal-directed attention and information processing) as opposed to higher-level executive functions, which showed no between-group differences.},
  added-at = {2017-07-24T01:10:12.000+0200},
  author = {Bott, Nicholas and Quintin, Eve-Marie and Saggar, Manish and Kienitz, Eliza and Royalty, Adam and Hong, Daniel Wei-Chen and Liu, Ning and hsuan Chien, Yin and Hawthorne, Grace and Reiss, Allan L.},
  biburl = {https://www.bibsonomy.org/bibtex/2adbbb686e1d9dab8584abd9444ab64b8/vngudivada},
  doi = {10.1016/j.tsc.2014.03.005},
  interhash = {184113917236460a94a3b0d993267083},
  intrahash = {adbbb686e1d9dab8584abd9444ab64b8},
  journal = {Thinking Skills and Creativity},
  keywords = {Creativity STEM},
  month = sep,
  pages = {120--128},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Creativity training enhances goal-directed attention and information processing},
  url = {https://doi.org/10.1016%2Fj.tsc.2014.03.005},
  volume = 13,
  year = 2014
}

@article{haydenstemhighschoolincreasing,
  abstract = {The iQUEST (investigations for Quality Understanding and Engagement for Students and Teachers) project is designed to promote student interest and attitudes toward careers in science, technology, engineering, and mathematics (STEM). The project targets seventh- and eighth-grade science classrooms that serve high percentages of Hispanic students. The project design, student summer camp program, and professional development model have led to successful increases in student performance. The iQUEST student summer camp findings show that underserved populations of both female and male students experienced increased interest and attitudes toward science and technology. The iQUEST professional development model seeks to transform middle school science teachers from digital immigrants to advocates for technology being a critical part of student learning through integration of innovative technology experiences in formal science settings. Classroom observations illustrate how teachers have successfully implemented lessons that engage students in hands-on investigations, leading to deeper understanding of science and, therefore improving the potential of underrepresented students competing in STEM fields.},
  added-at = {2017-07-23T20:39:11.000+0200},
  author = {Hayden, K. and Ouyang, Y. and Scinski, L. and Olszewski, B. and Bielefeldt, T.},
  biburl = {https://www.bibsonomy.org/bibtex/277cdd95b9d14b07cce0323d8d47e724b/vngudivada},
  interhash = {34b34a04451ffb2dfe99874c5c91e5f7},
  intrahash = {77cdd95b9d14b07cce0323d8d47e724b},
  journal = {Contemporary Issues in Technology and Teacher Education},
  keywords = {STEM},
  number = 1,
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Increasing student interest and attitudes in STEM: Professional development and activities to engage and inspire learners},
  url = {http://www.citejournal.org/volume-11/issue-1-11/science/increasing-student-interest-and-attitudes-in-stem-professional-development-and-activities-to-engage-and-inspire-learners},
  volume = 11,
  year = {STEM HighSchool},
  yyear = {2011}
}

@article{sjaastad2012sources,
  abstract = { The objectives of this article were to investigate to which extent and in what ways persons influence students' choice of science, technology, engineering, and mathematics (STEM) in tertiary education, and to assess the suitability of an analytical framework for describing this influence. In total, 5,007 Norwegian STEM students completed a questionnaire including multiple-choice as well as open-ended questions about sources of inspiration for their educational choice. Using the conceptualisation of significant persons suggested by Woelfel and Haller, the respondents' descriptions of parents and teachers are presented in order to elaborate on the different ways these significant persons influence a STEM-related educational choice. Parents engaged in STEM themselves are models, making the choice of STEM familiar, and they help youngsters define themselves through conversation and support, thus being definers. Teachers are models by displaying how STEM might bring fulfilment in someone's life and by giving pupils a positive experience with the subjects. They help young people discover their STEM abilities, thus being definers. Celebrities are reported to have minor influence on STEM-related educational choices. Both qualitative and quantitative analyses indicate that interpersonal relationships are key factors in order to inspire and motivate a choice of STEM education. Implications for recruitment issues and for research on interpersonal influence are discussed. It is suggested that initiatives to increase recruitment to STEM might be aimed at parents and other persons in interpersonal relationships with youth as a target group. },
  added-at = {2017-07-23T20:48:35.000+0200},
  author = {Sjaastad, Jørgen},
  biburl = {https://www.bibsonomy.org/bibtex/201f2c63d9292af1944f09acef528e44f/vngudivada},
  doi = {10.1080/09500693.2011.590543},
  eprint = {http://dx.doi.org/10.1080/09500693.2011.590543},
  interhash = {3b2392f8281e06366cd54b7e3e1a3b3e},
  intrahash = {01f2c63d9292af1944f09acef528e44f},
  journal = {International Journal of Science Education},
  keywords = {HighSchool STEM},
  number = 10,
  pages = {1615-1636},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Sources of Inspiration: The role of significant persons in young people's choice of science in higher education},
  url = {http://dx.doi.org/10.1080/09500693.2011.590543},
  volume = 34,
  year = 2012
}

@book{bast2016semantic,
  abstract = {This article provides a comprehensive overview of the broad area of semantic search on text and knowledge bases. In a nutshell, semantic search is “search with meaning”. This “meaning” can refer to various parts of the search process: understanding the query (instead of just finding matches of its components in the data), understanding the data (instead of just searching it for such matches), or representing knowledge in a way suitable for meaningful retrieval. Semantic search is studied in a variety of different communities with a variety of different views of the problem. In this survey, we classify this work according to two dimensions: the type of data (text, knowledge bases, combinations of these) and the kind of search (keyword, structured, natural language). We consider all nine combinations. The focus is on fundamental techniques, concrete systems, and benchmarks. The survey also considers advanced issues: ranking, indexing, ontology matching and merging, and inference. It also provides a succinct overview of fundamental natural language processing techniques: POS-tagging, named-entity recognition and disambiguation, sentence parsing, and distributional semantics. The survey is as self-contained as possible, and should thus also serve as a good tutorial for newcomers to this fascinating and highly topical field.


This monograph provides a comprehensive overview of the broad area of semantic search on text and knowledge bases. In a nutshell, semantic search is “search with meaning”. This “meaning” can refer to various parts of the search process: understanding the query (instead of just finding matches of its components in the data), understanding the data (instead of just searching it for such matches), or representing knowledge in a way suitable for meaningful retrieval.

Semantic search is studied in a variety of different communities with a variety of different views of the problem. Semantic Search on Text and Knowledge Bases classifies this work according to two dimensions: the type of data (text, knowledge bases, combinations of these) and the kind of search (keyword, structured, natural language). All nine combinations are considered. The focus is on fundamental techniques, concrete systems, and benchmarks. The monograph also considers advanced issues: ranking, indexing, ontology matching and merging, and inference. It also provides a succinct overview of fundamental natural language processing techniques: POS-tagging, named-entity recognition and disambiguation, sentence parsing, and distributional semantics.

Semantic Search on Text and Knowledge Bases is as self-contained as possible, and should thus also serve as a good tutorial for newcomers to this fascinating and highly topical field.},
  added-at = {2017-06-03T20:52:32.000+0200},
  address = {Boston, Massachusetts},
  author = {Bast, Hannah},
  biburl = {https://www.bibsonomy.org/bibtex/2e0574bb8a9d5bc849fb1117945fffe87/vngudivada},
  interhash = {2847192356e306ddb8bb2bc0710da9b0},
  intrahash = {e0574bb8a9d5bc849fb1117945fffe87},
  keywords = {IR NLP SemanticSearch},
  publisher = {Now Publishers Inc},
  refid = {953426077},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Semantic search on text and knowledge bases},
  year = 2016
}

@book{ingalls2013mathematical,
  abstract = {Systems techniques are integral to current research in molecular cell biology, and system-level investigations are often accompanied by mathematical models. These models serve as working hypotheses: they help us to understand and predict the behavior of complex systems. This book offers an introduction to mathematical concepts and techniques needed for the construction and interpretation of models in molecular systems biology. It is accessible to upper-level undergraduate or graduate students in life science or engineering who have some familiarity with calculus, and will be a useful reference for researchers at all levels.

The first four chapters cover the basics of mathematical modeling in molecular systems biology. The last four chapters address specific biological domains, treating modeling of metabolic networks, of signal transduction pathways, of gene regulatory networks, and of electrophysiology and neuronal action potentials. Chapters 3--8 end with optional sections that address more specialized modeling topics. Exercises, solvable with pen-and-paper calculations, appear throughout the text to encourage interaction with the mathematical techniques. More involved end-of-chapter problem sets require computational software. Appendixes provide a review of basic concepts of molecular biology, additional mathematical background material, and tutorials for two computational software packages (XPPAUT and MATLAB) that can be used for model simulation and analysis.},
  added-at = {2017-06-06T20:23:55.000+0200},
  address = {Cambridge, MA},
  author = {Ingalls, Brian P.},
  biburl = {https://www.bibsonomy.org/bibtex/2d4ad8d9752f701448634d704e006af5c/vngudivada},
  interhash = {0e79acd7d17cb14772ed567dcfd695bd},
  intrahash = {d4ad8d9752f701448634d704e006af5c},
  isbn = {9781461935667 1461935660 0262315637 9780262315630},
  keywords = {SystemsBiology},
  publisher = {The MIT Press},
  refid = {853455800},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Mathematical modeling in systems biology: an introduction},
  year = 2013
}

@article{castillo2011adversarial,
  abstract = {Web search engines have become indispensable tools for finding content. As the popularity of the Web has increased, the efforts to exploit the Web for commercial, social, or political advantage have grown, making it harder for search engines to discriminate between truthful signals of content quality and deceptive attempts to game search engines' rankings. This problem is further complicated by the open nature of the Web, which allows anyone to write and publish anything, and by the fact that search engines must analyze ever-growing numbers of Web pages. Moreover, increasing expectations of users, who over time rely on Web search for information needs related to more aspects of their lives, further deepen the need for search engines to develop effective counter-measures against deception.

In this monograph, we consider the effects of the adversarial relationship between search systems and those who wish to manipulate them, a field known as "Adversarial Information Retrieval". We show that search engine spammers create false content and misleading links to lure unsuspecting visitors to pages filled with advertisements or malware. We also examine work over the past decade or so that aims to discover such spamming activities to get spam pages removed or their effect on the quality of the results reduced.

Research in Adversarial Information Retrieval has been evolving over time, and currently continues both in traditional areas (e.g., link spam) and newer areas, such as click fraud and spam in social media, demonstrating that this conflict is far from over.},
  added-at = {2017-06-03T21:22:04.000+0200},
  author = {Castillo, Carlos and Davison, Brian D.},
  biburl = {https://www.bibsonomy.org/bibtex/2cd231f71fa7d3cca4b6690eb2129f1d2/vngudivada},
  doi = {10.1561/1500000021},
  interhash = {f7dca6d95760cdb1415f99d27f92046a},
  intrahash = {cd231f71fa7d3cca4b6690eb2129f1d2},
  journal = {Foundations and Trends® in Information Retrieval},
  keywords = {AdversialWebSearch ErbSearch IR},
  number = 5,
  pages = {377-486},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Adversarial Web Search},
  url = {http://dx.doi.org/10.1561/1500000021},
  volume = 4,
  year = 2011
}

@article{goldberg2017neural,
  abstract = {Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries.

The second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning.

Table of Contents: Preface / Acknowledgments / Introduction / Learning Basics and Linear Models / From Linear Models to Multi-layer Perceptrons / Feed-forward Neural Networks / Neural Network Training / Features for Textual Data / Case Studies of NLP Features / From Textual Features to Inputs / Language Modeling / Pre-trained Word Representations / Using Word Embeddings / Case Study: A Feed-forward Architecture for Sentence Meaning Inference / Ngram Detectors: Convolutional Neural Networks / Recurrent Neural Networks: Modeling Sequences and Stacks / Concrete Recurrent Neural Network Architectures / Modeling with Recurrent Networks / Conditioned Generation / Modeling Trees with Recursive Neural Networks / Structured Output Prediction / Cascaded, Multi-task and Semi-supervised Learning / Conclusion / Bibliography / Author's Biography},
  added-at = {2017-06-26T04:18:48.000+0200},
  author = {Goldberg, Yoav},
  biburl = {https://www.bibsonomy.org/bibtex/26e399bc8d09ebb7024e5733e9a3ece18/vngudivada},
  doi = {10.2200/S00762ED1V01Y201703HLT037},
  eprint = {https://doi.org/10.2200/S00762ED1V01Y201703HLT037},
  interhash = {457f378e4a0b61ee74f78f436134d90a},
  intrahash = {6e399bc8d09ebb7024e5733e9a3ece18},
  journal = {Synthesis Lectures on Human Language Technologies},
  keywords = {DeepLearning NLP NeuralNetwork},
  number = 1,
  pages = {1 -- 309},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Neural Network Methods for Natural Language Processing},
  volume = 10,
  year = 2017
}

@article{chater2006probabilistic,
  abstract = {Remarkable progress in the mathematics and computer science of probability has led to a revolution in the scope of probabilistic models. In particular, `sophisticated' probabilistic methods apply to structured relational systems such as graphs and grammars, of immediate relevance to the cognitive sciences. This Special Issue outlines progress in this rapidly developing field, which provides a potentially unifying perspective across a wide range of domains and levels of explanation. Here, we introduce the historical and conceptual foundations of the approach, explore how the approach relates to studies of explicit probabilistic reasoning, and give a brief overview of the field as it stands today.},
  added-at = {2017-06-05T23:49:20.000+0200},
  author = {Chater, Nick and Tenenbaum, Joshua B. and Yuille, Alan},
  biburl = {https://www.bibsonomy.org/bibtex/25445d1f4b506094304ae5e90e19efe5a/vngudivada},
  interhash = {ff0d6859f78d4e07eb2bf164b997b794},
  intrahash = {5445d1f4b506094304ae5e90e19efe5a},
  journal = {Trends in Cognitive Sciences},
  keywords = {CognitiveScience PGM Probability},
  number = 7,
  pages = {287 - 291},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Probabilistic models of cognition: Conceptual foundations},
  volume = 10,
  year = 2006
}

@article{papastergiou2009digital,
  abstract = {The aim of this study was to assess the learning effectiveness and motivational appeal of a computer game for learning computer memory concepts, which was designed according to the curricular objectives and the subject matter of the Greek high school Computer Science (CS) curriculum, as compared to a similar application, encompassing identical learning objectives and content but lacking the gaming aspect. The study also investigated potential gender differences in the game’s learning effectiveness and motivational appeal. The sample was 88 students, who were randomly assigned to two groups, one of which used the gaming application (Group A, N = 47) and the other one the non-gaming one (Group B, N = 41). A Computer Memory Knowledge Test (CMKT) was used as the pretest and posttest. Students were also observed during the interventions. Furthermore, after the interventions, students’ views on the application they had used were elicited through a feedback questionnaire. Data analyses showed that the gaming approach was both more effective in promoting students’ knowledge of computer memory concepts and more motivational than the non-gaming approach. Despite boys’ greater involvement with, liking of and experience in computer gaming, and their greater initial computer memory knowledge, the learning gains that boys and girls achieved through the use of the game did not differ significantly, and the game was found to be equally motivational for boys and girls. The results suggest that within high school CS, educational computer games can be exploited as effective and motivational learning environments, regardless of students’ gender.},
  added-at = {2017-07-23T23:26:21.000+0200},
  author = {Papastergiou, Marina},
  biburl = {https://www.bibsonomy.org/bibtex/2e1f7d8e777f850272e505a7b89d6acb0/vngudivada},
  doi = {10.1016/j.compedu.2008.06.004},
  interhash = {404d0c48323d9924e0f3d227d678883a},
  intrahash = {e1f7d8e777f850272e505a7b89d6acb0},
  journal = {Computers {\&} Education},
  keywords = {HighSchool Pedagogy STEM},
  month = jan,
  number = 1,
  pages = {1--12},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Digital Game-Based Learning in high school Computer Science education: Impact on educational effectiveness and student motivation},
  url = {https://doi.org/10.1016%2Fj.compedu.2008.06.004},
  volume = 52,
  year = 2009
}

@misc{authors2017schaums,
  added-at = {2017-06-04T05:05:49.000+0200},
  author = {Authors, Multiple},
  biburl = {https://www.bibsonomy.org/bibtex/2d162ade4e8168c0ecd851a180f23ea55/vngudivada},
  interhash = {a520b1fb0e2f7aac20c176fa0a69b3bb},
  intrahash = {d162ade4e8168c0ecd851a180f23ea55},
  keywords = {Book Probability SchaumsOutline Statistics},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Schaum's Outline of Probability and Statistics},
  year = 2017
}

@article{santos2015search,
  abstract = {Ranking in information retrieval has been traditionally approached as a pursuit of relevant information, under the assumption that the users’ information needs are unambiguously conveyed by their submitted queries. Nevertheless, as an inherently limited representation of a more complex information need, every query can arguably be considered ambiguous to some extent. In order to tackle query ambiguity, search result diversification approaches have recently been proposed to produce rankings aimed to satisfy the multiple possible information needs underlying a query. In this survey, we review the published literature on search result diversification. In particular, we discuss the motivations for diversifying the search results for an ambiguous query and provide a formal definition of the search result diversification problem. In addition, we describe the most successful approaches in the literature for producing and evaluating diversity in multiple search domains. Finally, we also discuss recent advances as well as open research directions in the field of search result diversification.},
  added-at = {2017-06-03T21:52:04.000+0200},
  author = {Santos, Rodrygo L. T. and Macdonald, Craig and Ounis, Iadh},
  biburl = {https://www.bibsonomy.org/bibtex/2a86576bfb73639b342e834e954eefd76/vngudivada},
  doi = {10.1561/1500000040},
  interhash = {642288661ab17da96beecd790725e811},
  intrahash = {a86576bfb73639b342e834e954eefd76},
  issn = {1554-0669},
  journal = {Foundations and Trends in Information Retrieval},
  keywords = {IR},
  number = 1,
  pages = {1-90},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Search Result Diversification},
  url = {http://dx.doi.org/10.1561/1500000040},
  volume = 9,
  year = 2015
}

@article{larson2012spoken,
  abstract = {Speech media, that is, digital audio and video containing spoken content, has blossomed in recent years. Large collections are accruing on the Internet as well as in private and enterprise settings. This growth has motivated extensive research on techniques and technologies that facilitate reliable indexing and retrieval. Spoken content retrieval (SCR) requires the combination of audio and speech processing technologies with methods from information retrieval (IR). SCR research initially investigated planned speech structured in document-like units, but has subsequently shifted focus to more informal spoken content produced spontaneously, outside of the studio and in conversational settings. This survey provides an overview of the field of SCR encompassing component technologies, the relationship of SCR to text IR and automatic speech recognition and user interaction issues. It is aimed at researchers with backgrounds in speech technology or IR who are seeking deeper insight on how these fields are integrated to support research and development, thus addressing the core challenges of SCR.


Speech media, i.e., digital audio and video containing spoken content, has blossomed in recent years. Large collections are accruing on the Internet as well as in private and enterprise settings. This growth has motivated extensive research work on techniques and technologies that facilitate reliable indexing and retrieval. Spoken content retrieval (SCR) requires a combination of audio and speech processing technologies with methods from information retrieval (IR). SCR research initially investigated planned speech structured in document-like units, but has subsequently shifted focus to more informal spoken content produced spontaneously, outside of the studio and in conversational settings. Spoken Content Retrieval: A Survey of Techniques and Technologies provides an overview of the field of SCR encompassing component technologies, the relationship of SCR to text IR and automatic speech recognition and user interaction issues. It can be read sequentially from beginning to end, but is also written modularly, making it possible to read parts of the survey selectively. Spoken Content Retrieval: A Survey of Techniques and Technologies includes an extensive bibliography including over 300 references. The bibliography was selected with the goal of providing a comprehensive selection of entry points into the literature that would allow further exploration of the issues covered. The text is an invaluable reference for researchers with backgrounds in speech technology or information retrieval who are seeking deeper insight on how these fields are integrated to support research and development addressing the core challenges of SCR.},
  added-at = {2017-06-03T21:05:03.000+0200},
  author = {Larson, Martha and Jones, Gareth J. F.},
  biburl = {https://www.bibsonomy.org/bibtex/2e54b590261920379adaa4c0e6fe13000/vngudivada},
  doi = {10.1561/1500000020},
  interhash = {a434968ee83b9db209f6698f5c7a1c7c},
  intrahash = {e54b590261920379adaa4c0e6fe13000},
  issn = {1554-0669},
  journal = {Foundations and Trends in Information Retrieval},
  keywords = {IR SpokenLanguage},
  number = {4–5},
  pages = {235-422},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Spoken Content Retrieval: A Survey of Techniques and Technologies},
  url = {http://dx.doi.org/10.1561/1500000020},
  volume = 5,
  year = 2012
}

@article{annetta2009investigating,
  abstract = {The popularity of video games has transcended entertainment crossing into the world of education. While the literature base on educational gaming is growing, there is still a lack of systematic study of this emerging technology’s efficacy. This quasi-experimental study evaluated a teacher created video game on genetics in terms of its affective and cognitive impact on student users. While statistical results indicated no differences (p > .05) in student learning as measured by our instrument, there were significant differences (p < .05) found in the participants’ level of engagement while interfacing with the video game. Implications on this emerging line of inquiry are discussed.},
  added-at = {2017-07-23T23:22:22.000+0200},
  author = {Annetta, Leonard A. and Minogue, James and Holmes, Shawn Y. and Cheng, Meng-Tzu},
  biburl = {https://www.bibsonomy.org/bibtex/262b9d36fb1021a46f8196d508239b296/vngudivada},
  doi = {10.1016/j.compedu.2008.12.020},
  interhash = {e950e447ac44ff0c90d8ec4f8c59a440},
  intrahash = {62b9d36fb1021a46f8196d508239b296},
  journal = {Computers {\&} Education},
  keywords = {HighSchool Pedagogy STEM},
  month = aug,
  number = 1,
  pages = {74--85},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Investigating the impact of video games on high school students' engagement and learning about genetics},
  url = {https://doi.org/10.1016%2Fj.compedu.2008.12.020},
  volume = 53,
  year = 2009
}

@article{liu2009learning,
  abstract = {Learning to rank for Information Retrieval (IR) is a task to automatically construct a ranking model using training data, such that the model can sort new objects according to their degrees of relevance, preference, or importance. Many IR problems are by nature ranking problems, and many IR technologies can be potentially enhanced by using learning-to-rank techniques. The objective of this tutorial is to give an introduction to this research direction. Specifically, the existing learning-to-rank algorithms are reviewed and categorized into three approaches: the pointwise, pairwise, and listwise approaches. The advantages and disadvantages with each approach are analyzed, and the relationships between the loss functions used in these approaches and IR evaluation measures are discussed. Then the empirical evaluations on typical learning-to-rank methods are shown, with the LETOR collection as a benchmark dataset, which seems to suggest that the listwise approach be the most effective one among all the approaches. After that, a statistical ranking theory is introduced, which can describe different learning-to-rank algorithms, and be used to analyze their query-level generalization abilities. At the end of the tutorial, we provide a summary and discuss potential future work on learning to rank.},
  added-at = {2017-06-03T21:24:10.000+0200},
  author = {Liu, Tie-Yan},
  biburl = {https://www.bibsonomy.org/bibtex/22f2f637ec5ca1f937543ea8ffd65e112/vngudivada},
  doi = {10.1561/1500000016},
  interhash = {7ac5804a617fcacd504a55de462fd7a9},
  intrahash = {2f2f637ec5ca1f937543ea8ffd65e112},
  journal = {Foundations and Trends® in Information Retrieval},
  keywords = {IR Ranking},
  number = 3,
  pages = {225-331},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Learning to Rank for Information Retrieval},
  url = {http://dx.doi.org/10.1561/1500000016},
  volume = 3,
  year = 2009
}

@book{pearl2009causality,
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
  added-at = {2017-06-05T23:14:48.000+0200},
  address = {Cambridge, Massachusetts},
  author = {Pearl, Judea},
  biburl = {https://www.bibsonomy.org/bibtex/2299e2147177792474dcaf60e845d503c/vngudivada},
  edition = {Second},
  interhash = {e1ad305042e608819815bbbc7a91a821},
  intrahash = {299e2147177792474dcaf60e845d503c},
  keywords = {BayesianMethods Book PGM},
  publisher = {Cambridge University Press},
  refid = {967412040},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Causality: models, reasoning, and inference},
  year = 2009
}

@article{hmelosilver2006goals,
  abstract = {This paper describes an analysis of facilitation of a student-centered problem-based learning group. The focus of this analysis was to understand the goals and strategies of an expert facilitator in support of collaborative learning. This was accomplished through interaction analysis using video data and stimulated recall to examine two PBL group meetings. In this paper, we examine how specific strategies were used to support the PBL goals of helping students construct causal explanations, reason effectively, and become self-directed learners while maintaining a student-centered learning process. Being able to articulate these strategies is an important step in helping others learn the art of PBL facilitation.},
  added-at = {2017-07-24T01:37:15.000+0200},
  author = {Hmelo-Silver, Cindy E. and Barrows, Howard S.},
  biburl = {https://www.bibsonomy.org/bibtex/2b7a017327540e8242b5493659690212e/vngudivada},
  doi = {10.7771/1541-5015.1004},
  interhash = {277ed31603c17b2b8f9a74633210e841},
  intrahash = {b7a017327540e8242b5493659690212e},
  journal = {Interdisciplinary Journal of Problem-Based Learning},
  keywords = {PBL STEM},
  month = may,
  number = 1,
  publisher = {Purdue University},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Goals and Strategies of a Problem-based Learning Facilitator},
  url = {https://doi.org/10.7771%2F1541-5015.1004},
  volume = 1,
  year = 2006
}

@inproceedings{collobert2008unified,
  abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
  acmid = {1390177},
  added-at = {2017-07-07T13:54:13.000+0200},
  address = {New York, NY},
  author = {Collobert, Ronan and Weston, Jason},
  biburl = {https://www.bibsonomy.org/bibtex/206dca17c32bf0b9c57cb744a9ec1cb08/vngudivada},
  booktitle = {Proceedings of the 25th International Conference on Machine Learning},
  doi = {10.1145/1390156.1390177},
  interhash = {3f9fd54a89bf8be67e9de4c766dcba83},
  intrahash = {06dca17c32bf0b9c57cb744a9ec1cb08},
  isbn = {978-1-60558-205-4},
  keywords = {DeepLearning NLP},
  location = {Helsinki, Finland},
  numpages = {8},
  pages = {160--167},
  publisher = {ACM},
  series = {ICML '08},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
  url = {http://doi.acm.org/10.1145/1390156.1390177},
  year = 2008
}

@book{ogrady2010contemporary,
  abstract = {Contemporary Linguistics is one of the most comprehensive introduction to the fundamentals of linguistics, balancing engaging aspects of language study with solid coverage of the basics. Up-to-date scholarship, a direct approach, and a lucid writing style makes it appealing to instructors and beginning students alike and a resource that many students continue to use beyond the classroom.},
  added-at = {2017-07-05T01:02:18.000+0200},
  address = {New York, NY},
  author = {O'Grady, William Delaney and Archibald, John and Aronoff, Mark and Rees-Miller, Janie},
  biburl = {https://www.bibsonomy.org/bibtex/2b0b9f9bc96a2f297f51adb0a0772140d/vngudivada},
  interhash = {6c66e9fb53896bf2709d94e4caef389e},
  intrahash = {b0b9f9bc96a2f297f51adb0a0772140d},
  isbn = {978-0312555283},
  keywords = {Book Linguistics},
  publisher = {St. Martin's Press},
  refid = {708358836},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Contemporary linguistics an introduction},
  year = 2010
}

@article{wainwright2008graphical,
  abstract = {The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.},
  added-at = {2017-06-03T21:40:17.000+0200},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  biburl = {https://www.bibsonomy.org/bibtex/28ef96a2bc501bd0d73daae2a99a3a654/vngudivada},
  doi = {10.1561/2200000001},
  interhash = {2d7afa0e81253b2d2b86968141dba8a7},
  intrahash = {8ef96a2bc501bd0d73daae2a99a3a654},
  issn = {1935-8237},
  journal = {Foundations and Trends in Machine Learning},
  keywords = {BayesianMethods PGM},
  number = {1–2},
  pages = {1-305},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Graphical Models, Exponential Families, and Variational Inference},
  url = {http://dx.doi.org/10.1561/2200000001},
  volume = 1,
  year = 2008
}

@electronic{anderson2006future,
  abstract = {Examines the rise of the niche in today's economy, thanks to a breakdown in the barrier between supply and demand and the increasing availability of everything to everyone, and assesses the implications of this new economic model for business.},
  added-at = {2017-06-11T21:56:54.000+0200},
  address = {New York, N.Y.},
  author = {Anderson, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/218a5f471f709d602af60dcfdda0d522d/vngudivada},
  interhash = {9b04b129705c7c2989bb4f11e6ed8ed1},
  intrahash = {18a5f471f709d602af60dcfdda0d522d},
  isbn = {1401384641 9781401384647},
  keywords = {Book Entrepreneurship Innovation},
  publisher = {Hyperion},
  refid = {74279026},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {The long tail: why the future of business is selling less of more},
  year = 2006
}

@article{barnickel2014large,
  abstract = {To reduce the increasing amount of time spent on literature search in the life sciences, several methods for automated knowledge extraction have been developed. Co-occurrence based approaches can deal with large text corpora like MEDLINE in an acceptable time but are not able to extract any specific type of semantic relation. Semantic relation extraction methods based on syntax trees, on the other hand, are computationally expensive and the interpretation of the generated trees is difficult. Several natural language processing (NLP) approaches for the biomedical domain exist focusing specifically on the detection of a limited set of relation types. For systems biology, generic approaches for the detection of a multitude of relation types which in addition are able to process large text corpora are needed but the number of systems meeting both requirements is very limited. We introduce the use of SENNA (“Semantic Extraction using a Neural Network Architecture”), a fast and accurate neural network based Semantic Role Labeling (SRL) program, for the large scale extraction of semantic relations from the biomedical literature. A comparison of processing times of SENNA and other SRL systems or syntactical parsers used in the biomedical domain revealed that SENNA is the fastest Proposition Bank (PropBank) conforming SRL program currently available. 89 million biomedical sentences were tagged with SENNA on a 100 node cluster within three days. The accuracy of the presented relation extraction approach was evaluated on two test sets of annotated sentences resulting in precision/recall values of 0.71/0.43. We show that the accuracy as well as processing speed of the proposed semantic relation extraction approach is sufficient for its large scale application on biomedical text. The proposed approach is highly generalizable regarding the supported relation types and appears to be especially suited for general-purpose, broad-scale text mining systems. The presented approach bridges the gap between fast, cooccurrence-based approaches lacking semantic relations and highly specialized and computationally demanding NLP approaches.},
  added-at = {2017-07-07T13:50:26.000+0200},
  author = {Barnickel, Thorsten and Weston, Jason and Collobert, Ronan and Mewes, Hans-Werner and Stümpflen, Volker},
  biburl = {https://www.bibsonomy.org/bibtex/2191ac4c8c8164ef8583883f94fd7cc0f/vngudivada},
  doi = {10.1371/journal.pone.0006393},
  interhash = {64e598cd9e303d44ff7ff33586d145b6},
  intrahash = {191ac4c8c8164ef8583883f94fd7cc0f},
  journal = {PLOS ONE},
  keywords = {NLP RoleLabeling},
  month = {07},
  number = 7,
  pages = {1-6},
  publisher = {Public Library of Science},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Large Scale Application of Neural Network Based Semantic Role Labeling for Automated Relation Extraction from Biomedical Texts},
  url = {https://doi.org/10.1371/journal.pone.0006393},
  volume = 4,
  year = 2014
}

@book{amabile2011progress,
  abstract = {What really sets the best managers above the rest? It’s their power to build a cadre of employees who have great inner work lives—consistently positive emotions; strong motivation; and favorable perceptions of the organization, their work, and their colleagues. The worst managers undermine inner work life, often unwittingly.

As Teresa Amabile and Steven Kramer explain in The Progress Principle, seemingly mundane workday events can make or break employees’ inner work lives. But it’s forward momentum in meaningful work—progress—that creates the best inner work lives. Through rigorous analysis of nearly 12,000 diary entries provided by 238 employees in 7 companies, the authors explain how managers can foster progress and enhance inner work life every day.

The book shows how to remove obstacles to progress, including meaningless tasks and toxic relationships. It also explains how to activate two forces that enable progress: (1) catalysts—events that directly facilitate project work, such as clear goals and autonomy—and (2) nourishers—interpersonal events that uplift workers, including encouragement and demonstrations of respect and collegiality.

Brimming with honest examples from the companies studied, The Progress Principle equips aspiring and seasoned leaders alike with the insights they need to maximize their people’s performance.},
  added-at = {2017-07-24T00:40:43.000+0200},
  address = {Cambridge, Massachusetts},
  author = {Amabile, Teresa M. and Kramer, Steven J.},
  biburl = {https://www.bibsonomy.org/bibtex/236c3ee8ca9c183423c26dfc48346d156/vngudivada},
  interhash = {95c50e3bf9872d5410488b85fbbfc132},
  intrahash = {36c3ee8ca9c183423c26dfc48346d156},
  isbn = {978-1422198575},
  keywords = {Book Creativity Engagement STEM},
  month = {july},
  publisher = {Harvard Business Review Press},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {The Progress Principle: Using Small Wins to Ignite Joy, Engagement, and Creativity at Work},
  url = {http://progressprinciple.com/},
  year = 2011
}

@book{burke2013health,
  abstract = {A hands-on, analytics road map for health industry leaders
The industry-wide transformation taking place across the health and life sciences ecosystem is mandating that organizations adopt new decision-making capabilities, based on science and real-world information. Analytics will be a required competency for the modern health enterprise; this book is about how to "cross the chasm." The ultimate analytics guide for the health industry leader, this essential book equips business leaders with little-to-no experience in analytics to understand how to incorporate analytics as a cornerstone of their 21st century competitive business strategy.

Paints the picture for a new health enterprise, one focused on the patient
Explores the financial components of this new operating model, using analytics to optimize the tradeoffs between cost and value
Deals with the rising role of the consumer, using analytics to create a completely new health engagement model with individual recipients of care
Looks at how analytics can drive innovations in care practice, patient-experienced medical outcomes, and analytically driven novel therapies optimized for the individual patient
Presents a variety of text, tables, and graphics illustrating the various concepts being described
Within each section and chapter, Health Analytics assesses the current landscape, proposing a new model/concept, sharing real-world stories of how the old and new world come together, and framing a "how-to" for the reader in terms of growing that particular set of capabilities in their own enterprises.},
  added-at = {2017-06-26T18:33:32.000+0200},
  address = {New York, NY},
  author = {Burke, Jason},
  biburl = {https://www.bibsonomy.org/bibtex/2608c7a330729e1543724b33b458837b1/vngudivada},
  interhash = {ea322315ce3dbc6e44f51cb093344d55},
  intrahash = {608c7a330729e1543724b33b458837b1},
  isbn = {978-1118383049},
  keywords = {Book Healthcare},
  publisher = {John Wiley},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Health Analytics: Gaining the Insights to Transform Health Care},
  year = 2013
}

@article{pearl2010introduction,
  abstract = {This paper summarizes recent advances in causal inference and underscores the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underlie all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: those about (1) the effects of potential interventions, (2) probabilities of counterfactuals, and (3) direct and indirect effects (also known as "mediation"). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both. The tools are demonstrated in the analyses of mediation, causes of effects, and probabilities of causation.},
  added-at = {2017-06-05T23:25:47.000+0200},
  author = {Pearl, Judea},
  biburl = {https://www.bibsonomy.org/bibtex/21643fb67ea6b7802c76541f937543667/vngudivada},
  interhash = {c98f6599d66e37fb0ecdf7454f732cd3},
  intrahash = {1643fb67ea6b7802c76541f937543667},
  journal = {The International Journal of Biostatistics},
  keywords = {BayesianMethods CausalInference PGM},
  number = 2,
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {An Introduction to Causal Inference},
  volume = 6,
  year = 2010
}

@book{aggarwal2012mining,
  abstract = {Text mining applications have experienced tremendous advances because of web 2.0 and social networking applications. Recent advances in hardware and software technology have lead to a number of unique scenarios where text mining algorithms are learned.

Mining Text Data introduces an important niche in the text analytics field, and is an edited volume contributed by leading international researchers and practitioners focused on social networks & data mining. This book contains a wide swath in topics across social networks & data mining. Each chapter contains a comprehensive survey including the key research content on the topic, and the future directions of research in the field. There is a special focus on Text Embedded with Heterogeneous and Multimedia Data which makes the mining process much more challenging. A number of methods have been designed such as transfer learning and cross-lingual mining for such cases.

Mining Text Data simplifies the content, so that advanced-level students, practitioners and researchers in computer science can benefit from this book. Academic and corporate libraries, as well as ACM, IEEE, and Management Science focused on information security, electronic commerce, databases, data mining, machine learning, and statistics are the primary buyers for this reference book.},
  added-at = {2017-06-06T01:24:43.000+0200},
  address = {New York, NY},
  author = {Aggarwal, Charu C. and Zhai, ChengXiang and Media, Springer Science+Business},
  biburl = {https://www.bibsonomy.org/bibtex/2f45d534211b630243e16a69882cf9968/vngudivada},
  interhash = {728844fdb6d350dc18ac5a7539b8359a},
  intrahash = {f45d534211b630243e16a69882cf9968},
  keywords = {NLP TextMining},
  publisher = {Springer},
  refid = {864854660},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Mining text data},
  year = 2012
}

@article{milyavskaya2012inspired,
  abstract = {The present investigation examined the effects of trait and goal inspiration on goal progress. Undergraduate students reported three goals they intended to pursue throughout the semester and completed measures of trait and goal inspiration as well as measures of personality traits. Participants then reported on goal progress three times at monthly intervals throughout the semester. Result showed that trait inspiration predicted goal progress, and that this effect was fully mediated by goal inspiration and held after controlling for the Big Five personality traits. Additional within-person analyses of goal inspiration showed that most of the variance in goal inspiration was due to between-person individual differences. Furthermore, analyses of the direction of causality between goal inspiration and goal progress revealed a bi-directional relationship. Discussion focused on the implications and future directions for research on inspiration.},
  added-at = {2017-07-23T23:52:50.000+0200},
  author = {Milyavskaya, Marina and Ianakieva, Iana and Foxen-Craft, Emily and Colantuoni, Agnes and Koestner, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/26b98fab860765a442de3a809c1be5328/vngudivada},
  doi = {10.1016/j.paid.2011.08.031},
  interhash = {bf03f01f371c7cf6fbedd3ea348e1017},
  intrahash = {6b98fab860765a442de3a809c1be5328},
  journal = {Personality and Individual Differences},
  keywords = {Inspiration STEM},
  month = jan,
  number = 1,
  pages = {56--60},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Inspired to get there: The effects of trait and goal inspiration on goal progress},
  url = {https://doi.org/10.1016%2Fj.paid.2011.08.031},
  volume = 52,
  year = 2012
}

@book{buttcher2010information,
  abstract = {Information retrieval is the foundation for modern search engines. This textbook offers an introduction to the core topics underlying modern search technologies, including algorithms, data structures, indexing, retrieval, and evaluation. The emphasis is on implementation and experimentation; each chapter includes exercises and suggestions for student projects. Wumpus—a multiuser open-source information retrieval system developed by one of the authors and available online—provides model implementations and a basis for student work. The modular structure of the book allows instructors to use it in a variety of graduate-level courses, including courses taught from a database systems perspective, traditional information retrieval courses with a focus on IR theory, and courses covering the basics of Web retrieval. In addition to its classroom use, Information Retrieval will be a valuable reference for professionals in computer science, computer engineering, and software engineering.},
  added-at = {2017-06-18T22:29:41.000+0200},
  address = {Cambridge, Massachusetts},
  author = {B\"{u}ttcher, Stefan and Clarke, Charles L. A. and Cormack, Gordon V.},
  biburl = {https://www.bibsonomy.org/bibtex/21eccde8c05f5c182fc5c3b24a3ceb9fe/vngudivada},
  interhash = {ed0342a5c4fbbaf95945230afa074e12},
  intrahash = {1eccde8c05f5c182fc5c3b24a3ceb9fe},
  isbn = {978-0262026512},
  keywords = {Book IR},
  publisher = {The MIT Press},
  refid = {805082494},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Information retrieval: implementing and evaluating search engines},
  year = 2010
}

@article{master2017programming,
  abstract = {The gender gap in science, technology, engineering, and math (STEM) engagement is large and persistent. This gap is significantly larger in technological fields such as computer science and engineering than in math and science. Gender gaps begin early; young girls report less interest and self-efficacy in technology compared with boys in elementary school. In the current study (N = 96), we assessed 6-year-old children’s stereotypes about STEM fields and tested an intervention to develop girls’ STEM motivation despite these stereotypes. First-grade children held stereotypes that boys were better than girls at robotics and programming but did not hold these stereotypes about math and science. Girls with stronger stereotypes about robotics and programming reported lower interest and self-efficacy in these domains. We experimentally tested whether positive experience with programming robots would lead to greater interest and self-efficacy among girls despite these stereotypes. Children were randomly assigned either to a treatment group that was given experience in programming a robot using a smartphone or to control groups (no activity or other activity). Girls given programming experience reported higher technology interest and self-efficacy compared with girls without this experience and did not exhibit a significant gender gap relative to boys’ interest and self-efficacy. These findings show that children’s views mirror current American cultural messages about who excels at computer science and engineering and show the benefit of providing young girls with chances to experience technological activities.},
  added-at = {2017-08-20T20:28:55.000+0200},
  author = {Master, Allison and Cheryan, Sapna and Moscatelli, Adriana and Meltzoff, Andrew N.},
  biburl = {https://www.bibsonomy.org/bibtex/21af99f775caeb853d9af07e4a8b58977/vngudivada},
  description = {Six-year-olds held stereotypes that boys are better at robots and programming.
Stereotypes about robots were stronger than stereotypes about math and science.
Girls given programming experience showed higher technology interest and self-efficacy.
Experience also eliminated gender differences in technology interest and self-efficacy.
Providing girls with positive STEM experiences is beneficial.},
  doi = {10.1016/j.jecp.2017.03.013},
  interhash = {e0e8014fc2514663311863ffe3027dcb},
  intrahash = {1af99f775caeb853d9af07e4a8b58977},
  journal = {Journal of Experimental Child Psychology},
  keywords = {Motivation Programming STEM},
  month = aug,
  pages = {92--106},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Programming experience promotes higher {STEM} motivation among first-grade girls},
  url = {https://doi.org/10.1016%2Fj.jecp.2017.03.013},
  volume = 160,
  year = 2017
}

@article{ertmer2006jumping,
  abstract = {While problem-based learning (PBL) has a relatively long history of successful use in medical and pre-professional schools, it has yet to be widely adopted by K–12 teachers. This may be due, in part, to the numerous challenges teachers experience when implementing PBL. In this paper, we describe specific hurdles that teachers are likely to encounter during the implementation process and provide specific suggestions for supporting teachers’ classroom efforts. Implementation challenges relate to 1) creating a culture of collaboration and interdependence, 2) adjusting to changing roles, and 3) scaffolding student learning and performance. By supporting teachers’ initial and ongoing efforts, we anticipate that more teachers will recognize the potential of PBL as an effective instructional approach for developing learners who are flexible thinkers and successful problem solvers.},
  added-at = {2017-07-24T01:39:32.000+0200},
  author = {Ertmer, Peggy A. and Simons, Krista D.},
  biburl = {https://www.bibsonomy.org/bibtex/2a3390f4422bf0b20d7fdd8970f1a19c5/vngudivada},
  doi = {10.7771/1541-5015.1005},
  interhash = {8342995088313c74bdc656b8b28596b3},
  intrahash = {a3390f4422bf0b20d7fdd8970f1a19c5},
  journal = {Interdisciplinary Journal of Problem-Based Learning},
  keywords = {K-12 PBL STEM},
  month = may,
  number = 1,
  publisher = {Purdue University},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Jumping the {PBL} Implementation Hurdle: Supporting the Efforts of K{\textendash}12 Teachers},
  url = {https://doi.org/10.7771%2F1541-5015.1005},
  volume = 1,
  year = 2006
}

@article{wang2016learning,
  abstract = {As a major breakthrough in artificial intelligence, deep learning has achieved very impressive success in solving grand challenges in many fields including speech recognition, natural language processing, computer vision, image and video processing, and multimedia. This article provides a historical overview of deep learning and focus on its applications in object recognition, detection, and segmentation, which are key challenges of computer vision and have numerous applications to images and videos. The discussed research topics on object recognition include image classification on ImageNet, face recognition, and video classification. The detection part covers general object detection on ImageNet, pedestrian detection, face landmark detection (face alignment), and human landmark detection (pose estimation). On the segmentation side, the article discusses the most recent progress on scene labeling, semantic segmentation, face parsing, human parsing and saliency detection. Object recognition is considered as whole-image classification, while detection and segmentation are pixelwise classification tasks. Their fundamental differences will be discussed in this article. Fully convolutional neural networks and highly efficient forward and backward propagation algorithms specially designed for pixelwise classification task will be introduced. The covered application domains are also much diversified. Human and face images have regular structures, while general object and scene images have much more complex variations in geometric structures and layout. Videos include the temporal dimension. Therefore, they need to be processed with different deep models. All the selected domain applications have received tremendous attentions in the computer vision and multimedia communities. Through concrete examples of these applications, we explain the key points which make deep learning outperform conventional computer vision systems. (1) Different than traditional pattern recognition systems, which heavily rely on manually designed features, deep learning automatically learns hierarchical feature representations from massive training data and disentangles hidden factors of input data through multi-level nonlinear mappings. (2) Different than existing pattern recognition systems which sequentially design or train their key components, deep learning is able to jointly optimize all the components and crate synergy through close interactions among them. (3) While most machine learning models can be approximated with neural networks with shallow structures, for some tasks, the expressive power of deep models increases exponentially as their architectures go deep. Deep models are especially good at learning global contextual feature representation with their deep structures. (4) Benefitting from the large learning capacity of deep models, some classical computer vision challenges can be recast as high-dimensional data transform problems and can be solved from new perspectives. Finally, some open questions and future works regarding to deep learning in object recognition, detection, and segmentation will be discussed.

As a major breakthrough in artificial intelligence, deep learning has achieved impressive success on solving grand challenges in many fields including speech recognition, natural language processing, computer vision, image and video processing, and multimedia. This monograph provides a historical overview of deep learning and focuses on its applications in object recognition, detection, and segmentation, which are key challenges of computer vision and have numerous applications to images and videos.

Specifically the topics covered under object recognition include image classification on ImageNet, face recognition, and video classification. In detection, the monograph covers general object detection on ImageNet, pedestrian detection, face landmark detection (face alignment), and human landmark detection (pose estimation). Finally, within segmentation, it covers the most recent progress on scene labeling, semantic segmentation, face parsing, human parsing, and saliency detection. Concrete examples of these applications explain the key points that make deep learning outperform conventional computer vision systems.

Deep Learning in Object Recognition, Detection, and Segmentation provides a comprehensive introductory overview of a topic that is having major impact on many areas of research in signal processing, computer vision, and machine learning. This is a must-read for students and researchers new to these fields.},
  added-at = {2017-06-03T19:47:54.000+0200},
  author = {Wang, Xiaogang},
  biburl = {https://www.bibsonomy.org/bibtex/202f0a892edf40cdf1b036d5a5bce44a8/vngudivada},
  doi = {10.1561/2000000071},
  interhash = {ed59691fd5399d6e304fe2d2522f4eb5},
  intrahash = {02f0a892edf40cdf1b036d5a5bce44a8},
  issn = {1932-8346},
  journal = {Foundations and Trends in Signal Processing},
  keywords = {DIP DeepLearning},
  number = 4,
  pages = {217-382},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Deep Learning in Object Recognition, Detection, and Segmentation},
  url = {http://dx.doi.org/10.1561/2000000071},
  volume = 8,
  year = 2016
}

@article{hirschberg2015advances,
  abstract = {Natural language processing employs computational techniques for the purpose of learning, understanding, and producing human language content. Early computational approaches to language research focused on automating the analysis of the linguistic structure of language and developing basic technologies such as machine translation, speech recognition, and speech synthesis.Today's researchers refine and make use of such tools in real-world applications, creating spoken dialogue systems and speech-to-speech translation engines, mining social media for information about health or finance, and identifying sentiment and emotion toward products and services.We describe successes and challenges in this rapidly advancing area.},
  added-at = {2017-06-06T20:59:44.000+0200},
  author = {Hirschberg, Julia and Manning, Christopher D.},
  biburl = {https://www.bibsonomy.org/bibtex/24862a0bc4d9bc9b4e347ee7d6cb16f24/vngudivada},
  interhash = {e5e918e1d94924ee46b334b7c608d614},
  intrahash = {4862a0bc4d9bc9b4e347ee7d6cb16f24},
  journal = {Science},
  keywords = {NLP Survey},
  number = 6245,
  pages = {261 -- 266},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Advances in natural language processing},
  volume = 349,
  year = 2015
}

@book{goodfellow2016learning,
  abstract = {Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning.

The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models.

Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  added-at = {2017-06-26T02:44:53.000+0200},
  address = {Cambridge, Massachusetts},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  biburl = {https://www.bibsonomy.org/bibtex/2175f81afff897a68829e4d30c080a8fb/vngudivada},
  interhash = {62814dec510d5c55b0b38ad85a6c748d},
  intrahash = {175f81afff897a68829e4d30c080a8fb},
  isbn = {978-0262035613},
  keywords = {Book DeepLearning ML MachineLearning ParExcellence},
  privnote = {https://www.pdf-archive.com/2016/04/07/deep-learning/deep-learning.pdf},
  publisher = {The MIT Press},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Deep Learning},
  year = 2016
}

@book{steele2010beautiful,
  abstract = {Visualization is the graphic presentation of data -- portrayals meant to reveal complex information at a glance. Think of the familiar map of the New York City subway system, or a diagram of the human brain. Successful visualizations are beautiful not only for their aesthetic design, but also for elegant layers of detail that efficiently generate insight and new understanding.

This book examines the methods of two dozen visualization experts who approach their projects from a variety of perspectives -- as artists, designers, commentators, scientists, analysts, statisticians, and more. Together they demonstrate how visualization can help us make sense of the world.

Explore the importance of storytelling with a simple visualization exercise
Learn how color conveys information that our brains recognize before we're fully aware of it
Discover how the books we buy and the people we associate with reveal clues to our deeper selves
Recognize a method to the madness of air travel with a visualization of civilian air traffic
Find out how researchers investigate unknown phenomena, from initial sketches to published papers},
  added-at = {2017-06-27T04:28:41.000+0200},
  address = {Sebastopol, California},
  author = {Steele, Julie and Iliinsky, Noah P. N.},
  biburl = {https://www.bibsonomy.org/bibtex/21b1983bcf71547fe38fac8ef09c8172b/vngudivada},
  interhash = {fddb03d847aafca59c1053d0360b9a63},
  intrahash = {1b1983bcf71547fe38fac8ef09c8172b},
  isbn = {9781449379865},
  keywords = {Book Visualization},
  publisher = {O'Reilly},
  refid = {757869215},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Beautiful visualization: looking at data through the eyes of experts},
  year = 2010
}

@article{wilks2009natural,
  abstract = {The main argument of this paper is that Natural Language Processing (NLP) does, and will continue to, underlie the Semantic Web (SW), including its initial construction from unstructured sources like the World Wide Web (WWW), whether its advocates realise this or not. Chiefly, we argue, such NLP activity is the only way up to a defensible notion of meaning at conceptual levels (in the original SW diagram) based on lower level empirical computations over usage. Our aim is definitely not to claim logic-bad, NLP-good in any simple-minded way, but to argue that the SW will be a fascinating interaction of these two methodologies, again like the WWW (which has been basically a field for statistical NLP research) but with deeper content. Only NLP technologies (and chiefly information extraction) will be able to provide the requisite RDF knowledge stores for the SW from existing unstructured text databases in the WWW, and in the vast quantities needed. There is no alternative at this point, since a wholly or mostly hand-crafted SW is also unthinkable, as is a SW built from scratch and without reference to the WWW. We also assume that, whatever the limitations on current SW representational power we have drawn attention to here, the SW will continue to grow in a distributed manner so as to serve the needs of scientists, even if it is not perfect. The WWW has already shown how an imperfect artefact can become indispensable.

Natural Language Processing as a Foundation of the Semantic Web argues that Natural Language Processing (NLP) does, and will continue to, underlie the Semantic Web (SW), including its initial construction from unstructured sources like the World Wide Web, in several different ways, and whether its advocates realise this or not. Chiefly, it argues, such NLP activity is the only way up to a defensible notion of meaning at conceptual levels based on lower level empirical computations over usage. The claim being made is definitely not logic-bad, NLP-good in any simple-minded way, but that the SW will be a fascinating interaction of these two methodologies, like the WWW (which, as the authors explain, has been a fruitful field for statistical NLP research) but with deeper content. Only NLP technologies (and chiefly information extraction) will be able to provide the requisite resource description framework (RDF) knowledge stores for the SW from existing WWW (unstructured) text databases, and in the vast quantities needed. There is no alternative at this point, since a wholly or mostly hand-crafted SW is also unthinkable, as is a SW built from scratch and without reference to the WWW. It is also assumed here that, whatever the limitations on current SW representational power drawn attention to here, the SW will continue to grow in a distributed manner so as to serve the needs of scientists, even if it is not perfect. The WWW has already shown how an imperfect artefact can become indispensable. Natural Language Processing as a Foundation of the Semantic Web will appeal to researchers, practitioners and anyone with an interest in NLP, the philosophy of language, cognitive science, the Semantic Web and Web Science generally, as well as providing a magisterial and controversial overview of the history of artificial intelligence.},
  added-at = {2017-06-03T19:50:18.000+0200},
  author = {Wilks, Yorick and Brewster, Christopher},
  biburl = {https://www.bibsonomy.org/bibtex/20411804700881b5694a55143e2862508/vngudivada},
  doi = {10.1561/1800000002},
  interhash = {be60460ee94b1b3add2ca1a1198ed7c4},
  intrahash = {0411804700881b5694a55143e2862508},
  issn = {1555-077X},
  journal = {Foundations and Trends in Web Science},
  keywords = {NLP NLU SemanticWeb},
  number = {3–4},
  pages = {199-327},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Natural Language Processing as a Foundation of the Semantic Web},
  url = {http://dx.doi.org/10.1561/1800000002},
  volume = 1,
  year = 2009
}

@book{walker2015essential,
  abstract = {Like most good educational interventions, problem-based learning (PBL) did not grow out of theory, but out of a practical problem. Medical students were bored, dropping out, and unable to apply what they had learned in lectures to their practical experiences a couple of years later. Neurologist Howard S. Barrows reversed the sequence, presenting students with patient problems to solve in small groups and requiring them to seek relevant knowledge in an effort to solve those problems. Out of his work, PBL was born.The book is divided into four sections, each containing contributions by leaders in the field. Chapters in the first section focus on the structure of PBL and the critical elements of the approach. Articulating the underlying problems to be addressed, the role of facilitators, and the process to be followed in achieving a successful PBL intervention are all discussed. The second section explores how PBL has been adapted to function in areas outside medicine, from climate science to teacher education, while the third section explores how the methodology has been combined with other approaches to teaching and learning, such as learning by design and project-based learning. The fourth section assesses the impact of PBL techniques on improving both research and teaching. An epilogue speculates about the future of PBL, synthesizing contributions from the previous chapters and suggesting key themes for further exploration},
  added-at = {2017-07-24T01:32:06.000+0200},
  address = {West Lafayette, Indiana},
  author = {Walker, Andrew and Leary, Heather and Hmelo-Silver, Cindy E. and Ertmer, Peggy A.},
  biburl = {https://www.bibsonomy.org/bibtex/2797c5944cb3b1e28dfa5af58f2780743/vngudivada},
  interhash = {63682dc314ecc130a06c44e481709532},
  intrahash = {797c5944cb3b1e28dfa5af58f2780743},
  keywords = {Book PBL},
  publisher = { Purdue University Press},
  refid = {979880961},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Essential readings in problem-based learning: exploring and extending the legacy of Howard S. Barrows},
  year = 2015
}

@book{ross2014first,
  abstract = {A First Course in Probability, Ninth Edition, features clear and intuitive explanations of the mathematics of probability theory, outstanding problem sets, and a variety of diverse examples and applications. This book is ideal for an upper-level undergraduate or graduate level introduction to probability for math, science, engineering and business students. It assumes a background in elementary calculus.},
  added-at = {2017-06-16T13:32:03.000+0200},
  address = {Boston, MA},
  author = {Ross, Sheldon M.},
  biburl = {https://www.bibsonomy.org/bibtex/265f22896e69fa109f3f52fc5d6fdf5bc/vngudivada},
  edition = {Nineth},
  interhash = {80bd9c497db97ee59662a99ebb8f9fb5},
  intrahash = {65f22896e69fa109f3f52fc5d6fdf5bc},
  isbn = {032179477X 9780321794772},
  keywords = {ML Probability},
  publisher = {Pearson},
  refid = {907018160},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {A first course in probability},
  year = 2014
}

@phdthesis{greenwood2005opendomain,
  abstract = {Question answering aims to develop techniques that can go beyond the retrieval of relevant documents in order to return exact answers to natural language questions, such as “How tall is the Eiffel Tower?”, “Which cities have a subway system?”, and “Who is Alberto Tomba?”. Answering natural language questions requires more complex processing of text than employed by current information retrieval systems. A number of question answering systems have been developed which are capable of carrying out the processing required to achieve high levels of accuracy. However, little work has been reported on techniques for quickly finding exact answers.


This thesis investigates a number of novel techniques for performing open-domain question answering. Investigated techniques include: manual and automatically constructed question analysers, document retrieval specifically for question answering, semantic type answer extraction, answer extraction via automatically acquired surface matching text patterns, principled target processing combined with document retrieval for definition questions, and various approaches to sentence simplification which aid in the generation of concise definitions.


The novel techniques in this thesis are combined to create two end-to-end question answering systems which allow answers to be found quickly. AnswerFinder answers factoid questions such as “When was Mozart born?”, whilst Varro builds definitions for terms such as “aspirin”, “Aaron Copland”, and “golden parachute”. Both systems allow users to find answers to their questions using web documents retrieved by Google™. Together these two systems demonstrate that the techniques developed in this thesis can be successfully used to provide quick effective open-domain question answering.},
  added-at = {2017-06-03T22:03:59.000+0200},
  author = {Greenwood, Mark Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/260f3659c86f4bbe4a606282622793dba/vngudivada},
  interhash = {cf62d1d77c3586b866e9efb6b8aea2fc},
  intrahash = {60f3659c86f4bbe4a606282622793dba},
  keywords = {IR NLP QASystem},
  school = {University of Sheffield},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Open-Domain Question Answering},
  year = 2005
}

@article{merz2011latent,
  abstract = {Interactions among the dimensions of the Five Factor Model (FFM) have not typically been evaluated in mental health research, with the extant literature focusing on bivariate relationships with psychological constructs of interest. This study used latent profile analysis to mimic higher-order interactions to identify homogenous personality profiles using the FFM, and also examined relationships between resultant profiles and affect, self-esteem, depression, anxiety, and coping efficacy. Participants (N = 371) completed self-report and daily diary questionnaires. A 3-profile solution provided the best fit to the data; the profiles were characterized as well-adjusted, reserved, and excitable. The well-adjusted group reported better psychological functioning in validation analyses. The reserved and excitable groups differed on anxiety, with the excitable group reporting generally higher anxiety than the reserved group. Latent profile analysis may be a parsimonious way to model personality heterogeneity.},
  added-at = {2017-07-23T23:56:31.000+0200},
  author = {Merz, Erin L. and Roesch, Scott C.},
  biburl = {https://www.bibsonomy.org/bibtex/2412245d2e1baa1e185f57a56206ad218/vngudivada},
  doi = {10.1016/j.paid.2011.07.022},
  interhash = {5806644ec67ad50f9e0c4ce9eecd0256},
  intrahash = {412245d2e1baa1e185f57a56206ad218},
  journal = {Personality and Individual Differences},
  keywords = {Personality STEM Trait},
  month = dec,
  number = 8,
  pages = {915--919},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {A latent profile analysis of the Five Factor Model of personality: Modeling trait interactions},
  url = {https://doi.org/10.1016%2Fj.paid.2011.07.022},
  volume = 51,
  year = 2011
}

@book{pearl1991probabilistic,
  abstract = {Textbook offers an accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. For graduate-level courses in AI, operations research, and applied probability.},
  added-at = {2017-06-05T23:34:31.000+0200},
  address = {New York, NY},
  author = {Pearl, Judea},
  biburl = {https://www.bibsonomy.org/bibtex/2bf7fe6cbcd48ad73b2381d30f0df3dbe/vngudivada},
  interhash = {9f8ac7d474f314a59e092b96848796cb},
  intrahash = {bf7fe6cbcd48ad73b2381d30f0df3dbe},
  keywords = {BayesianMethods Book CausalInference PGM},
  publisher = {Morgan Kaufmann Publishers},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference},
  year = 1991
}

@book{pinker2010language,
  abstract = {A three-year-old toddler is "a grammatical genius"--master of most constructions, obeying adult rules of language. To Pinker, a Massachusetts Institute of Technology psycholinguist, the explanation for this miracle is that language is an instinct, an evolutionary adaptation that is partly "hard-wired" into the brain and partly learned. In this exciting synthesis--an entertaining, totally accessible study that will regale language lovers and challenge professionals in many disciplines--Pinker builds a bridge between "innatists" like MIT linguist Noam Chomsky, who hold that infants are biologically programmed for language, and "social interactionists" who contend that they acquire it largely from the environment. If Pinker is right, the origins of language go much further back than 30,000 years ago (the date most commonly given in textbooks)--perhaps to Homo habilis , who lived 2.5 million years ago, or even eons earlier. Peppered with mind-stretching language exercises, the narrative first unravels how babies learn to talk and how people make sense of speech. Professor and co-director of MIT's Center for Cognitive Science, Pinker demolishes linguistic determinism, which holds that differences among languages cause marked differences in the thoughts of their speakers. He then follows neurolinguists in their quest for language centers in the brain and for genes that might help build brain circuits controlling grammar and speech. Pinker also argues that claims for chimpanzees' acquisition of language (via symbols or American Sign Language) are vastly exaggerated and rest on skimpy data. Finally, he takes delightful swipes at "language mavens" like William Safire and Richard Lederer, accusing them of rigidity and of grossly underestimating the average person's language skills. Pinker's book is a beautiful hymn to the infinite creative potential of language.},
  added-at = {2017-07-05T00:58:16.000+0200},
  address = {New York, NY},
  author = {Pinker, Steven},
  biburl = {https://www.bibsonomy.org/bibtex/21afc4e3676210b0a3c6396a191dbe718/vngudivada},
  interhash = {b11c3d224a9bd07563070d93032370c5},
  intrahash = {1afc4e3676210b0a3c6396a191dbe718},
  isbn = {978-0061336461},
  keywords = {Book Linguistics},
  publisher = {Harper Perennial},
  refid = {705785671},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {The language instinct: how the mind creates language},
  year = 2010
}

@article{rhodes2011creating,
  abstract = {In August 2009, a new urban public high school featuring project-based science, technology, engineering, and mathematics (STEM) education opened with a population of African-American, low-income, and special needs students. A planning team comprised of lead teachers and the school principal sought to create a positive school culture with a clear vision and core values that would engender relational trust, a strong sense of community, and principal and teacher co-leadership. The culture was to be supported by social structures instituted through teams, professional development, student orientations, venues for instructional innovation, and informal gathering places. This account, written by the principal, a teacher, and a university researcher, presents a first-hand narrative of how the school culture was created.},
  added-at = {2017-07-23T20:29:35.000+0200},
  author = {Rhodes, Virginia and Stevens, Douglas and Hemmings, Annette},
  biburl = {https://www.bibsonomy.org/bibtex/2bd8adb3c5945ca51813b7eaad0e061dd/vngudivada},
  comment = {Volume 94, Number 3, Spring 2011},
  interhash = {c9b4594aaa95e359eae6d4da6e261dc0},
  intrahash = {bd8adb3c5945ca51813b7eaad0e061dd},
  journal = {The High School Journal},
  keywords = {HighSchool STEM},
  number = 3,
  pages = {82--94},
  privnote = {Volume 94, Number 3, Spring 2011},
  publisher = {The University of North Carolina Press},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Creating Positive Culture in a New Urban High School},
  url = {https://muse.jhu.edu/article/433343},
  volume = 94,
  year = 2011
}

@misc{distributionscoreteam2009handbook,
  added-at = {2017-06-11T22:06:32.000+0200},
  author = {distributions Core Team, R-forge},
  biburl = {https://www.bibsonomy.org/bibtex/2a0f9fdc652748b538a34e54599e1c7af/vngudivada},
  interhash = {efa76401a0dfe3984535ee4a4dcb7a64},
  intrahash = {a0f9fdc652748b538a34e54599e1c7af},
  keywords = {Book Distributions Probability},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Handbook on probability distributions},
  year = 2009
}

@book{murphy2012machine,
  abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package -- PMTK (probabilistic modeling toolkit) -- that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  added-at = {2017-06-26T02:41:15.000+0200},
  address = {Cambridge, Massachusetts},
  author = {Murphy, Kevin},
  biburl = {https://www.bibsonomy.org/bibtex/294c5976b40f97a89988be5024e16ac60/vngudivada},
  interhash = {997486e4e92adad73560717a6a07bf82},
  intrahash = {94c5976b40f97a89988be5024e16ac60},
  isbn = {978-0262018029},
  keywords = {Book MachineLearning ParExcellence},
  publisher = {The MIT Press},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Machine Learning: A Probabilistic Perspective},
  year = 2012
}

@article{baroni2014count,
  abstract = {Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts.},
  added-at = {2017-07-08T21:10:31.000+0200},
  author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, Germán},
  biburl = {https://www.bibsonomy.org/bibtex/2cb7a3604218c1220dfd90e9c033ad4da/vngudivada},
  interhash = {3d2cadcbf34e520b64c0919d40686914},
  intrahash = {cb7a3604218c1220dfd90e9c033ad4da},
  keywords = {DeepLearning NLP word2vec},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors},
  year = 2014
}

@article{beier2008literature,
  added-at = {2017-08-20T20:18:24.000+0200},
  author = {Beier, Margaret E. and Rittmayer, Ashley D.},
  biburl = {https://www.bibsonomy.org/bibtex/29bdac195526bd6ec5fc53e1ecb119c0c/vngudivada},
  interhash = {8c5cf999cb150ab9f99b2babfe42b29a},
  intrahash = {9bdac195526bd6ec5fc53e1ecb119c0c},
  journal = {SWE-AWE-CASEE ARP Resources},
  keywords = {Motivation STEM},
  pages = {1 - 10},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Literature Overview: Motivational factors in STEM: Interest and self-concept},
  url = {http://www.engr.psu.edu/awe/misc/arps/arp_selfconcept_overview_122208.pdf},
  year = 2008
}

@phdthesis{hazley2016successful,
  abstract = {Computer Science (CS) and Computer Engineering (CE) fields typically have not been successful at recruiting or retaining women students. Research indicates several reasons for this shortage but mainly from three perspectives: social issues, exposure/prior knowledge and curriculum issues in K-12 settings. This mixed-methods research addresses a gap in the literature by investigating the motivation and self-regulation behaviors of successful female students who are studying computer science and computer engineering. The findings in phase one of this study indicated that learning and performance approach goals predicted adaptive strategic self-regulation behaviors including strategy use, knowledge building and engagement. Learning avoidance goals predicted lack of regulation. Task approach goals predicted knowledge building and engagement (each negatively) and task avoid goals predicted strategy use, knowledge building and engagement (each negatively). Engagement positively predicted course grades while lack of regulation negatively predicted course grades. Learning avoidance had a significant negative indirect effect on course grades through lack of regulation. Learning approach was associated with better regulation (i.e., lower lack of regulation) and higher grades. Performance approach had a significant positive indirect effect on course grades through engagement. Performance avoidance had a significant negative indirect effect on course grades
through lack of regulation. Task avoidance had a significant negative indirect effect on course grades through engagement. Task approach, contrary to the hypothesized direct of effect, had a significant negative indirect effect on course grades through engagement.

Four themes emerged during the qualitative phase of this study. These included: The Gender Effect (students gender impacted their behavior and confidence), Lack of CS \& CE experience (students' prior knowledge impacted how they studied and how competent they felt), Key Influences (students were influenced by the work and academic environment, family members, classmates and professors) and Problem Solvers (students responded to the rigor of the program by being aggressive problem solvers). This research has implications for how we can support other female students who have the potential or desire to excel in computing fields. K-12 school systems and undergraduate programs should take steps to create intentional programming that introduce, educate and help female students persist in computer science and computer engineering programs. Educators and parents should engage female students in math and science coursework and extracurricular activities early in life, especially where formal CS or CE programs do not exist.},
  added-at = {2017-08-20T21:12:00.000+0200},
  address = {Lincoln, Nebraska},
  author = {Hazley, Melissa P.},
  biburl = {https://www.bibsonomy.org/bibtex/2ffe2b47e048ff73a707e796ef919050e/vngudivada},
  interhash = {6b42fbbcc36df1e731fd973a4fb75845},
  intrahash = {ffe2b47e048ff73a707e796ef919050e},
  keywords = {ComputerScience Motivation STEM},
  month = {april},
  school = { University of Nebraska},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Successful Female Students in Undergraduate Computer Science and Computer Engineering: Motivation, Self-regulation, and Qualitative Characteristics},
  url = {http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1263&context=cehsdiss},
  year = 2016
}

@book{lipschultz2012schaums,
  abstract = {Linear algebra has in recent years become an essential part of the mathematical background required by
mathematicians and mathematics teachers, engineers, computer scientists, physicists, economists, and
statisticians, among others. This requirement reflects the importance and wide applications of the subject
matter.
This book is designed for use as a textbook for a formal course in linear algebra or as a supplement to all
current standard texts. It aims to present an introduction to linear algebra which will be found helpful to all
readers regardless of their fields of specification. More material has been included than can be covered in most
first courses. This has been done to make the book more flexible, to provide a useful book of reference, and to
stimulate further interest in the subject.
Each chapter begins with clear statements of pertinent definitions, principles, and theorems together with
illustrative and other descriptive material. This is followed by graded sets of solved and supplementary
problems. The solved problems serve to illustrate and amplify the theory, and to provide the repetition of basic
principles so vital to effective learning. Numerous proofs, especially those of all essential theorems, are
included among the solved problems. The supplementary problems serve as a complete review of the material
of each chapter.
The first three chapters treat vectors in Euclidean space, matrix algebra, and systems of linear equations.
These chapters provide the motivation and basic computational tools for the abstract investigations of vector
spaces and linear mappings which follow. After chapters on inner product spaces and orthogonality and on
determinants, there is a detailed discussion of eigenvalues and eigenvectors giving conditions for representing
a linear operator by a diagonal matrix. This naturally leads to the study of various canonical forms,
specifically, the triangular, Jordan, and rational canonical forms. Later chapters cover linear functions and
the dual space V*, and bilinear, quadratic, and Hermitian forms. The last chapter treats linear operators on
inner product spaces.
The main changes in the fourth edition have been in the appendices. First of all, we have expanded
Appendix A on the tensor and exterior products of vector spaces where we have now included proofs on the
existence and uniqueness of such products. We also added appendices covering algebraic structures, including
modules, and polynomials over a field. Appendix D, ‘‘Odds and Ends,’’ includes the Moore–Penrose
generalized inverse which appears in various applications, such as statistics. There are also many additional
solved and supplementary problems.},
  added-at = {2017-06-04T05:10:22.000+0200},
  address = {New York, NY},
  author = {Lipschultz, Seymour},
  biburl = {https://www.bibsonomy.org/bibtex/22c3ddce5641e20591c6c1a89c04f2f82/vngudivada},
  interhash = {7a91fd54218ea8208c3b624aa77b49d3},
  intrahash = {2c3ddce5641e20591c6c1a89c04f2f82},
  keywords = {Book LinearAlgebra SchaumsOutline},
  publisher = {Mcgraw-Hill},
  refid = {870313970},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Schaum's outline of linear algebra},
  year = 2012
}

@article{smith2012trying,
  abstract = {Feeling like one exerts more effort than others may influence women's feelings of belonging with science, technology, engineering, and math (STEM) and impede their motivation. In Study 1, women STEM graduate students perceived they exerted more effort than peers to succeed. For women, but not men, this effort expenditure perception predicted a decreased sense of belonging, which in turn decreased motivation. Study 2 tested whether the male-dominated status of a field triggers such effort expectations. We created a fictional ``eco-psychology'' graduate program, which when depicted as male-dominated resulted in women expecting to exert relatively more effort and decreased their interest in pursuing the field. Study 3 found emphasizing effort as expected (and normal) to achieve success elevated women's feelings of belonging and future motivation. Results suggest effort expenditure perceptions are an indicator women use to assess their fit in STEM. Implications for enhancing women's participation in STEM are discussed.},
  added-at = {2017-08-20T20:20:51.000+0200},
  author = {Smith, Jessi L. and Lewis, Karyn L. and Hawthorne, Lauren and Hodges, Sara D.},
  biburl = {https://www.bibsonomy.org/bibtex/2096e61372f60a3a51feefc9ed75e22d5/vngudivada},
  doi = {10.1177/0146167212468332},
  interhash = {5773b89aea88985fa0f7e42bf4df6eab},
  intrahash = {096e61372f60a3a51feefc9ed75e22d5},
  journal = {Personality and Social Psychology Bulletin},
  keywords = {Motivation STEM},
  month = nov,
  number = 2,
  pages = {131--143},
  publisher = {{SAGE} Publications},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {When Trying Hard Isn't Natural},
  url = {https://doi.org/10.1177%2F0146167212468332},
  volume = 39,
  year = 2012
}

@book{madsen2014datadriven,
  abstract = {Healthcare is changing, and data is the catalyst
Data is taking over in a powerful way, and it's revolutionizing the healthcare industry. You have more data available than ever before, and applying the right analytics can spur growth. Benefits extend to patients, providers, and board members, and the technology can make centralized patient management a reality. Despite the potential for growth, many in the industry and government are questioning the value of data in health care, wondering if it's worth the investment.

Data-Driven Healthcare: How Analytics and BI are Transforming the Industry tackles the issue and proves why BI is not only worth it, but necessary for industry advancement. Healthcare BI guru Laura Madsen challenges the notion that data have little value in healthcare, and shows how BI can ease regulatory reporting pressures and streamline the entire system as it evolves. Madsen illustrates how a data-driven organization is created, and how it can transform the industry.

Learn why BI is a boon to providers
Create powerful infographics to communicate data more effectively
Find out how Big Data has transformed other industries, and how it applies to healthcare
Data-Driven Healthcare: How Analytics and BI are Transforming the Industry provides tables, checklists, and forms that allow you to take immediate action in implementing BI in your organization. You can't afford to be behind the curve. The industry is moving on, with or without you. Data-Driven Healthcare: How Analytics and BI are Transforming the Industry is your guide to utilizing data to advance your operation in an industry where data-fueled growth will be the new norm.},
  added-at = {2017-06-26T18:40:11.000+0200},
  address = {New York, NY},
  author = {Madsen, Laura B.},
  biburl = {https://www.bibsonomy.org/bibtex/2193ac29843acc6a2aa0840f4dc909b4e/vngudivada},
  interhash = {0f8b49a6ac88307ae4021f4ddf9ccda9},
  intrahash = {193ac29843acc6a2aa0840f4dc909b4e},
  isbn = {978-1118772218},
  keywords = {Book Healthcare},
  publisher = {John Wiley},
  refid = {936319823},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Data-driven healthcare: how analytics and BI are transforming the industry},
  year = 2014
}

@inbook{lopez2005aqualog,
  abstract = {As semantic markup becomes ubiquitous, it will become important to be able to ask queries and obtain answers, using natural language (NL) expressions, rather than the keyword-based retrieval mechanisms used by the current search engines. AquaLog is a portable question-answering system which takes queries expressed in natural language and an ontology as input and returns answers drawn from the available semantic markup. We say that AquaLog is portable, because the configuration time required to customize the system for a particular ontology is negligible. AquaLog combines several powerful techniques in a novel way to make sense of NL queries and to map them to semantic markup. Moreover it also includes a learning component, which ensures that the performance of the system improves over time, in response to the particular community jargon used by the end users. In this paper we describe the current version of the system, in particular discussing its portability, its reasoning capabilities, and its learning mechanism.},
  added-at = {2017-06-03T22:00:05.000+0200},
  address = {Berlin, Germany},
  author = {Lopez, Vanessa and Pasin, Michele and Motta, Enrico},
  biburl = {https://www.bibsonomy.org/bibtex/211d7a8a71e4c7e9a1d0eb9aeec7be395/vngudivada},
  booktitle = {The Semantic Web: Research and Applications: Second European Semantic Web Conference, ESWC 2005, Heraklion, Crete, Greece, May 29--June 1, 2005. Proceedings},
  doi = {10.1007/11431053_37},
  editor = {G{\'o}mez-P{\'e}rez, Asunci{\'o}n and Euzenat, J{\'e}r{\^o}me},
  interhash = {4e479b5f89b7cbb2d3a0e077df2d78de},
  intrahash = {11d7a8a71e4c7e9a1d0eb9aeec7be395},
  isbn = {978-3-540-31547-6},
  keywords = {IR NLP QASystem},
  pages = {546--562},
  publisher = {Springer},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {AquaLog: An Ontology-Portable Question Answering System for the Semantic Web},
  url = {http://dx.doi.org/10.1007/11431053_37},
  year = 2005
}

@article{nenkova2011automatic,
  abstract = {It has now been 50 years since the publication of Luhn’s seminal paper on automatic summarization. During these years the practical need for automatic summarization has become increasingly urgent and numerous papers have been published on the topic. As a result, it has become harder to find a single reference that gives an overview of past efforts or a complete view of summarization tasks and necessary system components. This article attempts to fill this void by providing a comprehensive overview of research in summarization, including the more traditional efforts in sentence extraction as well as the most novel recent approaches for determining important content, for domain and genre specific summarization and for evaluation of summarization. We also discuss the challenges that remain open, in particular the need for language generation and deeper semantic understanding of language that would be necessary for future advances in the field.},
  added-at = {2017-06-06T01:20:09.000+0200},
  author = {Nenkova, Ani and McKeown, Kathleen},
  biburl = {https://www.bibsonomy.org/bibtex/2f39e8cfc53f29e6524bf044b6961b35c/vngudivada},
  doi = {10.1561/1500000015},
  interhash = {1af64e334c84a6ead255b5f10e73a253},
  intrahash = {f39e8cfc53f29e6524bf044b6961b35c},
  journal = {Foundations and Trends in Information Retrieval},
  keywords = {NLP TextSummarization},
  number = {2–3},
  pages = {103-233},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Automatic Summarization},
  url = {http://dx.doi.org/10.1561/1500000015},
  volume = 5,
  year = 2011
}

@phdthesis{edzie2014exploring,
  abstract = {Nationally, the need for an increase in interest, enrollment, and degrees awarded from science, technology, engineering and mathematics (STEM) degree programs continues to suffer. While students are enrolling in collegiate STEM degree programs, it is not occurring at a rate that meets the workforce demand. In addition to the concern that there is not a sufficient amount of collegiate STEM majors, there is a concern over too few females enrolling and persisting in collegiate STEM degree programs.

This mixed methods sequential exploratory research study considered the factors that influence and motivate undergraduate female students to enroll and persist in collegiate STEM degree programs. The research study was conducted in four phases. The first phase of the study focused on exploring the factors that influenced first-year female freshmen to enroll in a collegiate STEM degree program. Qualitative data were collected from undergraduate females enrolled in a STEM degree program. The second phase, instrument development, involved developing a survey instrument that consisted of 15 questions. The survey included a combination of (a) the Motivated Student Learning Questionnaire, (b) the questions developed from the findings from the qualitative phase, and (c) a demographic section. In the third phase of the research study, quantitative data collection, the survey instrument was administered to a sample of undergraduate female STEM majors. The fourth phase integrated the findings from the qualitative and quantitative phases.


Five factors were considered as being significant to undergraduate female STEM majors when choosing a collegiate degree program: (a) helping others in their career, (b) having access to pre-collegiate STEM exposure, (c) obtaining information about STEM career pathways, (d) establishing relationships with influential stakeholders, and (e) developing confidence in math and science. The findings from this study illustrate the role of K-12 STEM educators, pre-collegiate STEM outreach programs, and STEM education policymakers in influencing and motivating female students to enroll and persist in collegiate STEM degree programs.},
  added-at = {2017-08-20T21:04:04.000+0200},
  address = {Lincoln, Nebraska},
  author = {Edzie, Rosemary L.},
  biburl = {https://www.bibsonomy.org/bibtex/2ee77d6597f19ae73d61b0c6987190633/vngudivada},
  interhash = {3c31e47755c5d181b41fa2c8b7201891},
  intrahash = {ee77d6597f19ae73d61b0c6987190633},
  keywords = {sys:relevantfor:ecu-cc-research Motivation STEM Women},
  month = may,
  school = { University of Nebraska},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Exploring the Factors that Influence and Motivate Female Students to Enroll and Persist in Collegiate STEM Degree Programs: A Mixed Methods Study},
  url = {http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1175&context=cehsedaddiss},
  year = 2014
}

@article{li2014semantic,
  abstract = {Relevance is the most important factor to assure users’ satisfaction in search and the success of a search engine heavily depends on its performance on relevance. It has been observed that most of the dissatisfaction cases in relevance are due to term mismatch between queries and documents (e.g., query “NY times” does not match well with a document only containing “New York Times”), because term matching, i.e., the bag-of-words approach, still functions as the main mechanism of modern search engines. It is not exaggerated to say, therefore, that mismatch between query and document poses the most critical challenge in search. Ideally, one would like to see query and document match with each other, if they are topically relevant. Recently, researchers have expended significant effort to address the problem. The major approach is to conduct semantic matching, i.e., to perform more query and document understanding to represent the meanings of them, and perform better matching between the enriched query and document representations. With the availability of large amounts of log data and advanced machine learning techniques, this becomes more feasible and significant progress has been made recently. This survey gives a systematic and detailed introduction to newly developed machine learning technologies for query document matching (semantic matching) in search, particularly web search. It focuses on the fundamental problems, as well as the state-of-the-art solutions of query document matching on form aspect, phrase aspect, word sense aspect, topic aspect, and structure aspect. The ideas and solutions explained may motivate industrial practitioners to turn the research results into products. The methods introduced and the discussions made may also stimulate academic researchers to find new research directions and approaches. Matching between query and document is not limited to search and similar problems can be found in question answering, online advertising, cross-language information retrieval, machine translation, recommender systems, link prediction, image annotation, drug design, and other applications, as the general task of matching between objects from two different spaces. The technologies introduced can be generalized into more general machine learning techniques, which is referred to as learning to match in this survey.},
  added-at = {2017-06-03T21:35:53.000+0200},
  author = {Li, Hang and Xu, Jun},
  biburl = {https://www.bibsonomy.org/bibtex/29fd12f52c1b7a256b9689092b4811777/vngudivada},
  doi = {10.1561/1500000035},
  interhash = {c5e8cb032e644c1eb34044b252fe8b2a},
  intrahash = {9fd12f52c1b7a256b9689092b4811777},
  journal = {Foundations and Trends in Information Retrieval},
  keywords = {IR Search SemanticMatching},
  number = 5,
  pages = {343-469},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Semantic Matching in Search},
  url = {http://dx.doi.org/10.1561/1500000035},
  volume = 7,
  year = 2014
}

@article{penney2011review,
  abstract = {This article reviews the literature linking the Big Five personality traits with job performance in order to identify the most promising directions for future research. Specifically, we recommend expanding the criterion domain to include internal and external service-oriented behavior as well as adaptive performance. We also review situational moderators of the personality–performance relationship and suggest additional moderators at the task, social, and organizational levels. Finally, we discuss trait interactions and explain why we expect that our capability to predict employee behavior will be considerably improved by considering the interaction among traits.},
  added-at = {2017-07-24T00:02:52.000+0200},
  author = {Penney, Lisa M. and David, Emily and Witt, L.A.},
  biburl = {https://www.bibsonomy.org/bibtex/2f519130c860f11fc1dc59cd1bd2a2499/vngudivada},
  doi = {10.1016/j.hrmr.2010.10.005},
  interhash = {4b2118563015ae7076b230aaa6d9b0a5},
  intrahash = {f519130c860f11fc1dc59cd1bd2a2499},
  journal = {Human Resource Management Review},
  keywords = {JobPerrformance Personality Trait},
  month = dec,
  number = 4,
  pages = {297--310},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {A review of personality and performance: Identifying boundaries, contingencies, and future research directions},
  url = {https://doi.org/10.1016%2Fj.hrmr.2010.10.005},
  volume = 21,
  year = 2011
}

@book{mcelreath2015statistical,
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds readers’ knowledge of and confidence in statistical modeling. Reflecting the need for even minor programming in today’s model-based statistics, the book pushes readers to perform step-by-step calculations that are usually automated. This unique computational approach ensures that readers understand enough of the details to make reasonable choices and interpretations in their own modeling work.

The text presents generalized linear multilevel models from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. It covers from the basics of regression to multilevel models. The author also discusses measurement error, missing data, and Gaussian process models for spatial and network autocorrelation.

By using complete R code examples throughout, this book provides a practical foundation for performing statistical inference. Designed for both PhD students and seasoned professionals in the natural and social sciences, it prepares them for more advanced or specialized statistical modeling.

Web Resource
The book is accompanied by an R package (rethinking) that is available on the author’s website and GitHub. The two core functions (map and map2stan) of this package allow a variety of statistical models to be constructed from standard model formulas.},
  added-at = {2017-06-26T01:03:47.000+0200},
  address = {Boca Raton, FL},
  author = {McElreath, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/2686527e27f114d7509ecfaa63e7ef579/vngudivada},
  interhash = {8557e096d81ffc63ebaea35fba8722c0},
  intrahash = {686527e27f114d7509ecfaa63e7ef579},
  isbn = {978-1482253443},
  keywords = {BayesianMethods Book},
  publisher = {Chapman and Hall/CRC},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan},
  year = 2015
}

@book{walck2007handbook,
  abstract = {In experimental work e.g. in physics one often encounters problems where a standard statistical probability density function is applicable. It is often of great help to be able to handle these in different ways such as calculating probability contents or generating random numbers. For these purposes there are excellent text-books in statistics e.g. the classical work of Maurice G. Kendall and Alan Stuart [1,2] or more modern text-books as [3] and others. Some books are particularly aimed at experimental physics or even specifically at particle physics [4,5,6,7,8]. Concerning numerical methods a valuable references worth mentioning is [9] which has been surpassed by a new edition [10]. Also hand-books, especially [11], has been of great help throughout. However, when it comes to actual applications it often turns out to be hard to find detailed explanations in the literature ready for implementation. This work has been collected over many years in parallel with actual experimental work. In this way some material may be “historical” and sometimes be na¨ıve and have somewhat clumsy solutions not always made in the mathematically most stringent may. We apologize for this but still hope that it will be of interest and help for people who is struggling to find methods to solve their statistical problems in making real applications and not only learning statistics as a course. Even if one has the skill and may be able to find solutions it seems worthwhile to have easy and fast access to formulæ ready for application. Similar books and reports exist e.g. [12,13] but we hope the present work may compete in describing more distributions, being more complete, and including more explanations on relations given. The material could most probably have been divided in a more logical way but we have chosen to present the distributions in alphabetic order. In this way it is more of a hand-book than a proper text-book. After the first release the report has been modestly changed. Minor changes to correct misprints is made whenever found. In a few cases subsections and tables have been added. These alterations are described on page 182. In October 1998 the first somewhat bigger revision was made where in particular a lot of material on the non-central sampling distributions were added.},
  added-at = {2017-06-11T22:03:56.000+0200},
  author = {Walck, Christian},
  biburl = {https://www.bibsonomy.org/bibtex/2166943bdefe170fdc5399f14b4f2ff17/vngudivada},
  interhash = {4de9af4427e57638a31c50255d6e64a1},
  intrahash = {166943bdefe170fdc5399f14b4f2ff17},
  keywords = {Book Probability},
  publisher = {University of Stockholm},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Hand-book on statistical distributions for
experimentalists},
  year = 2007
}

@article{prager2007opendomain,
  abstract = {Open-Domain Question Answering is an introduction to the field of Question Answering (QA). It covers the basic principles of QA along with a selection of systems that have exhibited interesting and significant techniques, so it serves more as a tutorial than as an exhaustive survey of the field. Starting with a brief history of the field, it goes on to describe the architecture of a QA system before analysing in detail some of the specific approaches that have been successfully deployed by academia and industry designing and building such systems. Open-Domain Question Answering is both a guide for beginners who are embarking on research in this area, and a useful reference for established researchers and practitioners in this field.},
  added-at = {2017-06-03T21:56:48.000+0200},
  author = {Prager, John},
  biburl = {https://www.bibsonomy.org/bibtex/2df52fba3cec55100a4ed76c9a5469848/vngudivada},
  doi = {10.1561/1500000001},
  interhash = {7b1e6f90008776393186fe343c6e0157},
  intrahash = {df52fba3cec55100a4ed76c9a5469848},
  issn = {1554-0669},
  journal = {Foundations and Trends® in Information Retrieval},
  keywords = {IR QASystem},
  number = 2,
  pages = {91-231},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Open-Domain Question–Answering},
  url = {http://dx.doi.org/10.1561/1500000001},
  volume = 1,
  year = 2007
}

@article{dave2014computational,
  abstract = {Computational Advertising, popularly known as online advertising or Web advertising, refers to finding the most relevant ads matching a particular context on the Web. The context depends on the type of advertising and could mean – content where the ad is shown, the user who is viewing the ad or the social network of the user. Computational Advertising (CA) is a scientific sub-discipline at the intersection of information retrieval, statistical modeling, machine learning, optimization, large scale search and text analysis. The core problem addressed in Computational Advertising is of match-making between the ads and the context.

CA is prevalent in three major forms on the Web. One of the forms involves showing textual ads relevant to a query on the search page, known as Sponsored Search. On the other hand, showing textual ads relevant to a third party webpage content is known as Contextual Advertising. The third form of advertising also deals with the placement of ads on third party Web pages, but the ads in this form are rich multimedia ads – image, video, audio, flash. The business model with rich media ads is slightly different from the ones with textual ads. These ads are also called banner ads, and this form of advertising is known as Display Advertising.

Both Sponsored Search and Contextual Advertising involve retrieving relevant ads for different types of content (query and Web page). As ads are short and are mainly written to attract the user, retrieval of ads pose challenges like vocabulary mismatch between the query/content and the ad. Also, as the user’s probability of examining an ad decreases with the position of the ad in the ranked list, it is imperative to keep the best ads at the top positions. Display Advertising poses several challenges including modeling user behaviour and noisy page content and bid optimization on the advertiser’s side. Additionally, online advertising faces challenges like false bidding, click spam and ad spam. These challenges are prevalent in all forms of advertising. There has been a lot of research work published in different areas of CA in the last one and a half decade. The focus of this survey is to discuss the problems and solutions pertaining to the information retrieval, machine learning and statistics domain of CA. This survey covers techniques and approaches that deal with several issues mentioned above.

Research in Computational Advertising has evolved over time and currently continues both in traditional areas (vocabulary mismatch, query rewriting, click prediction) and recently identified areas (user targeting, mobile advertising, social advertising). In this study, we predominantly focus on the problems and solutions proposed in traditional areas in detail and briefly cover the emerging areas in the latter half of the survey. To facilitate future research, a discussion of available resources, list of public benchmark datasets and future directions of work is also provided in the end.},
  added-at = {2017-06-03T21:33:09.000+0200},
  author = {Dave, Kushal and Varma, Vasudeva},
  biburl = {https://www.bibsonomy.org/bibtex/2c0b2cde1449655020fd91e74c5ea9469/vngudivada},
  doi = {10.1561/1500000045},
  interhash = {cbdbf9f027f5d42fba28964e82af74ad},
  intrahash = {c0b2cde1449655020fd91e74c5ea9469},
  journal = {Foundations and Trends in Information Retrieval},
  keywords = {ComputationalAdvertising IR},
  number = {4–5},
  pages = {263-418},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Computational Advertising: Techniques for Targeting Relevant Ads},
  url = {http://dx.doi.org/10.1561/1500000045},
  volume = 8,
  year = 2014
}

@article{lehman2006preparing,
  abstract = {Calls for reform in science education stress the need for inquiry-based, integrative methods that provide students with opportunities to solve authentic problems. Project INSITE, a four-year professional development effort in Indiana, was designed to help teachers integrate problem-centered science methods in their classrooms. This approach was characterized by use of a meaningful driving question anchored in a real-world context; student-conducted investigations that resulted in the creation of products; collaboration among students, teachers, and the community; and use of technology as a tool for gathering and sharing information. Quantitative and qualitative evaluations of the project suggest that it was generally successful in promoting positive teacher perceptions, fostering learner-centered classroom approaches, and leading to implementation of inquiry-based science in many classrooms.},
  added-at = {2017-07-24T01:46:26.000+0200},
  author = {Lehman, James D. and George, Melissa and Buchanan, Peggy and Rush, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2e5ef8b5ff4f90971d3344088b326da51/vngudivada},
  doi = {10.7771/1541-5015.1007},
  interhash = {5c01f5c391daa80b4861d4af53b98f7d},
  intrahash = {e5ef8b5ff4f90971d3344088b326da51},
  journal = {Interdisciplinary Journal of Problem-Based Learning},
  keywords = {PBL STEM},
  month = may,
  number = 1,
  publisher = {Purdue University (bepress)},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Preparing Teachers to Use Problem-centered Inquiry-based Science:  Lessons from a Four-Year Professional Development Project},
  url = {https://doi.org/10.7771%2F1541-5015.1007},
  volume = 1,
  year = 2006
}

@article{kanhabua2015temporal,
  abstract = {Temporal dynamics and how they impact upon various components of information retrieval (IR) systems have received a large share of attention in the last decade. In particular, the study of relevance in information retrieval can now be framed within the so-called temporal IR approaches, which explain how user behavior, document content and scale vary with time, and how we can use them in our favor in order to improve retrieval effectiveness. This survey provides a comprehensive overview of temporal IR approaches, centered on the following questions: what are temporal dynamics, why do they occur, and when and how to leverage temporal information throughout the search cycle and architecture. We first explain the general and wide aspects associated to temporal dynamics by focusing on the web domain, from content and structural changes to variations of user behavior and interactions. Next, we pinpoint several research issues and the impact of such temporal characteristics on search, essentially regarding processing dynamic content, temporal query analysis and time-aware ranking. We also address particular aspects of temporal information extraction (for instance, how to timestamp documents and generate temporal profiles of text). To this end, we present existing temporal search engines and applications in related research areas, e.g., exploration, summarization, and clustering of search results, as well as future event retrieval and prediction, where the time dimension also plays an important role.},
  added-at = {2017-06-03T21:26:38.000+0200},
  author = {Kanhabua, Nattiya and Blanco, Roi and Nørvåg, Kjetil},
  biburl = {https://www.bibsonomy.org/bibtex/2600d4de8f9f3359d9f1b130a0f177577/vngudivada},
  doi = {10.1561/1500000043},
  interhash = {19f5fe69e368d764ea828add85e2ed28},
  intrahash = {600d4de8f9f3359d9f1b130a0f177577},
  journal = {Foundations and Trends® in Information Retrieval},
  keywords = {IR TemporalIR},
  number = 2,
  pages = {91-208},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Temporal Information Retrieval},
  url = {http://dx.doi.org/10.1561/1500000043},
  volume = 9,
  year = 2015
}

@article{ginsca2015credibility,
  abstract = {Credibility, as the general concept covering trustworthiness and expertise, but also quality and reliability, is strongly debated in philosophy, psychology, and sociology, and its adoption in computer science is therefore fraught with difficulties. Yet its importance has grown in the information access community because of two complementing factors: on one hand, it is relatively difficult to precisely point to the source of a piece of information, and on the other hand, complex algorithms, statistical machine learning, artificial intelligence, make decisions on behalf of the users, with little oversight from the users themselves. This survey presents a detailed analysis of existing credibility models from different information seeking research areas, with focus on the Web and its pervasive social component. It shows that there is a very rich body of work pertaining to different aspects and interpretations of credibility, particularly for different types of textual content (e.g., Web sites, blogs, tweets), but also to other modalities (videos, images, audio) and topics (e.g., health care). After an introduction placing credibility in the context of other sciences and relating it to trust, we argue for a quartic decomposition of credibility: expertise and trustworthiness, well documented in the literature and predominantly related to information source, and quality and reliability, raised to the status of equal partners because the source is often impossible to detect, and predominantly related to the content. The second half of the survey provides the reader with access points to the literature, grouped by research interests. Section 3 reviews general research directions: the factors that contribute to credibility assessment in human consumers of information; the models used to combine these factors; the methods to predict credibility. A smaller section is dedicated to informing users about the credibility learned from the data. Sections 4, 5, and 6 go further into details, with domain-specific credibility, social media credibility, and multimedia credibility, respectively. While each of them is best understood in the context of Sections 1 and 2, they can be read independently of each other. The last section of this survey addresses a topic not commonly considered under “credibility”: the credibility of the system itself, independent of the data creators. This is a topic of particular importance in domains where the user is professionally motivated and where there are no concerns about the credibility of the data (e.g. e-discovery and patent search). While there is little explicit work in this direction, we argue that this is an open research direction that is worthy of future exploration. Finally, as an additional help to the reader, an appendix lists the existing test collections that cater specifically to some aspect of credibility. Overall, this review will provide the reader with an organised and comprehensive reference guide to the state of the art and the problems at hand, rather than a final answer to the question of what credibility is for computer science. Even within the relatively limited scope of an exact science, such an answer is not possible for a concept that is itself widely debated in philosophy and social sciences.},
  added-at = {2017-06-03T21:42:58.000+0200},
  author = {Ginsca, Alexandru L. and Popescu, Adrian and Lupu, Mihai},
  biburl = {https://www.bibsonomy.org/bibtex/253e221292a4f60b3ac015fc1a1d49f40/vngudivada},
  doi = {10.1561/1500000046},
  interhash = {573e2e1ed7f8fd570d16425146857459},
  intrahash = {53e221292a4f60b3ac015fc1a1d49f40},
  issn = {1554-0669},
  journal = {Foundations and Trends® in Information Retrieval},
  keywords = {Credibility IR},
  number = 5,
  pages = {355-475},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Credibility in Information Retrieval},
  url = {http://dx.doi.org/10.1561/1500000046},
  volume = 9,
  year = 2015
}

@book{elton2016healthcare,
  abstract = {"Success in the new healthcare industry demands deep and profound strategic changes Healthcare Disrupted is a guide to developing vital new operating and business models in the healthcare industry's fast-changing landscape. Based on original research conducted by Accenture, this book explains how the biggest industry trends--consumer-focused care; value- and outcomes-based reimbursement; new digital consumers; the use of big data and advanced analytics; and rising numbers of strategic, collaborative, and inventive partnerships--are compelling biopharmaceutical and medical technology companies to change, and the implications of those changes on the rest of the industry. This book provides an informed, insightful view of the current state of the healthcare industry, as well as insight on what's to come. It digs deep into the issues to examine why and how established companies, and those new to the industry, must evolve to keep pace. It demonstrates how real-world data (from Electronic Medical Records, health wearables, the health Internet of Things (IoT), digital media, social media, and other sources) is combining with scalable technologies and advanced analytics to fundamentally change how healthcare is delivered. Healthcare Disrupted also reveals how this shift in healthcare delivery significantly improves patient and economic outcomes. The health care industry is increasingly focusing on: the patient as a partner; patient outcomes; and the quality, convenience, and affordability of care management. But does the industry really understand the implications of this shift in focus? To lead, and to thrive, many organizations will need to change their definition of innovation. They will need to lessen the time and lower the cost from discovery to market, and ensure the drugs they develop deliver a better patient outcome at an economically viable cost. This book provides practical insights on getting ahead of the curve by: Discovering how profoundly the healthcare industry is changing Finding your organization's place in the new order of modern health care Learning how to survive in the face of transformational trends Evolving your strategies for innovation, testing, providing care, and more The industry is turning toward an overall system in which value and patient outcomes are rewarded over volume, and performance is measured accordingly. Healthcare Disrupted shows how to adapt and stay relevant in the new age of digital medicine"--},
  added-at = {2017-06-26T18:24:49.000+0200},
  address = {New York, NY},
  author = {Elton, Jeff and O'Riordan, Anne},
  biburl = {https://www.bibsonomy.org/bibtex/22f41c5512af277fcd311f6de42ada403/vngudivada},
  interhash = {088cefe286ff803e3eac22f29afb988e},
  intrahash = {2f41c5512af277fcd311f6de42ada403},
  isbn = {978-1-119-17188-1},
  keywords = {Book Healthcare},
  publisher = {John Wiley},
  refid = {914221701},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Healthcare disrupted: next generation business models and strategies},
  year = 2016
}

@article{hung2006model,
  abstract = {Well-designed problems are crucial for the success of problem-based learning (PBL). Previous discussions about designing problems for PBL have been rather general and inadequate in guiding educators and practitioners to design effective PBL problems. This paper introduces the 3C3R PBL problem design model as a conceptual framework for systematically designing optimal PBL problems. The 3C3R model comprises two classes of components: core components and processing components. Core components—including content, context, and connection—support content and conceptual learning, while processing components—consisting of researching, reasoning, and reflecting—concern students’ cognitive processes and problem-solving skills. This paper discusses the model in terms of its theoretical basis, component functions, and the techniques used in designing PBL problems.},
  added-at = {2017-07-24T01:43:34.000+0200},
  author = {Hung, Woei},
  biburl = {https://www.bibsonomy.org/bibtex/2e88c5cd39932ea8e00b74f010c545e36/vngudivada},
  doi = {10.7771/1541-5015.1006},
  interhash = {0a88c16ca8a56eeedcbad9d9c20147fb},
  intrahash = {e88c5cd39932ea8e00b74f010c545e36},
  journal = {Interdisciplinary Journal of Problem-Based Learning},
  keywords = {PBL STEM},
  month = may,
  number = 1,
  publisher = {Purdue University},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {The 3C3R Model: A Conceptual Framework for Designing Problems in {PBL}},
  url = {https://doi.org/10.7771%2F1541-5015.1006},
  volume = 1,
  year = 2006
}

@article{craft2014passion,
  abstract = {Literature suggests that whilst creativity is frequently seen as ubiquitous and taken for granted (Dawson, Tan, & McWilliam, 2011; Livingston, 2010) there is evidence that creative approaches in higher education can be seen as unnecessary work (Chao, 2009; Clouder et al., 2008; Gibson, 2010; McWilliam et al., 2008), and creative teaching is not always recognised or valued (Clouder et al., 2008; Dawson et al., 2011; Gibson, 2010). Forming part of a cross-cultural study of creative teaching (although reporting on only one part of it), the research explored student and lecturer perspectives in four universities in England, Malaysia and Thailand, using mixed methods within an interpretive frame. This paper reports on findings from the English University site. Key elements of creative teaching in this site were having a passion for the subject and for using sensitised pedagogical strategies, driven by an awareness of student perspective and relationship. Crucial goals were fostering independent thinking; striving for equality through conversation and collaboration; and orchestrating for knowledge-building. The lecturers’ passion for the subject was a powerful engine for creative teaching across all academic disciplines spanning the arts, the humanities, and STEM (science, technology, engineering and mathematics) subjects.},
  added-at = {2017-07-24T00:57:47.000+0200},
  author = {Craft, Anna and Hall, Emese and Costello, Rebecca},
  biburl = {https://www.bibsonomy.org/bibtex/2cd16190548a12d876bae12e028ac1f0d/vngudivada},
  doi = {10.1016/j.tsc.2014.03.003},
  interhash = {aa0c1ab049620547d8a5bba77384ddc9},
  intrahash = {cd16190548a12d876bae12e028ac1f0d},
  journal = {Thinking Skills and Creativity},
  keywords = {Creativity Passion STEM},
  month = sep,
  pages = {91--105},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Passion: Engine of creative teaching in an English university?},
  url = {https://doi.org/10.1016%2Fj.tsc.2014.03.003},
  volume = 13,
  year = 2014
}

@article{cormack2008email,
  abstract = {Spam is information crafted to be delivered to a large number of recipients, in spite of their wishes. A spam filter is an automated tool to recognize spam so as to prevent its delivery. The purposes of spam and spam filters are diametrically opposed: spam is effective if it evades filters, while a filter is effective if it recognizes spam. The circular nature of these definitions, along with their appeal to the intent of sender and recipient make them difficult to formalize. A typical email user has a working definition no more formal than "I know it when I see it." Yet, current spam filters are remarkably effective, more effective than might be expected given the level of uncertainty and debate over a formal definition of spam, more effective than might be expected given the state-of-the-art information retrieval and machine learning methods for seemingly similar problems. But are they effective enough? Which are better? How might they be improved? Will their effectiveness be compromised by more cleverly crafted spam?

We survey current and proposed spam filtering techniques with particular emphasis on how well they work. Our primary focus is spam filtering in email; Similarities and differences with spam filtering in other communication and storage media — such as instant messaging and the Web — are addressed peripherally. In doing so we examine the definition of spam, the user's information requirements and the role of the spam filter as one component of a large and complex information universe. Well-known methods are detailed sufficiently to make the exposition self-contained, however, the focus is on considerations unique to spam. Comparisons, wherever possible, use common evaluation measures, and control for differences in experimental setup. Such comparisons are not easy, as benchmarks, measures, and methods for evaluating spam filters are still evolving. We survey these efforts, their results and their limitations. In spite of recent advances in evaluation methodology, many uncertainties (including widely held but unsubstantiated beliefs) remain as to the effectiveness of spam filtering techniques and as to the validity of spam filter evaluation methods. We outline several uncertainties and propose experimental methods to address them.

Email Spam Filtering: A Systematic Review surveys current and proposed spam filtering techniques with particular emphasis on how well they work. The primary focus is on spam filtering in email, while similarities and differences with spam filtering in other communication and storage media - such as instant messaging and the Web - are addressed peripherally. Email Spam Filtering: A Systematic Review examines the definition of spam, the user's information requirements and the role of the spam filter as one component of a large and complex information universe. Well known methods are detailed sufficiently to make the exposition self-contained; however, the focus is on considerations unique to spam. Comparisons, wherever possible, use common evaluation measures and control for differences in experimental setup. Such comparisons are not easy, as benchmarks, measures and methods for evaluating spam filters are still evolving. The author surveys these efforts, their results and their limitations. In spite of recent advances in evaluation methodology, many uncertainties (including widely held but unsubstantiated beliefs) remain as to the effectiveness of spam filtering techniques and as to the validity of spam filter evaluation methods. Email Spam Filtering: A Systematic Review outlines several uncertainties and proposes experimental methods to address them. Email Spam Filtering: A Systematic Review is a highly recommended read for anyone conducting research in the area or charged with controlling spam in a corporate environment.},
  added-at = {2017-06-03T22:07:04.000+0200},
  author = {Cormack, Gordon V.},
  biburl = {https://www.bibsonomy.org/bibtex/2f105eaf96c4f7e5fa04208bda83ee430/vngudivada},
  doi = {10.1561/1500000006},
  interhash = {b2312417b7b9d0d4a4b9d5bc68f99c3e},
  intrahash = {f105eaf96c4f7e5fa04208bda83ee430},
  issn = {1554-0669},
  journal = {Foundations and Trends in Information Retrieval},
  keywords = {Email IR ML SpamFiltering},
  number = 4,
  pages = {335-455},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Email Spam Filtering: A Systematic Review},
  url = {http://dx.doi.org/10.1561/1500000006},
  volume = 1,
  year = 2008
}

@techreport{massachusettsdepartmentofhighereducation2011increasing,
  abstract = {
The purpose of this report is to profile STEM Pipeline funded programs that have employed promising practices to increase student interest in STEM (Science, Technology, Engineering and Mathematics).

More specifically the intent of this report is to:

● Describe how the programs were designed
● Document what the programs accomplished
● Summarize program evaluation methods used

Programs were selected for inclusion in this report if they met all of the following criteria:

I. Program Design Criteria

A. Designed to increase student interest in STEM

B. Employed practices that research indicates are effective in affecting STEM interest, such as:

	o Content was related to real-world application, ideally through hands-on learning
	o Critical thinking, collaboration, and small group work was encouraged
	o Caring adults (i.e., parents, teachers or STEM professionals) participated
	 Information about STEM career opportunities was provided

II. Program Outcome Criteria

A. Provided data on student interest in STEM
B. Assessed changes in STEM interest in relation to program participation
C. Assessed interest in STEM more generally, beyond the program itself
D. Collected student interest data directly from students

Programs funded by the STEM Pipeline during the 2009-2010 year (whether by the 2007 Networks Grants or by the 2008 Student Interest Grants) were considered for inclusion. The review included all current and historical materials submitted to the Department of Higher Education as part of a statewide evaluation process, as well as reports completed by local evaluators. Frequently, a single organization implemented two or three programs with different approaches and objectives and local evaluators conducted tailored evaluations specifically for each program. As a result, each program was considered separately for inclusion in this report.},
  added-at = {2017-08-20T20:40:35.000+0200},
  author = {{Massachusetts Department of Higher Education}},
  biburl = {https://www.bibsonomy.org/bibtex/25aa040cf37e87c9ee0989a618f30c81f/vngudivada},
  interhash = {b16257e49d1f6e14d3f3dc8d543c3b1b},
  intrahash = {5aa040cf37e87c9ee0989a618f30c81f},
  keywords = {sys:relevantfor:ecu-cc-research BestPractices Motivation STEM},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Increasing Student Interest in Science, Technology, Engineering, and Math (STEM): Massachusetts STEM Pipeline Fund Programs Using Promising Practices},
  year = 2011
}

@book{roelleke2013information,
  abstract = {Information Retrieval (IR) models are a core component of IR research and IR systems. The past decade brought a consolidation of the family of IR models, which by 2000 consisted of relatively isolated views on TF-IDF (Term-Frequency times Inverse-Document-Frequency) as the weighting scheme in the vector-space model (VSM), the probabilistic relevance framework (PRF), the binary independence retrieval (BIR) model, BM25 (Best-Match Version 25, the main instantiation of the PRF/BIR), and language modelling (LM). Also, the early 2000s saw the arrival of divergence from randomness (DFR).

Regarding intuition and simplicity, though LM is clear from a probabilistic point of view, several people stated: "It is easy to understand TF-IDF and BM25. For LM, however, we understand the math, but we do not fully understand why it works."

This book takes a horizontal approach gathering the foundations of TF-IDF, PRF, BIR, Poisson, BM25, LM, probabilistic inference networks (PIN's), and divergence-based models. The aim is to create a consolidated and balanced view on the main models.

A particular focus of this book is on the "relationships between models." This includes an overview over the main frameworks (PRF, logical IR, VSM, generalized VSM) and a pairing of TF-IDF with other models. It becomes evident that TF-IDF and LM measure the same, namely the dependence (overlap) between document and query. The Poisson probability helps to establish probabilistic, non-heuristic roots for TF-IDF, and the Poisson parameter, average term frequency, is a binding link between several retrieval models and model parameters.},
  added-at = {2017-06-26T04:15:16.000+0200},
  address = {Williston, VT},
  author = {Roelleke, Thomas},
  biburl = {https://www.bibsonomy.org/bibtex/2f239543855d04d24df6fc3be97cd0517/vngudivada},
  interhash = {68e2dc976e7f41376c3a1614f21933a2},
  intrahash = {f239543855d04d24df6fc3be97cd0517},
  isbn = {978-1627050784},
  keywords = {Book IR},
  publisher = {Morgan \& Claypool Publishers},
  series = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Information Retrieval Models: Foundations and Relationships},
  year = 2013
}

@book{tailor2015patient,
  abstract = {In The Patient Revolution, author Krisa Tailor—a noted expert in health care innovation and management—explores, through the lens of design thinking, how information technology will take health care into the experience economy. In the experience economy, patients will shift to being empowered consumers who are active participants in their own care. Tailor explores this shift by creating a vision for a newly designed health care system that's focused on both sickness and wellness, and is driven by data and analytics. The new system seamlessly integrates health into our daily lives, and delivers care so uniquely personalized that no two people are provided identical treatments. Connected through data, everyone across the health care ecosystem, including clinicians, insurers, and researchers, will be able to meet individuals wherever they are in their health journey to reach the ultimate goal of keeping people healthy.

The patient revolution has just begun and an exciting journey awaits us.

Praise for the patient revolution

"A full 50% of the US population has at least one chronic disease that requires ongoing monitoring and treatment. Our current health care system is woefully inadequate in providing these individuals with the treatment and support they need. This disparity can only be addressed through empowering patients to better care for themselves and giving providers better tools to care for their patients. Both of those solutions will require the development and application of novel technologies. In Krisa Tailor's book The Patient Revolution, a blueprint is articulated for how this could be achieved, culminating in a vision for a learning health system within 10 years."

—Ricky Bloomfield, MD, Director, Mobile Technology Strategy; Assistant Professor, Duke Medicine

"In The Patient Revolution, Krisa Tailor astutely points out that 80% of health is impacted by factors outside of the health care system. Amazon unfortunately knows more about our patients than we do. The prescriptive analytics she describes will allow health care providers to use big data to optimize interventions at the level of the individual patient. The use of analytics will allow providers to improve quality, shape care coordination, and contain costs. Advanced analytics will lead to personalized care and ultimately empowered patients!"

—Linda Butler, MD, Vice President of Medical Affairs/Chief Medical Officer/Chief Medical Information Officer, Rex Healthcare

"The Patient Revolution provides a practical roadmap on how the industry can capture value by making health and care more personalized, anticipatory, and intuitive to patient needs."

—Ash Damle, CEO, Lumiata

"Excellent read. For me, health care represents a unique economy—one focused on technology, but requiring a deep understanding of humanity. Ms. Tailor begins the exploration of how we provide care via the concepts of design thinking, asking how we might redesign care with an eye toward changing the experience. She does an excellent job deconstructing this from the patient experience. I look forward to a hopeful follow-up directed at changing the provider culture."

—Alan Pitt, MD, Chief Medical Officer, Avizia

"Whether you're a health care provider looking to gain an understanding of the health care landscape, a health data scientist, or a seasoned business pro, you'll come away with a deeper, nuanced understanding of today's evolving health care system with this book. Krisa Tailor ties together—in a comprehensive, unique way—the worlds of health care administration, clinical practice, design thinking, and business strategy and innovation."

—Steven Chan, MD, MBA, University of California, Davis},
  added-at = {2017-06-26T18:29:56.000+0200},
  address = {New York, NY},
  author = {Tailor, Krisa},
  biburl = {https://www.bibsonomy.org/bibtex/2b5ae65d1216ec52123c2a2f3af4aa575/vngudivada},
  interhash = {8b462979fe1b6a8c19decad238c42734},
  intrahash = {b5ae65d1216ec52123c2a2f3af4aa575},
  isbn = {978-1-119-13000-0},
  keywords = {Book Healthcare},
  publisher = {John Wiley},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {The Patient Revolution: How Big Data and Analytics Are Transforming the Health Care Experience},
  year = 2015
}

@book{segaran2009beautiful,
  abstract = {In this insightful book, you'll learn from the best data practitioners in the field just how wide-ranging -- and beautiful -- working with data can be. Join 39 contributors as they explain how they developed simple and elegant solutions on projects ranging from the Mars lander to a Radiohead video. Explore the opportunities and challenges involved in working with the vast number of datasets made available by the Web; learn how to visualize trends in urban crime, using maps and data mashups; discover the challenges of designing a data processing system that works within the constraints of space travel; learn how crowdsourcing and transparency have combined to advance the state of drug research; understand how new data can automatically trigger alerts when it matches or overlaps pre-existing data; learn about the massive infrastructure required to create, capture, and process DNA data.},
  added-at = {2017-06-27T04:25:53.000+0200},
  address = {Sebastopol, California},
  author = {Segaran, Toby and Hammerbacher, Jeff},
  biburl = {https://www.bibsonomy.org/bibtex/2c31b17225bd5b7163d1a81a2bd23bbe6/vngudivada},
  interhash = {7a69c4856e84bc8a56679e09c7a622bf},
  intrahash = {c31b17225bd5b7163d1a81a2bd23bbe6},
  isbn = {9780596157111},
  keywords = {Book DataStories},
  publisher = {O'Reilly},
  refid = {912418613},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Beautiful data: the stories behind elegant data solutions},
  year = 2009
}

@article{kiryakov2004semantic,
  abstract = {The Semantic Web realization depends on the availability of a critical mass of metadata for the web content, associated with the respective formal knowledge about the world. We claim that the Semantic Web, at its current stage of development, is in a state of a critical need of metadata generation and usage schemata that are specific, well-defined and easy to understand. This paper introduces our vision for a holistic architecture for semantic annotation, indexing, and retrieval of documents with regard to extensive semantic repositories. A system (called KIM), implementing this concept, is presented in brief and it is used for the purposes of evaluation and demonstration. A particular schema for semantic annotation with respect to real-world entities is proposed. The underlying philosophy is that a practical semantic annotation is impossible without some particular knowledge modelling commitments. Our understanding is that a system for such semantic annotation should be based upon a simple model of real-world entity classes, complemented with extensive instance knowledge. To ensure the efficiency, ease of sharing, and reusability of the metadata, we introduce an upper-level ontology (of about 250 classes and 100 properties), which starts with some basic philosophical distinctions and then goes down to the most common entity types (people, companies, cities, etc.). Thus it encodes many of the domain-independent commonsense concepts and allows straightforward domain-specific extensions. On the basis of the ontology, a large-scale knowledge base of entity descriptions is bootstrapped, and further extended and maintained. Currently, the knowledge bases usually scales between 10^5 and 10^6 descriptions. Finally, this paper presents a semantically enhanced information extraction system, which provides automatic semantic annotation with references to classes in the ontology and to instances. The system has been running over a continuously growing document collection (currently about 0.5 million news articles), so it has been under constant testing and evaluation for some time now. On the basis of these semantic annotations, we perform semantic based indexing and retrieval where users can mix traditional information retrieval (IR) queries and ontology-based ones. We argue that such large-scale, fully automatic methods are essential for the transformation of the current largely textual web into a Semantic Web.},
  acmid = {1741313},
  added-at = {2017-06-03T20:47:34.000+0200},
  address = {Amsterdam, The Netherlands},
  author = {Kiryakov, Atanas and Popov, Borislav and Terziev, Ivan and Manov, Dimitar and Ognyanoff, Damyan},
  biburl = {https://www.bibsonomy.org/bibtex/2922654fc613d64101c933ba0136143af/vngudivada},
  doi = {10.1016/j.websem.2004.07.005},
  interhash = {bb27ee2d3a5fc8952903c7ca59a17bd7},
  intrahash = {922654fc613d64101c933ba0136143af},
  issue_date = {December, 2004},
  journal = {Web Semant.},
  keywords = {IR SemanticAnnotation},
  month = dec,
  number = 1,
  pages = {49--79},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Semantic Annotation, Indexing, and Retrieval},
  url = {http://dx.doi.org/10.1016/j.websem.2004.07.005},
  volume = 2,
  year = 2004
}

@book{bertsekas2008introduction,
  abstract = {An intuitive, yet precise introduction to probability theory, stochastic processes, and probabilistic models used in science, engineering, economics, and related fields. The 2nd edition is a substantial revision of the 1st edition, involving a reorganization of old material and the addition of new material. The length of the book has increased by about 25 percent. The main new feature of the 2nd edition is thorough introduction to Bayesian and classical statistics.

The book is the currently used textbook for "Probabilistic Systems Analysis," an introductory probability course at the Massachusetts Institute of Technology, attended by a large number of undergraduate and graduate students. The book covers the fundamentals of probability theory (probabilistic models, discrete and continuous random variables, multiple random variables, and limit theorems), which are typically part of a first course on the subject, as well as the fundamental concepts and methods of statistical inference, both Bayesian and classical. It also contains, a number of more advanced topics, from which an instructor can choose to match the goals of a particular course. These topics include transforms, sums of random variables, a fairly detailed introduction to Bernoulli, Poisson, and Markov processes.

The book strikes a balance between simplicity in exposition and sophistication in analytical reasoning. Some of the more mathematically rigorous analysis has been just intuitively explained in the text, but is developed in detail (at the level of advanced calculus) in the numerous solved theoretical problems. },
  added-at = {2017-06-04T04:56:12.000+0200},
  address = {Belmont, Massechusetss},
  author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
  biburl = {https://www.bibsonomy.org/bibtex/239f14185f9a9097e1aee4bddd91ca9d1/vngudivada},
  interhash = {3b8c5df9fa2fa05dd2ad4671737b0fb7},
  intrahash = {39f14185f9a9097e1aee4bddd91ca9d1},
  keywords = {Book ParExcellence Probability},
  publisher = {Athena Scientific},
  refid = {494160015},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Introduction to probability},
  year = 2008
}

@article{juola2008authorship,
  abstract = {Authorship attribution, the science of inferring characteristics of the author from the characteristics of documents written by that author, is a problem with a long history and a wide range of application. Recent work in "non-traditional" authorship attribution demonstrates the practicality of automatically analyzing documents based on authorial style, but the state of the art is confusing. Analyses are difficult to apply, little is known about type or rate of errors, and few "best practices" are available. In part because of this confusion, the field has perhaps had less uptake and general acceptance than is its due.

This review surveys the history and present state of the discipline, presenting some comparative results when available. It shows, first, that the discipline is quite successful, even in difficult cases involving small documents in unfamiliar and less studied languages; it further analyzes the types of analysis and features used and tries to determine characteristics of well-performing systems, finally formulating these in a set of recommendations for best practices.},
  added-at = {2017-06-03T21:47:47.000+0200},
  author = {Juola, Patrick},
  biburl = {https://www.bibsonomy.org/bibtex/2ac86d9581df6bfe98f0ac4367bfce5f3/vngudivada},
  doi = {10.1561/1500000005},
  interhash = {7352a1cea868cf37f0d85d9e7dfd6eb1},
  intrahash = {ac86d9581df6bfe98f0ac4367bfce5f3},
  issn = {1554-0669},
  journal = {Foundations and Trends in Information Retrieval},
  keywords = {AuthorshipAttribution IR},
  number = 3,
  pages = {233-334},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Authorship Attribution},
  url = {http://dx.doi.org/10.1561/1500000005},
  volume = 1,
  year = 2008
}

@article{sutton2012introduction,
  abstract = {Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide variety of fields.

In modern applications of machine learning, predicting a single class label is often not enough. Instead we want to predict a large number of variables that depend on each other, such as a class label for every word in a document or for every region in an image. This structured prediction problem is significantly harder than the simple classification problem because we want to learn how the different labels depend on each other. Conditional random fields provide a powerful solution to this problem. They combine the advantages of classification and graphical modeling as they join the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. In the past ten years, there has been an explosion of interest in CRFs with applications as diverse as natural language processing, computer vision, and bioinformatics. An Introduction to Conditional Random Fields provides a comprehensive tutorial aimed at application-oriented practitioners seeking to apply CRFs. This survey does not assume previous knowledge of graphical modeling, and so is intended to be useful to practitioners in a wide variety of fields. It includes discussion of feature construction, inference, and parameter estimation in CRFs. Additionally, the monograph also includes sections on practical "tips of the trade" for CRFs that are difficult to find in the published literature.},
  added-at = {2017-06-03T19:41:59.000+0200},
  author = {Sutton, Charles and McCallum, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/2ee87295cfed9ff062198ea188894cd56/vngudivada},
  doi = {10.1561/2200000013},
  interhash = {46e01d00212fd904da346e81b179b5a0},
  intrahash = {ee87295cfed9ff062198ea188894cd56},
  issn = {1935-8237},
  journal = {Foundations and Trends® in Machine Learning},
  keywords = {CRF ML Survey},
  number = 4,
  pages = {267-373},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {An Introduction to Conditional Random Fields},
  url = {http://dx.doi.org/10.1561/2200000013},
  volume = 4,
  year = 2012
}

@article{wang2014development,
  abstract = {This study aimed to follow up on the research conducted by Ho, Wang, and Cheng (2013) and to develop a model of the scientific imagination using group concept mapping. Participants included five outstanding elementary school teachers and four researchers from southern Taiwan. The framework developed by Trochim (1989) was used as the basis for the construction of concept mappings of the scientific imagination through five panel discussions among the experts. A review of the literature, qualitative interviews, classroom observation, and document analyses were performed on group concept mapping, and independent relevant documents were used for data validation. A qualitative method was employed for data analysis. Finally, we developed the personality, developmental process, picture-in-mind, and surroundings (3PS) model of scientific imagination. Research results indicated that the scientific imagination model not only enhanced understanding of scientific imagination but also applied to daily experiences. The results of the present study are relevant to future projects and research in this domain, including the development of academic-based checklists to foster scientific imagination, the establishment of appropriate assessment tools, and the formulation of a specific curriculum for teaching the concept of scientific imagination.},
  added-at = {2017-07-24T01:04:06.000+0200},
  author = {Wang, Chia-Chi and Ho, Hsiao-Chi and Wu, Jing-Jyi and Cheng, Ying-Yao},
  biburl = {https://www.bibsonomy.org/bibtex/25024d0422de5c15bc26cf56c7a394a6e/vngudivada},
  doi = {10.1016/j.tsc.2014.04.001},
  interhash = {2f40cf197bb332772be63ee407121c3a},
  intrahash = {5024d0422de5c15bc26cf56c7a394a6e},
  journal = {Thinking Skills and Creativity},
  keywords = {ConceptMapping STEM ScientificImagination},
  month = sep,
  pages = {106--119},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Development of the scientific imagination model: A concept-mapping perspective},
  url = {https://doi.org/10.1016%2Fj.tsc.2014.04.001},
  volume = 13,
  year = 2014
}

@article{golder2006usage,
  abstract = {Collaborative tagging describes the process by which many users add metadata in the form of keywords to shared content. Recently, collaborative tagging has grown in popularity on the web, on sites that allow users to tag bookmarks, photographs and other content. In this paper we analyze the structure of collaborative tagging systems as well as their dynamic aspects. Specifically, we discovered regularities in user activity, tag frequencies, kinds of tags used, bursts of popularity in bookmarking and a remarkable stability in the relative proportions of tags within a given URL. We also present a dynamic model of collaborative tagging that predicts these stable patterns and relates them to imitation and shared knowledge.},
  added-at = {2017-06-25T18:14:10.000+0200},
  author = {Golder, S. A. and Huberman, Bernardo A.},
  biburl = {https://www.bibsonomy.org/bibtex/2022b919309b60e22fc6324286985ce76/vngudivada},
  doi = {10.1177/0165551506062337},
  interhash = {df675e16fcba9cd0f6afc5c9f2a8a723},
  intrahash = {022b919309b60e22fc6324286985ce76},
  journal = {Journal of Information Science},
  keywords = {Tagging TopicMap},
  month = apr,
  number = 2,
  pages = {198--208},
  publisher = {{SAGE} Publications},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Usage patterns of collaborative tagging systems},
  url = {https://doi.org/10.1177%2F0165551506062337},
  volume = 32,
  year = 2006
}

@article{chen2006citespace,
  abstract = {This article describes the latest development of a generic approach to detecting and visualizing emerging trends and transient patterns in scientific literature. The work makes substantial theoretical and methodological contributions to progressive knowledge domain visualization. A specialty is conceptualized and visualized as a time-variant duality between two fundamental concepts in information science: research fronts and intellectual bases. A research front is defined as an emergent and transient grouping of concepts and underlying research issues. The intellectual base of a research front is its citation and co-citation footprint in scientific literature—an evolving network of scientific publications cited by research-front concepts. Kleinberg's (2002) burst-detection algorithm is adapted to identify emergent research-front concepts. Freeman's (1979) betweenness centrality metric is used to highlight potential pivotal points of paradigm shift over time. Two complementary visualization views are designed and implemented: cluster views and time-zone views. The contributions of the approach are that (a) the nature of an intellectual base is algorithmically and temporally identified by emergent research-front terms, (b) the value of a co-citation cluster is explicitly interpreted in terms of research-front concepts, and (c) visually prominent and algorithmically detected pivotal points substantially reduce the complexity of a visualized network. The modeling and visualization process is implemented in CiteSpace II, a Java application, and applied to the analysis of two research fields: mass extinction (1981–2004) and terrorism (1990–2003). Prominent trends and pivotal points in visualized networks were verified in collaboration with domain experts, who are the authors of pivotal-point articles. Practical implications of the work are discussed. A number of challenges and opportunities for future studies are identified.},
  added-at = {2017-06-25T18:18:30.000+0200},
  author = {Chen, Chaomei},
  biburl = {https://www.bibsonomy.org/bibtex/2a08a9bbbd8284db6f263d706647be2ff/vngudivada},
  doi = {10.1002/asi.20317},
  interhash = {7c93cd7c2d93ec5a09091a42f875143c},
  intrahash = {a08a9bbbd8284db6f263d706647be2ff},
  issn = {1532-2890},
  journal = {Journal of the American Society for Information Science and Technology},
  keywords = {Tagging TopicMap Visualization},
  number = 3,
  pages = {359--377},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {CiteSpace II: Detecting and visualizing emerging trends and transient patterns in scientific literature},
  url = {http://dx.doi.org/10.1002/asi.20317},
  volume = 57,
  year = 2006
}

@book{baker2008atoms,
  abstract = {Whether all human languages are fundamentally the same or different has been a subject of debate for ages. This problem has deep philosophical implications: If languages are all the same, it implies a fundamental commonality--and thus mutual intelligibility--of human thought.We are now on the verge of solving this problem. Using a twenty-year-old theory proposed by the world's greatest living linguist, Noam Chomsky, researchers have found that the similarities among languages are more profound than the differences. Languages whose grammars seem completely incompatible may in fact be structurally almost identical, except for a difference in one simple rule. The discovery of these rules and how they may vary promises to yield a linguistic equivalent of the Periodic Table of the Elements: a single framework by which we can understand the fundamental structure of all human language. This is a landmark breakthrough both within linguistics, which will herewith finally become a full-fledged science, and in our understanding of the human mind.},
  added-at = {2017-07-05T00:45:55.000+0200},
  address = {New York, NY},
  author = {Baker, Mark C.},
  biburl = {https://www.bibsonomy.org/bibtex/2ccd53a750bf70e9f60742c9f9848fe69/vngudivada},
  interhash = {bb064977c9f09dad2aa88f5c630545b5},
  intrahash = {ccd53a750bf70e9f60742c9f9848fe69},
  isbn = {978-0465005222},
  keywords = {Book Linguistics},
  publisher = {Basic Books},
  refid = {815384427},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {The Atoms of Language: The Mind's Hidden Rules of Grammar},
  year = 2008
}

@article{ho2013analysis,
  abstract = {Scientific inventions arise from the exercise of a rich imagination. This study aimed to explore the mechanisms and factors influencing the Scientific Imagination Process of elementary school students. Five award-winning science teachers and nine students recruited from a southern city of Taiwan participated in this study. The five teachers had an average seniority of 24.6 years and had won numerous major awards in the International Exhibition for Young Inventors (IEYI). The nine students had been instructed by these teachers with regard to their entries to the IEYI. Data were collected via teacher interviews, student interviews, and classroom observations. Data were analysed using qualitative methods and coded using ATLAS.ti software. This study provided multiple forms of evidence to ensure research validity. The results identified three stages in the Scientific Imagination Process: Initiation Stage, Dynamic Adjustment Stage, and Virtual Implementation Stage. Each stage was found to have its own key components. Additionally, individuals were influenced by both internal (e.g., motivation and personal dispositions) and external (e.g., family environment, teacher guidance, peer interactions, and multiple life experiences) factors during the process of scientific imagination. Several implications and suggestions for further research were also discussed.},
  added-at = {2017-07-24T01:06:48.000+0200},
  author = {Ho, Hsiao-Chi and Wang, Chia-Chi and Cheng, Ying-Yao},
  biburl = {https://www.bibsonomy.org/bibtex/23199265ddabfab4b2298cbfc23e6e8b4/vngudivada},
  doi = {10.1016/j.tsc.2013.04.003},
  interhash = {fdcd2d7288afe0882bd1ac04d84f55bb},
  intrahash = {3199265ddabfab4b2298cbfc23e6e8b4},
  journal = {Thinking Skills and Creativity},
  keywords = {STEM ScientificImagination},
  month = dec,
  pages = {68--78},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Analysis of the Scientific Imagination Process},
  url = {https://doi.org/10.1016%2Fj.tsc.2013.04.003},
  volume = 10,
  year = 2013
}

@article{raju2010future,
  abstract = {Unfortunately, education is not an especially glorified field in our nation; teachers work long hours at often low rates, which often drives highly qualified candidates from the profession. ...] our nation recognizes and works to change this, it will continue to be difficult to achieve such an ideal goal.},
  added-at = {2017-07-23T20:44:52.000+0200},
  author = {Raju, P. K. and Clayson, Ashley},
  biburl = {https://www.bibsonomy.org/bibtex/2ffbdf8da65369dd1a50b2eb12cabfec7/vngudivada},
  interhash = {5ed04a9c55b3b70fd2ad1aea721c8ae4},
  intrahash = {ffbdf8da65369dd1a50b2eb12cabfec7},
  journal = {Journal of STEM Education: Innovations and Research},
  keywords = {HighSchool STEM},
  month = oct,
  number = 5,
  pages = {25-28},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {The Future of STEM Education: An Analysis of Two National Reports},
  url = {http://search.proquest.com.jproxy.lib.ecu.edu/docview/858459431?accountid=10639},
  volume = 11,
  year = 2010
}

@techreport{ashcraft2012girls,
  added-at = {2017-08-20T20:45:39.000+0200},
  author = {Ashcraft, Catherine and Eger, Elizabeth and Friend, Michelle},
  biburl = {https://www.bibsonomy.org/bibtex/2916a036631f28f5945fecc7d72cb423c/vngudivada},
  institution = {National Center for Women \& Information Technology (NCWIT)},
  interhash = {58fb40a7380b8c22954414a3aff0e2c3},
  intrahash = {916a036631f28f5945fecc7d72cb423c},
  keywords = {IT Motivation STEM Women},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Girls in IT: The Facts},
  url = {https://www.ncwit.org/sites/default/files/resources/girlsinit_thefacts_fullreport2012.pdf},
  year = 2012
}

@article{savery2006overview,
  abstract = {Problem-based learning (PBL) is an instructional approach that has been used successfully for over 30 years and continues to gain acceptance in multiple disciplines. It is an instructional (and curricular) learner-centered approach that empowers learners to conduct research, integrate theory and practice, and apply knowledge and skills to develop a viable solution to a defined problem. This overview presents a brief history, followed by a discussion of the similarities and differences between PBL and other experiential approaches to teaching, and identifies some of the challenges that lie ahead for PBL.},
  added-at = {2017-07-24T01:35:21.000+0200},
  author = {Savery, John R.},
  biburl = {https://www.bibsonomy.org/bibtex/23f7605f047652a64be9b1a34a11c0183/vngudivada},
  doi = {10.7771/1541-5015.1002},
  interhash = {a8898a3f672d8ef3e57872fc49b53976},
  intrahash = {3f7605f047652a64be9b1a34a11c0183},
  journal = {Interdisciplinary Journal of Problem-Based Learning},
  keywords = {PBL STEM},
  month = may,
  number = 1,
  publisher = {Purdue University},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Overview of Problem-based Learning: Definitions and Distinctions},
  url = {https://doi.org/10.7771%2F1541-5015.1002},
  volume = 1,
  year = 2006
}

@article{darwish2014arabic,
  abstract = {In the past several years, Arabic Information Retrieval (IR) has garnered significant attention. The main research interests have focused on retrieval of formal language, mostly in the news domain, with ad hoc retrieval, OCR document retrieval, and cross-language retrieval. The literature on other aspects of retrieval continues to be sparse or non-existent, though some of these aspects have been investigated by industry. Others aspects of Arabic retrieval that have received attention include document image retrieval, speech search, social media and web search, and filtering. However, efforts on different aspects of Arabic retrieval continue to be deficient and severely lacking behind efforts in other languages. The survey covers: 1) general properties of the Arabic language; 2) some of the aspects of Arabic that affect retrieval; 3) Arabic processing necessary for effective Arabic retrieval; 4) Arabic retrieval in public IR evaluations; 5) specialized retrieval problems, namely Arabic-English CLIR, Arabic Document Image Retrieval, Arabic Social Search, Arabic Web Search, Question Answering, Image retrieval, and Arabic Speech Search; 6) Arabic IR and NLP resources; and 7) open IR problems that require further attention.

Arabic is ranked as the seventh largest language on the Internet but it has also been the fastest growing language in the last decade in terms of users. At this rate of growth, Arabic users should have the fourth largest user population on the Internet by 2020. Given these facts, it is not surprising that Arabic Information Retrieval (IR) has garnered significant attention. The main research interests have focused on retrieval of formal language, mostly in the news domain, with ad hoc retrieval, OCR document retrieval, and cross-language retrieval. The literature on other aspects of retrieval continues to be sparse or non-existent, though some of these aspects have been investigated by industry. Others aspects of Arabic retrieval that have received attention include document image retrieval, speech search, filtering, and social media and web search. However, efforts within different aspects of Arabic retrieval continue to be deficient and severely lacking behind efforts in other languages.

Arabic Information Retrieval reviews Arabic IR including the nature of the Arabic language, the techniques used for pre-processing the language, the latest research in Arabic IR in different domains, and the open areas in Arabic IR. It covers general properties of the Arabic language, aspects of Arabic that affect retrieval, Arabic processing necessary for effective Arabic retrieval, Arabic retrieval in public IR evaluations, Arabic IR and NLP resources, and specialized retrieval problems such as Arabic-English CLIR, Arabic Document Image Retrieval, Arabic Social Search, Arabic Web Search, Question Answering, Image retrieval, and Arabic Speech Search. Lastly, it also discusses open IR problems that require further attention.},
  added-at = {2017-06-03T21:01:14.000+0200},
  author = {Darwish, Kareem and Magdy, Walid},
  biburl = {https://www.bibsonomy.org/bibtex/29b2d5d0cbbe8b7baf8aeaaca85009678/vngudivada},
  doi = {10.1561/1500000031},
  interhash = {e26b2de2d9a031daa2575eb0439d8605},
  intrahash = {9b2d5d0cbbe8b7baf8aeaaca85009678},
  issn = {1554-0669},
  journal = {Foundations and Trends in Information Retrieval},
  keywords = {Arabic IR},
  number = 4,
  pages = {239-342},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Arabic Information Retrieval},
  url = {http://dx.doi.org/10.1561/1500000031},
  volume = 7,
  year = 2014
}

@book{andcatherineforbes2011statistical,
  added-at = {2017-06-11T22:00:54.000+0200},
  address = {Hoboken, N.J.},
  author = {and Catherine Forbes and Evans, Merran and Hastings, Nicholas and Peacock, Brian},
  biburl = {https://www.bibsonomy.org/bibtex/2464c87c56e242f185349a528db8c582b/vngudivada},
  edition = {Fourth},
  interhash = {8d7e3d68d0cd0195c09cf089de7b3392},
  intrahash = {464c87c56e242f185349a528db8c582b},
  isbn = {9780470390634},
  keywords = {Probability},
  publisher = {Wiley},
  refid = {840842779},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Statistical distributions},
  year = 2011
}

@article{chen2010structure,
  abstract = {A multiple-perspective co-citation analysis method is introduced for characterizing and interpreting the structure and dynamics of co-citation clusters. The method facilitates analytic and sense making tasks by integrating network visualization, spectral clustering, automatic cluster labeling, and text summarization. Co-citation networks are decomposed into co-citation clusters. The interpretation of these clusters is augmented by automatic cluster labeling and summarization. The method focuses on the interrelations between a co-citation cluster's members and their citers. The generic method is applied to a three-part analysis of the field of Information Science as defined by 12 journals published between 1996 and 2008: 1) a comparative author co-citation analysis (ACA), 2) a progressive ACA of a time series of co-citation networks, and 3) a progressive document co-citation analysis (DCA). Results show that the multiple-perspective method increases the interpretability and accountability of both ACA and DCA networks.},
  added-at = {2017-06-25T18:38:15.000+0200},
  author = {Chen, Chaomei and Ibekwe{-}Sanjuan, Fidelia and Hou, Jianhua},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {https://www.bibsonomy.org/bibtex/2a4f36da3f5a1ec093530a13fea4d9db2/vngudivada},
  interhash = {1903b97707c204ccc43f96c422ad98db},
  intrahash = {a4f36da3f5a1ec093530a13fea4d9db2},
  journal = {CoRR},
  keywords = {Clustering TopicMap},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {The Structure and Dynamics of Co-Citation Clusters: {A} Multiple-Perspective Co-Citation Analysis},
  url = {http://arxiv.org/abs/1002.1985},
  volume = {abs/1002.1985},
  year = 2010
}

@inproceedings{repenning2010scalable,
  abstract = {Game design appears to be a promising approach to interest K-12 students in Computer Science. Unfortunately, balancing motivational and educational concerns is truly challenging. Over a number of years, we have explored how to achieve a functional balance by creating a curriculum that combines increasingly complex game designs, computational thinking patterns and authoring tools. Scalable Game Design is a research project exploring new strategies of how to scale up from after school and summer programs into required curriculum of public schools through game design approaches. The project includes inner city schools, remote rural areas and Native American communities. A requirement checklist of computational thinking tools regarding curriculum, teacher training, standards and authoring tools has been developed and is being tested with thousands of students.},
  acmid = {1734357},
  added-at = {2017-07-23T20:19:43.000+0200},
  address = {New York, NY},
  author = {Repenning, Alexander and Webb, David and Ioannidou, Andri},
  biburl = {https://www.bibsonomy.org/bibtex/2492d90070a8590dad46f417342e48877/vngudivada},
  booktitle = {Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
  doi = {10.1145/1734263.1734357},
  interhash = {f94850f8640bea761337ace7f5842186},
  intrahash = {492d90070a8590dad46f417342e48877},
  isbn = {978-1-4503-0006-3},
  keywords = {ComputationalThinking GameDesign},
  location = {Milwaukee, Wisconsin, USA},
  numpages = {5},
  pages = {265--269},
  publisher = {ACM},
  series = {SIGCSE '10},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Scalable Game Design and the Development of a Checklist for Getting Computational Thinking into Public Schools},
  url = {http://doi.acm.org/10.1145/1734263.1734357},
  year = 2010
}

@techreport{shell2015understanding,
  abstract = {Students' motivation and strategic engagement have been identified as playing crucial roles in their success in STEM and CS classes. Numerous motivational constructs have been identified including goals, instrumentality of the course, mindsets, emotional/affective reactions, and self-efficacy. These are thought to motivate students to achieve and to drive the self-regulation and engagement necessary for student-centered learning. Despite sometimes lengthy histories of research in these constructs and behaviors, there are still many questions about how students are motivated in their courses and how they can become effective self-directed, engaged learners. This talk will discuss research findings from five years of classroom research in introductory computer science courses. We have employed comprehensive pre- and post-survey questionnaires assessing student motivation, affect, and strategic engagement and examined impacts on grades and learning and the dynamics of motivation change across the semester. Courses have included computer science majors as well as engineering and other STEM and non-STEM undergraduates. We will talk about our findings and discuss implications for CS and STEM teaching and instruction in the undergraduate classroom.},
  added-at = {2017-08-20T20:56:46.000+0200},
  author = {Shell, Duane F. and Soh, Leen-Kiat},
  biburl = {https://www.bibsonomy.org/bibtex/20283271d1391701eb4d18e229f76305f/vngudivada},
  interhash = {64dc0f07baac287d1b39173445c6d6f5},
  intrahash = {0283271d1391701eb4d18e229f76305f},
  keywords = {ComputerScience Motivation STEM},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Understanding Student Motivation and Strategic Engagement in Computer Science and STEM Courses},
  type = {DBER Speaker Series},
  url = {http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1084&context=dberspeakers},
  year = 2015
}

@article{vansoom2014profiling,
  abstract = {The low success rate of first-year college students in Science, Technology, Engineering, and Mathematics (STEM) programs has spurred many academic achievement studies in which explanatory factors are studied. In this study, we investigated from a person-oriented perspective whether different motivational and academic self-concept profiles could be discerned between male and female first-year college students in STEM and whether differences in early academic achievement were associated with these student groups. Data on autonomous motivation, academic self-concept, and early academic achievement of 1,400 first-year STEM college students were collected. Cluster analyses were used to distinguish motivational profiles based on the relative levels of autonomous motivation and academic self-concept for male and female students. Differences in early academic achievement of the various profiles were studied by means of ANCOVA. Four different motivational profiles were discerned based on the dimensions of autonomous motivation (A) and academic self-concept (S): students scoring high and respectively low on both dimensions (HA-HS or LA-LS), and students scoring high on one dimension and low on the other (HA-LS or LA-HS). Also gender differences were found in this study: male students with high levels of academic self-concept and autonomous motivation had higher academic achievement compared to male students with low levels on both motivational dimensions. For female students, motivational profiles were not associated with academic achievement. The findings partially confirm the internal and external validity of the motivational theories underpinning this study and extend the present insights on identifying subgroup(s) of at risk students in contemporary STEM programs at university level.},
  added-at = {2017-08-20T21:17:56.000+0200},
  author = {Van Soom, Carolien and Donche, Vincent},
  biburl = {https://www.bibsonomy.org/bibtex/207a3daccf59d641ba0aab7b0fa499c05/vngudivada},
  doi = {10.1371/journal.pone.0112489},
  interhash = {39f6fb9889d67baab3dda5b8f7bbfd53},
  intrahash = {07a3daccf59d641ba0aab7b0fa499c05},
  journal = {PLOS ONE},
  keywords = {sys:relevantfor:ecu-cc-research Motivation Persistence STEM},
  month = {11},
  number = 11,
  pages = {1-13},
  publisher = {Public Library of Science},
  timestamp = {2019-03-25T17:08:17.000+0100},
  title = {Profiling First-Year Students in STEM Programs Based on Autonomous Motivation and Academic Self-Concept and Relationship with Academic Achievement},
  url = {https://doi.org/10.1371/journal.pone.0112489},
  volume = 9,
  year = 2014
}

@book{mitkov2005oxford,
  abstract = {Thirty-eight chapters, commissioned from experts all over the world, describe major concepts, methods, and applications in computational linguistics. Part I, Linguistic Fundamentals, provides an overview of the field suitable for senior undergraduates and non-specialists from other fields of linguistics and related disciplines. Part II describes current tasks, techniques, and tools in Natural Language Processing and aims to meet the needs of post-doctoral workers and others embarking on computational language research. Part III surveys current Applications.

    The book is a state-of-the-art reference to one of the most active and productive fields in linguistics. It will be of interest and practical use to a wide range of linguists, as well as to researchers in such fields as informatics, artificial intelligence, language engineering, and cognitive science.

    An excellent reference book that provides a wealth of information and enables the experienced reader to enter quickly into new subject areas of computational linguistics and natural language processing...The particular strengths of The Oxford Handbook of Computational Linguistics are the comprehensive computation-oriented discussions of the fundamental linguistic issues and the broad coverage of NLP methods and resources. It thus extensively accounts for the theoretical and methodological backgrounds of CL and NLP.},
  added-at = {2017-12-23T00:31:18.000+0100},
  address = {Oxford, UK},
  author = {Mitkov, Ruslan},
  biburl = {https://www.bibsonomy.org/bibtex/27e7607473ee36c9c814d7fe19d024565/vngudivada},
  interhash = {7ab6fa26c99b38f273119f5b3403f839},
  intrahash = {7e7607473ee36c9c814d7fe19d024565},
  isbn = {978-0199276349},
  keywords = {Book Handbook Linguistics},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The Oxford Handbook of Computational Linguistics},
  year = 2005
}

@book{berman2016simplification,
  abstract = {Description
Data Simplification: Taming Information With Open Source Tools addresses the simple fact that modern data is too big and complex to analyze in its native form. Data simplification is the process whereby large and complex data is rendered usable. Complex data must be simplified before it can be analyzed, but the process of data simplification is anything but simple, requiring a specialized set of skills and tools.

This book provides data scientists from every scientific discipline with the methods and tools to simplify their data for immediate analysis or long-term storage in a form that can be readily repurposed or integrated with other data. View more >

Key Features
Discusses data simplification principles, methods, and tools that must be studied and mastered
Provides open source tools, free utilities, and snippets of code that can be reused and repurposed to simplify data
Explains how to best utilize indexes to search, retrieve, and analyze textual data
Shows the data scientist how to apply ontologies, classifications, classes, properties, and instances to data using tried and true methods
Readership
Researchers in academia and graduate students in Computer Science with an interest in machine learning.

Table of Contents
1. The Simple Life
2. Structuring Text
3. Indexing Text
4. Understanding Your Data
5. Identifying and Deidentifying Data
6. Giving Meaning to Data
7. Object-oriented data
8. Problem simplification },
  added-at = {2017-12-06T00:51:50.000+0100},
  address = {New York, NY},
  author = {Berman, Jules J.},
  biburl = {https://www.bibsonomy.org/bibtex/24b2ad32d880eee2801bc59908af16531/vngudivada},
  interhash = {5dfb768f81ff2f7a6d58b24fc1ca006d},
  intrahash = {4b2ad32d880eee2801bc59908af16531},
  isbn = {978-0128037812},
  keywords = {sys:relevantfor:ecu-cc-research BigData Book DataCleaning},
  publisher = {Morgan Kaufmann},
  refid = {949752529},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Data simplification: taming information with open source tools},
  year = 2016
}

@book{salzmann2014language,
  abstract = {Since 1993, professors have turned to Language, Culture, and Society for its comprehensive coverage of all critical aspects of linguistic anthropology. The revised and updated sixth edition features:

    A greatly expanded discussion of the sociocultural context of language, including a new chapter on gender and a thoroughly revised and broadened chapter on race, ethnicity, and class

    End-of-chapter resource manuals and study guides with key terms, questions for discussion, group and individual projects, objective study questions and problems (with answers), and suggestions for further reading

    Additional exercises on phonetics and syntax that reflect contemporary research

    Sidebars and boxes throughout the book that provide ethnographic detail and illustrate the practical experience of conducting linguistic research

    A global perspective and a focus on transnational and multilingual anthropology

    Expanded sections on written languages and theoretical and historical perspectives of linguistic anthropology.},
  added-at = {2017-12-22T18:48:52.000+0100},
  address = {Boulder, Colorado},
  author = {Salzmann, Zdenek and Stanlaw, James and Adachi, Nobuko},
  biburl = {https://www.bibsonomy.org/bibtex/29a95701feab05c9c05018c4adf500a2c/vngudivada},
  edition = {Sixth},
  interhash = {251823c1c0a09f14d89bcde6d6c21133},
  intrahash = {9a95701feab05c9c05018c4adf500a2c},
  isbn = {978-0813349541},
  keywords = {Book Culture Linguistics},
  publisher = {Westview Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Language, Culture, and Society: An Introduction to Linguistic Anthropology},
  year = 2014
}

@book{dechaine2012linguistics,
  abstract = {The fascinating, fun, and friendly way to understand the science behind human language

    Linguistics is the scientific study of human language. Linguistics students study how languages are constructed, how they function, how they affect society, and how humans learn language. From understanding other languages to teaching computers to communicate, linguistics plays a vital role in society. Linguistics For Dummies tracks to a typical college-level introductory linguistics course and arms you with the confidence, knowledge, and know-how to score your highest.

    Understand the science behind human language
    Grasp how language is constructed
    Score your highest in college-level linguistics
    If you're enrolled in an introductory linguistics course or simply have a love of human language, Linguistics For Dummies is your one-stop resource for unlocking the science of the spoken word.

    Learn to:

    The components of any language
    Language's social dimensions
    What language reveals about the human brain
    How writing changes a language
    Understand the science behind human language

    Linguistics is the scientific study of human language. Whether you're currently enrolled in a course or just want to explore the subject, Linguistics For Dummies helps you understand some of the primary streams of linguistics: what language is for (communication), how language works (pattern formation), what language reveals about the mind (cognition), and how written language shapes society (technology).

    Get to know the basics — determine the traits that all languages share and get to know the design features which ensure that language can convey meaning

    Discover the building blocks of language — investigate how individual speech sounds combine to form words, which form sentences, and how those are strung together to make conversation

    Explore the evolution of language — consider how new languages are born, chart how language changes over time, and examine how languages are lost

    Study the relationship between the human brain and language — find out how children and adults learn language and how language activates your brain

    Delve into the written word — see how different writing systems develop, how writing a language down changes it, and how it changes you!

    The architecture of words
    How linguistics categorizes into grammatical categories
    What makes a conversation tick
    The social variables that affect language
    How linguistic archeology connects Sanskrit and English
    Why dying languages matter and how they can be saved
    What brain damage does to language
    How electronic writing streamlines language},
  added-at = {2017-12-22T22:24:23.000+0100},
  address = {New York, NY},
  author = {Dechaine, Rose-Marie and Vatikiotis-Bateson, Strang Burtonand Eric},
  biburl = {https://www.bibsonomy.org/bibtex/2709d3dd9bfedd6f7a53fe0466bba9148/vngudivada},
  interhash = {a41f6318feb132d766c6feb0f37faefd},
  intrahash = {709d3dd9bfedd6f7a53fe0466bba9148},
  isbn = {978-1118091692},
  keywords = {Book Linguistics},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Linguistics For Dummies},
  year = 2012
}

@article{bigler2014analysis,
  abstract = {Amendments passed as part of the No Child Left Behind Act in 2006 made some forms of single-sex (SS) public education legal in the United States. Proponents offer a host of arguments in favor of such schooling. This chapter identifies and evaluates five broad rationales for SS schooling. We conclude that empirical evidence fails to support proponents' claims but nonetheless suggests ways in which to improve coeducation. Specifically, we (a) show that the purported benefits of SS schooling arise from factors confounded with, but not causally linked to, single-sex composition; (b) challenge claims that biological sex is an effective marker of differences relevant to instruction; (c) argue that sexism on the part of teachers and peers persists in SS contexts; and (d) critique the notion that gender per se "disappears" in SS contexts. We also address societal implications of the use of sex-segregated education and conclude that factors found to be beneficial for students should be implemented within coeducational schools.},
  added-at = {2017-08-21T01:49:08.000+0200},
  author = {Bigler, R.S. and Hayes, A.R. and Liben, L.S.},
  biburl = {https://www.bibsonomy.org/bibtex/21ce4a82e9f11a00ee800342aee17df6a/vngudivada},
  interhash = {08f4cbda1e525122c22f346834d3f9df},
  intrahash = {1ce4a82e9f11a00ee800342aee17df6a},
  journal = {Adv Child Dev Behav.},
  keywords = {Gender},
  pages = {225 - 60},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Analysis and evaluation of the rationales for single-sex schooling},
  volume = 47,
  year = 2014
}

@book{creswell2015educational,
  abstract = {"Educational Research: Planning, Conducting, and Evaluating Quantitative and Qualitative Research offers a truly balanced, inclusive, and integrated overview of the processes involved in educational research. This text first examines the general steps in the research process and then details the procedures for conducting specific types of quantitative, qualitative, and mixed methods studies. Direct guidance on reading research is offered throughout the text, and interactive features provide opportunities for practice."--Publisher's description.},
  added-at = {2017-09-05T00:39:43.000+0200},
  author = {Creswell, John W.},
  biburl = {https://www.bibsonomy.org/bibtex/25f12a4c4b8168cbc3538abdc81477030/vngudivada},
  edition = {Fifth},
  interhash = {7d7d4b8037cb4424335fe35ac2a8656e},
  intrahash = {5f12a4c4b8168cbc3538abdc81477030},
  isbn = {978-0133831535},
  keywords = {sys:relevantfor:ecu-cc-research Book EducationResearch},
  publisher = {Pearson},
  refid = {864808781},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Educational research: planning, conducting, and evaluating quantitative and qualitative research},
  year = 2015
}

@article{benamati2013undergraduate,
  abstract = {The MIS field suffers from two pressing workforce issues, underrepresentation of women and inadequate supply of entry level talent. To examine these issues, this study develops an instrument to measure attitudes toward MIS (Attitude toward success, usefulness, confidence in learning, and effectance motivation) and perceptions of both MIS and MIS professionals. Data from 1102 college students collected over a five-year period were then used to test gender differences within and across time periods. In spite of recent efforts, little progress has been made to improve attitudes and perceptions. However, in contrast to expectations, views appear not so different across gender.},
  added-at = {2017-08-29T22:03:25.000+0200},
  author = {Benamati, John and Rajkumar, T.M.},
  biburl = {https://www.bibsonomy.org/bibtex/247db49f87c431ce35c2220d461458ac5/vngudivada},
  interhash = {26821f533d3bbd3514a6b2b26d576179},
  intrahash = {47db49f87c431ce35c2220d461458ac5},
  journal = {Communications of the Association for Information Systems},
  keywords = {sys:relevantfor:ecu-cc-research Gender Survey},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Undergraduate Student Attitudes Toward MIS: Instrument Development and Changing Perceptions of the Field Across Gender and Time},
  url = {http://aisel.aisnet.org/cais/vol33/iss1/14},
  volume = 33,
  year = 2013
}

@book{beranger2016ethics,
  abstract = {Faced with the exponential development of Big Data and both its legal and economic repercussions, we are still slightly in the dark concerning the use of digital information.  In the perpetual balance between confidentiality and transparency, this data will lead us to call into question how we understand certain paradigms, such as the Hippocratic Oath in medicine. As a consequence, a reflection on the study of the risks associated with the ethical issues surrounding the design and manipulation of this “massive data” seems to be essential. This book provides a direction and ethical value to these significant volumes of data. It proposes an ethical analysis model and recommendations to better keep this data in check. This empirical and ethico-technical approach brings together the first aspects of a moral framework directed toward thought, conscience and the responsibility of citizens concerned by the use of data of a personal nature.

Defines Big Data applications in health
Presents the ethical value of the medical datasphere via the description of a model of an ethical analysis of Big Data
Provides the recommendations and steps necessary for successful management and governance of personal health data
Helps readers determine what conditions are essential for the development of the study of Big Data},
  added-at = {2017-12-06T00:21:03.000+0100},
  address = {New York, NY},
  author = {B\'{e}ranger, J\'{e}r\^{o}me},
  biburl = {https://www.bibsonomy.org/bibtex/206d4352ec24cfbb46490e97324f36d8c/vngudivada},
  interhash = {d76444fbe130174eca720dd72f939311},
  intrahash = {06d4352ec24cfbb46490e97324f36d8c},
  isbn = {9781785480256},
  keywords = {sys:relevantfor:ecu-cc-research BigData Book Ethics},
  publisher = {ISTE Press - Elsevier},
  refid = {930257766},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Big data and ethics: the medical datasphere},
  year = 2016
}

@book{hymes1964language,
  added-at = {2017-12-22T18:50:06.000+0100},
  address = {New York, NY},
  author = {Hymes, Dell},
  biburl = {https://www.bibsonomy.org/bibtex/21bc3e6353f5287197868595f93272395/vngudivada},
  interhash = {0e7f58da95a13606a976b3d172e27c12},
  intrahash = {1bc3e6353f5287197868595f93272395},
  keywords = {Book Culture Language},
  publisher = {Harper \& Row},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Language In Culture and Society: A Reader in Linguistics and Anthropology},
  year = 1964
}

@article{kim2015subspace,
  abstract = {Data attacks on state estimation modify part of system measurements such that the tempered measurements cause incorrect system state estimates. Attack techniques proposed in the literature often require detailed knowledge of system parameters. Such information is difficult to acquire in practice. The subspace methods presented in this paper, on the other hand, learn the system operating subspace from measurements and launch attacks accordingly. Conditions for the existence of an unobservable subspace attack are obtained under the full and partial measurement models. Using the estimated system subspace, two attack strategies are presented. The first strategy aims to affect the system state directly by hiding the attack vector in the system subspace. The second strategy misleads the bad data detection mechanism so that data not under attack are removed. Performance of these attacks are evaluated using the IEEE 14-bus network and the IEEE 118-bus network.},
  added-at = {2017-10-09T03:08:56.000+0200},
  author = {Kim, Jinsub and Tong, Lang and Thomas, Robert J.},
  biburl = {https://www.bibsonomy.org/bibtex/298ca00118db36a997765bcfe30db75d1/vngudivada},
  doi = {10.1109/tsp.2014.2385670},
  interhash = {d1206a8bd8222487be2ef81fd23c464f},
  intrahash = {98ca00118db36a997765bcfe30db75d1},
  journal = {{IEEE} Transactions on Signal Processing},
  keywords = {CPS Cybersecurity},
  month = mar,
  number = 5,
  pages = {1102--1114},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Subspace Methods for Data Attack on State Estimation: A Data Driven Approach},
  url = {https://doi.org/10.1109%2Ftsp.2014.238567},
  volume = 63,
  year = 2015
}

@book{bowcher2016society,
  abstract = {This collection of original articles covers a range of research connecting with the work of the eminent linguist Ruqaiya Hasan. It contains contributions from M.A.K. Halliday, G. Williams, D. Butt, D. Miller and M. Berry among others, an interview with Ruqaiya Hasan, and notes from the contributors about their connection with Ruqaiya Hasan's work.},
  added-at = {2017-12-22T18:33:26.000+0100},
  address = {Basingstoke, United Kingdom},
  biburl = {https://www.bibsonomy.org/bibtex/26e1595eb29674563d5a8b214f44e968b/vngudivada},
  editor = {Bowcher, Wendy L. and Liang, Jennifer Yameng},
  interhash = {e287256661fff50dbd0e1b48640bdeca},
  intrahash = {6e1595eb29674563d5a8b214f44e968b},
  isbn = {978-1137402851},
  keywords = {Book Language},
  publisher = {Palgrave Macmillan},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Society in Language, Language in Society: Essays in Honour of Ruqaiya Hasan},
  year = 2016
}

@inproceedings{hoegh2009examining,
  abstract = {Concerns have been raised with respect to the recent decline in enrollment in undergraduate computer science majors. Women are one subpopulation that is severely underrepresented. To better understand the factors that discourage students, both males and females, from pursuing degrees in computer science, a valid and reliable survey is needed. This type of instrument would support the quantitative tracking of attitudinal changes with respect to the field overtime as well as attitudinal comparisons across various subpopulations. This paper describes a survey which is being developed based on current research in computer science education at the Colorado School of Mines through support of the National Science Foundation. Based on the results of a factor analysis and with respect to the pilot population (Colorado School of Mines undergraduate students), there is evidence to support the assertion that this instrument is accurately measuring the five constructs that it was designed to assess.},
  added-at = {2017-08-29T22:34:39.000+0200},
  address = {Piscataway, NJ},
  author = {Hoegh, Andrew and Moskal, Barbara M.},
  biburl = {https://www.bibsonomy.org/bibtex/2e005d984018d95c904d3fd61308adecb/vngudivada},
  booktitle = {Proceedings of the 39th IEEE International Conference on Frontiers in Education Conference},
  interhash = {950839d651eaff21a50c9027dd84fa1c},
  intrahash = {e005d984018d95c904d3fd61308adecb},
  keywords = {ComputerScience Survey},
  pages = {1306--1311},
  publisher = {IEEE Press},
  series = {FIE'09},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Examining Science and Engineering Students' Attitudes Toward Computer Science},
  url = {http://dl.acm.org/citation.cfm?id=1733663.1733967},
  year = 2009
}

@book{pinker2015sense,
  abstract = {Why is so much writing so bad, and how can we make it better? Is the English language being corrupted by texting and social media? Do the kids today even care about good writing—and why should we care? From the author of The Better Angels of Our Nature and the forthcoming Enlightenment Now

    In this entertaining and eminently practical book, the cognitive scientist, dictionary consultant, and New York Times–bestselling author Steven Pinker rethinks the usage guide for the twenty-first century. Using examples of great and gruesome modern prose while avoiding the scolding tone and Spartan tastes of the classic manuals, he shows how the art of writing can be a form of pleasurable mastery and a fascinating intellectual topic in its own right. The Sense of Style is for writers of all kinds, and for readers who are interested in letters and literature and are curious about the ways in which the sciences of mind can illuminate how language works at its best.

    Steven Pinker is currently chair of the Usage Panel of The American Heritage Dictionary.},
  added-at = {2017-12-22T19:45:42.000+0100},
  address = {London, United Kingdom},
  author = {Pinker, Steven},
  biburl = {https://www.bibsonomy.org/bibtex/2c33c203b99865d528f2d8a0ba545660d/vngudivada},
  interhash = {f025d4149ef5a9f7a5a34370435f7ab8},
  intrahash = {c33c203b99865d528f2d8a0ba545660d},
  isbn = {978-0143127796},
  keywords = {Book Language Style Writing},
  publisher = {Penguin Books},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The Sense of Style: The Thinking Person's Guide to Writing in the 21st Century},
  year = 2015
}

@techreport{zumbrunn2011encouraging,
  abstract = {Self-regulated learning (SLR) is recognized as an important predictor of student academic motivation and achievement. This process requires students to independently plan, monitor, and assess their learning. However, few students naturally do this well. This paper provides a review of the literature including: the definition of SRL; an explanation of the relationship between SRL and motivation in the classroom; specific SRL strategies for student use; approaches for encouraging student SRL; and a discussion of some of the challenges educators might encounter while teaching students to be self-regulated, life-long learners.},
  added-at = {2017-08-20T21:44:51.000+0200},
  address = {Richmond, Virginia},
  author = {Zumbrunn, Sharon and Tadlock, Joseph and Roberts, Elizabeth D.},
  biburl = {https://www.bibsonomy.org/bibtex/286003c9dd964e2f4dc61eab2f464c3c9/vngudivada},
  institution = {Metropolitan Educational Research Consortium (MERC), Virginia Commonwealth University},
  interhash = {af4926dd8d376dd084d55156e31745c1},
  intrahash = {86003c9dd964e2f4dc61eab2f464c3c9},
  keywords = {Motivation SelfRegulatedLearning},
  month = oct,
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Encouraging Self-Regulated Learning in the Classroom: A Review of the Literature},
  url = {http://www.self-regulation.ca/uploads/5/6/2/6/56264915/encouraging_self_regulated_learning_in_the_classroom.pdf},
  year = 2011
}

@techreport{wiebe2003computer,
  added-at = {2017-08-29T22:44:42.000+0200},
  address = {Raleigh, North Carolina},
  author = {Wiebe, E.N. and Williams, L. and Yang, K. and Miller, C.},
  biburl = {https://www.bibsonomy.org/bibtex/236bb06539c1086e85d02fac8e8a53d33/vngudivada},
  institution = {NC State University},
  interhash = {25092d8efb58873d0a17efb174b3f137},
  intrahash = {36bb06539c1086e85d02fac8e8a53d33},
  keywords = {sys:relevantfor:ecu-cc-research ComputerScience Survey},
  number = {NCSU CSC TR-2003-1},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Computer science attitude survey},
  type = {Technical Report},
  year = 2003
}

@book{mertler2016introduction,
  abstract = {Introduction to Educational Research guides readers through the various steps of the research methods process to help plan and compose their first educational research project. Through comprehensive chapter content and in-text exercises, readers learn how to prepare a research plan, gather and analyze data, address research questions and hypotheses, and organize reports of their projects. The book is practical and student-friendly; Dr. Craig A. Mertler uses a conversational writing style with non-technical language to help students clearly understand and apply research concepts with no prior familiarity with the principles, procedures, or terminology.

This chapter deals with mixed-methods research, a group of approaches to conducting educational research studies that combines both quantitative and qualitative data. While that description may seem somewhat basic and straightforward, there are many important aspects to consider. In this chapter, we will look at the characteristics of mixed-methods research, along with various designs and other important decisions to be made during the process of conducting mixed-methods research studies.

Characteristics of Mixed-Methods Research

The major characteristic of mixed-methods research is that it combines quantitative and qualitative approaches by including both quantitative and qualitative data in a single research study (Gay, Mills, & Airasian, 2009). Creswell and Plano Clark (2011) define mixed-methods research as those studies that include at least one quantitative strand and one qualitative strand. A strand is a component of a study that encompasses the basic process of conducting quantitative or qualitative research: posing a research question, collecting and analyzing data, and interpreting the results. However, these are merely “surface-level” descriptions of the characteristics of mixed-methods research. We will look more closely at specific characteristics in a moment.

Before examining various characteristics of this approach to conducting research, it is important to understand when and how mixed-methods research began. Creswell and Plano Clark (2011) date the beginnings of mixed-methods research back to the mid- to late 1980s. Methodology experts and writers from all around the world seemed to have been simultaneously working on similar ideas regarding the combination of quantitative and qualitative methods. Up to this point in time, many qualitative researchers and quantitative researchers did not see the legitimacy in the other approach to doing research. However, members of both research camps began to realize, on a deeper level, the value of the alternate approach. For example, quantitative researchers began to see that qualitative data could play an important role in quantitative research; similarly, qualitative researchers began to see that reporting only qualitative views of the world – and of a few individuals – would not permit generalization of the findings to many other individuals and audiences (Creswell & Plano Clark, 2011). Over the past decade or more, interest in the use of mixed-methods research as a means for studying educational topics and phenomenon has grown substantially.

Between the late 1980s and today, definitions and descriptions of mixed-methods research have shifted and morphed, and they continue to do so. While having a singular definition is desirable for many researchers, Creswell and Plano Clark (2011) have instead offered a definition of core characteristics of mixed-methods research. They suggest that their core characteristics provide a broader definition of mixed-methods research, since they combine methods, philosophies, and a research design orientation. These characteristics also highlight the key components that should be considered when designing and conducting a mixed-methods study. These six core characteristics focus on activities of the mixed-methods researcher and include the following actions:

-       Collecting and analyzing persuasively and rigorously both qualitative and quantitative data, based on research questions [emphasis added]

-       Mixing – or integrating or linking – the two forms of data either concurrently by combining or merging them, sequentially by having one build on the other, or embedding one within the other

-       Giving priority to one or to both forms of data, again based on the research questions and the emphasis of the research [emphasis added]

-       Using these procedures in a single research study or in multiple phases of a program of research

-       Framing these procedures within philosophical worldviews and theoretical lenses

-       Combining the procedures into specific research designs that direct the plan for conducting the study (p. 5)

These core characteristics provide an extremely comprehensive perspective on the critical aspects of engaging in mixed-methods research. You will see them integrated into our discussions as we proceed through this chapter.

Not unlike any other approach to conducting research, when preparing a research study that will use a mixed methodology, the researcher must provide a justification for the use of this approach. Researchers would need to do this even if they were engaged in a study that was purely qualitative or purely quantitative. There are specific situations that would more likely warrant a research approach that capitalizes on the combination of quantitative and qualitative data. Creswell and Plano Clark (2011) have described six scenarios or examples of research problems that are best suited for mixed-methods research:

-       A need exists because one data source may be insufficient. As you know, qualitative data provide understanding through greater depth, whereas quantitative data provide broader, more general understanding. Each approach has its advantages and limitations. Qualitative data may provide a deep examination of a phenomenon of interest but only with respect to a handful of participants. On the other hand, quantitative data can provide information across a much broader sampling of participants, but the depth of that information is certainly limited. Depending on the goals of a research study – as well as its guiding research questions – one type of data alone may not tell the complete picture or adequately answer the research questions. Additionally, the results from the analysis of qualitative data and those from the collection of quantitative data may be contradictory, which could not have been discovered if only one type or the other was collected and analyzed. Using both types of data in a single research study provides depth as well as breadth.

-       A need exists to explain initial results. Sometimes researchers find themselves in situations where the results of the study do not provide complete understanding of the research problem; further explanation is needed. This additional explanation can be provided through the collection and analysis of a second set of data that helps explain the results of the initial set of data. For example, quantitative data can be used to provide numerical expressions of the relationships among variables or differences between groups, but detailed understanding of what those relationships mean or from where the differences came (i.e., the meanings behind the results of the statistical tests) can be provided only by qualitative data collection and analysis, as a follow-up to the initial collection of quantitative data.

-       A need exists to generalize exploratory findings. As you know from your studies of qualitative research methods, in some research investigations entered into by researchers, the research questions are not known, the variables cannot yet be identified, and the goals of the research cannot be specified at the outset of the study. In these scenarios, an initial phase focused on the collection of qualitative data is necessary simply to explore the setting or participants involved. Once there is enhanced general knowledge of the research situation, the qualitative phase can be followed up with a quantitative study to generalize and test what was learned from the initial exploration.

-       A need exists to enhance a study with a second method. In some research situations, a second method can be added to provide enhanced understanding of some phase of research that has been conducted. For example, a researcher could add a qualitative component to enhance an experimental, correlational, or causal-comparative study. Similarly, quantitative data could be added to enhance the findings of an ethnographic, narrative, or grounded theory research study. In these situations, however, the second method is embedded or nested within the primary method. The design of this approach should not be confused with the one described above, where the second method is used as a follow-up to the initial method of data collection.

-       A need exists to best employ a theoretical stance. There may be a particular research situation where a theoretical perspective dictates the need to collect both quantitative and qualitative data. All data could either be collected simultaneously or sequentially, with one form of data building on the other. The application of a particular theoretical viewpoint may determine this specific need.

-       A need exists to understand a research objective through multiple research phases. Many research studies require multiple research phases – which may or may not be viewed as individual, separate studies – whereby researchers may need to connect several seemingly independent studies to achieve the overall research goal. This is a common approach used in comprehensive and/or multiyear evaluation or other types of longitudinal studies. As with the previous need, data may be collected simultaneously or sequentially. If the phases of data collection are simultaneous, or occur relatively close in time, we refer to the study as a multiphase mixed-methods research study; if the phases of data collection are distinctly separated by substantial periods of time, we might refer to the study as a multiproject mixed-methods study.

These scenarios illustrate situations in which mixed-methods research would be an appropriate design for investigating the particular problems. Although this list is not necessarily exhaustive, these cases and explanations can certainly serve as justifications for the researchers’ need to use a particular mixed-methods research design. In many cases, researchers may combine some of these six explanations to provide the most accurate justification for the use of mixed-methods designs. As you will also see later in the chapter, these six research problems lay the groundwork for and parallel the various designs of mixed-methods research we will examine shortly.

The Mixed-Methods Research Process

As you might expect, the process for conducting mixed-methods research closely parallels the general process for conducting educational research, presented in Chapter 2. That being said, there are some unique aspects to consider in the process of conducting mixed-methods research. The entire process is outlined and described below (Creswell, 2005; Fraenkel, Wallen, & Hyun, 2012); however, if you compare these steps with those described in Chapter 2, you will notice the additional, unique aspects in the process of conducting mixed-methods research:

1.     Identification of the research problem to be studied. As we have seen numerous times in this book, the clear identification and specification of a research topic is the first step in any study. Consider that you may want to include both quantitative and qualitative data; however, do not become overly concerned about balancing the two forms of data at this point in the process.

2.     Determination of whether a mixed-methods study is feasible.  If you believe that your study will benefit from the use of quantitative and qualitative data, you must consider what this entails. First and foremost, you must have well-developed skills in gathering both quantitative and qualitative data. Gathering both types of data will also be more time-consuming; so you must factor in the desired timetable for your study. Additionally, you must have the appropriate skills to analyze both types of data. Finally, it is important to consider the make-up of potential audiences for your research – those audiences should be able to understand and have an appreciation for the complexity of mixed-methods designs. If any of the above conditions are not adequately satisfied, a mixed-methods study is likely not feasible.

3.     Development of a clear and sound rationale for doing a mixed-methods study. Provided the study is feasible, you should consider and be prepared to answer questions of why you are collecting both quantitative and qualitative data, why both types of data are necessary, and how the study will be enhanced as a result of doing so. Again, if you cannot be clear and explicit in providing a rationale, a mixed-methods study may not be appropriate.

4.     Identification of the appropriate mixed-methods design to guide your data collection. We will discuss mixed-methods designs shortly, but for now you will need to determine the following aspects of your data collection strategy:

-       The priority you will give to quantitative and qualitative data

-       The sequence of your data collection, if you do not plan to collect both types of data simultaneously

-       The specific forms of quantitative and qualitative data you will collect

The determinations you make regarding these three items will typically align with a particular mixed-methods research design, which you should identify for inclusion in your final research report.

5.     Development of research questions for both quantitative and qualitative methods. In a mixed-methods study, researchers typically delineate research questions that pertain specifically to the analysis of quantitative data and ones that pertain specifically to the analysis of qualitative data. It is possible also to add research questions that can be answered by the combination of the interpretations of both kinds of analysis. Depending on the nature of your study, some of the research questions may need to emerge during the course of the study. For example, if you are collecting quantitative data to be followed with collection of qualitative data, those qualitative research questions will likely depend on the outcomes of the quantitative data analysis. (It is important to note that Steps 4 and 5 may occur in reverse order or concurrently.)

6.     Review of related literature and development of a written review. Reviewing related literature provides the same benefits we have discussed in previous chapters – guiding aspects of your study and contextualizing your study. You should develop a thorough written review of the pertinent body of literature to be included in your final research report.

7.     Collection of data. Qualitative and quantitative procedures for the collection of data, which will be described in full in Chapters 11 and 12, are appropriate for mixed-methods research as well. In fact, there are no differences in data collection procedures – quantitative data in a mixed-methods study are collected just as quantitative data in any study would be, and the same holds true for qualitative data. The only caveat, however, is that care must be taken to ensure that data are collected so they parallel the mixed-methods research design you specified earlier.

8.     Analysis of data. Similarly, data analysis proceeds just as presented in Chapters 11 and 13. The exception is that you must determine – based on the mixed-methods design you are using – whether you will analyze quantitative data separately from quantitative data or integrate the two types of data analysis. Again, you must ensure that you are following the particular process outlined by the specific mixed-methods research design you chose.

9.     Development of conclusions and recommendations. You must draw conclusions, inferences, and recommendations directly from the interpretation of results of the data analysis. Once again, however, you must ensure that you are interpreting the data appropriately; in other words, you must determine if the interpretations of your analytical results will be drawn separately and sequentially, or if they will be done in an integrated, concurrent manner.

10.  Preparation of a final research report.  The final step in conducting a mixed-methods research study is to prepare the final research report. There is some variation in developing a report of mixed-methods research, compared with a report of just quantitative or just qualitative research. Specifically, the report should parallel your data analysis and interpretation of results. For example, if your study involved separate data collection, analysis, and interpretation for your quantitative data and qualitative data, your report should contain two separate sections for the collection, analysis, and interpretation – one for each type of data. In contrast, if your analysis and interpretation were integrated into one process across both types of data, you should include only one section reporting the combination of quantitative and qualitative data. Thus, the data analysis section is an attempt to converge both types of data into a single set of results and interpretations, relating directly back to the research problem and guiding questions.

References

Creswell, J.W. (2005). Educational research: Planning, conducting, and evaluating quantitative and qualitative research (2nd ed.). Upper Saddle River, NJ: Merrill/Prentice Hall.

Creswell, J.W., & Plano Clark, V.L. (2011). Designing and conducting mixed methods research (2nd ed.). Los Angeles, CA: Sage.

Fraenkel, J.R., Wallen, N.E., & Hyun, H. (2012). How to design and evaluate research in education (8th ed.). Boston, MA: McGraw-Hill.

Gay, L.R., Mills, G.E., & Airasian, P. (2009). Educational research: Competencies for analysis and applications (9th ed.). Upper Saddle River, NJ: Merrill.},
  added-at = {2017-08-27T23:09:44.000+0200},
  address = {Los Angeles, California},
  author = {Mertler, Craig A.},
  biburl = {https://www.bibsonomy.org/bibtex/2a0e22b88e3e77d4f034ad5dc2199af02/vngudivada},
  interhash = {4a8856cd80195140bbd3245689b1be11},
  intrahash = {a0e22b88e3e77d4f034ad5dc2199af02},
  isbn = {978-1483375489},
  keywords = {MixedMethods},
  publisher = {SAGE Publications, Inc},
  refid = {923878541},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Introduction to Educational Research},
  year = 2016
}

@book{ogrady2009contemporary,
  abstract = {Contemporary Linguistics is one of the most comprehensive introduction to the fundamentals of linguistics, balancing engaging aspects of language study with solid coverage of the basics. Up-to-date scholarship, a direct approach, and a lucid writing style makes it appealing to instructors and beginning students alike and a resource that many students continue to use beyond the classroom.},
  added-at = {2017-12-22T22:09:21.000+0100},
  address = {New York, NY},
  author = {O'Grady, William and Archibald, John and Aronoff, Mark and Rees-Miller, Janie},
  biburl = {https://www.bibsonomy.org/bibtex/2146a454ce1fa99fddb36106486b8df21/vngudivada},
  edition = {Sixth},
  interhash = {70db4413bc1d61bf9e9369eaa82d58a9},
  intrahash = {146a454ce1fa99fddb36106486b8df21},
  isbn = {978-0312555283},
  keywords = {Book Linguistics},
  publisher = {Bedford/St. Martin's},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Contemporary Linguistics},
  year = 2009
}

@book{bondi2010keyness,
  abstract = {This is corpus linguistics with a text linguistic focus. The volume concerns lexical inequality, the fact that some words and phrases share the quality of being key – and thereby reflect or promote important themes – in some textual contexts, while others do not. The patterning of words which differ in their centrality to text meaning is of increasing interest to corpus linguistics. At the same time software resources are yielding increasingly more detailed ways of identifying and studying the linkages between key words and phrases in text databases. This volume brings together work from some of the leading researchers in this field. It presents thirteen studies organized in three sections, the first containing a series of studies exploring the nature of keyness itself, then a set of five studies looking at keyness in specific discourse contexts, and then three studies with an educational focus.},
  added-at = {2017-12-24T03:14:56.000+0100},
  address = {Amsterdam, Netherlands},
  biburl = {https://www.bibsonomy.org/bibtex/2c800e3b0019bb4ef326196ddaff258cc/vngudivada},
  editor = {Bondi, Marina and Scott, Mike},
  interhash = {3ddad8f22b30f845e0a7a6eb0f0707ff},
  intrahash = {c800e3b0019bb4ef326196ddaff258cc},
  isbn = {978-9027223173},
  keywords = {Book Corpus},
  publisher = {John Benjamins Publishing Company},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Keyness in Texts},
  year = 2010
}

@book{grossman1997grammatically,
  abstract = {A humorous guide to correct grammar for those who wish to learn from funny stories, poems, and jokes rather than a textbook provides a bottom-line explanation of the linguistic complexities to help the reader remember the do's and don'ts of modern English.},
  added-at = {2017-12-22T16:21:35.000+0100},
  address = {New York, NY},
  author = {Grossman, Ellie},
  biburl = {https://www.bibsonomy.org/bibtex/2fc7d164c7d049dfaaeaf8c0daaeece01/vngudivada},
  interhash = {1f3254be992ce4d37eab225aa64330e2},
  intrahash = {fc7d164c7d049dfaaeaf8c0daaeece01},
  isbn = {978-0786881697},
  keywords = {Book Grammar},
  publisher = {Hyperion},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The Grammatically Correct Handbook: A Lively and Unorthodox Review Of Common English for the Linguistically Challenged},
  year = 1997
}

@book{sadker2009still,
  abstract = {Despite decades of effort to create fair classrooms and schools, gender bias is alive and well, and in some ways growing. School practices continue to send boys and girls down different life paths, too often treating them not as different genders but as different species. Teachers and parents often miss the subtle signs of sexism in classrooms. Through firsthand observations and up-to-the-minute research, Still Failing at Fairness brings the gender issue into focus.

The authors provide an in-depth account of how girls' and boys' educations are compromised from elementary school through college, and offer practical advice for teachers and parents who want to make a positive difference. The authors examine today's pressing issues -- the lack of enforcement for Title IX, the impact of the backlash against gender equity, the much-hyped "boys' crisis," hardwired brain differences, and the recent growth of singlesex public schools. This book documents how teaching, current testing practices, and subtle cultural attitudes continue to short-circuit both girls and boys of every race, social class, and ethnicity. Hard-hitting and remarkably informative, Still Failing at Fairness is "a fascinating look into America's classrooms.},
  added-at = {2017-08-21T02:11:43.000+0200},
  author = {Sadker, David and Zittleman, Karen R.},
  biburl = {https://www.bibsonomy.org/bibtex/20a557fa4f191c2230232676516784654/vngudivada},
  interhash = {6f9e6ccde7111d3f9340f5196f3397fd},
  intrahash = {0a557fa4f191c2230232676516784654},
  isbn = {978-1416552475},
  keywords = {sys:relevantfor:ecu-cc-research Book Gender},
  publisher = {Scribner},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Still Failing at Fairness: How Gender Bias Cheats Girls and Boys in School and What We Can Do About It},
  year = 2009
}

@book{chapelle2013encyclopedia,
  abstract = {A ground-breaking resource available either online or as a 10-volume print set bringing together historic and emerging areas of research within applied linguistics.

    Combines individual entries ranging from 1,500 to 4,000 words, with longer, essay-style contributions giving a detailed overview of key developments and ideas.

    Includes over 1,100 entries written by an international team of scholars from over 40 countries.

    Covers 27 key areas of the field, including Language Learning and Teaching, Bilingual and Multilingual Education, Assessment and Testing, Corpus Linguistics,
    Conversation Analysis, Discourse, Cognitive Second Language Acquisition, Language, Policy and Planning, Literacy, and Technology and Language.

    Features over 200 entries on the philosophy and history of applied linguistics and biographies of key applied linguists.

    Updates and new articles available twice a year, enabling the work to stay relevant and cutting-edge},
  added-at = {2017-12-23T14:44:31.000+0100},
  address = {New York, NY},
  biburl = {https://www.bibsonomy.org/bibtex/2fc76ecf90262d9895d229cebe0ce249e/vngudivada},
  editor = {Chapelle, Carol A.},
  interhash = {20bd03b3d8ccc3ebaf7fbb42e817797b},
  intrahash = {fc76ecf90262d9895d229cebe0ce249e},
  isbn = {978-1405194730},
  keywords = {Book Encyclopedia Linguistics},
  publisher = {Wiley-Blackwell},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The Encyclopedia of Applied Linguistics, 10 Volume Set},
  year = 2013
}

@book{meyer2002english,
  abstract = {English Corpus Linguistics is a step-by-step guide to creating and analyzing linguistic corpora. It discusses the role that corpus linguistics plays in linguistic theory, demonstrating that corpora have proven to be very useful resources for linguists who believe that their theories and descriptions of English should be based on real rather than contrived data. The author shows how to collect and computerize data for inclusion in a corpus; how to annotate the data; and how to conduct a linguistic analysis of it once it has been created.},
  added-at = {2017-12-23T15:47:59.000+0100},
  address = {Cambridge, UK},
  author = {Meyer, Charles F.},
  biburl = {https://www.bibsonomy.org/bibtex/23f940ccccca157bb8d9d7d2e1afa3b46/vngudivada},
  interhash = {992f8790a21a70f9da00ad5c0ad58260},
  intrahash = {3f940ccccca157bb8d9d7d2e1afa3b46},
  isbn = {978-0521004909},
  keywords = {Book Corpus Linguistics},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {English Corpus Linguistics: An Introduction},
  year = 2002
}

@inproceedings{clayton2009gender,
  added-at = {2017-08-21T05:27:32.000+0200},
  author = {Clayton, K.L. and {von Hellens}, L.A. and Nielsen, S.H.},
  biburl = {https://www.bibsonomy.org/bibtex/29aa8bf627b12d9864566cb291afdd004/vngudivada},
  booktitle = {Proceedings of the Special Interest Group on Management Information System's 47th Annual Conference on Computer Personnel Research},
  interhash = {943c51c3e990b8cc245c7d92ce104990},
  intrahash = {9aa8bf627b12d9864566cb291afdd004},
  keywords = {sys:relevantfor:ecu-cc-research ComputerScience Gender Motivation},
  pages = {153-158},
  series = {SIGMIS CPR},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Gender stereotypes prevail in ICT: A research review},
  year = 2009
}

@book{manning1999foundations,
  abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.

Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.},
  added-at = {2017-12-23T00:52:42.000+0100},
  address = {Cambridge, Massachusetts},
  author = {Manning, Christopher and Sch\"{u}tze, Hinrich},
  biburl = {https://www.bibsonomy.org/bibtex/217324441df9bbec9dc3f3d8a2fec7d25/vngudivada},
  interhash = {a81df02f92f266a51183fe936f588a08},
  intrahash = {17324441df9bbec9dc3f3d8a2fec7d25},
  isbn = {978-0262133609},
  keywords = {Book NLP},
  publisher = {The MIT Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Foundations of Statistical Natural Language Processing},
  year = 1999
}

@inproceedings{karagiannidis2016cpsenabled,
  abstract = {This work presents the architectural concept for real-time/stored data monitoring and control of a small packaged plant for urban sewer-mining, the corresponding ICT infrastructure and web based operator dashboard. An interoperable communication platform is designed in order to enable acquisition, integration, processing, and distribution of monitoring, alerting, management and control functions from the physical sensors to the Internet, using Sensor Web Enablement standards through a Service Oriented Architecture approach. Thus, fostering the paradigm of Cyber-Physical System (CPS) for sustainable water solutions and resources utilization. A responsive and user-friendly graphical interface has been designed with reporting, configuration and data analysis capabilities. The ICT and web platform has been developed using low cost embedded devices, WLAN infrastructure and open source S/W. It has been integrated and tested at a pilot packaged plant installation consisting of a membrane bioreactor coupled with nano-filtration and reverse osmosis membranes and industrial water quality sensors and probes.},
  added-at = {2017-10-09T00:49:37.000+0200},
  author = {Karagiannidis, Lazaros and Vrettopoulos, Michalis and Amditis, Angelos and Makri, Effie and Gkonos, Nikolaos},
  biburl = {https://www.bibsonomy.org/bibtex/24e4828904aeb3794cc7971846fa49ee4/vngudivada},
  booktitle = {2016 International Workshop on Cyber-physical Systems for Smart Water Networks ({CySWater})},
  doi = {10.1109/cyswater.2016.7469056},
  interhash = {87af3370d738ee03678c1ad6f0615d38},
  intrahash = {4e4828904aeb3794cc7971846fa49ee4},
  keywords = {sys:relevantfor:ecu-cc-research CPS},
  month = apr,
  publisher = {{IEEE}},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {A {CPS}-enabled architecture for sewer mining systems},
  url = {https://doi.org/10.1109%2Fcyswater.2016.7469056},
  year = 2016
}

@book{elliot2017handbook,
  abstract = {Now completely revised (over 90% new), this handbook established the concept of competence as an organizing framework for the field of achievement motivation. With an increased focus on connecting theory to application, the second edition incorporates diverse perspectives on why and how individuals are motivated to work toward competence in school, work, sports, and other settings. Leading authorities present cutting-edge findings on the psychological, sociocultural, and biological processes that shape competence motivation across development, analyzing the role of intelligence, self-regulated learning, emotions, creativity, gender and racial stereotypes, self-perceptions, achievement values, parenting practices, teacher behaviors, workplace environments, and many other factors. As a special bonus, purchasers of the second edition can download a supplemental e-book featuring several notable, highly cited chapters from the first edition.

New to This Edition
*Most chapters are new, reflecting over a decade of theoretical and methodological developments.
*Each chapter now has an applied as well as conceptual focus, showcasing advances in intervention research.
*Additional topics: self-regulation in early childhood, self-determination theory, challenge and threat appraisals, performance incentives, achievement emotions, job burnout, gene-environment interactions, class-based models of competence, and the impact of social group membership.
*Supplemental e-book featuring selected chapters from the prior edition.

You are holding in your hands a complete encyclopedia of current thinking on motivation and, more generally, the psychology of achievement. I cannot imagine a more esteemed group of authors to guide readers through the fundamentals of this area--and right up to the cutting edge.

Like the first edition of this handbook, the second edition brings together the world's greatest experts on competence motivation to provide readers with the most important ideas and recent discoveries. And it does something else that is new to this edition and very significant: it discusses exciting applications in key domains of everyday life, including education, business, and athletics. More than ever, this is a 'must-read' handbook for researchers, practitioners, and students.

This handbook is the best evidence to date that the field of motivation is alive and well, and that it provides a valuable framework for addressing complex, real-world challenges, such as the racial achievement gap and the underrepresentation of women in STEM fields. Kudos to the editors and chapter authors for assuring the continued vitality of our field.},
  added-at = {2017-08-20T21:40:26.000+0200},
  address = {New York},
  author = {Elliot, Andrew J. and Dweck, Carol S. and Yeager, David S.},
  biburl = {https://www.bibsonomy.org/bibtex/227dfb7bf6d8c01cc6043683b91be43e9/vngudivada},
  interhash = {91180916ef0a6718d8e9a92434587f4d},
  intrahash = {27dfb7bf6d8c01cc6043683b91be43e9},
  isbn = {9781462529643 146252964X},
  keywords = {Motivation},
  publisher = {Guilford Publications},
  refid = {985330092},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Handbook of Competence and Motivation: Theory and Application},
  url = {http://public.eblib.com/choice/PublicFullRecord.aspx?p=4837497},
  year = 2017
}

@book{hymes1974foundations,
  abstract = {Sociolinguistics is conceived here as a fundamental critical perspective on the whole of the study of language. The scientific problems within present linguistics, the book contends, combine with social problems of the society in which linguists participate to press linguistics to discover ethnographic foundations. The work of providing such foundations largely remains to be done. Working out the implications of these three principles requires a new mode of description of linguistic features and relationships, a mode which can treat the verbal means of a community as a part of its organization of communicative means.

    In Part One, Dell Hymes indicates the place of linguistic inquiry as part of an inquiry into communicable conduct in general. Part Two demonstrates the mutual relation between linguistics and other disciplines that contribute to the common larger field—sociology, social anthropology, education, folklore, and poetics are discussed. In Part Three the author argues that problems within linguistic inquiry suggest social foundations of linguistics deeper than presently assumed, such that social meaning and stylistic function must be taken into account systematically, and social life seen as a source of the organization of linguistic means.},
  added-at = {2017-12-22T18:50:40.000+0100},
  address = {Philadelphia, PA},
  author = {Hymes, Dell},
  biburl = {https://www.bibsonomy.org/bibtex/2f53f14c1105f33ceea8b77157151f78d/vngudivada},
  interhash = {124e127d44be583cd254500f3fa7f46d},
  intrahash = {f53f14c1105f33ceea8b77157151f78d},
  isbn = {978-0812210651},
  keywords = {Book Culture Language},
  publisher = {University of Pennsylvania Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Foundations in Sociolinguistics: An Ethnographic Approach},
  year = 1974
}

@inproceedings{lee2010foundations,
  abstract = {This paper argues that cyber-physical systems present a sub-stantial intellectual challenge that requires changes in both theories of computation and dynamical systems theory. The CPS problem is not the union of cyber and physical problems, but rather their intersection, and as such it demands models that embrace both. Two complementary approaches are identified: cyberizing the physical (CtP) means to endow physical subsystems with cyber-like abstractions and interfaces; and physicalizing the cyber (PtC) means to endow software and network components with abstractions and interfaces that represent their dynamics in time.},
  acmid = {1837462},
  added-at = {2017-10-01T18:14:02.000+0200},
  address = {New York, NY},
  author = {Lee, Edward A.},
  biburl = {https://www.bibsonomy.org/bibtex/29f6a9f08e3831f4191eed52ec0cddf2f/vngudivada},
  booktitle = {Proceedings of the 47th Design Automation Conference},
  doi = {10.1145/1837274.1837462},
  interhash = {a01bc284a54e5b8545948c1b541e55ac},
  intrahash = {9f6a9f08e3831f4191eed52ec0cddf2f},
  isbn = {978-1-4503-0002-5},
  keywords = {sys:relevantfor:ecu-cc-research CPS EmbeddedSystem},
  location = {Anaheim, California},
  pages = {737--742},
  publisher = {ACM},
  series = {DAC '10},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {CPS Foundations},
  url = {http://doi.acm.org/10.1145/1837274.1837462},
  year = 2010
}

@book{dawson2016language,
  abstract = {Language Files: Materials for an Introduction to Language and Linguistics has become one of the most widely adopted, consulted, and authoritative introductory textbooks to linguistics ever written. The scope of the text makes it suitable for use in a wide range of courses, while its unique organization into student-friendly, self-contained sections allows for tremendous flexibility in course design.

    The twelfth edition has been significantly revised, clarified, and updated throughout—with particular attention to the chapters on phonetics, phonology, pragmatics, and especially psycholinguistics. The restructured chapter on psycholinguistics makes use of recent research on language in the brain and includes expanded coverage of language processing disorders, introducing students to current models of speech perception and production and cutting-edge research techniques. In addition, exercises have been updated, and icons have been added to the text margins throughout the book, pointing instructors and students to useful and engaging audio files, videos, and other online resources on the accompanying Language Files website, which has also been significantly expanded.},
  added-at = {2017-12-22T21:43:27.000+0100},
  address = {Columbus, Ohio},
  biburl = {https://www.bibsonomy.org/bibtex/20e3c91b8e89e2dc0e0d8a170d8a61d6c/vngudivada},
  edition = {Twelfth},
  editor = {Dawson, Hope C. and Phelan, Michael},
  interhash = {8f915bba28e3bfb4950dd07dae165bb1},
  intrahash = {0e3c91b8e89e2dc0e0d8a170d8a61d6c},
  isbn = {978-0814252703},
  keywords = {Book Language},
  publisher = {Ohio State University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Language Files: Materials for an Introduction to Language and Linguistics},
  year = 2016
}

@book{christiansen2016creating,
  abstract = {Language is a hallmark of the human species; the flexibility and unbounded expressivity of our linguistic abilities is unique in the biological world. In this book, Morten Christiansen and Nick Chater argue that to understand this astonishing phenomenon, we must consider how language is created: moment by moment, in the generation and understanding of individual utterances; year by year, as new language learners acquire language skills; and generation by generation, as languages change, split, and fuse through the processes of cultural evolution. Christiansen and Chater propose a revolutionary new framework for understanding the evolution, acquisition, and processing of language, offering an integrated theory of how language creation is intertwined across these multiple timescales.

    Christiansen and Chater argue that mainstream generative approaches to language do not provide compelling accounts of language evolution, acquisition, and processing. Their own account draws on important developments from across the language sciences, including statistical natural language processing, learnability theory, computational modeling, and psycholinguistic experiments with children and adults. Christiansen and Chater also consider some of the major implications of their theoretical approach for our understanding of how language works, offering alternative accounts of specific aspects of language, including the structure of the vocabulary, the importance of experience in language processing, and the nature of recursive linguistic structure.

    Our understanding of language -- its evolution, acquisition, and processing -- is undergoing a seismic shift and this engaging, ambitious book clarifies and motivates the new exciting landscape.

    This book is unique in its attempt to take a usage-based and unified approach to the sciences of language: its evolution, historical change, processing, and acquisition. It covers an extraordinarily wide range of relevant and up-to-date literature from which it builds an important theoretical approach. It provides the foundation for asking all the fundamental questions in language research.

    Christiansen and Chater's Creating Language presents a compelling account of how acquisition and processing mutually constrain one another in shaping both linguistic performance and the nature of language. Then, to top it off, they fearlessly touch the linguistic third rail, language evolution, and the time scales shift from milliseconds and months, to millennia. The book will excite controversy, but it most certainly will excite.},
  added-at = {2017-12-22T21:52:43.000+0100},
  address = {Cambridge, Massachusetts},
  author = {Christiansen, Morten H. and Chater, Nick},
  biburl = {https://www.bibsonomy.org/bibtex/284207fb4949c4ffed0452f0b244ea0c2/vngudivada},
  interhash = {04d0015b1b53f47dfee2db891f94727a},
  intrahash = {84207fb4949c4ffed0452f0b244ea0c2},
  isbn = {978-0262034319},
  keywords = {Book Language},
  publisher = {The MIT Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Creating Language: Integrating Evolution, Acquisition, and Processing},
  year = 2016
}

@book{jurafsky2009speech,
  abstract = {An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology – at all levels and with all modern technologies – this text takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. The authors cover areas that traditionally are taught in different courses, to describe a unified vision of speech and language processing. Emphasis is on practical applications and scientific evaluation. An accompanying Website contains teaching materials for instructors, with pointers to language processing resources on the Web. The Second Edition offers a significant amount of new and extended material.},
  added-at = {2017-12-23T00:58:33.000+0100},
  address = {Upper Saddle River, New Jersey},
  author = {Jurafsky, Daniel and Martin, James H.},
  biburl = {https://www.bibsonomy.org/bibtex/23bbad91fe5fdd1d29f9070d8648eab77/vngudivada},
  edition = {Second},
  interhash = {4692a9082b44edb24457fd59b65d7b83},
  intrahash = {3bbad91fe5fdd1d29f9070d8648eab77},
  isbn = {978-0131873216},
  keywords = {Book NLP},
  publisher = {Prentice Hall},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Speech and Language Processing},
  year = 2009
}

@book{ofsciences2016century,
  abstract = {"Cyber-physical systems (CPS) are "engineered systems that are built from, and depend upon, the seamless integration of computational algorithms and physical components." CPS can be small and closed, such as an artificial pancreas, or very large, complex, and interconnected, such as a regional energy grid. CPS engineering focuses on managing inter- dependencies and impact of physical aspects on cyber aspects, and vice versa. With the development of low-cost sensing, powerful embedded system hardware, and widely deployed communication networks, the reliance on CPS for system functionality has dramatically increased. These technical developments in combination with the creation of a workforce skilled in engineering CPS will allow the deployment of increasingly capable, adaptable, and trustworthy systems. Engineers responsible for developing CPS but lacking the appropriate education or training may not fully understand at an appropriate depth, on the one hand, the technical issues associated with the CPS software and hardware or, on the other hand, techniques for physical system modeling, energy and power, actuation, signal processing, and control. In addition, these engineers may be designing and implementing life-critical systems without appropriate formal training in CPS methods needed for verification and to assure safety, reliability, and security. A workforce with the appropriate education, training, and skills will be better positioned to create and manage the next generation of CPS solutions. A 21st Century Cyber-Physical Systems Education examines the intellectual content of the emerging field of CPS and its implications for engineering and computer science education. This report is intended to inform those who might support efforts to develop curricula and materials; faculty and university administrators; industries with needs for CPS workers; and current and potential students about intellectual foundations, workforce requirements, employment opportunities, and curricular needs"--Publisher's description.},
  added-at = {2017-10-09T00:16:52.000+0200},
  address = {Washington, D.C.},
  author = {of Sciences, National Academies and Engineering},
  biburl = {https://www.bibsonomy.org/bibtex/2fcfa33dc2f46f3266383072d0bf00b04/vngudivada},
  interhash = {c237773a22db83093383a106c2ea11d9},
  intrahash = {fcfa33dc2f46f3266383072d0bf00b04},
  keywords = {sys:relevantfor:ecu-cc-research Book CPS},
  publisher = {National Academies Press},
  refid = {967682836},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {A 21st century cyber-physical systems education},
  url = {http://dx.doi.org/10.17226/24x},
  year = 2016
}

@article{hartwell1985grammar,
  added-at = {2017-12-22T16:14:12.000+0100},
  author = {Hartwell, Patrick},
  biburl = {https://www.bibsonomy.org/bibtex/288910aa489cdeb0b33002d9c41f51c49/vngudivada},
  interhash = {f41f6b4b8ffdce922e0e2dda2c3fdea0},
  intrahash = {88910aa489cdeb0b33002d9c41f51c49},
  journal = {College English},
  keywords = {Book Grammar},
  number = 2,
  pages = {105-127},
  publisher = {National Council of Teachers of English},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Grammar, Grammars, and the Teaching of Grammar},
  url = {http://www.jstor.org/stable/376562},
  volume = 47,
  year = 1985
}

@book{strunk1999elements,
  abstract = {This book's unique tone, wit and charm have conveyed the principles of English style to millions of readers. Use the fourth edition of ``the little book'' to make a big impact with writing.},
  added-at = {2017-12-22T19:44:43.000+0100},
  address = {London, England},
  author = {Strunk, William and White, E. B.},
  biburl = {https://www.bibsonomy.org/bibtex/2f00e6d789fe348df6297e89a54a03f48/vngudivada},
  edition = {Fourth},
  interhash = {819c786de82b3cc3350abee70fb9d25c},
  intrahash = {f00e6d789fe348df6297e89a54a03f48},
  isbn = {978-0205309023},
  keywords = {Book Language Style Writing},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The Elements of Style},
  year = 1999
}

@book{bunnik2016challenges,
  abstract = {This book brings together an impressive range of academic and intelligence professional perspectives to interrogate the social, ethical and security upheavals in a world increasingly driven by data. Written in a clear and accessible style, it offers fresh insights to the deep reaching implications of Big Data for communication, privacy and organisational decision-making. It seeks to demystify developments around Big Data before evaluating their current and likely future implications for areas as diverse as corporate innovation, law enforcement, data science, journalism, and food security. The contributors call for a rethinking of the legal, ethical and philosophical frameworks that inform the responsibilities and behaviours of state, corporate, institutional and individual actors in a more networked, data-centric society. In doing so, the book addresses the real world risks, opportunities and potentialities of Big Data.},
  added-at = {2017-12-06T00:26:59.000+0100},
  address = {New York, NY},
  author = {Bunnik, Anno and Cawley, Anthony and Mulqueen, Michael and Zwitter, Andrej},
  biburl = {https://www.bibsonomy.org/bibtex/20ef509af60e30a29d7cdd39dde2e8b23/vngudivada},
  interhash = {eed47ac762e43df0a01f610c537c61d9},
  intrahash = {0ef509af60e30a29d7cdd39dde2e8b23},
  isbn = {978-1349948840},
  keywords = {BigData Book Ethics},
  publisher = {Palgrave Macmillan},
  refid = {970012530},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Big data challenges: society, security, innovation and ethics},
  year = 2016
}

@book{lee2017introduction,
  abstract = {The most visible use of computers and software is processing information for human consumption. The vast majority of computers in use, however, are much less visible. They run the engine, brakes, seatbelts, airbag, and audio system in your car. They digitally encode your voice and construct a radio signal to send it from your cell phone to a base station. They command robots on a factory floor, power generation in a power plant, processes in a chemical plant, and traffic lights in a city. These less visible computers are called embedded systems, and the software they run is called embedded software. The principal challenges in designing and analyzing embedded systems stem from their interaction with physical processes. This book takes a cyber-physical approach to embedded systems, introducing the engineering concepts underlying embedded systems as a technology and as a subject of study. The focus is on modeling, design, and analysis of cyber-physical systems, which integrate computation, networking, and physical processes.

The second edition offers two new chapters, several new exercises, and other improvements. The book can be used as a textbook at the advanced undergraduate or introductory graduate level and as a professional reference for practicing engineers and computer scientists. Readers should have some familiarity with machine structures, computer programming, basic discrete mathematics and algorithms, and signals and systems.},
  added-at = {2017-10-10T00:45:24.000+0200},
  address = {Cambridge, Massachusetts},
  author = {Lee, Edward Ashford and Seshia, Sanjit A.},
  biburl = {https://www.bibsonomy.org/bibtex/225722ad24025aab72fa0c93da9c4cf5c/vngudivada},
  interhash = {fc84fe6fe7d6ab80eb30a6ae747e614c},
  intrahash = {25722ad24025aab72fa0c93da9c4cf5c},
  keywords = {sys:relevantfor:ecu-cc-research Book CPS EmbeddedSystem},
  publisher = {The MIT Press},
  refid = {990143318},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Introduction to embedded systems: a cyber-physical systems approach},
  year = 2017
}

@book{straus2014grammar,
  abstract = {The Blue Book of Grammar and Punctuation is a concise, entertaining workbook and guide to English grammar, punctuation, and usage. This user-friendly resource includes simple explanations of grammar, punctuation, and usage; scores of helpful examples; dozens of reproducible worksheets; and pre- and post-tests to help teach grammar to students of all ages. Appropriate for virtually any age range, this authoritative guide makes learning English grammar and usage simple and fun. This updated Eleventh Edition reflects the latest updates to English usage and grammar and features a fully revised two-color design and lay-flat binding for easy photocopying.

    Clear and concise, easy-to-follow, offering "just the facts"
    Fully updated to reflect the latest rules in grammar and usage along with new quizzes
    Ideal for students from seventh grade through adulthood in the US and abroad
    For anyone who wants to understand the major rules and subtle guidelines of English grammar and usage, The Blue Book of Grammar and Punctuation offers comprehensive, straightforward instruction.},
  added-at = {2017-12-22T20:57:30.000+0100},
  address = {New York, NY},
  author = {Straus, Jane and Kaufman, Lester and Stern, Tom},
  biburl = {https://www.bibsonomy.org/bibtex/2070d40ed43bf4a30faebc376599a0781/vngudivada},
  edition = {Eleventh},
  interhash = {133ea9650db548d3d70b1f56eeb0c345},
  intrahash = {070d40ed43bf4a30faebc376599a0781},
  isbn = {978-1118785560},
  keywords = {Book Grammar},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The Blue Book of Grammar and Punctuation: An Easy-to-Use Guide with Clear Rules, Real-World Examples, and Reproducible Quizzes},
  year = 2014
}

@book{johnson2017educational,
  abstract = {Assuming no prior knowledge, Educational Research by R. Burke Johnson and Larry Christensen offers a comprehensive, easily digestible introductory research methods text for undergraduate and graduate students.  Readers will develop an understanding of the multiple research methods and strategies used in education and related fields; how to read and critically evaluate published research; and the ability to write a proposal, construct a questionnaire, and conduct an empirical research study on their own. Students rave about the clarity of this best seller and its usefulness for their studies, enabling them to become critical consumers and users of research. },
  added-at = {2017-09-02T21:35:34.000+0200},
  address = {Thousand Oaks, California},
  author = {Johnson, Burke and Christensen, Larry B.},
  biburl = {https://www.bibsonomy.org/bibtex/20723d413395125d9592043619830ab10/vngudivada},
  edition = {Sixth},
  interhash = {ee40f5dc330cd6383624cc260ae7ec91},
  intrahash = {0723d413395125d9592043619830ab10},
  isbn = {9781483391601},
  keywords = {sys:relevantfor:ecu-cc-research Book EducationResearch ResearchMethod},
  publisher = {SAGE Publications, Inc},
  refid = {950202240},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Educational research: quantitative, qualitative, and mixed approaches},
  url = {https://edge.sagepub.com/johnson6e/student-resources/chapter-1/action-plan},
  year = 2017
}

@book{lang2017handbook,
  added-at = {2017-10-19T02:29:25.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/28795099847921c27110ada400f6b91fa/vngudivada},
  doi = {10.18608/hla17},
  editor = {Lang, Charles and Siemens, George and Wise, Alyssa and Ga\u{s}evi\'{c}, Dragan},
  interhash = {73a853fb02085827962de98ddf79f0c5},
  intrahash = {8795099847921c27110ada400f6b91fa},
  keywords = {sys:relevantfor:ecu-cc-research Book Handbook LearningAnalytics},
  month = may,
  publisher = {Society for Learning Analytics Research ({SoLAR})},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Handbook of Learning Analytics},
  url = {https://doi.org/10.18608%2Fhla17},
  year = 2017
}

@book{lu2014computational,
  abstract = {In the past few decades the use of increasingly large text corpora has grown rapidly in language and linguistics research. This was enabled by remarkable strides in natural language processing (NLP) technology, technology that enables computers to automatically and efficiently process, annotate and analyze large amounts of spoken and written text in linguistically and/or pragmatically meaningful ways. It has become more desirable than ever before for language and linguistics researchers who use corpora in their research to gain an adequate understanding of the relevant NLP technology to take full advantage of its capabilities.

	This volume provides language and linguistics researchers with an accessible introduction to the state-of-the-art NLP technology that facilitates automatic annotation and analysis of large text corpora at both shallow and deep linguistic levels. The book covers a wide range of computational tools for lexical, syntactic, semantic, pragmatic and discourse analysis, together with detailed instructions on how to obtain, install and use each tool in different operating systems and platforms. The book illustrates how NLP technology has been applied in recent corpus-based language studies and suggests effective ways to better integrate such technology in future corpus linguistics research.

	This book provides language and linguistics researchers with a valuable reference for corpus annotation and analysis.

	Computational Methods for Corpus Annotation and Analysis’ is an excellent book for corpus linguists who are interested in using advanced corpus queries. It presents the latest computational tools for corpus annotation and analysis in a very accessible manner. The advice and resources in the book are also very practical and useful. It is highly recommended to researchers and students of corpus linguistics.},
  added-at = {2017-12-24T21:32:32.000+0100},
  address = {New York, NY},
  author = {Lu, Xiaofei},
  biburl = {https://www.bibsonomy.org/bibtex/2257bc016d1f6d3689b5c7e33800ec1f7/vngudivada},
  interhash = {3d7926e5e16f95a309df17de3920499a},
  intrahash = {257bc016d1f6d3689b5c7e33800ec1f7},
  isbn = {978-9401786447},
  keywords = {Annotation Book, Corpus,},
  publisher = {Springer},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Computational Methods for Corpus Annotation and Analysis},
  year = 2014
}

@book{poesio2016anaphora,
  abstract = {This book lays out a path leading from the linguistic and cognitive basics, to classical rule-based and machine learning algorithms, to today’s state-of-the-art approaches, which use advanced empirically grounded techniques, automatic knowledge acquisition, and refined linguistic modeling to make a real difference in real-world applications. Anaphora and coreference resolution both refer to the process of linking textual phrases (and, consequently, the information attached to them) within as well as across sentence boundaries, and to the same discourse referent.

The book offers an overview of recent research advances, focusing on practical, operational approaches and their applications. In part I (Background), it provides a general introduction, which succinctly summarizes the linguistic, cognitive, and computational foundations of anaphora processing and the key classical rule- and machine-learning-based anaphora resolution algorithms. Acknowledging the central importance of shared resources, part II (Resources) covers annotated corpora, formal evaluation, preprocessing technology, and off-the-shelf anaphora resolution systems. Part III (Algorithms) provides a thorough description of state-of-the-art anaphora resolution algorithms, covering enhanced machine learning methods as well as techniques for accomplishing important subtasks such as mention detection and acquisition of relevant knowledge. Part IV (Applications) deals with a selection of important anaphora and coreference resolution applications, discussing particular scenarios in diverse domains and distilling a best-practice model for systematically approaching new application cases. In the concluding part V (Outlook), based on a survey conducted among the contributing authors, the prospects of the research field of anaphora processing are discussed, and promising new areas of interdisciplinary cooperation and emerging application scenarios are identified.

    Given the book’s design, it can be used both as an accompanying text for advanced lectures in computational linguistics, natural language engineering, and computer science, and as a reference work for research and independent study. It addresses an audience that includes academic researchers, university lecturers, postgraduate students, advanced undergraduate students, industrial researchers, and software engineers.},
  added-at = {2017-12-22T15:12:12.000+0100},
  address = {New York, NY},
  biburl = {https://www.bibsonomy.org/bibtex/21a56c98788cc3f7add679603d0ee4f72/vngudivada},
  editor = {Poesio, Massimo and Stuckardt, Roland and Versley, Yannick},
  interhash = {783dbc98bb34a66aae15afb9249c225a},
  intrahash = {1a56c98788cc3f7add679603d0ee4f72},
  isbn = {978-3662479087},
  keywords = {Book NLP Parsing},
  publisher = {Springer},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Anaphora Resolution: Algorithms, Resources, and Applications},
  year = 2016
}

@inbook{orton2016bringing,
  abstract = {Computation is reshaping modern science and mathematics practices, but relatively few students have access to, or take, courses that adequately prepare them for the increasingly technological nature of these fields. Further, students who do study computational topics tend to not reflect the greater student body, with female and minority students being disproportionately underrepresented. To address these issues, we investigate the approach of embedding computational thinking content into required high school mathematics and science coursework. Using data from a 3-year implementation, we present results showing differences in attitudes towards computing by gender, while also finding similar gaps do not correlate with aptitude. Using pre/post mea sures, we then show female participants expressed improved confidence with computational thinking and interest in STEM careers. Additionally, we report a dosage effect, where participating in more activities resulted in greater learning gains, providing evidence in support of embedding computational thinking enhanced activities across high school curriculum.},
  added-at = {2017-08-29T22:24:33.000+0200},
  author = {Orton, Kai and Weintrop, David and Beheshti, Elham and Horn, Michael and Jona, Kemi and Wilensky, Uri},
  biburl = {https://www.bibsonomy.org/bibtex/24f207426d5db81477bc1d0301159e191/vngudivada},
  booktitle = {12th International Conference of the Learning Sciences, ICLS 2016: Transforming Learning, Empowering Learners, Proceedings},
  interhash = {fc8d8005b2fb09ba843212bdc94a0c1f},
  intrahash = {4f207426d5db81477bc1d0301159e191},
  keywords = {ComputationalThinking HighSchool},
  pages = {705--712},
  publisher = {International Society of the Learning Sciences (ISLS)},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Bringing computational thinking into high school mathematics and science classrooms},
  volume = 2,
  year = 2016
}

@book{leech2016frequencies,
  abstract = {Word Frequencies in Written and Spoken English is a landmark volume in the development of vocabulary frequency studies. Whereas previous books have in general given frequency information about the written language only, this book provides information on both speech and writing. It not only gives information about the language as a whole, but also about the differences between spoken and written English, and between different spoken and written varieties of the language. The frequencies are derived from a wide ranging and up-to-date corpus of English: the British National Corpus, which was compiled from over 4,000 written texts and spoken transcriptions representing the present day language in the UK. The book is based on a new version of the corpus (available from 2001) providing more accurate grammatical information, which is essential (for example) for distinguishing words like leaves (noun) and leaves (verb) with different meanings. The book begins with a general introduction, explaining why such information is important and highlighting interesting linguistic findings that emerge from the statistical analysis of the British National Corpus vocabulary. It also contains twenty four 'interest boxes' which highlight and comment on different aspects of frequency - for example, the most common colour words in English in order of frequency, and a comparison of male words (e.g. man) and female words (e.g. woman) in terms of their frequency. },
  added-at = {2017-12-24T03:21:43.000+0100},
  address = {Abingdon, United Kingdom},
  author = {Leech, Geoffrey and Rayson, Paul and Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/2c5484894aef97b598266f0e0023637de/vngudivada},
  interhash = {20997b4b9d49f6de66d8d55412842369},
  intrahash = {c5484894aef97b598266f0e0023637de},
  isbn = {978-1138151314},
  keywords = {Book Corpus},
  publisher = {Routledge},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Word Frequencies in Written and Spoken English: based on the British National Corpus},
  year = 2016
}

@book{folse2016teaching,
  abstract = {Keys to Teaching Grammar to English Language Learners:  A Practical Handbook  is not intended to be an exhaustive reference book about ESL grammar. Written for classroom teachers (K-12, ESL, EFL), this book teaches the most common ESL grammar points in an accessible way through real ESL errors together with suggested teaching techniques. Relevant grammar terminology is explained.

    The four objectives of this book are to help teachers: (1) identify common ESL grammar points and understand the details associated with each one; (2) improve their ability to answer any grammar question on the spot (when on the “hot seat”); (3) anticipate common ESL errors by grammar point, by first language, and/or by proficiency level; and (4) develop more effective grammar/language learning lessons. These objectives are for all teachers, whether they are teaching grammar directly or indirectly  in a variety of classes – including a grammar class, a writing class, a speaking class, an ESP class, or a K-12 class.

    In the Second Edition, all chapters have been updated and substantively revised. The number of marginal (gray) boxes with tips and extra information has doubled. A 16th Key, on Negating, and three new appendixes have been added. One of the new appendixes provides a sample exercise from an actual ESL textbook plus relevant notes about the designing of grammar activities and suggestions for teaching each grammar point.

    Also added to each Key is a section on the vocabulary items (e.g., collocations) that are related to the teaching of that particular grammar point. This information is unique to this edition and cannot be found elsewhere on the market. },
  added-at = {2017-12-22T16:08:46.000+0100},
  address = {Ann Arbor, MI},
  biburl = {https://www.bibsonomy.org/bibtex/28459617099a0fbb115fef42f6311c337/vngudivada},
  edition = {Second},
  editor = {Folse, Keith S.},
  interhash = {0fc72e06ee3e4d8cc7bd74a223e6dc49},
  intrahash = {8459617099a0fbb115fef42f6311c337},
  isbn = {978-0472036677},
  keywords = {Book Grammar},
  publisher = {University of Michigan Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Keys to Teaching Grammar to English Language Learners},
  year = 2016
}

@article{faul2007gpower,
  abstract = {G*Power (Erdfelder, Faul, {\&} Buchner, 1996) was designed as a general stand-alone power analysis program for statistical tests commonly used in social and behavioral research. G*Power 3 is a major extension of, and improvement over, the previous versions. It runs on widely used computer platforms (i.e., Windows XP, Windows Vista, and Mac OS X 10.4) and covers many different statistical tests of thet, F, and $\chi$2 test families. In addition, it includes power analyses forz tests and some exact tests. G*Power 3 provides improved effect size calculators and graphic options, supports both distribution-based and design-based input modes, and offers all types of power analyses in which users might be interested. Like its predecessors, G*Power 3 is free.},
  added-at = {2017-08-30T02:13:04.000+0200},
  author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
  biburl = {https://www.bibsonomy.org/bibtex/25fcdf1e697205971e95705039e5fc75b/vngudivada},
  day = 01,
  doi = {10.3758/BF03193146},
  interhash = {2d8d086d637d91e09ea828c41648e736},
  intrahash = {5fcdf1e697205971e95705039e5fc75b},
  issn = {1554-3528},
  journal = {Behavior Research Methods},
  keywords = {StatisticalPower},
  month = may,
  number = 2,
  pages = {175--191},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences},
  url = {https://doi.org/10.3758/BF03193146},
  volume = 39,
  year = 2007
}

@book{barnbrook1996language,
  abstract = {This book is a first-stop introduction to corpus-based language research. It takes the reader systematically through the practical problems and benefits including the points to be reviewed before using computers, obtaining corpus material, the main analytical tools and the most important applications of computerised natural language processing. Each chapter offers guidance on programming where appropriate at a level suitable for readers with no prior experience, and provides exercises to help the reader to apply the principles covered. Case studies are used to show how the techniques are used in genuine research situations.},
  added-at = {2017-12-24T21:32:32.000+0100},
  address = {Edinburgh, UK},
  author = {Barnbrook, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/2cca6e672a20d2724fb9101d3c4ab8bc4/vngudivada},
  interhash = {3c51766a67fa4759755b34cbf2a4cd6d},
  intrahash = {cca6e672a20d2724fb9101d3c4ab8bc4},
  isbn = {978-0748607853},
  keywords = {Book Linguistics},
  publisher = {Edinburgh University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Language and Computers: A Practical Introduction to the Computer Analysis of Language},
  year = 1996
}

@article{griol2013architecture,
  abstract = {Animated characters are beginning to be used as pedagogical tools, as they have the power to capture students' attention and foster their motivation for discovery and learning. However, in order for them to be widely employed and accepted as a learning resource, they must be easy to use and friendly. In this paper we present an architecture that facilitates building interactive pedagogical chatbots that can interact with students in natural language. Our proposal provides a modular and scalable framework to develop such systems efficiently. Additionally, we present Geranium, a system that helps children to appreciate and protect their environment with an interactive chatbot developed following our scheme.},
  added-at = {2017-08-25T00:58:31.000+0200},
  author = {Griol, David and Callejas, Zoraida},
  biburl = {https://www.bibsonomy.org/bibtex/28d614ad81b531a1b76b80795f5d97115/vngudivada},
  doi = {10.5772/55791},
  interhash = {5cda569b8ee6c155982ec6693b6da628},
  intrahash = {8d614ad81b531a1b76b80795f5d97115},
  journal = {International Journal of Advanced Robotic Systems},
  keywords = {sys:relevantfor:ecu-cc-research Chatbot EducationalInnovation},
  number = 3,
  pages = 175,
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {An Architecture to Develop Multimodal Educative Applications with Chatbots},
  url = {http://dx.doi.org/10.5772/55791},
  volume = 10,
  year = 2013
}

@book{weisser2016practical,
  abstract = {This is the first book of its kind to provide a practical and student-friendly guide to corpus linguistics that explains the nature of electronic data and how it can be collected and analyzed.

    Designed to equip readers with the technical skills necessary to analyze and interpret language data, both written and (orthographically) transcribed.

    Introduces a number of easy-to-use, yet powerful, free analysis resources consisting of standalone programs and web interfaces for use with Windows, Mac OS X, and Linux.

    Each section includes practical exercises, a list of sources and further reading, and illustrated step-by-step introductions to analysis tools.

    Requires only a basic knowledge of computer concepts in order to develop the specific linguistic analysis skills required for understanding/analyzing corpus data.

    This textbook makes Practical Corpus Linguistics accessible to everyone. The focus on methodological and technical aspects and the instructive dimension of the book - nothing is considered obvious or already known - make it very useful to any corpus linguist aiming at a better understanding of his/her data. Through the various exercises, it is very easy to test one's comprehension and the reader gradually gains confidence.

    The educational, sometimes entertaining tone as well as the glossary also contribute to gradually enhance the reader's learning capacities in a field in which many feel insecure. It should accompany scholars at the beginning of any research to raise awareness about technical issues that are too often overlooked.

    Designed to help readers analyze and interpret language data, both written and (orthographically) transcribed, Practical Corpus Linguistics offers a step-by-step guide to this burgeoning field of linguistics research.

    In this accessible introduction, Martin Weisser not only explains the nature of electronic data, illustrating how it can be collected and prepared for analysis purposes, but also demonstrates ways of comparing the results of readers' own smaller-scale analyses to facts obtained from general reference corpora.

    Each section includes practical exercises, a list of sources and further reading, and illustrated step-by-step introductions to analysis tools.

    Seamlessly balancing computer science with linguistic theory, Practical Corpus Linguistics is  essential reading for anyone interested in developing the specific linguistic analysis skills required for analyzing corpus data.},
  added-at = {2017-12-23T00:17:24.000+0100},
  address = {New York, NY},
  biburl = {https://www.bibsonomy.org/bibtex/20281fa44f9671c8b76978093c3a9fa9e/vngudivada},
  editor = {Weisser, Martin},
  interhash = {7c488d38cd6ba984754017f45069b348},
  intrahash = {0281fa44f9671c8b76978093c3a9fa9e},
  isbn = {978-1118831885},
  keywords = {Book Corpus Linguistics},
  publisher = {Wiley-Blackwell},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Practical Corpus Linguistics: An Introduction to Corpus-Based Language Analysis},
  year = 2016
}

@article{dorn2015empirical,
  abstract = {Student attitudes play an important role in shaping learning experiences. However, few validated instruments exist for measuring student attitude development in a discipline-specific way. In this paper, we present the design, development, and validation of the computing attitudes survey (CAS). The CAS is an extension of the Colorado Learning Attitudes about Science Survey and measures novice to expert attitude shifts about the nature of knowledge and problem solving in computer science. Factor analysis with a large, multi-institutional data-set identified and confirmed five subscales on the CAS related to different facets of attitudes measured on the survey. We then used the CAS in a pre–post format to demonstrate its usefulness in studying attitude shifts during CS1 courses and its responsiveness to varying instructional conditions. The most recent version of the CAS is provided in its entirety along with a discussion of the conditions under which its validity has been demonstrated. },
  added-at = {2017-08-29T22:17:58.000+0200},
  author = {Dorn, Brian and Tew, Allison Elliott},
  biburl = {https://www.bibsonomy.org/bibtex/27505f207ad5cbb83eeaff89d0635464b/vngudivada},
  description = {Student attitudes play an important role in shaping how students learn from their experiences. However, few validated instruments exist for measuring student attitude development about Computer Science in a discipline-specific way. The Computing Attitudes Survey (CAS) is an extension of the Colorado Learning Attitudes about Science Survey and measures novice-to-expert attitude shifts about the nature of knowledge and problem solving in computer science. It has five subscales related to different facets of attitudes measured on the survey (transfer, personal interest, problem solving strategies, real world connections, and fixed mindset). Validity and reliability of the CAS has been evaluated with first-year undergraduate students in a variety of classes for both majors and non-majors in computing fields at multiple institutions.},
  doi = {10.1080/08993408.2015.1014142},
  eprint = {http://dx.doi.org/10.1080/08993408.2015.1014142},
  interhash = {d117b370cd2eee5ca494082cbdd6efe0},
  intrahash = {7505f207ad5cbb83eeaff89d0635464b},
  journal = {Computer Science Education},
  keywords = {sys:relevantfor:ecu-cc-research Survey},
  number = 1,
  pages = {1 - 36},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Empirical validation and application of the computing attitudes survey},
  url = {http://dx.doi.org/10.1080/08993408.2015.1014142},
  volume = 25,
  year = 2015
}

@book{biber1998corpus,
  abstract = {This book is about investigating the way people use language in speech and writing. It introduces the corpus-based approach to the study of language, based on analysis of large databases of real language examples and illustrates exciting new findings about language and the different ways that people speak and write. The book is important both for its step-by-step descriptions of research methods and for its findings about grammar and vocabulary, language use, language learning, and differences in language use across texts and user groups.},
  added-at = {2017-12-23T20:51:32.000+0100},
  address = {Cambridge, UK},
  author = {Biber, Douglas and Conrad, Susan and Reppen, Randi},
  biburl = {https://www.bibsonomy.org/bibtex/2f8634df79fbc1f615e559a271be677ff/vngudivada},
  interhash = {01dc1dabe4d837cffa13d90ec599ad46},
  intrahash = {f8634df79fbc1f615e559a271be677ff},
  isbn = {978-0521499576},
  keywords = {Book Corpus Linguistics},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Corpus Linguistics: Investigating Language Structure and Use},
  year = 1998
}

@article{barker2004making,
  added-at = {2017-08-21T05:32:14.000+0200},
  author = {Barker, L.J. and Garvin-Doxas, K.},
  biburl = {https://www.bibsonomy.org/bibtex/2d9b16b5aa5da3a7c9af06fe56b06579a/vngudivada},
  interhash = {c17c175ffbeeff95b9a1ab1c2776f86e},
  intrahash = {d9b16b5aa5da3a7c9af06fe56b06579a},
  journal = {Computer Science Education},
  keywords = {sys:relevantfor:ecu-cc-research ComputerScience Gender Motivation},
  number = 1,
  pages = {119-145},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Making visible the behaviors that influence learning environment: A qualitative exploration of computer science classrooms},
  url = {http://dblp.uni-trier.de/db/journals/csedu/csedu14.html#BarkerG04},
  volume = 14,
  year = 2004
}

@book{payne2006exploring,
  abstract = {Designed for those beginning to study linguistics, this is a lively introduction to two key aspects of the structure of language: syntax (the structure of sentences) and morphology (the structure of words). It shows students in a step-by-step fashion how to analyze the syntax and morphology of any language, by clearly describing the basic methods and techniques, and providing almost 100 practical exercises based on data from a rich variety of the world's languages. Written in an engaging style and complete with a comprehensive glossary, Exploring Language Structure explains linguistic concepts by using clear analogies from everyday life. It introduces a range of essential topics in syntax and morphology, such as rules, categories, word classes, grammatical relations, multi-clause constructions and typology. Providing a solid foundation in morphology and syntax, this is the perfect introductory text for beginning students, and will fully prepare them for more advanced courses in linguistic analysis.},
  added-at = {2017-12-22T22:15:07.000+0100},
  address = {Cambridge, United Kingdom},
  author = {Payne, Thomas},
  biburl = {https://www.bibsonomy.org/bibtex/2a575706929e9b38968860926ef3805da/vngudivada},
  interhash = {3ee8c5c93b974b6117d6c210498c2d20},
  intrahash = {a575706929e9b38968860926ef3805da},
  isbn = {978-0521855426},
  keywords = {Book Linguistics},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Exploring Language Structure: A Student's Guide},
  year = 2006
}

@book{yu2015automatic,
  abstract = {This book provides a comprehensive overview of the recent advancement in the field of automatic speech recognition with a focus on deep learning models including deep neural networks and many of their variants. This is the first automatic speech recognition book dedicated to the deep learning approach. In addition to the rigorous mathematical treatment of the subject, the book also presents insights and theoretical foundation of a series of highly successful deep learning models.},
  added-at = {2017-12-03T04:40:25.000+0100},
  address = {New York, NY},
  author = {Yu, Dong and Deng, Li},
  biburl = {https://www.bibsonomy.org/bibtex/2aec371f0cde48e0f58a84149889970c5/vngudivada},
  interhash = {fbaf07e52074d95a31a2d8b43a4e6f91},
  intrahash = {aec371f0cde48e0f58a84149889970c5},
  isbn = {978-1447157786},
  keywords = {sys:relevantfor:ecu-cc-research Book DeepLearning SpeechRecognition},
  publisher = {Springer},
  refid = {899003467},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Automatic recognition: speech a deep learning approach},
  year = 2015
}

@techreport{liston2008evaluating,
  added-at = {2017-08-21T05:37:51.000+0200},
  address = {Boulder, Colorado},
  author = {Liston, C. and Peterson, K. and Ragan, V.},
  biburl = {https://www.bibsonomy.org/bibtex/2658993a088e46dbc3d3ead77bacb6214/vngudivada},
  institution = {National Center for Women \& Information Technology},
  interhash = {547b393dcc349128c11594243bad1973},
  intrahash = {658993a088e46dbc3d3ead77bacb6214},
  keywords = {sys:relevantfor:ecu-cc-research ComputerScience Gender Motivation},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Evaluating Promising Practices in Informal Information Technology (IT) Education for Girls: Women in IT - Survey Results},
  year = 2008
}

@book{frechtling1997userfriendly,
  added-at = {2017-08-28T01:33:30.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2a08d8372ac5251d3c5528288f4a29d11/vngudivada},
  editor = {Frechtling, Joy and Sharp, Laure},
  interhash = {d80af93cd9bc870d63d1434a10e772d7},
  intrahash = {a08d8372ac5251d3c5528288f4a29d11},
  isbn = {978-0788174315},
  keywords = {MixedMethods},
  publisher = {Westat, Inc.},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {User-Friendly Handbook for Mixed Method Evaluations},
  url = {https://www.nsf.gov/pubs/1997/nsf97153/start.htm},
  year = 1997
}

@article{watt2006motivation,
  abstract = {This study examined the continued gender imbalance in mathematics participation in senior high, which has implications for adolescents' future careers. It confirms persisting greater male participation in maths-related educational and occupational choices among this sample of Australian youth. Gendered course-taking in maths, along with participation intentions for maths-related careers, were explained using the Expectancy-Value framework of Eccles and colleagues (Eccles (Parsons) et al., 1983; Wigfield & Eccles, 2000). The influences of maths-related self-perceptions, intrinsic and utility values, and perceived task difficulty were assessed using longitudinal data from a sample of 442 adolescents spanning Grades 9 through 11 in Sydney, Australia. Gendered maths-related self-perceptions and intrinsic values were the major influences on gendered educational participation in senior high maths, which subsequently predicted maths-related career aspirations--over and above prior mathematical achievement. Utility value showed a curvilinear relationship with maths-related occupational intentions moderated by gender, whereby girls with the highest utility values planned for highly maths-related careers, while boys with mid through high utility values planned similarly highly maths-related careers. Recommendations focus on ways to enhance participation in maths, especially for girls.},
  added-at = {2017-08-21T13:28:16.000+0200},
  author = {Watt, Helen},
  biburl = {https://www.bibsonomy.org/bibtex/2f8c840f5a6ed31b4adb2f77f526e0382/vngudivada},
  interhash = {a2679f4e9a06ce6bf635a4a6e05c0d0a},
  intrahash = {f8c840f5a6ed31b4adb2f77f526e0382},
  journal = {Educational Research and Evaluation},
  keywords = {sys:relevantfor:ecu-cc-research Motivation},
  pages = {305 - 322},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The role of motivation in gendered educational and occupational trajectories related to maths},
  url = {https://eric.ed.gov/?id=EJ752999},
  volume = 12,
  year = 2006
}

@book{grus2015science,
  abstract = {Data science libraries, frameworks, modules, and toolkits are great for doing data science, but they’re also a good way to dive into the discipline without actually understanding data science. In this book, you’ll learn how many of the most fundamental data science tools and algorithms work by implementing them from scratch.

If you have an aptitude for mathematics and some programming skills, author Joel Grus will help you get comfortable with the math and statistics at the core of data science, and with hacking skills you need to get started as a data scientist. Today’s messy glut of data holds answers to questions no one’s even thought to ask. This book provides you with the know-how to dig those answers out.

Get a crash course in Python
Learn the basics of linear algebra, statistics, and probability—and understand how and when they're used in data science
Collect, explore, clean, munge, and manipulate data
Dive into the fundamentals of machine learning
Implement models such as k-nearest Neighbors, Naive Bayes, linear and logistic regression, decision trees, neural networks, and clustering
Explore recommender systems, natural language processing, network analysis, MapReduce, and databases},
  added-at = {2017-12-03T04:18:25.000+0100},
  address = {Sebastopol, California},
  author = {Grus, Joel},
  biburl = {https://www.bibsonomy.org/bibtex/2b679cee68be9cdc86283ec75f5b3930e/vngudivada},
  interhash = {54ad4ebbf7d129d9ee62fd4c7cb00a66},
  intrahash = {b679cee68be9cdc86283ec75f5b3930e},
  isbn = {9781491901427},
  keywords = {sys:relevantfor:ecu-cc-research Book DataScience},
  publisher = {O'Reilly& Associates},
  refid = {945702637},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Data science from scratch},
  year = 2015
}

@book{bauer1999language,
  abstract = {Language is a part of us all and is tightly woven into human experience. Yet, although research into language has increased at a phenomenal rate over the last fifty years, misconceptions abound.

    This illuminating and highly readable collection of essays explores some of the myths, for example: standards of children's speech and writing have declined; women talk too much; the 'purity' of the English language is under threat; some languages are more attractive to the ear or are harder to learn than others; the media has a detrimental effect on language. These widely held views are questioned and shown to be based on inadequate or false information, or simply, not to be true. Other essays explore spelling problems, attitudes towards accents, controversies over changes in language, and the belief that some languages have no grammar.

    A unique collection of original essays by 21 of the world's leading linguists. The topics discussed focus on some of the most popular myths about language: The Media Are Ruining English; Children Can't Speak or Write Properly Anymore; America is Ruining the English Language. The tone is lively and entertaining throughout and there are cartoons from Doonesbury andThe Wizard of Id to illustrate some of the points. The book should have a wide readership not only amongst students who want to read leading linguists writing about popular misconceptions but also amongst the large number of people who enjoy reading about language in general.},
  added-at = {2017-12-22T18:59:01.000+0100},
  address = {London, United Kingdom},
  author = {Bauer, Laurie and Trudgill, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/278769d86a52d0c36411ef6ff354427ac/vngudivada},
  interhash = {245cedb0cafc160d17fefefe56ce25c6},
  intrahash = {78769d86a52d0c36411ef6ff354427ac},
  isbn = {978-0140260236},
  keywords = {Book Language},
  publisher = {Penguin Books},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Language Myths},
  year = 1999
}

@book{anderson2017exploring,
  abstract = {This introductory textbook does an amazing job of covering a lot of territory in a very accessible manner for a wide range of readers, with well thought-out, engaging activities. Ideal for a variety of applied linguistics courses as well as for those studying corpus linguistics.' - Randi Reppen, Northern Arizona University, USA 'Anderson \& Corbett do an impressive job of addressing the niceties of corpus literacy for an extremely wide audience, without compromising on rigour. This will undoubtedly become a valuable textbook for students and teachers on a variety of applied and corpus linguistics courses.

    This is an essential guide to using digital resources in the study of English language and linguistics. Assuming no prior experience, it introduces the fundamentals of online corpora and equips readers with the skills needed to search and interpret corpus data. Later chapters focus on specific elements of linguistic analysis, namely vocabulary, grammar, discourse and pronunciation. Examples from five major online corpora illustrate key issues to consider in corpus analysis, while case studies and activities help students get to grips with the wide range of resources that are available and select those that best suit their needs.

    Perfect for students of corpus linguistics and applied linguistics, this engaging and accessible guide opens the door to an ever-expanding world of online resources. It is also ideal for anyone who is curious about how the English language works and has a desire to explore its many written and spoken forms.},
  added-at = {2017-12-24T01:44:55.000+0100},
  address = {Basingstoke, United Kingdom},
  author = {Anderson, Wendy and Corbett, John},
  biburl = {https://www.bibsonomy.org/bibtex/232b170f317373a2a531aef3da8f6c59b/vngudivada},
  edition = {Second},
  interhash = {904eed2d71e210d7bf7b90e9941640c6},
  intrahash = {32b170f317373a2a531aef3da8f6c59b},
  isbn = {978-1137438096},
  keywords = {Book Corpus},
  publisher = {Palgrave},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Exploring English with Online Corpora},
  year = 2017
}

@book{biber2009register,
  abstract = {This book describes the most important kinds of texts in English and introduces the methodological techniques used to analyse them. Three analytical approaches are introduced and compared, describing a wide range of texts from the perspectives of register, genre and style. The primary focus of the book is on the analysis of registers.

  	Part 1 introduces an analytical framework for studying registers, genre conventions, and styles.

  	Part 2 provides detailed descriptions of particular text varieties in English, including spoken interpersonal varieties (conversation, university office hours, service encounters), written varieties (newspapers, academic prose, fiction), and emerging electronic varieties (e-mail, internet forums, text messages).

  	Finally, Part 3 introduces advanced analytical approaches using corpora, and discusses theoretical concerns, such as the place of register studies in linguistics, and practical applications of register analysis. Each chapter ends with three types of activities: reflection and review activities, analysis activities, and larger project ideas.

	This outstanding and highly accessible guide should definitely be useful to students of linguistics and language teachers who may wish to incorporate textual analyses in their teaching. Particularly, the volume can be used as a textbook for courses on register, genre, and style in applied English programs worldwide. clearly, the information presented in this practice-oriented volume has been compiled through years of work and research and serves as an essential reading for those interested in the analysis of registers and other related textual studies.},
  added-at = {2017-12-24T21:32:32.000+0100},
  address = {Cambridge, UK},
  author = {Biber, Douglas and Conrad, Susan},
  biburl = {https://www.bibsonomy.org/bibtex/2c502f6f38fb97ea18018502be1554b05/vngudivada},
  interhash = {e453be29a72b7624e23fa7ffadd4adf6},
  intrahash = {c502f6f38fb97ea18018502be1554b05},
  isbn = {978-0521677899},
  keywords = {Book Genre Linguistics Registers Style},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Register, Genre, and Style},
  year = 2009
}

@article{davies2009million,
  abstract = {The Corpus of Contemporary American English (COCA), which was released online in early 2008, is the first large and diverse corpus of American English. In this paper, we first discuss the design of the corpus -- which contains more than 385 million words from 1990–2008 (20 million words each year), balanced between spoken, fiction, popular magazines, newspapers, and academic journals. We also discuss the unique relational databases architecture, which allows for a wide range of queries that are not available (or are quite difficult) with other architectures and interfaces. To conclude, we consider insights from the corpus on a number of cases of genre-based variation and recent linguistic variation, including an extended analysis of phrasal verbs in contemporary American English.},
  added-at = {2017-12-24T02:16:34.000+0100},
  author = {Davies, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/2b0d3cfaaeb3f041f608f9e4bf2678603/vngudivada},
  doi = {10.1075/ijcl.14.2.02dav},
  interhash = {14002a4019215948afb034a1a7f00ba3},
  intrahash = {b0d3cfaaeb3f041f608f9e4bf2678603},
  journal = {International Journal of Corpus Linguistics},
  keywords = {Corpus},
  number = 2,
  pages = {159-190},
  publisher = {John Benjamins Publishing Company},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The 385+ million word Corpus of Contemporary American English (1990-2008+): Design, architecture, and linguistic insights},
  volume = 14,
  year = 2009
}

@book{oakley2017mindshift,
  abstract = {"Mindshift reveals how we can overcome stereotypes and preconceived ideas about what is possible for us to learn and become. At a time when we are constantly being asked to retrain and reinvent ourselves to adapt to new technologies and changing industries, this book shows us how we can uncover and develop talents we didn't realize we had--no matter what our age or background. We're often told to "follow our passions." But in Mindshift, Dr. Barbara Oakley shows us how we can broaden our passions. Drawing on the latest neuroscientific insights, Dr. Oakley shepherds us past simplistic ideas of "aptitude" and "ability," which provide only a snapshot of who we are now--with little consideration about how we can change. Even seemingly "bad" traits, such as a poor memory, come with hidden advantages--like increased creativity. Profiling people from around the world who have overcome learning limitations of all kinds, Dr. Oakley shows us how we can turn perceived weaknesses, such as impostor syndrome and advancing age, into strengths. People may feel like they're at a disadvantage if they pursue a new field later in life; yet those who change careers can be fertile cross-pollinators: They bring valuable insights from one discipline to another. Dr. Oakley teaches us strategies for learning that are backed by neuroscience so that we can realize the joy and benefits of a learning lifestyle. Mindshift takes us deep inside the world of how people change and grow. Our biggest stumbling blocks can be our own preconceptions, but with the right mental insights, we can tap into hidden potential and create new opportunities"--},
  added-at = {2017-08-25T04:55:12.000+0200},
  address = {New York, NY},
  author = {Oakley, Barbara},
  biburl = {https://www.bibsonomy.org/bibtex/22b96170b949f47c40ceda33927535ad6/vngudivada},
  interhash = {000c53aec7dbc90338a8056f766d04fd},
  intrahash = {2b96170b949f47c40ceda33927535ad6},
  keywords = {sys:relevantfor:ecu-cc-research Bool Learning},
  publisher = {TarcherPerigee},
  refid = {954224181},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Mindshift: discover your hidden potential at any stage of life},
  year = 2017
}

@article{weintrop2016defining,
  abstract = {Science and mathematics are becoming computational endeavors. This fact is reflected in the recently released Next Generation Science Standards and the decision to include "computational thinking" as a core scientific practice. With this addition, and the increased presence of computation in mathematics and scientific contexts, a new urgency has come to the challenge of defining computational thinking and providing a theoretical grounding for what form it should take in school science and mathematics classrooms. This paper presents a response to this challenge by proposing a definition of computational thinking for mathematics and science in the form of a taxonomy consisting of four main categories: data practices, modeling and simulation practices, computational problem solving practices, and systems thinking practices. In formulating this taxonomy, we draw on the existing computational thinking literature, interviews with mathematicians and scientists, and exemplary computational thinking instructional materials. This work was undertaken as part of a larger effort to infuse computational thinking into high school science and mathematics curricular materials. In this paper, we argue for the approach of embedding computational thinking in mathematics and science contexts, present the taxonomy, and discuss how we envision the taxonomy being used to bring current educational efforts in line with the increasingly computational nature of modern science and mathematics.},
  added-at = {2017-08-29T22:21:02.000+0200},
  author = {Weintrop, David and Beheshti, Elham and Horn, Michael and Orton, Kai and Jona, Kemi and Trouille, Laura and Wilensky, Uri},
  biburl = {https://www.bibsonomy.org/bibtex/2eaaffac924767bf06edbfcb90cde4ad7/vngudivada},
  interhash = {005e0eb439386ab914229427ab3a1e0c},
  intrahash = {eaaffac924767bf06edbfcb90cde4ad7},
  journal = {Journal of Science Education and Technology},
  keywords = {sys:relevantfor:ecu-cc-research ComputationalThinking HighSchool},
  month = feb,
  number = 1,
  pages = {127 - 147},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Defining Computational Thinking for Mathematics and Science Classrooms},
  volume = 25,
  year = 2016
}

@book{rosenfelder2010language,
  abstract = {Create plausible and realistic languages for RPGs, fantasy and science fiction, movies or video games, or international communication... or just learn about how languages work from an unusual, light-hearted perspective. The Language Construction Kit on zompist.com has helped a generation of conlangers to understand and create languages. It's expanded here with coverage of semantics and pragmatics, language families, writing systems, and sample wordlists, as well as an annotated sample grammar. Second revised edition.},
  added-at = {2017-12-22T21:59:34.000+0100},
  address = {Chicago, Illinois},
  author = {Rosenfelder, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/29073e34025d3a06b67e5ae71faa50ef4/vngudivada},
  interhash = {0e349f5c6bf98f01eafc0b6a49588ec4},
  intrahash = {9073e34025d3a06b67e5ae71faa50ef4},
  isbn = {978-0984470006},
  keywords = {Book Language},
  publisher = {Yonagu Books},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The Language Construction Kit},
  year = 2010
}

@book{hume2010basics,
  abstract = {Learning a language involves so much more than just rote memorization of rules. Basics of Language for Language Learners, 2nd edition, by Peter W. Culicover and Elizabeth V. Hume, systematically explores all the aspects of language central to second language learning: the sounds of language, the different grammatical structures, the social functions of communication, and the psychology of language learning and use.

    Unlike books specific to one single language, Basics of Language will help students of all languages. Readers will gain insight into the structure and use of their own language and will therefore see more clearly how the language they are learning differs from their first language. Language instructors will find the approach provocative, and the book will stimulate many new and effective ideas for teaching. Both a textbook and a reference work, Basics of Language will enhance the learning experience for anyone taking a foreign language course as well as the do-it-yourself learner.

    A new section, ``Tools and Strategies for Language Learning,'' has been added to this second edition. It comprises three chapters that focus on brain training, memory and using a dictionary. In addition, the section ``Thinking Like a Native Speaker'' has been substantially updated to include more discussion of errors made by language learners.},
  added-at = {2017-12-22T22:04:33.000+0100},
  address = {Columbus, Ohio},
  biburl = {https://www.bibsonomy.org/bibtex/23252242b6ec1b6d721c0fe1c24ebbfab/vngudivada},
  edition = {Twelfth},
  editor = {Hume, Elizabeth V. and Culicover, Peter W.},
  interhash = {30e7c63f84959c8f6a4787323ee5341a},
  intrahash = {3252242b6ec1b6d721c0fe1c24ebbfab},
  isbn = {978-0814251720},
  keywords = {Book Language},
  publisher = {Ohio State University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Basics of Language for Language Learners},
  year = 2010
}

@book{dale2000handbook,
  abstract = {This study explores the design and application of natural language text-based processing systems, based on generative linguistics, empirical corpus analysis, and artificial neural networks. It emphasizes the practical tools to accommodate the selected system.},
  added-at = {2017-12-24T21:32:32.000+0100},
  address = {Boca Raton, Florida},
  biburl = {https://www.bibsonomy.org/bibtex/205ef0b3c4a73457b456a6b63a7948537/vngudivada},
  editor = {Dale, Robert and Moisl, Hermann and Somers, Harold},
  interhash = {3e6dfe766cb6b070f3e065bea76374e2},
  intrahash = {05ef0b3c4a73457b456a6b63a7948537},
  isbn = {978-0824790004},
  keywords = {Book Handbook NLP},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Handbook of Natural Language Processing},
  year = 2000
}

@book{garner2016garners,
  abstract = {With more than a thousand new entries and more than 2,300 word-frequency ratios, the magisterial fourth edition of this book-now renamed Garner's Modern English Usage (GMEU)-reflects usage lexicography at its finest. Garner explains the nuances of grammar and vocabulary with thoroughness, finesse, and wit. He discourages whatever is slovenly, pretentious, or pedantic.

    GMEU is the liveliest and most compulsively readable reference work for writers of our time. It delights while providing instruction on skillful, persuasive, and vivid writing. Garner liberates English from two extremes: both from the hidebound "purists" who mistakenly believe that split infinitives and sentence-ending prepositions are malfeasances and from the linguistic relativists who believe that whatever people say or write must necessarily be accepted.

    The judgments here are backed up not just by a lifetime of study but also by an empirical grounding in the largest linguistic corpus ever available. In this fourth edition, Garner has made extensive use of corpus linguistics to include ratios of standard terms as compared against variants in modern print sources. No other resource provides as comprehensive, reliable, and empirical a guide to current English usage.

    For all concerned with writing and editing, GMEU will prove invaluable as a desk reference. Garner illustrates with actual examples, cited with chapter and verse, all the linguistic blunders that modern writers and speakers are prone to, whether in word choice, syntax, phrasing, punctuation, or pronunciation. No matter how knowledgeable you may already be, you're sure to learn from every single page of this book.},
  added-at = {2017-12-22T16:28:05.000+0100},
  address = {Oxfor, U.K.},
  author = {Garner, Bryan},
  biburl = {https://www.bibsonomy.org/bibtex/291db4de714cdbd50c5beb3434f8996c7/vngudivada},
  interhash = {787f2be8ae6f0f91ecdb7bc2d348ee6f},
  intrahash = {91db4de714cdbd50c5beb3434f8996c7},
  isbn = {978-0190491482},
  keywords = {Book English},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Garner's Modern English Usage},
  year = 2016
}

@book{liben2014gender,
  added-at = {2017-08-21T02:00:08.000+0200},
  address = {Boca Raton, Florida},
  biburl = {https://www.bibsonomy.org/bibtex/29358220de118a9afc77fcaceb6d3d282/vngudivada},
  editor = {Liben, Lynn S and Bigler, Rebecca S.},
  interhash = {91507327c47220c2eb02fed030b20436},
  intrahash = {9358220de118a9afc77fcaceb6d3d282},
  isbn = {978-0124115828},
  keywords = {sys:relevantfor:ecu-cc-research Book Gender},
  publisher = {Academic Press},
  series = {Advances in Child Development and Behavior},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The Role of Gender in Educational Contexts and Outcomes},
  volume = 47,
  year = 2014
}

@book{roelleke2013information,
  abstract = {Information Retrieval (IR) models are a core component of IR research and IR systems. The past decade brought a consolidation of the family of IR models, which by 2000 consisted of relatively isolated views on TF-IDF (Term-Frequency times Inverse-Document-Frequency) as the weighting scheme in the vector-space model (VSM), the probabilistic relevance framework (PRF), the binary independence retrieval (BIR) model, BM25 (Best-Match Version 25, the main instantiation of the PRF/BIR), and language modelling (LM). Also, the early 2000s saw the arrival of divergence from randomness (DFR).

Regarding intuition and simplicity, though LM is clear from a probabilistic point of view, several people stated: "It is easy to understand TF-IDF and BM25. For LM, however, we understand the math, but we do not fully understand why it works."

This book takes a horizontal approach gathering the foundations of TF-IDF, PRF, BIR, Poisson, BM25, LM, probabilistic inference networks (PIN's), and divergence-based models. The aim is to create a consolidated and balanced view on the main models.

A particular focus of this book is on the "relationships between models." This includes an overview over the main frameworks (PRF, logical IR, VSM, generalized VSM) and a pairing of TF-IDF with other models. It becomes evident that TF-IDF and LM measure the same, namely the dependence (overlap) between document and query. The Poisson probability helps to establish probabilistic, non-heuristic roots for TF-IDF, and the Poisson parameter, average term frequency, is a binding link between several retrieval models and model parameters.},
  added-at = {2017-11-04T06:12:07.000+0100},
  address = {San Rafael, California},
  author = {Roelleke, Thomas},
  biburl = {https://www.bibsonomy.org/bibtex/28a86693c0207f339903d4d954ef1ade5/vngudivada},
  doi = {10.2200/S00494ED1V01Y201304ICR027},
  eprint = {https://doi.org/10.2200/S00494ED1V01Y201304ICR027},
  interhash = {68e2dc976e7f41376c3a1614f21933a2},
  intrahash = {8a86693c0207f339903d4d954ef1ade5},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {sys:relevantfor:ecu-cc-research IR IRModel SynthesisLecture},
  number = 3,
  pages = {1-163},
  publisher = {Morgan \& Claypool},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Information Retrieval Models: Foundations and Relationships},
  volume = 5,
  year = 2013
}

@book{garside2016corpus,
  abstract = {Corpus Annotation gives an up-to-date picture of this fascinating new area of research, and will provide essential reading for newcomers to the field as well as those already involved in corpus annotation. Early chapters introduce the different levels and techniques of corpus annotation. Later chapters deal with software developments, applications, and the development of standards for the evaluation of corpus annotation. While the book takes detailed account of research world-wide, its focus is particularly on the work of the UCREL (University Centre for Computer Corpus Research on Language) team at Lancaster University, which has been at the forefront of developments in the field of corpus annotation since its beginnings in the 1970s.},
  added-at = {2017-12-24T21:32:32.000+0100},
  address = {Abingdon, United Kingdom},
  author = {Garside, R.G. and Leech, Geoffrey and Mcenery, Anthony Mark},
  biburl = {https://www.bibsonomy.org/bibtex/24ca9c82cbf1b6b0808fd64c69649afd0/vngudivada},
  interhash = {922e4251720897acc6f18f5ea0cda02f},
  intrahash = {4ca9c82cbf1b6b0808fd64c69649afd0},
  isbn = {978-1138148581},
  keywords = {Annotation Book Linguistics},
  publisher = {Routledge},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Corpus Annotation: Linguistic Information from Computer Text Corpora},
  year = 2016
}

@article{anderson2008because,
  added-at = {2017-08-21T05:22:11.000+0200},
  author = {Anderson, N. and Lankshear, C. and Timms, C. and Courtney, L.},
  biburl = {https://www.bibsonomy.org/bibtex/2fd2d3193e9d47c8dd01a12606d23ebb7/vngudivada},
  interhash = {e3d3bbb7af048bf284aca3850b992914},
  intrahash = {fd2d3193e9d47c8dd01a12606d23ebb7},
  journal = {Computers \& Education},
  keywords = {sys:relevantfor:ecu-cc-research ComputerScience Motivation},
  number = 4,
  pages = {1304 - 1318},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Because it's boring, irrelevant and I don't like computers: Why high school girls avoid professionally-oriented ICT subjects},
  volume = 50,
  year = 2008
}

@book{editorsoftheamericanheritagedictionaries2015american,
  abstract = {The American Heritage Dictionary of the English Language is the premier resource for anyone who wants to know precisely what words mean, where they come from, and how to use them effectively. It is renowned for presenting accurate and comprehensible definitions, etymologies based on the latest research, and authoritative usage guidance from the celebrated American Heritage Usage Panel.

    The fifth edition of the dictionary, published in 2011, included 10,000 new words and senses, as well as 4,000 new full-color images. This comprehensive update continues the tradition of exhaustive research and thorough review. Nearly 1,000 revisions to definitions and etymologies, 100 new words and senses, updated charts and tables, and new usage advice make this version the most current print dictionary of its size available today.

    The American Heritage Dictionary combines clear, precise definitions with useful features that make it easier to choose your words and express yourself clearly. Your words really do define you. Make the most of them with the guidance of this respected work of reference.},
  added-at = {2017-12-22T16:01:20.000+0100},
  address = {Boston, MA},
  biburl = {https://www.bibsonomy.org/bibtex/29b88b678301a332bc11536c02e370d1f/vngudivada},
  edition = {Fifth},
  editor = {{Editors of the American Heritage Dictionaries}},
  interhash = {f0ba6cb0a1edaa9ba9ace4f9f1462cb6},
  intrahash = {9b88b678301a332bc11536c02e370d1f},
  isbn = {978-0544454453},
  keywords = {Book Dictionar NLP},
  publisher = {Houghton Mifflin Harcourt},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {American Heritage Dictionary of the English Language},
  year = 2015
}

@book{weisser2009essential,
  abstract = {A gentle introduction to programming for students and researchers interested in conducting computer-based analysis in linguistics, this book is an ideal starting point for linguists approaching programming for the first time. Assuming no background knowledge of programming, the author introduces basic notions and techniques needed for linguistic programming and helps readers to develop their understanding of electronic texts.The book includes many examples based on diverse topics in linguistics in order to demonstrate the applicability of the concepts at the heart of programming. Practical examples are designed to help the reader to* Identify basic issues in handling language data, including Unicode processing* Conduct simple analyses in morphology/morphosyntax, and phonotactics* Understanding techniques for matching linguistic patterns* Learn to convert data into formats and data structures suitable for linguistic analysis* Create frequency lists from corpus materials to gather basic descriptive statistics on texts* Understand, obtain and 'clean up' web-based data* Design graphical user interfaces for writing more efficient and easy-to-use analysis tools. Two different types of exercise help readers to learn to interpret and understand illustrative sample code, and then develop algorithmic thinking and solution strategies through turning a series of instructions into sample programs. Readers will be equipped with the necessary tools for designing their own extended projects.},
  added-at = {2017-12-23T14:52:31.000+0100},
  address = {Edinburgh, UK},
  author = {Weisser, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/223d0aa65b141426da4aac8941c308ca4/vngudivada},
  interhash = {a523e19d8d3e87b5fc58da588506174c},
  intrahash = {23d0aa65b141426da4aac8941c308ca4},
  isbn = {978-0748638567},
  keywords = {Book Linguistics},
  publisher = {Edinburgh University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Essential Programming for Linguistics},
  year = 2009
}

@techreport{wilson2010running,
  abstract = {Computer science and the technologies it enables now lie at the heart of our economy, our daily lives, and scientific enterprise. As the digital age has transformed the world and workforce, U.S. K–12 education has fallen woefully behind in preparing students with the fundamental computer science knowledge and skills they need for future success. To be a well-educated citizen as we move toward an ever-more computing-intensive world and to be prepared for the jobs of the 21st Century, students must have a deeper understanding of the fundamentals of computer science.

The report finds that roughly two-thirds of the country have few computer science education standards for secondary school education, and most states treat high school computer science courses as simply an elective and not part of a student’s core education},
  added-at = {2017-08-21T04:31:53.000+0200},
  address = {New York, NY},
  author = {Wilson, Cameron and Sudol, Leigh Ann and Stephenson, Chris and Stehlik, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/276116ddbd1656ace244f981bdcc43a22/vngudivada},
  institution = {Association for Computing Machinery (ACM) and the Computer Science Teachers Association (CSTA)},
  interhash = {6c9b04e54d1447046f4d6ebeb9424c69},
  intrahash = {76116ddbd1656ace244f981bdcc43a22},
  keywords = {sys:relevantfor:ecu-cc-research ComputerScience K-12},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Running on Empty: The Failure to Teach K–12 Computer Science in the Digital Age},
  url = {http://runningonempty.acm.org/},
  year = 2010
}

@book{pustejovsky2012natural,
  abstract = {Create your own natural language training corpus for machine learning. Whether you're working with English, Chinese, or any other natural language, this hands-on book guides you through a proven annotation development cycle—the process of adding metadata to your training corpus to help ML algorithms work more efficiently. You don't need any programming or linguistics experience to get started.

	Using detailed examples at every step, you'll learn how the MATTER Annotation Development Process helps you Model, Annotate, Train, Test, Evaluate, and Revise your training corpus. You also get a complete walkthrough of a real-world annotation project.

	Define a clear annotation goal before collecting your dataset (corpus)

	Learn tools for analyzing the linguistic content of your corpus

	Build a model and specification for your annotation project

	Examine the different annotation formats, from basic XML to the Linguistic Annotation Framework

	Create a gold standard corpus that can be used to train and test ML algorithms

	Select the ML algorithms that will process your annotated data

	Evaluate the test results and revise your annotation task

	Learn how to use lightweight software for annotating texts and adjudicating the annotations

	This book is a perfect companion to O'Reilly's Natural Language Processing with Python.},
  added-at = {2017-12-24T21:32:32.000+0100},
  address = {Sebastopol, California},
  author = {Pustejovsky, James and Stubbs, Amber},
  biburl = {https://www.bibsonomy.org/bibtex/25eef05c72d76e9dc7aa6966dbeef7b06/vngudivada},
  interhash = {41898481aad4ab742f3cd6d966fb3d25},
  intrahash = {5eef05c72d76e9dc7aa6966dbeef7b06},
  isbn = {978-1449306663},
  keywords = {Annotation Book Linguistics},
  publisher = {O'Reilly Media},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Natural Language Annotation for Machine Learning: A Guide to Corpus-Building for Applications},
  year = 2012
}

@book{aijmer2014corpus,
  abstract = {Corpus linguistics is a long-established method which uses authentic language data, stored in extensive computer corpora, as the basis for linguistic research. Moving away from the traditional intuitive approach to linguistics, which used made-up examples, corpus linguistics has made a significant contribution to all areas of the field. Until very recently, corpus linguistics has focused almost exclusively on syntax and the lexicon; however corpus-based approaches to the other subfields of linguistics are now rapidly emerging, and this is the first handbook on corpus pragmatics as a field. Bringing together a team of leading scholars from around the world, this handbook looks at how the use of corpus data has informed research into different key aspects of pragmatics, including pragmatic principles, pragmatic markers, evaluation, reference, speech acts, and conversational organisation.},
  added-at = {2017-12-23T14:29:05.000+0100},
  address = {Cambridge, UK},
  biburl = {https://www.bibsonomy.org/bibtex/25fbf8abd626a974f649b832d72906136/vngudivada},
  editor = {Aijmer, Karin and R\"{u}hlemann, Christoph},
  interhash = {e8f87469655de3cd112fa3e289ce17eb},
  intrahash = {5fbf8abd626a974f649b832d72906136},
  isbn = {978-1107015043},
  keywords = {Book Corpus Handbook},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Corpus Pragmatics: A Handbook},
  year = 2014
}

@book{chomsky2015syntactic,
  abstract = {2015 Reprint of 1957 Edition. Full facsimile of the original edition. Not reproduced with Optical Recognition Software. American linguist Paul Postal wrote in 1964 that most of the "syntactic conceptions prevalent in the United States" were "versions of the theory of phrase structure grammars in the sense of Chomsky". British linguist John Lyons wrote in 1966 that "no work has had a greater influence upon the current linguistic theory than Chomsky's Syntactic Structures." Prominent historian of linguistics R. H. Robins wrote in 1967 that the publication of Chomsky's "Syntactic Structures" was "probably the most radical and important change in direction in descriptive linguistics and in linguistic theory that has taken place in recent years". Another historian of linguistics Frederick Newmeyer considers "Syntactic Structures" "revolutionary" for two reasons. Firstly, it showed that a formal yet non-empiricist theory of language was possible and more importantly, it demonstrated this possibility in a practical sense by formally treating a fragment of English grammar. Secondly, it put syntax at the center of the theory of language. Syntax was recognized as the focal point of language production, in which a finite set of rules can produce an infinite number of sentences. As a result, morphology and phonology were relegated in importance. "Syntactic Structures" also initiated an interdisciplinary dialog between philosophers of language and linguists. American philosopher John Searle wrote that "Chomsky's work is one of the most remarkable intellectual achievements of the present era, comparable in scope and coherence to the work of Keynes or Freud. It has done more than simply produce a revolution in linguistics; it has created a new discipline of generative grammar and is having a revolutionary effect on two other subjects, philosophy and psychology". With its formal and logical treatment of language, Syntactic Structures also brought linguistics and the new field of computer science closer together.

    Chomsky's book on syntactic structures is one of the first serious attempts on the part of a linguist to construct within the tradition of scientific theory-construction a comprehensive theory of language which may be understood in the same sense that a chemical, biological theory is ordinarily understood by experts in those fields. It is not a mere reorganization of the data into a new kind of library catalog, nor another speculative philosophy about the nature of Man and Language, but rather a rigorous explication of our intuitions about our language in terms of an overt axiom system, the theorems derivable from it, explicit results which may be compared with new data and other intuitions, all based plainly on an overt theory of the internal structure of languages; and it may well provide an opportunity for the application of explicit measures of simplicity to decide preference of one form over another form of grammar."Robert B. Lees in: 'Language' "I had already decided I wanted to be a linguist when I discovered this book. But it is unlikely that I would have stayed in the field without it. It has been the single most inspiring book on linguistics in my whole career.

    Chomsky's book on syntactic structures is one of the first serious attempts on the part of a linguist to construct within the tradition of scientific theory-construction a comprehensive theory of language which may be understood in the same sense that a chemical, biological theory is ordinarily understood by experts in those fields. It is not a mere reorganization of the data into a new kind of library catalog, nor another speculative philosophy about the nature of Man and Language, but rather a rigorous explication of our intuitions about our language in terms of an overt axiom system, the theorems derivable from it, explicit results which may be compared with new data and other intuitions, all based plainly on an overt theory of the internal structure of languages; and it may well provide an opportunity for the application of explicit measures of simplicity to decide preference of one form over another form of grammar.},
  added-at = {2017-12-23T00:38:21.000+0100},
  address = {Eastford, Connecticut},
  biburl = {https://www.bibsonomy.org/bibtex/293a243f070db4b0f870317ee2e730a7b/vngudivada},
  editor = {Chomsky, Noam},
  interhash = {f6bda1fa527383a620f9ade9d94aa8cb},
  intrahash = {93a243f070db4b0f870317ee2e730a7b},
  isbn = {978-1614278047},
  keywords = {Book Grammar Syntax},
  publisher = {Martino Fine Books},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Syntactic Structures},
  year = 2015
}

@book{crawford2015doing,
  abstract = {Doing Corpus Linguistics offers a practical step-by-step introduction to corpus linguistics, making use of widely available corpora and of a register analysis-based theoretical framework to provide students in Applied Linguistics and TESOL with the understanding and skills necessary to meaningfully analyze corpora and carry out successful corpus-based research. Divided into three parts – Introduction to Doing Corpus Linguistics and Register Analysis; Searches in Available Corpora; and Building Your Own Corpus, Analyzing Your Quantitative Results, and Making Sense of Data - the book emphasizes hands-on experience with performing language analysis research and in interpreting findings in a meaningful and engaging way. Readers are given multiple opportunities to analyze and apply language data by completing smaller tasks and corpus projects using publicly available corpora. The book also takes readers through the process of building a specialized corpus designed to answer a specific research question and provides detailed information on completing a final research project that includes both a written paper and an oral presentation of their specific research projects. Doing Corpus Linguistics provides students in applied linguistics and TESOL with the opportunity to gain proficiency in the technical and interpretive aspects of corpus research and to encourage them to participate in the growing field of corpus linguistics.},
  added-at = {2017-12-24T21:32:32.000+0100},
  address = {Abingdon, United Kingdom},
  author = {Crawford, William and Csomay, Eniko},
  biburl = {https://www.bibsonomy.org/bibtex/22994d06c8e986653bd72769c30a4cfdf/vngudivada},
  interhash = {d90762cd4f95d8f7ec32a61fab5d9289},
  intrahash = {2994d06c8e986653bd72769c30a4cfdf},
  isbn = {978-1138024618},
  keywords = {Book Corpus},
  publisher = {Routledge},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Doing Corpus Linguistics},
  year = 2015
}

@book{dornyei2007research,
  abstract = {This is a very practical and accessible book that offers a comprehensive overview of research methodology in applied linguistics by describing the various stages of qualitative and quantitative investigations, from collecting the data to reporting the results. It also discusses 'mixed methods research', that is, the various combinations of qualitative and quantitative methodologies.},
  added-at = {2017-12-24T21:32:32.000+0100},
  address = {Oxford, UK},
  author = {D\"{o}rnyei, Zolt\'{a}n},
  biburl = {https://www.bibsonomy.org/bibtex/252b11e76834b844b133d67a3662fbfe3/vngudivada},
  interhash = {fe423db8139650429c5b11d9a7122f48},
  intrahash = {52b11e76834b844b133d67a3662fbfe3},
  isbn = {978-0194422581},
  keywords = {Book Linguistics},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Research Methods in Applied Linguistics},
  year = 2007
}

@book{hirtle2013making,
  abstract = {In his exploration of word meaning, Walter Hirtle examines an important and controversial topic in lexical semantics: polysemy, the capacity of words to manifest a range of different meanings when employed in different contexts. Building on the work of French linguist Gustave Guillaume, Making Sense out of Meaning is a speaker-oriented study that describes how speakers form word meaning and not, as in other theories, how listeners interpret the meaning of what they hear. Hirtle develops a general model of the ways in which words and word meaning may be realized in discourse contexts and addresses such issues as the demarcation of polysemy and monosemy, metaphorical meaning, parts of speech, and the concept of conversion or zero derivation. Bringing together both lexical and grammatical components, Hirtle shows that distinct lexical senses can be observed and their relations can be understood by focusing on speakers' use of verbs and nouns. A methodical and thoughtful work, Making Sense out of Meaning situates its central question by recalling traditional views of language’s relation to thought and argues for meaning as a valid object of scientific inquiry.},
  added-at = {2017-12-27T03:31:44.000+0100},
  address = { Kingston, Canada},
  author = {Hirtle, Walter},
  biburl = {https://www.bibsonomy.org/bibtex/20c8f7ad730d97e213d64c5e1002dcf4b/vngudivada},
  interhash = {662570a33c7626eb4e173f555ef212b3},
  intrahash = {0c8f7ad730d97e213d64c5e1002dcf4b},
  isbn = {978-0773542051},
  keywords = {Book Linguistics Semantics},
  publisher = {McGill-Queen's University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Making Sense out of Meaning: An Essay in Lexical Semantics},
  year = 2013
}

@book{huddleston2002cambridge,
  abstract = {The Cambridge Grammar of the English Language is the first comprehensive descriptive grammar of English to appear for over fifteen years, a period which has seen immense developments in linguistic theory at all levels. The principal authors, Rodney Huddleston and Geoffrey Pullum, are among the world's leading scholars in this area, and they have benefited from the expertise of an international team of distinguished contributors in preparing what will be the definitive grammar for decades to come. Each chapter comprises core definitions, detailed analyses, notes explaining alternative interpretations of difficult or controversial points, and brief notes on usage and history. Numerous cross-references and an exhaustive index ensure ease of access to information. An introductory section offers guidance as to how best to use the book is provided. Rodney Huddleston was until recently Professor in the Linguistics section of the Department of English at the University of Queensland, Australia, and has been publishing important books and papers on English grammar for thirty years. Geoffrey K. Pullum is Professor of Linguistics at the University of California, Santa Cruz, and is the author of 200 articles and books on English grammar and a variety of other topics in theoretical and applied linguistics.

  Although the title may suggest that this is a usage manual or style guide, it is actually a reference work that, in the authors' words, aims to "outline and illustrate the principles that govern the construction of words and sentences ... without recommending or condemning particular usage choices." With help from an impressive group of international scholars, linguistics professors Huddleston (English Grammar: An Outline) and Pullum (Phonetic Symbol Guide) here provide a comprehensive and detailed look at the principles of the English language. Chapters are divided into several subsections, with ample examples and explanations for each concept; both a conceptual and a lexical index are included. Although the writers attempt to "bridge a gap...between traditional grammar and the partial descriptions of English grammar proposed by those working in the field of linguistics," the book leans heavily toward the field of linguistics. This makes it more accessible to the informed student of linguistics than to the simple lover of the English language. It is nevertheless an authoritative addition to the fields of both English grammar and linguistics. Recommended for all academic libraries.},
  added-at = {2017-12-26T14:58:14.000+0100},
  address = {Cambridge},
  author = {Huddleston, Rodney and Pullum, Geoffrey K.},
  biburl = {https://www.bibsonomy.org/bibtex/2652691d4e85b0176a63e94048df5ac3e/vngudivada},
  interhash = {1dd39db4b1f47f9b7bc677c93a5e0619},
  intrahash = {652691d4e85b0176a63e94048df5ac3e},
  isbn = {978-0521431460},
  keywords = {Book Grammar Linguistics},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {The Cambridge Grammar of the English Language},
  year = 2002
}

@book{goddard2011semantic,
  abstract = {This lively textbook introduces students and scholars to practical and precise methods for articulating the meanings of words and sentences, and for revealing connections between language and culture. Topics range over emotions, speech acts, words for animals and artifacts, motion, activity verbs, causatives, discourse particles, and nonverbal communication. Alongside English, it features a wide range of other languages, including Malay, Chinese, Japanese, Polish, Spanish, and Australian Aboriginal languages. Undergraduates, graduate students and professional linguists alike will benefit from Goddard's wide-ranging summaries, clear explanations and analytical depth. Meaning is fundamental to language and linguistics. This book shows that the study of meaning can be rigorous, insightful and exciting.},
  added-at = {2017-12-27T15:05:47.000+0100},
  address = {Oxford, UK},
  author = {Goddard, Cliff},
  biburl = {https://www.bibsonomy.org/bibtex/2e35f6466b5474ae9c3b49c62a005ee2c/vngudivada},
  edition = {Second},
  interhash = {7cc060f7a6145d35f1fe1f04e3355934},
  intrahash = {e35f6466b5474ae9c3b49c62a005ee2c},
  isbn = {978-0199560288},
  keywords = {Book Linguistics Semantics},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Semantic Analysis: A Practical Introduction},
  year = 2011
}

@book{goddard2016words,
  abstract = {In a series of cross-cultural investigations of word meaning, Cliff Goddard and Anna Wierzbicka examine key expressions from different domains of the lexicon - concrete, abstract, physical, sensory, emotional, and social. They focus on complex and culturally important words in a range of languages that includes English, Russian, Polish, French, Warlpiri, and Malay. Some are basic like men, women, and children or abstract nouns like trauma and violence; others describe qualities such as hot, hard, and rough, emotions like happiness and sadness, or feelings like pain. They ground their discussions in real examples from different cultures and draw on work ranging from Leibniz, Locke, and Bentham, to popular works such as autobiographies and memoirs, and the Dalai Lama on happiness.

	The book opens with a review of the neglected status of lexical semantics in linguistics. The authors consider a range of analytical issues including lexical polysemy, semantic change, the relationship between lexical and grammatical semantics, and the concepts of semantic molecules and templates. Their fascinating book is for everyone interested in the relations between meaning, culture, ideas, and words.},
  added-at = {2017-12-27T04:02:27.000+0100},
  address = {Oxford, UK},
  author = {Goddard, Cliff and Wierzbicka, Anna},
  biburl = {https://www.bibsonomy.org/bibtex/2047f842e9ceba370d49e2a31d09bcf40/vngudivada},
  interhash = {48f2ff4bcf98be28bfd6ec7c5ba0ddce},
  intrahash = {047f842e9ceba370d49e2a31d09bcf40},
  isbn = {978-0198783558},
  keywords = {Book Linguistics Semantics},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Words and Meanings: Lexical Semantics Across Domains, Languages, and Cultures},
  year = 2016
}

@book{ide2017handbook,
  abstract = {This handbook offers a thorough treatment of the science of linguistic annotation. Leaders in the field guide the reader through the process of modeling, creating an annotation language, building a corpus and evaluating it for correctness. Essential reading for both computer scientists and linguistic researchers.

	Linguistic annotation is an increasingly important activity in the field of computational linguistics because of its critical role in the development of language models for natural language processing applications. Part one of this book covers all phases of the linguistic annotation process, from annotation scheme design and choice of representation format through both the manual and automatic annotation process, evaluation, and iterative improvement of annotation accuracy.  The second part of the book includes case studies of annotation projects across the spectrum of linguistic annotation types, including morpho-syntactic tagging, syntactic analyses, a range of semantic analyses (semantic roles, named entities, sentiment and opinion), time and event and spatial analyses, and discourse level analyses including discourse structure, co-reference, etc. Each case study addresses the various phases and processes discussed in the chapters of part one.

	Nancy Ide is Professor of Computer Science at Vassar College in Poughkeepsie, New York, USA. She has been in the field of computational linguistics for over 30 years and made significant contributions to research in word sense disambiguation, computational lexicography, discourse analysis, and the use of semantic web technologies for language data. She is founder of the Text Encoding Initiative (TEI), the first major standard for representing electronic language data, and later developed the XML Corpus Encoding Standard (XCES). More recently, she co-developed the ISO LAF/GrAF representation format for linguistically annotated data. She has also developed major corpora for American English, including the Open American National Corpus (OANC) and the Manually Annotated Sub-Corpus (MASC), and has been a pioneer in efforts to foster open data and resources. Professor Ide is Co-Editor-in-Chief of the journal Language Resources and Evaluation and Editor of the Springer book series Text, Speech, and Language Technology.

	James Pustejovsky is the TJX Feldberg professor of computer science at Brandeis University in Waltham, Massachusetts, United States. His expertise includes theoretical and computational modeling of language, specifically: Computational linguistics, Lexical semantics, Knowledge representation, temporal and spatial reasoning and Extraction. His main topics of research are Natural language processing generally, and in particular, the computational analysis of linguistic meaning. He proposed Generative Lexicon theory in lexical semantics. His other interests include temporal reasoning, event semantics, spatial language, language annotation, computational linguistics, and machine learning.},
  added-at = {2017-12-24T21:32:32.000+0100},
  address = {New York, NY},
  biburl = {https://www.bibsonomy.org/bibtex/2a073b2ba77a52d68839ff4880ef2d68c/vngudivada},
  editor = {Ide, Nancy and Pustejovsky, James},
  interhash = {99bfb9e7a728acc86e4217b798156509},
  intrahash = {a073b2ba77a52d68839ff4880ef2d68c},
  isbn = {978-9402408799},
  keywords = {Annotation Book Linguistics},
  publisher = {Springer},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Handbook of Linguistic Annotation},
  year = 2017
}

@book{carter2006cambridge,
  abstract = {A major reference grammar offering comprehensive coverage of spoken and written English based on real everyday usage. A major reference grammar from the world's leading grammar publisher. It offers clear explanations of spoken and written English based on real everyday usage. The clear two-part structure makes the book particularly user-friendly. The accompanying CD-ROM makes the Cambridge Grammar of English even more accessible with: The whole book in handy, searchable format. Audio recordings of all the examples from the book. Links to the Cambridge Advanced Learner's Dictionary online for instant definitions of new vocabulary.

	Ronald Carter is Professor of Modern English Language at the University of Nottingham, UK. He is the author, co-author and editor of over 40 books and 100 academic papers in the field of language education, applied linguistics, literary language study and English grammar and vocabulary. He is the author (with Michael McCarthy) of the Cambridge Grammar of English (CUP, 2006) and English Grammar Today (CUP, 2011, with Michael McCarthy, Geraldine Mark and Anne O'Keeffe).},
  added-at = {2017-12-26T18:30:54.000+0100},
  address = {Cambridge, UK},
  author = {Carter, Ronald and McCarthy, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/28acc2e8d91320fc9dcf9de5fa79e9a56/vngudivada},
  interhash = {fde4ff8fafd0b0310c1317a69d9f1c06},
  intrahash = {8acc2e8d91320fc9dcf9de5fa79e9a56},
  isbn = {978-0521-67439-3},
  keywords = {Book Grammar},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Cambridge Grammar of English: A Comprehensive Guide},
  year = 2006
}

@book{quirk2010comprehensive,
  abstract = {Description a comprehensive grammar of the English language is the most thorough and most definitive grammar of modern English ever written.

    Professors Quirk, Greenbaum, Leech, and Svartvik, who are acknowledged to be leading authorities in the field of English language, have made this the climax of twenty years of collaborative work.

    Building on the strength of their previous work, especially a grammar of comprehensive English. Unchallenged since its publication in 1972 as the standard reference work in the field, the authors have produced an even more comprehensive and perceptive synthesis of current grammatical description. They have drawn collectively on the most recent research, including new findings not only in grammar but also in the neighboring fields of semantics, pragmatics, and text linguistics.

    Discourse features are prominent throughout, as well as being the theme of an entire chapter entitled from sentence to text. The grammatically analyzed data in the survey of English usage, together with other collections of material, both American and British, has again proved an invaluable source of new insights, and, equally important, has provided authentic examples of use.

    Copious examples for clarification are a hallmark of the work of professors Quirk, Greenbaum, Leech, and Svartvik, and of course are of immense help to all students and teachers of language, but particularly those whose first language is not English.},
  added-at = {2017-12-25T14:56:38.000+0100},
  address = {London, UK},
  author = {Quirk, Randolph and Greenbaum, Sidney and Leech, Geoffrey and Svartvik, Jan},
  biburl = {https://www.bibsonomy.org/bibtex/24d0103fa9bd9e307414e53a1c54da4e7/vngudivada},
  interhash = {49a0a8ef56e4aa98614419fd97058de4},
  intrahash = {4d0103fa9bd9e307414e53a1c54da4e7},
  isbn = {978-8131733431},
  keywords = {Book Corpus Grammar},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {A Comprehensive Grammar of The English Language},
  year = 2010
}

@incollection{leech1992corpora,
  added-at = {2017-12-25T02:52:59.000+0100},
  address = {{Berlin/New} York},
  author = {Leech, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/241ed2d00f82929d80646c95b00d69a98/vngudivada},
  booktitle = {Directions in Corpus Linguistics. Proceedings of the Nobel Symposium 82},
  editor = {Svartvik, Jan},
  interhash = {8eed7e4a011113ed7c03898c1fa30bf9},
  intrahash = {41ed2d00f82929d80646c95b00d69a98},
  keywords = {Corpora LinguisticPerformance},
  pages = {105–122},
  publisher = {Mouton de Gruyter},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Corpora and theories of linguistic performance},
  volume = 65,
  year = 1992
}

@book{zipf1949human,
  abstract = {The principle of least effort is a broad theory that covers diverse fields from evolutionary biology to webpage design. It postulates that animals, people, even well designed machines will naturally choose the path of least resistance or "effort". This is perhaps best known or at least documented among researchers in the field of library and information science. Their principle states that an information seeking client will tend to use the most convenient search method, in the least exacting mode available. Information seeking behavior stops as soon as minimally acceptable results are found. This theory holds true regardless of the user's proficiency as a searcher, or their level of subject expertise. The principle of least effort is analogous to the path of least resistance. The principle was studied by linguist George Kingsley Zipf, author of this classic treatment of the subject. He theorized that the distribution of word use was due to the tendency to communicate efficiently with least effort and this theory is known as Zipf's Law.},
  added-at = {2017-12-24T21:32:32.000+0100},
  address = {Eastford, Connecticut},
  author = {Zipf, George Kingsley},
  biburl = {https://www.bibsonomy.org/bibtex/2e504e77e0d63e1193d6d9e4f93378a45/vngudivada},
  interhash = {cd7d27d0def3aca8edecfea67070ddc6},
  intrahash = {e504e77e0d63e1193d6d9e4f93378a45},
  isbn = {978-1614273127},
  keywords = {Book NLP},
  publisher = {Martino Fine Books},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology},
  year = 1949
}

@book{jones2017corpus,
  abstract = {Corpus Linguistics for Grammar provides an accessible and practical introduction to the use of corpus linguistics to analyse grammar, demonstrating the wider application of corpus data and providing readers with all the skills and information they need to carry out their own corpus-based research.

	This book:

	explores the kinds of corpora available and the tools which can be used to analyse them;

	looks at specific ways in which features of grammar can be explored using a corpus through analysis of areas such as frequency and colligation;

	contains exercises, worked examples and suggestions for further practice with each chapter;

	provides three illustrative examples of potential research projects in the areas of English Literature, TESOL and English Language.

	Corpus Linguistics for Grammar is essential reading for students undertaking corpus-based research into grammar, or studying within the areas of English Language, Literature, Applied Linguistics and TESOL.},
  added-at = {2017-12-24T22:26:14.000+0100},
  address = {Abingdon, United Kingdom},
  author = {Jones, Christian and Waller, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/2bea93cf4d78837b77cc619adac8c4dd5/vngudivada},
  interhash = {ae2809a6afacc10223ee932fb3364cd5},
  intrahash = {bea93cf4d78837b77cc619adac8c4dd5},
  isbn = {978-0415746410},
  keywords = {Book Corpus Grammar},
  publisher = {Routledge},
  series = {Routledge Corpus Linguistics Guides},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Corpus Linguistics for Grammar: A guide for research},
  year = 2017
}

@book{leech1981semantics,
  abstract = {In recent decades a revolution in linguistic thought has restored semantics to a central position in the study of language. Geoffrey Leech's book is a study of the issues thus brought to the fore.},
  added-at = {2017-12-27T03:35:29.000+0100},
  address = {Middlesex, UK},
  author = {Leech, Geoffrey N.},
  biburl = {https://www.bibsonomy.org/bibtex/22e8b9b7e25ecf57bb4e3ad646612790a/vngudivada},
  edition = {Second},
  interhash = {4e46d3e090380e32dc8f06992d9e038d},
  intrahash = {2e8b9b7e25ecf57bb4e3ad646612790a},
  isbn = {978-0140134872},
  keywords = {Book Linguistics Semantics},
  publisher = {Penguin},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Semantics: The Study of Meaning},
  year = 1981
}

@book{meyer2009introducing,
  abstract = {Are you looking for a genuine introduction to the linguistics of English that provides a broad overview of the subject that sustains students' interest and avoids excessive detail? Introducing English Linguistics accomplishes this goal in two ways. First, it takes a top-down approach to language, beginning with the largest unit of linguistic structure, the text, and working its way down through successively smaller structures (sentences, words, and finally speech sounds). The advantage of presenting language this way is that students are first given the larger picture - they study language in context - and then see how the smaller pieces of language are a consequence of the larger goals of linguistic communication. Second, the book does not contain invented examples, as is the case with most comparable texts, but instead takes its sample materials from the major computerized databases of spoken and written English, giving students a more realistic view of language.},
  added-at = {2017-12-25T03:48:19.000+0100},
  address = {Cambridge, UK},
  author = {Meyer, Charles F.},
  biburl = {https://www.bibsonomy.org/bibtex/2abaf09bd4382abde97b3ac7afd8124ac/vngudivada},
  interhash = {3373f9d8c3aa4192437fdeb9a4829cd6},
  intrahash = {abaf09bd4382abde97b3ac7afd8124ac},
  isbn = {978-0521541220},
  keywords = {Book Linguistics},
  publisher = {Cambridge University Press},
  series = {Cambridge Introductions to Language and Linguistics},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Introducing English Linguistics},
  year = 2009
}

@book{comrie1989language,
  abstract = {Since its first publication, Language Universals and Linguistic Typology has become established as the leading introductory account of one of the most productive areas of linguistics—the analysis, comparison, and classification of the common features and forms of the organization of languages. Adopting an approach to the subject pioneered by Greenberg and others, Bernard Comrie is particularly concerned with syntactico-semantic universals, devoting chapters to word order, case making, relative clauses, and causative constructions. His book is informed throughout by the conviction that an exemplary account of universal properties of human language cannot restrict itself to purely formal aspects, nor focus on analysis of a single language. Rather, it must also consider language use, relate formal properties to testable claims about cognition and cognitive development, and treat data from a wide range of languages. This second edition has been revised and updated to take full account of new research in universals and typology in the past decade, and more generally to consider how the approach advocated here relates to recent advances in generative grammatical theory.

  	This book is excellent and should be in every linguist's library. The author follows Greenberg rather than Chomsky in his approach to language universals. You do need some knowledge of linguistics in order to appreciate the book.

  	Bernard Comrie is chair of the department of linguistics at the University of Southern California. He is the author of many publications including Aspect and Tense, and is editor of Studies in Language.},
  added-at = {2017-12-26T04:23:13.000+0100},
  address = {Chicago, Illinois},
  author = {Comrie, Bernard},
  biburl = {https://www.bibsonomy.org/bibtex/28db6801337774829fd185b3728c1addf/vngudivada},
  edition = {Second},
  interhash = {26c5ccccaf5952d3ca1755b976878cf4},
  intrahash = {8db6801337774829fd185b3728c1addf},
  isbn = {978-0226114330},
  keywords = {Book Linguistics Typology},
  publisher = {University of Chicago Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Language Universals and Linguistic Typology: Syntax and Morphology},
  year = 1989
}

@book{halliday2004introduction,
  abstract = {Fully updated and revised, this fourth edition of Halliday's Introduction to Functional Grammar explains the principles of systemic functional grammar, enabling the reader to understand and apply them in any context. Halliday's innovative approach of engaging with grammar through discourse has become a worldwide phenomenon in linguistics.

	Updates to the new edition include:

	Recent uses of systemic functional linguistics to provide further guidance for students, scholars and researchers

	More on the ecology of grammar, illustrating how each major system serves to realize a semantic system

	A systematic indexing and classification of examples

	More from corpora, thus allowing for easy access to data

	Halliday's Introduction to Functional Grammar, Fourth Edition, is the standard reference text for systemic functional linguistics and an ideal introduction for students and scholars interested in the relation between grammar, meaning and discourse.},
  added-at = {2017-12-26T03:14:55.000+0100},
  author = {Halliday, Michael and Matthiessen, Christian},
  biburl = {https://www.bibsonomy.org/bibtex/2c168a1479202df915e309f2d4589d4f1/vngudivada},
  edition = {Fourth},
  interhash = {f1ae273ab22d26cf4dce2e4a2b37b5e2},
  intrahash = {c168a1479202df915e309f2d4589d4f1},
  isbn = {978-1444146608},
  keywords = {Book Grammar},
  publisher = {Routledge},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {An Introduction to Functional Grammar},
  year = 2004
}

@book{rich2007automata,
  abstract = {The theoretical underpinnings of computing form a standard part of almost every computer science curriculum. But the classic treatment of this material isolates it from the myriad ways in which the theory influences the design of modern hardware and software systems. The goal of this book is to change that. The book is organized into a core set of chapters (that cover the standard material suggested by the title), followed by a set of appendix chapters that highlight application areas including programming language design, compilers, software verification, networks, security, natural language processing, artificial intelligence, game playing, and computational biology.

	The core material includes discussions of finite state machines, Markov models, hidden Markov models (HMMs), regular expressions, context-free grammars, pushdown automata, Chomsky and Greibach normal forms, context-free parsing, pumping theorems for regular and context-free languages, closure theorems and decision procedures for regular and context-free languages, Turing machines, nondeterminism, decidability and undecidability, the Church-Turing thesis, reduction proofs, Post Correspondence problem, tiling problems, the undecidability of first-order logic, asymptotic dominance, time and space complexity, the Cook-Levin theorem, NP-completeness, Savitch's Theorem, time and space hierarchy theorems, randomized algorithms and heuristic search. Throughout the discussion of these topics there are pointers into the application chapters. So, for example, the chapter that describes reduction proofs of undecidability has a link to the security chapter, which shows a reduction proof of the undecidability of the safety of a simple protection framework.},
  added-at = {2017-12-26T01:07:01.000+0100},
  address = {London, UK},
  author = {Rich, Elaine A.},
  biburl = {https://www.bibsonomy.org/bibtex/22485d04ebafd553bae7525cdfeb40cf4/vngudivada},
  interhash = {66c52c0da04951fff815597e9c9a9b59},
  intrahash = {2485d04ebafd553bae7525cdfeb40cf4},
  isbn = {978-0132288064},
  keywords = {Automata Book Grammar},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Automata, Computability and Complexity: Theory and Applications},
  year = 2007
}

@book{szudarski2017corpus,
  abstract = {Corpus Linguistics for Vocabulary provides a practical introduction to using corpus linguistics in vocabulary studies. Using freely available corpus tools, the author provides a step-by-step guide on how corpora can be used to explore key vocabulary-related research questions and topics such as:

	The frequency of English words and how to choose which ones should be taught to learners;

	How spoken vocabulary differs from written vocabulary, and how academic vocabulary differs from general vocabulary;

	How vocabulary contributes to the structure of discourse, and the pragmatic functions it fulfills.

	Featuring case studies and tasks throughout, Corpus Linguistics for Vocabulary provides a clear and accessible guide and is essential reading for students and teachers wanting to understand, appreciate and conduct corpus-based research in vocabulary studies.},
  added-at = {2017-12-24T22:25:43.000+0100},
  address = {Abingdon, United Kingdom},
  author = {Szudarski, Pawel},
  biburl = {https://www.bibsonomy.org/bibtex/272690f3c3bc57a7edfdfff08f06ce5a7/vngudivada},
  interhash = {17277f77e1d4c1affb0341684f587cfd},
  intrahash = {72690f3c3bc57a7edfdfff08f06ce5a7},
  isbn = {978-1138187221},
  keywords = {Book Corpus Linguistics Vocabulary},
  publisher = {Routledge},
  series = {Routledge Corpus Linguistics Guides},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Corpus Linguistics for Vocabulary: A Guide for Research},
  year = 2017
}

@book{aarts2011oxford,
  abstract = {Written by Bas Aarts, one of Britain's leading grammarians, Oxford Modern English Grammar is a brand new and definitive guide to English grammar. This indispensable handbook covers both British and American English, and makes use of authentic spoken and written examples.

	Packed with tables, diagrams, and numerous example sentences, and assuming no prior knowledge of grammatical concepts on the part of the reader, this volume offers an unmatched guide to the structure of contemporary English. Arranged in three clear parts for ease of use, the Grammar's comprehensive coverage ranges from the very basic--such as word structure, simple and complex phrases, and clause types--to the more sophisticated topics that lie at the intersection of grammar and meaning, including tense and aspect, mood and modality, and information structuring. How do words formed by "compounding" differ from words formed by "conversion"? How many verbs in English can take a declarative clause functioning as direct object (i.e., "decide that..." or "believe that...")? What is the relationship between a matrix clause and a subordinate clause? What is the present futurate? The past futurate? The present perfect? How does the grammar of English encode such semantic notions as "possibility," "probability," "necessity," "obligation," "permission," "intention," or "ability"? Aarts answers all these questions, clearly and engagingly, deeply enriching the reader's understanding of the English language.

	Oxford Modern English Grammar will be invaluable for those with an interest in the English language, undergraduate students of all disciplines, and for anyone who would like a clear guide to English grammar and how to use it.},
  added-at = {2017-12-25T15:22:17.000+0100},
  address = {Oxford, UK},
  author = {Aarts, Bas},
  biburl = {https://www.bibsonomy.org/bibtex/27003d0a746433a3c376f011e68ef0bfb/vngudivada},
  interhash = {94acdf83c0cf39ef934bbd517354d753},
  intrahash = {7003d0a746433a3c376f011e68ef0bfb},
  isbn = {978-0199533190},
  keywords = {Book Grammar},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Oxford Modern English Grammar},
  year = 2011
}

@book{biber1999longman,
  abstract = {The Longman Grammar of Spoken and Written English is a revolutionary, corpus-based reference grammar of English, based on a groundbreaking research project to analyze the ways in which English grammar is really used. The book looks at four text types -- conversation, fiction, news reportage, and academic prose -- and reports statistical findings as well as examining the reasons that condition a particular grammatical choice.

	Encompasses a six-year research project into the ways in which language is really used.

	Reveals which structures, tenses, and lexical choices occur, and how their distribution differs in different text types.

	Analyzes natural language in each chapter, based on analysis of the real everyday conversations in the Longman Spoken American Corpus and the British National Corpus.

	Contains over 350 tables and graphs that show the frequency of constructions across different registers, from conversation to fiction to academic prose.

	Douglas Biber is Professor of Applied Linguistics (English Department) at Northern Arizona University. His books and articles deal with language use in spoken and written discourse.},
  added-at = {2017-12-25T15:12:06.000+0100},
  author = {Biber, Douglas and Johansson, Stig and Leech, Geoffrey and Conrad, Susan and Finegan, Edward},
  biburl = {https://www.bibsonomy.org/bibtex/2fb0c9dc3e9fc719450583f3752f1b4a9/vngudivada},
  interhash = {0f237721823b4154d3a94e17a8d6ebc9},
  intrahash = {fb0c9dc3e9fc719450583f3752f1b4a9},
  isbn = {978-0582237254},
  keywords = {Book Grammar},
  publisher = {Longman},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Longman Grammar of Spoken and Written English},
  year = 1999
}

@book{leech1983principles,
  abstract = {Over the years, pragmatics - the study of the use and meaning of utterances to their situations - has become a more and more important branch of linguistics, as the inadequacies of a purely formalist, abstract approach to the study of language have become more evident. This book presents a rhetorical model of pragmatics: that is, a model which studies linguistic communication in terms of communicative goals and principles of 'good communicative behavior'.

	In this respect, Geoffrey Leech argues for a rapprochement between linguistics and the traditional discipline of rhetoric. He does not reject the Chomskvan revolution of linguistics, but rather maintains that the language system in the abstract - i.e. the 'grammar' broadly in Chomsky's sense - must be studied in relation to a fully developed theory of language use. There is therefore a division of labor between grammar and rhetoric, or (in the study of meaning) between semantics and pragmatics.

	The book's main focus is thus on the development of a model of pragmatics within an overall functional model of language. In this it builds on the speech act theory of Austin and Searle, and the theory of conversational implicature of Grice, but at the same time enlarges pragmatics to include politeness, irony, phatic communion, and other social principles of linguistic behavior.},
  added-at = {2017-12-27T03:33:49.000+0100},
  address = {New York, NY},
  author = {Leech, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/2fa8eb9f6eeb18de7e509386ec640f913/vngudivada},
  interhash = {ff6161fe3d60c88bac970b598de33034},
  intrahash = {fa8eb9f6eeb18de7e509386ec640f913},
  isbn = {978-0582551107},
  keywords = {Book Linguistics Pragmatics},
  publisher = {Longman},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Principles of Pragmatics},
  year = 1983
}

@book{lowie2012essential,
  abstract = {Using numbers and research methods in your studies can be a little daunting at first. This book gives you straightforward guidance and no-nonsense advice on using, analysing and interpreting statistics and methodologies in empirical work. Each topic is introduced by an activity to help stimulate thought, and this is followed by a full explanation and exercises.

	Essential Statistics for Applied Linguistics:

		– explores different types of research and explains how to choose a methodology

		– gives clear explanations of statistical and mathematical terms

		– builds confidence and skills through hands-on exercises, examples and 'how-to' sections

		– offers online learning resources, including demonstrations with real data to be used with the software package SPSS

	Assuming no previous knowledge of methodology and statistics, this book will help you to acquire and strengthen your understanding, practice what you have learnt and feel comfortable with applying this new expertise to your own work.},
  added-at = {2017-12-26T01:07:24.000+0100},
  address = {Basingstoke, United Kingdom},
  author = {Lowie, Wander and Seton, Bregtje},
  biburl = {https://www.bibsonomy.org/bibtex/29e7ccb76f7fbfa1e90ed6f1e6022aca5/vngudivada},
  interhash = {bed2c309d4dff7fe293987fc8137f301},
  intrahash = {9e7ccb76f7fbfa1e90ed6f1e6022aca5},
  isbn = {978-0230304819},
  keywords = {Book Linguistics Statistics},
  publisher = {Palgrave},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Essential Statistics for Applied Linguistics},
  year = 2012
}

@book{croft2003typology,
  abstract = {The second edition of this essential textbook has been thoroughly rewritten and updated to reflect advances in typology and universals over the past decade. It reviews new methodologies such as the semantic map model and questions of syntactic argumentation; discussion of current debates over explanations for specific classes of universals; and comparison of the typological and generative approaches to language.

  	William Croft is Professor of Linguistics at the University of Manchester. His books include Studies in Typology and Diachrony for Joseph H. Greenberg (edited with Keith Denning and Suzanne Kemmer, 1990), Typology and Universals (Cambridge, 1990), Syntactic Categories and Grammatical Relations: the Cognitive Organization of Information (1991), Explaining Language Change: An Evolutionary Approach (2000), and Radical Construction Grammar: Syntactic Theory in Typological Perspective (2001).},
  added-at = {2017-12-26T04:18:12.000+0100},
  address = {Cambridge, UK},
  author = {Croft, William},
  biburl = {https://www.bibsonomy.org/bibtex/248c34ccc5b6a368bfdbf4528757ff97f/vngudivada},
  edition = {Second},
  interhash = {b20933d1d351980cd2664a9f76405f3e},
  intrahash = {48c34ccc5b6a368bfdbf4528757ff97f},
  isbn = {978-1107627529},
  keywords = {Book Linguistics Typology},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:45.000+0100},
  title = {Typology and Universals},
  year = 2003
}

@book{belew2001finding,
  abstract = {The World Wide Web is rapidly filling with more text than anyone could have imagined a short time ago. However, the task of determining which data is relevant has become appreciably harder. In this original new work Richard Belew brings a cognitive science perspective to the study of information as a computer science discipline. He introduces the idea of Finding Out About (FOA), the process of actively seeking out information relevant to a topic of interest. Belew describes all facets of FOA, ranging from creating a good characterization of what the user seeks to evaluating the successful performance of search engines. His volume clearly shows how to build many of the tools that are useful for searching collections of text and other media. While computer scientists make up the book's primary audience, Belew skillfully presents technical details in a manner that makes important themes accessible to readers more comfortable with words than equations. Resources are available from the book's web site},
  added-at = {2018-01-09T18:35:20.000+0100},
  address = {New York, NY},
  author = {Belew, Richard K.},
  biburl = {https://www.bibsonomy.org/bibtex/2c5b500508ad67e6a83de7b6708a4d65a/vngudivada},
  interhash = {152fb47c35a399842f0be4bec105be96},
  intrahash = {c5b500508ad67e6a83de7b6708a4d65a},
  isbn = {978-0521630283},
  keywords = {Book IR},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Finding Out About: A Cognitive Perspective on Search Engine Technology and the WWW},
  year = 2001
}

@book{aston1997handbook,
  abstract = {This textbook is designed to provide a detailed understanding of the principles and practices underlying the use of large language corpora in exploratory learning and English language teaching and research. It focuses on the largest and most representative corpus of spoken and written data yet compiled -- the British National Corpus -- and on the search tool SARA (SGML Aware Retrieval Application). The method adopted is to provide a graded series of exercises, each introducing at the same time new features of the software and new techniques or applications for computer-assisted language learning.},
  added-at = {2018-01-01T15:12:25.000+0100},
  address = {Edinburgh, UK},
  author = {Aston, Guy and Burnard, Lou},
  biburl = {https://www.bibsonomy.org/bibtex/22912fcb6d9da59589c5791c61ee39f42/vngudivada},
  interhash = {3598547b20b588c3b6ec653998ad89c5},
  intrahash = {2912fcb6d9da59589c5791c61ee39f42},
  isbn = {978-0748610556},
  keywords = {BNC Book Corpus Linguistics SARA},
  publisher = {Edinburgh University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The BNC Handbook: Exploring the British National Corpus with SARA},
  year = 1997
}

@book{okeeffe2012routledge,
  abstract = {The Routledge Handbook of Corpus Linguistics provides a timely overview of a dynamic and rapidly growing area with a widely applied methodology. Through the electronic analysis of large bodies of text, corpus linguistics demonstrates and supports linguistic statements and assumptions. In recent years it has seen an ever-widening application in a variety of fields: computational linguistics, discourse analysis, forensic linguistics, pragmatics and translation studies.

	Bringing together experts in the key areas of development and change, the handbook is structured around six themes which take the reader through building and designing a corpus to using a corpus to study literature and translation.

	A comprehensive introduction covers the historical development of the field and its growing influence and application in other areas. Structured around five headings for ease of reference, each contribution includes further reading sections with three to five key texts highlighted and annotated to facilitate further exploration of the topics. The Routledge Handbook of Corpus Linguistics is the ideal resource for advanced undergraduates and postgraduates.


  	Anne O'Keeffe is senior lecturer in Applied Linguistics, Department of English Language and Literature, Mary Immaculate College, University of Limerick, Ireland.  She is co-author of Introducing Pragmatics In Use (Routledge, 2011).

	Michael McCarthy is Emeritus Professor of Applied Linguistics at the University of Nottingham, UK, Adjunct Professor of Applied Linguistics at the Pennsylvania State University, USA, and Adjunct Professor of Applied Linguistics at the University of Limerick, Ireland.
  },
  added-at = {2017-12-31T14:55:26.000+0100},
  address = {Abingdon, UK},
  biburl = {https://www.bibsonomy.org/bibtex/2ed001bf49d6e2be19de9ac18103e9611/vngudivada},
  editor = {O'Keeffe, Anne and McCarthy, Michael},
  interhash = {b6fd50d3f6e308568d0fcf9a30d26746},
  intrahash = {ed001bf49d6e2be19de9ac18103e9611},
  isbn = {978-0415622639},
  keywords = {Book Corpus Linguistics},
  publisher = {Routledge},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The Routledge Handbook of Corpus Linguistics},
  year = 2012
}

@book{rudder2010language,
  abstract = {This workbook introduces the International Phonetic Alphabet as a way of accurately representing human speech and strengthening your command of pronunciation in any language. You will learn about vowels, consonants, phonemes \& allophones, syllables and utterances. Build a strong foundation for writing and pronouncing the IPA with these lessons. The course provides practice exercises, clear examples of each topic and plenty of opportunities to copy and work with IPA transcriptions. Extras include a selection of symbols used for transcribing English, vowel and consonant charts, copy practice worksheets for unfamiliar IPA symbols and helpful illustrations of speech anatomy. The book ends with answers to the exercises and a short topical index.},
  added-at = {2017-12-28T22:24:27.000+0100},
  address = {Seattle, Washington},
  author = {Rudder, Joshua},
  biburl = {https://www.bibsonomy.org/bibtex/2b4f50827bef8720707e3e1bb35c69884/vngudivada},
  interhash = {a2f22fe0aae8cbada20c25be4cd0e9a6},
  intrahash = {b4f50827bef8720707e3e1bb35c69884},
  isbn = {978-1453837085},
  keywords = {Book IPA Phonetics},
  publisher = {CreateSpace Independent Publishing Platform},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The IPA for Language Learning: An Introduction to the International Phonetic Alphabet},
  year = 2010
}

@book{song2013oxford,
  abstract = {This handbook provides a critical state-of-the-art overview of work in linguistic typology. It examines the directions and challenges of current research and shows how these reflect and inform work on the development of linguistic theory. It describes what typologists have revealed about language in general and discovered (and continue to discover) about the richly various ways in which meaning and expression are achieved in the world's languages.

	Typological research extends across all branches of linguistics. The degree to which the characteristics of language are universal or particular is crucial to the understanding of language and its relation to human nature and culture. This book is an essential source of reference for linguists of all theoretical persuasions. It is a vital companion for all those working in linguistic typology or undertaking linguistic fieldwork on one or more languages.},
  added-at = {2017-12-31T14:45:52.000+0100},
  address = {Oxford, UK},
  author = {Song, Jae Jung},
  biburl = {https://www.bibsonomy.org/bibtex/2b502280874afa896cf17c6ac7289bce7/vngudivada},
  interhash = {fb2564c7b10f5c9a73073647609b9b24},
  intrahash = {b502280874afa896cf17c6ac7289bce7},
  isbn = {978-0199658404},
  keywords = {Book Linguistics Typology},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The Oxford Handbook of Linguistic Typology},
  year = 2013
}

@conference{aoughlis2006electronic,
  abstract = {Automatic text analysis systems can lexically recognize a word only if it already exists in the electronic dictionary. The same thing is true for the NOOJ system analysis programs. One understands here by electronic dictionaries the lexical databases where all information is explicit because they are intended for computer programs use. These bases aim at the modelling of the language, which distinguishes them from the electronic lexicons created for particular applications needs. For technical languages or of speciality, work remains to be made to build dictionaries. Our work applies to the NOOJ system, with for immediate objective the installation of an electronic dictionary of French computer science terms (compound words), with an aim of analysing, automatically indexing texts. The linguistic aspects of the terminology are retained.},
  added-at = {2018-01-01T14:46:30.000+0100},
  address = {Vouliagmeni, Athens, Greece},
  author = {Aoughlis, Farida and Metais, Elisabeth},
  biburl = {https://www.bibsonomy.org/bibtex/27a0c545f1d78aeb5e4cf63e81dbe148d/vngudivada},
  booktitle = {Proceedings of the 10th WSEAS International Conference on Computers},
  interhash = {9575a694a9080454974e9d500dd320d5},
  intrahash = {7a0c545f1d78aeb5e4cf63e81dbe148d},
  keywords = {ComputerScience Dictionary Linguistics},
  month = jul,
  pages = {458 -- 462},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {An Electronic Dictionary of Computer Science Terminology},
  year = 2006
}

@book{seikel2015anatomy,
  abstract = {Anatomy \& Physiology for Speech, Language, and Hearing, Fifth Edition, provides a solid foundation in anatomical and physiological principles relevant to communication sciences and disorders. Ideal for speech-language pathology and audiology students, as well as practicing clinicians, the text integrates clinical information with everyday experiences to reveal how anatomy and physiology relate to the speech, language, and hearing systems. Combining comprehensive coverage with abundant, full-color illustrations and a strong practical focus, the text makes complex material approachable even for students with little or no background in anatomy and physiology. Thoroughly updated to reflect current trends, techniques, and best practices, the Fifth Edition of this acclaimed text is supported by innovative Anatesse learning software -- now accessible online via PC, Mac, and tablet devices -- featuring tutorials, interactive quizzes, and other resources to help students of all learning styles master the material and prepare for professional licensing exams.},
  added-at = {2018-01-03T04:48:33.000+0100},
  address = {Clifton Park, NY},
  author = {Seikel, J. Anthony and Drumright, David G and King, Douglas W},
  biburl = {https://www.bibsonomy.org/bibtex/2c0c4aa0cdceb6437400c829310f25e31/vngudivada},
  edition = {Fifth},
  interhash = {3b8b3926005271e61f410e66ce3b1b09},
  intrahash = {c0c4aa0cdceb6437400c829310f25e31},
  isbn = {978-1285198248},
  keywords = {Anatomy Book Hearing Physiology Speech},
  publisher = {Delmar Cengage Learning},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Anatomy \& Physiology for Speech, Language, and Hearing},
  year = 2015
}

@book{ladefoged1996sounds,
  abstract = {This book describes all the known ways in which the sounds of the world's languages differ. Encapsulating the work of two leading figures in the field, it will be a standard work of reference for researchers in phonetics, linguistics and speech science for many years to come. The scope of the book is truly global, with data drawn from nearly 400 languages, many of them investigated at first hand by the authors.},
  added-at = {2017-12-28T22:37:34.000+0100},
  address = {New York, NY},
  author = {Ladefoged, Peter and Maddieson, Ian},
  biburl = {https://www.bibsonomy.org/bibtex/223531dc65cbb30b69d43f839fed105e4/vngudivada},
  interhash = {6c7e13549a56e85b40487e88b2e4e280},
  intrahash = {23531dc65cbb30b69d43f839fed105e4},
  isbn = {978-0631198154},
  keywords = {Book Linguistics Phonetics},
  publisher = {Wiley-Blackwell},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The Sounds of the World's Languages},
  year = 1996
}

@book{krishnamurti2006dravidian,
  abstract = {The Dravidian languages are spoken by nearly 200 million people in South Asia and in diaspora communities around the world. They include Tamil, Malayalam, Kannada and Telugu, as well as over 20 non-literary languages. Bhadriraju Krishnamurti, one of the most eminent Dravidianists of our time, provides a linguistic overview of the Dravidian language family. He describes its history and writing system, discusses its structure and typology, and considers its lexicon. Distant and more recent contacts between Dravidian and other language groups are also covered.},
  added-at = {2018-01-05T04:05:34.000+0100},
  address = {Cambridge, UK},
  author = {Krishnamurti, Bhadriraju},
  biburl = {https://www.bibsonomy.org/bibtex/212ac4f0103440f012049745ba30ca25b/vngudivada},
  interhash = {b2b1ce0ffcbaba8250d1ff0b36595fd3},
  intrahash = {12ac4f0103440f012049745ba30ca25b},
  isbn = {978-0521025126},
  keywords = {Book Dravidian Linguistics},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The Dravidian Languages},
  year = 2006
}

@book{marsden2011vector,
  added-at = {2018-01-14T01:16:04.000+0100},
  address = {New York, NY},
  author = {Marsden, Jerrold E. and Tromba, Anthony},
  biburl = {https://www.bibsonomy.org/bibtex/20a3a942ef1541b6ae944744941b03a5d/vngudivada},
  edition = {Sixth},
  interhash = {c0a91b3e301c6032619ba2e3a6e0f8db},
  intrahash = {0a3a942ef1541b6ae944744941b03a5d},
  isbn = {978-1429215084},
  keywords = {Book VectorCalculus},
  publisher = {W. H. Freeman},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Vector Calculus},
  year = 2011
}

@book{harley2006english,
  abstract = {Where do new words come from? How are words put together? How do we assign meaning to words? These are among the most compelling questions for students of language and linguistics. English Words is a comprehensive and accessible introduction to the study of English words from a theoretically informed linguistic perspective.

	Assuming little or no background in linguistics, Harley approaches the study of words from several directions, including phonology, morphology, syntax, semantics, historical linguistics, and psycholinguistics. Using examples pulled from history, from Scrabble, and even from the funny pages, this book is sure to make word study a breeze for students and instructors alike.

	English Words: A Linguistic Introduction gives students a command of the basic theory in each area, skill in analyzing and understanding English words, and the foundation needed for more advanced study in linguistic theory or lexicology.

	Accessibly written to give students a command of basic theory, skills in analyzing English words, and the foundation needed for more advanced study in linguistic theory or lexicology

	Covers basic introductory material and investigates the structure of English vocabulary

	Introduces students to the technical study of words from relevant areas of linguistics: phonology, morphology, syntax, semantics, historical linguistics and psycholinguistics},
  added-at = {2017-12-27T23:07:17.000+0100},
  address = {New York, NY},
  author = {Harley, Heidi},
  biburl = {https://www.bibsonomy.org/bibtex/25450dd5d726ceead694eaa6e3564b6ef/vngudivada},
  interhash = {dc20e3961d323cfcda3dca416625c2c3},
  intrahash = {5450dd5d726ceead694eaa6e3564b6ef},
  isbn = {978-0631230328},
  keywords = {Book Linguistics},
  publisher = {Wiley-Blackwell},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {English Words: A Linguistic Introduction},
  year = 2006
}

@inproceedings{stubbs2011lightweight,
  abstract = {MAE and MAI are lightweight annotation and adjudication tools for corpus creation. DTDs are used to define the annotation tags and attributes, including extent tags, link tags, and non-consuming tags. Both programs are written in Java and use a stand-alone SQLite database for storage and retrieval of annotation data. Output is in stand-off XML.},
  added-at = {2017-12-30T00:36:25.000+0100},
  address = {Stroudsburg, Pennsylvania},
  author = {Stubbs, Amber},
  biburl = {https://www.bibsonomy.org/bibtex/20881987d97d873e1dee0f306dc6a1d97/vngudivada},
  booktitle = {Proceedings of the 5th Linguistic Annotation Workshop},
  interhash = {b6a6f386ce56fcbabde4fd5e3940af8a},
  intrahash = {0881987d97d873e1dee0f306dc6a1d97},
  isbn = {978-1-932432-93-0},
  keywords = {Annotation Corpus},
  pages = {129--133},
  publisher = {Association for Computational Linguistics},
  series = {LAW V '11},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {MAE and MAI: Lightweight Annotation and Adjudication Tools},
  year = 2011
}

@book{heine2015oxford,
  abstract = {This handbook compares the main analytic frameworks and methods of contemporary linguistics. It offers a unique overview of linguistic theory, revealing the common concerns of competing approaches. By showing their current and potential applications it provides the means by which linguists and others can judge what are the most useful models for the task in hand. Distinguished scholars from all over the world explain the rationale and aims of over thirty explanatory approaches to the description, analysis, and understanding of language. Each chapter considers the main goals of the model; the relation it proposes from between lexicon, syntax, semantics, pragmatics, and phonology; the way it defines the interactions between cognition and grammar; what it counts as evidence; and how it explains linguistic change and structure. The Oxford Handbook of Linguistic Analysis offers an indispensable guide for everyone researching any aspect of language including those in linguistics, comparative philology, cognitive science, developmental philology, cognitive science, developmental psychology, computational science, and artificial intelligence.

	This second edition has been updated to include seven new chapters looking at linguistic units in language acquisition, conversation analysis, neurolinguistics, experimental phonetics, phonological analysis, experimental semantics, and distributional typology.},
  added-at = {2017-12-31T04:09:58.000+0100},
  address = {Oxford, UK},
  biburl = {https://www.bibsonomy.org/bibtex/283a6c5a7cf61bb532e158b089d44b71e/vngudivada},
  edition = {Second},
  editor = {Heine, Bernd and Narrog, Heiko},
  interhash = {60e5409268e40a9fde891560b6b508ae},
  intrahash = {83a6c5a7cf61bb532e158b089d44b71e},
  isbn = {978-0199677078},
  keywords = {Book Linguistics},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The Oxford Handbook of Linguistic Analysis},
  year = 2015
}

@book{oconner2010grammarphobes,
  abstract = {Former New York Times Book Review editor and linguistic expert O'Conner ... updates her bestselling guide to grammar, an invigorating and entertaining dissection of our ever-evolving language. -- Publishers Weekly

	In this new edition of Woe Is I, Patricia T. O'Conner unties the knottiest grammar tangles and displays the same lively humor that has charmed and enlightened grateful readers for years. With new chapters on spelling and punctuation, and fresh insights into the rights, wrongs, and maybes of English grammar and usage, Woe Is I offers down-to-earth explanations and plain-English solutions to the language mysteries that bedevil all of us.},
  added-at = {2017-12-31T00:23:45.000+0100},
  address = {New York, NY},
  author = {O'Conner, Patricia T.},
  biburl = {https://www.bibsonomy.org/bibtex/21c83b2fe827eb77dad889bb8fae39950/vngudivada},
  interhash = {8c39433c5f7750250e35b430b5403abf},
  intrahash = {1c83b2fe827eb77dad889bb8fae39950},
  isbn = {978-1573223317},
  keywords = {Book Grammar},
  publisher = {Riverhead Books},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Woe is I: The Grammarphobe's Guide to Better English in Plain English},
  year = 2010
}

@book{oconner2000words,
  abstract = {Whether you are working on the novel that's been in the back of your mind for years or simply facing an increasing demand to write well at work or school, the fact remains: we all write more often these days, be it reports, e-mails, blog posts, or texts. But despite the increase in written communication, the fundamentals of good writing have been lost. Grammar maven Patricia T. O'Conner comes to the rescue with the most painless, practical, and funny writing book ever written. In short, snappy chapters filled with crystal-clear examples, amusing comparisons, and humorous allegories that cover everything from "Pronoun Pileups" and "Verbs That Zing" to "What to Do When You're Stuck," O'Conner provides simple, straightforward tips to help you sort through your thoughts and make your sentences strong.

  	Patricia T. O'Conner, a former editor at The New York Times Book Review, has written five books about the English language--the bestselling Woe Is I: The Grammarphobe's Guide to Better English in Plain English; Origins of the Specious: Myths and Misconceptions of the English Language (with Stewart Kellerman); Words Fail Me: What Everyone Who Writes Should Know About Writing; Woe Is I Jr.: The Younger Grammarphobe's Guide to Better English in Plain English; and You Send Me: Getting It Right When You Write Online (with Stewart Kellerman).},
  added-at = {2017-12-31T00:42:00.000+0100},
  address = {Boston, Massachusetts},
  author = {O'Conner, Patricia T.},
  biburl = {https://www.bibsonomy.org/bibtex/273b3ad2bc242e0eb28f7e9e76b1017db/vngudivada},
  interhash = {bf4719de656eff93eecc6e64987373b8},
  intrahash = {73b3ad2bc242e0eb28f7e9e76b1017db},
  isbn = {978-0156010870},
  keywords = {Book Grammar Vocabulary},
  publisher = {Mariner Books},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Words Fail Me: What Everyone Who Writes Should Know about Writing},
  year = 2000
}

@book{gibbons2014scaffolding,
  abstract = {The bestselling Scaffolding Language, Scaffolding Learning helped tens of thousands of mainstream elementary teachers ensure that their English language learners became full members of the school community with the language and content skills they needed for success. In the highly anticipated Second Edition, Pauline Gibbons updates her classic text with a multitude of practical ideas for the classroom, supported by the latest research in the field of ELL/ESL.

	With clear directions and classroom tested strategies for supporting students' academic progress, Gibbons shows how the teaching of language can be integrated seamlessly with the teaching of content, and how academic achievement can be boosted without sacrificing our own vision of education to the dictates of knee-jerk accountability. Rich examples of classroom discourse illustrate exactly how the scaffolding process works, while activities to facilitate conversation and higher-level thinking put the latest research on second language learning into action.},
  added-at = {2017-12-27T23:11:58.000+0100},
  address = {Portsmouth, New Hampshire},
  author = {Gibbons, Pauline},
  biburl = {https://www.bibsonomy.org/bibtex/27f952737a710fa9be9d095657a80f6b4/vngudivada},
  edition = {Second},
  interhash = {8d57a37aa8f61ff2838d4e5a832b23c7},
  intrahash = {7f952737a710fa9be9d095657a80f6b4},
  isbn = {978-0325056647},
  keywords = {Book Linguistics Vocabulary},
  publisher = {Heinemann},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Scaffolding Language, Scaffolding Learning: Teaching English Language Learners in the Mainstream Classroom},
  year = 2014
}

@book{disciullo1987definition,
  abstract = {On The Definition of Word develops a consistent and coherent approach to central questions about morphology and its relation to syntax. In sorting out the various senses in which the word word is used, it asserts that three concepts which have often been identified with each other are in fact distinct and not coextensive: listemes (linguistic objects permanently stored by the speaker); morphological objects (objects whose shape can be characterized in morphological terms of affixation and compounding); and syntactic atoms (objects that are unanalyzable units with respect to syntax).

	The first chapter defends the idea that listemes are distinct from the other two notions, and that all one can and should say about them is that they exist. A theory of morphological objects is developed in chapter two. Chapter three defends the claim that the morphological objects are a proper subset of the syntactic atoms, presenting the authors' reconstruction of the important and much-debated Lexical Integrity Hypothesis. A final chapter shows that there are syntactic atoms which are not morphological objects.

	Anne Marie Di Sciullo is in the Department of Linguistics at the University of Quebec. Edwin Williams is in the Department of Linguistics at the University of Massachusetts. On The Definition of Word is Linguistic Inquiry Monograph 14.},
  added-at = {2017-12-29T01:17:53.000+0100},
  address = {Cambridge, Massachusetts},
  author = {{Di Sciullo}, Anna Maria and Williams, Edwin},
  biburl = {https://www.bibsonomy.org/bibtex/26dfa7f02fc3183fe5dc34789aa1b8948/vngudivada},
  interhash = {4257e6681e5aef367277ec47e157051f},
  intrahash = {6dfa7f02fc3183fe5dc34789aa1b8948},
  isbn = {978-0262540476},
  keywords = {Book Linguistics},
  publisher = {The MIT Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {On the Definition of Word},
  year = 1987
}

@book{pullum1996phonetic,
  abstract = {Phonetic Symbol Guide is a comprehensive and authoritative encyclopedia of phonetic alphabet symbols, providing a complete survey of the hundreds of characters used by linguists and speech scientists to record the sounds of the world's languages.

	This fully revised second edition incorporates the major revisions to the International Phonetic Alphabet made in 1989 and 1993. Also covered are the American tradition of transcription stemming from the anthropological school of Franz Boas; the Bloch/Smith/Trager style of transcription; the symbols used by dialectologists of the English language; usages of specialists such as Slavicists, Indologists, Sinologists, and Africanists; and the transcription proposals found in all major textbooks of phonetics.

	With sixty-one new entries, an expanded glossary of phonetic terms, added symbol charts, and a full index, this book will be an indispensable reference guide for students and professionals in linguistics, phonetics, anthropology, philology, modern language study, and speech science.},
  added-at = {2017-12-28T22:28:20.000+0100},
  address = {Chicago, Illinois},
  author = {Pullum, Geoffrey K. and Ladusaw, William A.},
  biburl = {https://www.bibsonomy.org/bibtex/2146bca124714507a119ef35abeb5c2a8/vngudivada},
  interhash = {06f9e33e14ec13c72aa0cccce20bc306},
  intrahash = {146bca124714507a119ef35abeb5c2a8},
  isbn = {978-0226685366},
  keywords = {Book Phonetics},
  publisher = {University Of Chicago Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Phonetic Symbol Guide},
  year = 1996
}

@book{gheorghe2015elasticsearch,
  abstract = {Elasticsearch in Action teaches you how to build scalable search applications using Elasticsearch. You'll ramp up fast, with an informative overview and an engaging introductory example. Within the first few chapters, you'll pick up the core concepts you need to implement basic searches and efficient indexing. With the fundamentals well in hand, you'll go on to gain an organized view of how to optimize your design. Perfect for developers and administrators building and managing search-oriented applications.

  Modern search seems like magic—you type a few words and the search engine appears to know what you want. With the Elasticsearch real-time search and analytics engine, you can give your users this magical experience without having to do complex low-level programming or understand advanced data science algorithms. You just install it, tweak it, and get on with your work.


  Elasticsearch in Action teaches you how to write applications that deliver professional quality search. As you read, you'll learn to add basic search features to any application, enhance search results with predictive analysis and relevancy ranking, and use saved data from prior searches to give users a custom experience. This practical book focuses on Elasticsearch's REST API via HTTP. Code snippets are written mostly in bash using cURL, so they're easily translatable to other languages.

  What is Inside

  What is a great search application?
  Building scalable search solutions
  Using Elasticsearch with any language
  Configuration and tuning

  PART 1 CORE ELASTICSEARCH FUNCTIONALITY
  Introducing Elasticsearch
  Diving into the functionality
  Indexing, updating, and deleting data
  Searching your data
  Analyzing your data
  Searching with relevancy
  Exploring your data with aggregations
  Relations among documents

  PART 2 ADVANCED ELASTICSEARCH FUNCTIONALITY
  Scaling out
  Improving performance
  Administering your cluster},
  added-at = {2018-01-06T23:46:06.000+0100},
  address = {Shelter Island, New York},
  author = {Gheorghe, Radu and Hinman, Matthew Lee and Russo, Roy},
  biburl = {https://www.bibsonomy.org/bibtex/2e5782bd867101150930acde88c39d994/vngudivada},
  interhash = {2362eceb597c18a6c3194e01b4c3a90f},
  intrahash = {e5782bd867101150930acde88c39d994},
  isbn = {978-1617291623},
  keywords = {Book Elasticsearch IR},
  publisher = {Manning Publications},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Elasticsearch in Action},
  year = 2015
}

@book{radford1997syntactic,
  abstract = {For the most part, Andrew Radford's book does an excellent job of bringing someone new to the nuances of syntactic theory up to speed. As someone with only a general background in the field (but also possessing a deep love of syntactic theory), I consider myself to be a very good test subject.

	One of the book's strongest points is Radford's willingness to take the time and space necessary to explain each detailed step in a theory, derivation, etc. Although I did not need all of the detail all of the time, it was easy to skip those parts and move on. For those with VERY little background in syntactic theory, these carefully outlined steps will be quite helpful in understanding where Radford is taking the theories.

	Another item of importance is Radford's occasional display of alternate explanations for certain empirical paradigms. Although even more alternates would have been more satisfying to me, too many would have made the 550-page book too cumbersome for some.

	A third strong point for the book is its generous workbook sections, one for each chapter. In each section, the problems are carefully explained, and the examples are arranged in an order of progressive difficulty. By using the workbook sections, the student can much better grasp the ideas than by simply going over the chapter's supplied paradigms/answers.

	There are, however, a few drawbacks to be kept in mind. First, only the initial (and easiest) of the workbook section's problems typically is given an answer. For a student doing independent study (like me), or for one with a teacher who is inattentive to the workbook section, this poses a major problem in that difficulties cannot be easily rectified. To make matters more confusing, a few of the workbook problems address issues which are not directly commented on in the chapter.

	The other problem I noticed was the book's espousing certain ideas without any explanation thereof. For instance, the book subscribes to the binary merger theory but offers no real reasons for it (and also neglects to offer any current alternatives). This is not an argument against binary merger; I simply maintain that it would have been better for the reader to understand (even very basically) the author's reasoning behind this decision.

	Despite these two problems, however, I highly recommend this text to anyone wishing to enter into the academic world of theoretical syntax and explore many of its nuanced facets.},
  added-at = {2017-12-27T23:38:43.000+0100},
  address = {Cambridge, UK},
  author = {Radford, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/25cfd6e8f821c7c962c166291afb9ae42/vngudivada},
  interhash = {ed2a24c78630e4a2ab168055ba0cfc0c},
  intrahash = {5cfd6e8f821c7c962c166291afb9ae42},
  isbn = {978-0521477079},
  keywords = {Book Syntax},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Syntactic Theory and the Structure of English: A Minimalist Approach},
  year = 1997
}

@book{steever1998dravidian,
  abstract = {The Dravidian language family is the world’s fourth largest with over 220 million speakers.

	This authoritative reference is ideal both as an introduction to the language family and for linguists looking for new information, as well as for readers with a general interest in Dravidian and Indic culture. It contains twelve descriptions of the individual languages written by internationally recognized experts, as well as discussions about the internal structure of the language in relation to morphology, phonology, parts of speech, syntax and lexicon.},
  added-at = {2018-01-05T04:01:47.000+0100},
  address = {Abingdon, UK},
  biburl = {https://www.bibsonomy.org/bibtex/2d0c51ffd393c96789be74cceb5716a73/vngudivada},
  editor = {Steever, Sanford B.},
  interhash = {7b050ee92d1d7a079b6cb4f6703df3d4},
  intrahash = {d0c51ffd393c96789be74cceb5716a73},
  isbn = {978-0415412674},
  keywords = {Book Dravidian Linguistics},
  publisher = {Routledge},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The Dravidian Languages},
  year = 1998
}

@book{graves2012teaching,
  abstract = {Building on Michael Graves s bestseller, The Vocabulary Book, this new resource offers a comprehensive plan for vocabulary instruction that K-12 teachers can use with English language learners. It is broad enough to include instruction for students who are just beginning to build their English vocabularies, as well as for students whose English vocabularies are approaching those of native speakers. The authors describe a four-pronged program that follows these key components: providing rich and varied language experiences; teaching individual words; teaching word learning strategies; and fostering word consciousness. This user-friendly book integrates up-to-date research on best practices into each chapter and includes vignettes, classroom activities, sample lessons, a list of children s literature, and more.},
  added-at = {2017-12-27T19:50:23.000+0100},
  address = {New York, NY},
  author = {Graves, Michael F. and August, Diane and Mancilla-Martinez, Jeannette},
  biburl = {https://www.bibsonomy.org/bibtex/2cfa0afbf340f584cbd1cb1abd1f001d9/vngudivada},
  interhash = {299b1767c8687ec1296576680c080fa5},
  intrahash = {cfa0afbf340f584cbd1cb1abd1f001d9},
  isbn = {978-0807753750},
  keywords = {Book Linguistics Vocabulary},
  publisher = {Teachers College Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Teaching Vocabulary to English Language Learners},
  year = 2012
}

@mastersthesis{minshall2013computer,
  abstract = {This study investigated the technical vocabulary of computer science in order to create a Computer Science Word List (CSWL). The CSWL was intended as a pedagogical tool in the instruction of non-native English speakers who are studying computer science in UK universities. In order to create this technical word list, a corpus of 3,661,337 tokens was compiled from journal articles and conference proceedings covering the 10 sub-disciplines of computer science as defined by the Association for Computing Machinery (ACM). The CSWL was intended to be supplemental to both the General Service List (GSL) (West, 1953) and the Academic Word List (AWL) (Coxhead, 2000) and was created using the criteria established by Coxhead (2000) for word selection. The CSWL contained 433 headwords and in combination with the GSL and AWL accounted for 95.11\% of all tokens in the corpus. This was sufficient to meet the lexical threshold for sufficient understanding of a text as proposed by Laufer (1990). This study also conducted research into the technicality of the CSWL by comparison to other corpora, comparison to a technical dictionary and an investigation of the distribution of its headwords against the BNC frequency bands. Overall, the CSWL was found to be highly technical in nature. The final part of the research looked into the existence of multiword units in computer science literature to build a Computer Science Multi-Word List (CSMWL) from the same corpus. A total of 23 items comprised the CSMWL and they were again chosen using the same criteria of range and frequency as established by Coxhead (2000). The CSMWL showed that whilst multi-word units do exist in computer science literature, they are mostly compound nouns with domain specific meaning.},
  added-at = {2018-01-01T14:39:29.000+0100},
  author = {Minshall, Daniel E},
  biburl = {https://www.bibsonomy.org/bibtex/26e01e5c7cbade1f17c9ba9ea306a120f/vngudivada},
  interhash = {dd7fcdb67538e16c7e54ced52ca7d67e},
  intrahash = {6e01e5c7cbade1f17c9ba9ea306a120f},
  keywords = {Linguistics WordList},
  school = {Swansea University},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {A Computer Science Word List},
  year = 2013
}

@book{beck2013bringing,
  abstract = {Hundreds of thousands of teachers have used this highly practical guide to help K-12 students enlarge their vocabulary and get involved in noticing, understanding, and using new words. Grounded in research, the book explains how to select words for instruction, introduce their meanings, and create engaging learning activities that promote both word knowledge and reading comprehension. The authors are trusted experts who draw on extensive experience in diverse classrooms and schools. Sample lessons and vignettes, children's literature suggestions, end-of-chapter summaries, and "Your Turn" learning activities enhance the book's utility as a classroom resource, professional development tool, or course text.

	One of the 10 Books Every Teacher Should Read

	Reflects over a decade of advances in research-based vocabulary instruction.

	Chapters on vocabulary and writing; assessment; and differentiating instruction for struggling readers and English language learners, including coverage of response to intervention (RTI).

	Expanded discussions of content-area vocabulary and multiple-meaning words.

	Many additional examples showing what robust instruction looks like in action.

	Appendix with a useful menu of instructional activities.

	See also the authors' Creating Robust Vocabulary: Frequently Asked Questions and Extended Examples, which includes specific instructional sequences for different grade ranges, as well as Making Sense of Phonics, Second Edition: The Hows and Whys, by Isabel L. Beck and Mark E. Beck, an invaluable resource for K-3.

	Bringing Words to Life is, without doubt, one of the best and most influential books ever published on the topic of teaching and developing vocabulary. The second edition is even better than the first--it addresses recent issues such as the Common Core and RTI; provides a greater number of helpful, practical examples; addresses vocabulary and writing; takes on the challenge of providing effective vocabulary instruction for struggling readers and English learners; and offers guidance for teaching math, science, and social studies vocabulary in the intermediate grades. This book should be required reading for anyone seeking teacher or reading specialist certification.

	This is a book teachers will read, reread, and refer to many times to nurture a love of language through joyful, robust vocabulary instruction. The authors' research-based insights and advice about selecting words and systematically presenting them to learners from all backgrounds are both practical and manageable. Teachers will delight in nurturing a language-rich classroom where students revel in language, read better and deeper, speak effectively, and write with purpose and clarity.

	It's hard to believe, but the second edition of Bringing Words to Life is even more comprehensive than the first. Beck, McKeown, and Kucan have outdone themselves. Containing real-life examples and easy-to-implement strategies, this book is a 'must have' for vocabulary instruction. It will help teachers greatly as they work to meet the Common Core Standards.

	Perhaps the most distinctive feature of this book is its personal touch--reading it is like having a conversation with the authors about robust vocabulary instruction. The authors share their decision making, offer warnings about potential challenges, encourage thoughtful planning, and insist on follow-through. Teachers are amply supported in their learning by the 'Your Turn' and 'You Try It' features, whether working on their own or in a course on literacy instruction. This is a book designed by teachers for teachers.

	The book offers elementary, middle, and high school teachers concrete suggestions for choosing words and teaching them to students. New chapters in the second edition provide important updates for teachers who are data driven, who have students of varying ability levels and language backgrounds, and who focus on the reading-writing connection. Like the first edition, this book will make a significant contribution to teacher preparation and professional development. Specific recommendations for practice are illustrated with detailed examples of teachers implementing robust vocabulary instruction in their classrooms.

	Isabel L. Beck, PhD, is Professor Emerita of Education in the School of Education and Senior Scientist at the Learning Research and Development Center, both at the University of Pittsburgh.

	Margaret G. McKeown, PhD, is Clinical Professor of Education in the School of Education and Senior Scientist at the Learning Research and Development Center at the University of Pittsburgh.

	Linda Kucan, PhD, is Associate Professor in the Department of Instruction and Learning at the University of Pittsburgh School of Education.},
  added-at = {2017-12-27T23:09:00.000+0100},
  address = {New York, NY},
  author = {Beck, Isabel L. and McKeown, Margaret G. and Kucan, Linda},
  biburl = {https://www.bibsonomy.org/bibtex/21b6889803a31291d9069875f427d59cd/vngudivada},
  edition = {Second},
  interhash = {fcc6051895037311f5edb107aa8d83af},
  intrahash = {1b6889803a31291d9069875f427d59cd},
  isbn = {978-1462508167},
  keywords = {Book Linguistics Vocabulary},
  publisher = {The Guilford Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Bringing Words to Life: Robust Vocabulary Instruction},
  year = 2013
}

@book{kleppmann2017designing,
  abstract = {Data is at the center of many challenges in system design today. Difficult issues need to be figured out, such as scalability, consistency, reliability, efficiency, and maintainability. In addition, we have an overwhelming variety of tools, including relational databases, NoSQL datastores, stream or batch processors, and message brokers. What are the right choices for your application? How do you make sense of all these buzzwords?

In this practical and comprehensive guide, author Martin Kleppmann helps you navigate this diverse landscape by examining the pros and cons of various technologies for processing and storing data. Software keeps changing, but the fundamental principles remain the same. With this book, software engineers and architects will learn how to apply those ideas in practice, and how to make full use of data in modern applications.

Peer under the hood of the systems you already use, and learn how to use and operate them more effectively
Make informed decisions by identifying the strengths and weaknesses of different tools
Navigate the trade-offs around consistency, scalability, fault tolerance, and complexity
Understand the distributed systems research upon which modern databases are built
Peek behind the scenes of major online services, and learn from their architectures},
  added-at = {2018-01-07T19:21:34.000+0100},
  address = {Sebastopol, California},
  author = {Kleppmann, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2174547cf43f237ff19382146db660b76/vngudivada},
  interhash = {092ae166c4379328d15362d9c16d4234},
  intrahash = {174547cf43f237ff19382146db660b76},
  isbn = {978-1449373320},
  keywords = {DistributedComputing},
  publisher = {O'Reilly Media},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems},
  year = 2017
}

@book{berlin1999basic,
  abstract = {The work reported in this monograph was begun in the winter of 1967 in a graduate seminar at Berkeley. Many of the basic data were gathered by members of the seminar and the theoretical framework presented here was initially developed in the context of the seminar discussions.

	Much has been discovered since 1969, the date of original publication, regarding the psychophysical and neurophysical determinants of universal, cross-linguistic constraints on the shape of basic color lexicons, and something, albeit less, can now also be said with some confidence regarding the constraining effects of these language-independent processes of color perception and conceptualization on the direction of evolution of basic color term lexicons.

	While Berlin and Kay's research has revived interest in the subject much effort has gone into defending a flawed theory. For a more fruitful approach see the section on color terms in Wierzbicka, Anna (1996) Semantics: Primes and Universals. Oxford: Oxford UP.},
  added-at = {2017-12-29T01:16:12.000+0100},
  address = {Stanford University, Stanford, California},
  author = {Berlin, Brent and Kay, Paul},
  biburl = {https://www.bibsonomy.org/bibtex/2d4c4033a8b98cd7ea0bcb0b1d05b0bcd/vngudivada},
  interhash = {31af84fc43e15d1cce4cab6089bd8a10},
  intrahash = {d4c4033a8b98cd7ea0bcb0b1d05b0bcd},
  isbn = {978-1575861623},
  keywords = {Book Universals},
  publisher = {Center for the Study of Language and Information},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Basic Color Terms: Their Universality and Evolution},
  year = 1999
}

@book{internationalphoneticassociation1999handbook,
  abstract = {This book is a comprehensive guide to the International Phonetic Alphabet, widely used for over a century to transcribe the sounds of languages. The Handbook is in three parts: Part I contains an introduction to phonetic description and exemplification of the use of phonetic symbols; Part II consists of twenty-nine Illustrations of the application of the International Phonetic Alphabet to a range of languages; and Part III covers speech pathology, computer codings, and the history of the IPA. This is an essential reference work for phoneticians and linguists more generally.},
  added-at = {2017-12-28T22:32:14.000+0100},
  address = {Cambridge, UK},
  author = {{International Phonetic Association}},
  biburl = {https://www.bibsonomy.org/bibtex/2e418eea7653c813fa6708385256a4fac/vngudivada},
  interhash = {f075beefb0781a1efdc075a7ce219192},
  intrahash = {e418eea7653c813fa6708385256a4fac},
  isbn = {978-0521637510},
  keywords = {Book Handbook IPA},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Handbook of the International Phonetic Association: A Guide to the Use of the International Phonetic Alphabet},
  year = 1999
}

@book{wierzbicka1996semantics,
  abstract = {Conceptual primitives and semantic universals are the cornerstones of a semantic theory which Anna Wierzbicka has been developing for many years. Semantics: Primes and Universals is a major synthesis of her work, presenting a full and systematic exposition of that theory in a non-technical and readable way. It delineates a full set of universal concepts, as they have emerged from large-scale investigations across a wide range of languages undertaken by the author and her colleagues. On the basis of empirical cross-linguistic studies it vindicates the old notion of the "psychic unity of mankind", while at the same time offering a framework for the rigorous description of different languages and cultures.},
  added-at = {2017-12-29T01:16:50.000+0100},
  address = {Oxford, UK},
  author = {Wierzbicka, Anna},
  biburl = {https://www.bibsonomy.org/bibtex/2d30963e4b05f7ebb3835a622992a6fe5/vngudivada},
  interhash = {381389ad9dbf68435f49154198c31633},
  intrahash = {d30963e4b05f7ebb3835a622992a6fe5},
  isbn = {978-0198700036},
  keywords = {Book Semantics},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Semantics: Primes and Universals},
  year = 1996
}

@book{radford1997syntax,
  abstract = {This textbook is a concise, readable introduction to current work in syntactic theory, particularly to Chomsky's Minimalist program. It gives an overview of theoretical concepts and descriptive devices. The discussion is based on varieties of English (Modern Standard, Belfast, Shakespearean, Jamaican Creole) and does not assume prior knowledge of syntax. There are exercises and a glossary. It is an abridged version of Radford's major new textbook Syntactic Theory and the Structure of English: A Minimalist Approach (CUP, 1997).

	Radford is well known for his comprehensible textbooks of generative syntax. This book is very helpful for beginners to get access to Chomsky's revolutionary ideas in his minimalist program (MP), which is radically different from his previous versions of grammar. What is the particular of this book is that the reader does not need to learn Chomsky's arguments starting from his early works. The book leads the reader to the MP in a straightforward way. The book first reviews Chomsky's philosophy of language, and then demonstrates the operation of the principles and parameters in the framework by using the examples which can explain our intuition of English. To reduce the burden of the reader, as he did in his other books, the author uses vivid analogies to illustrate some critical concepts. There are also excises attached to each chapter. Besides, the book includes a glossary of the terms which appear in the book. If you want to learn about the MP or to read Chomsky's latest works, or if you want to find a textbook for your students in the course of generative syntax, this book, I believe, as a warm-up candidate, should be under your consideration.

	Radford's writing is, as always, clear and illuminating. Radford introduces the rationale behind the Minimalist Program and then proceeds to walk the reader through a minimalist analysis of syntactic phenomena in English. His progression goes from elements possibly familiar from similar proposals in other versions of Generative Grammar to new and even polemic proposals. It is an outstanding overview for readers who may not wish to delve into the paraphernalia of constructs and concepts from obsolete prior proposals. Worthy of note is the fact that analysis of Early Modern English and Belfast English are used to contrast and complement Modern English analysis. I wish more scientists and professors had Radford's clarity!},
  added-at = {2017-12-27T23:40:29.000+0100},
  address = {Cambridge, UK},
  author = {Radford, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/26ed38c44a3be9f8cb32114013d5c3d79/vngudivada},
  interhash = {c5ee4e0869ea775356326d4d781eb4f2},
  intrahash = {6ed38c44a3be9f8cb32114013d5c3d79},
  isbn = {978-0521589147},
  keywords = {Book Syntax},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Syntax: A Minimalist Introduction},
  year = 1997
}

@book{turnbull2016language,
  abstract = {Language Development From Theory to Practice provides a survey of key topics in language development, including research methods, theoretical perspectives, and major language milestones from birth to adolescence and beyond, and language diversity and language disorders. Each chapter bridges language development theory and practice by providing students with a theoretical and scientific foundation to the study of language development. The authors emphasize the relevance of the material to students' current and future experiences in clinical, educational, and research settings; emphasize multicultural considerations and how they affect language development; focus on using evidence-based practices for making educational and clinical decisions; show the relevance of a multidisciplinary perspective on the theory and practice of language development; and include a number of outstanding pedagogical features to motivate and engage students.

	The new edition builds on the strengths of the earlier editions while featuring a chapter reorganization that promotes better understanding, more detailed coverage of topics of particular interest to students, expanded categorization of language-development theories, and a variety of helpful new pedagogical features. The Enhanced Pearson eText features embedded videos and assessments.},
  added-at = {2018-01-03T04:41:49.000+0100},
  address = {London, UK},
  author = {Turnbull, Khara L. Pence and Justice, Laura M.},
  biburl = {https://www.bibsonomy.org/bibtex/201ec888bd9fa8127647bcbf4dcad8c09/vngudivada},
  edition = {Third},
  interhash = {5c4b03648c0554e1a622e69ebf1c57a8},
  intrahash = {01ec888bd9fa8127647bcbf4dcad8c09},
  isbn = {978-0134170428},
  keywords = {Book Development Language},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Language Development From Theory to Practice},
  year = 2016
}

@book{wierzbicka1992semantics,
  abstract = {Not everything that can be said in one language can be said in another. The lexicons of different languages seem to suggest different conceptual universes. Investigating cultures from a universal, language-independent perspective, this book rejects analytical tools derived from the English language and Anglo culture and proposes instead a "natural semantic metalanguage" formulated in English words but based on lexical universals. The outcome of two and a half decades of research, the metalanguage is made up of universal semantic primitives in terms of which all meanings--including the most culture-specific ones--can be described and compared in a precise and illuminating way. Integrating insights from linguistics, cultural anthropology, and cognitive psychology, and written in simple, non-technical language, Semantics, Culture, and Cognition is accessible not only to scholars and students, but also to the general reader interested in semantics and the relationship between language and culture.},
  added-at = {2017-12-29T01:19:25.000+0100},
  address = {Oxford, UK},
  author = {Wierzbicka, Anna},
  biburl = {https://www.bibsonomy.org/bibtex/25e29eb1e56b7a1f0faeb040cf1c76878/vngudivada},
  interhash = {2ad2266f67b5a041394b178b18209bce},
  intrahash = {5e29eb1e56b7a1f0faeb040cf1c76878},
  isbn = {978-0195073263},
  keywords = {Book Linguistics Semantics},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Semantics, Culture, and Cognition: Universal Human Concepts in Culture-Specific Configurations},
  year = 1992
}

@book{sobin2011syntactic,
  abstract = {Highly readable and eminently practical, Syntactic Analysis: The Basics focuses on bringing students with little background in linguistics up to speed on how modern syntactic analysis works.

	A succinct and practical introduction to understanding sentence structure, ideal for students who need to get up to speed on key concepts in the field

	Introduces readers to the central terms and concepts in syntax

	Offers a hands-on approach to understanding and performing syntactic analysis and introduces students to linguistic argumentation

	Includes numerous problem sets, helpfully graded for difficulty, with model answers provided at critical points

	Prepares readers for more advanced work with syntactic systems and syntactic analyses

	Ultimately, I would argue that this book succeeds with its goals by laying a broad, basic, and clear foundation in the philosophy of generative syntax, thus allowing undergraduates to learn the nature of scientific inquiry with languages in a trimester/quarter system or supporting graduate students with little or distant background to read and respond to primary literature with more confidence and understanding.

	This succinct, practical introduction to understanding sentence structure is ideal for students with little background in linguistics who need to get up to speed on how modern syntactic analysis works. Introducing the reader to the central terms and concepts in the field of syntax, it explains how to understand and operate syntactic analysis, as well as how to approach linguistic argumentation. Included are numerous problem sets, helpfully graded for difficulty, with model answers provided at critical points.

	Designed for either classroom use or self-study, Syntactic Analysis fills a gap in the available literature by providing a short and hands-on guide to understanding syntactic systems, and provides the reader with a strong foundation for more advanced work in the field.},
  added-at = {2017-12-27T19:51:10.000+0100},
  address = {New York, NY},
  author = {Sobin, Nicholas},
  biburl = {https://www.bibsonomy.org/bibtex/27af633b7abd596cf6374d6d36a84cf36/vngudivada},
  interhash = {dcb7ec439586c36deb7b9b78ba8947f9},
  intrahash = {7af633b7abd596cf6374d6d36a84cf36},
  isbn = {978-1444335071},
  keywords = {Book Syntax},
  publisher = {Wiley-Blackwell},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Syntactic Analysis: The Basics},
  year = 2011
}

@book{brinton2010linguistic,
  abstract = {This text is for advanced undergraduate and graduate students interested in contemporary English, especially those whose primary area of interest is English as a second language, primary or secondary-school education, English stylistics, theoretical and applied linguistics, or speech pathology. The emphasis is on empirical facts of English rather than any particular theory of linguistics; the text does not assume any background in language or linguistics. In this newly revised edition numerous example sentences are taken from the Corpus of Contemporary American English. A full glossary of key terms, an additional chapter on pedagogy and new sections on cognitive semantics and politeness have been added. Other changes include: completely updated print references; web links to sites of special interest and relevance; and a revised, reader-friendly layout. A companion website that includes a complete workbook with self-testing exercises and a comprehensive list of web links accompanies the book. The website can be found at the following address: http://dx.doi.org/10.1075/z.156.workbook.


	Students completing the text and workbook will acquire: a knowledge of the sound system of contemporary English; an understanding of the formation of English words; a comprehension of the structure of both simple and complex sentence in English; a recognition of complexities in the expression of meaning; an understanding of the context and function of use upon the structure of the language; and an appreciation of the importance of linguistic knowledge to the teaching of English to first and second-language learners.

	Laurel J. Brinton is Professor of English Language at the University of British Columbia.

	Donna M. Brinton is Senior Lecturer in TESOL at the University of Southern California's Rossier School of Education.

	The Linguistic Structure of Modern English is a revised edition of The Structure of Modern English by Laurel J. Brinton (2000).},
  added-at = {2017-12-27T19:42:16.000+0100},
  address = {Philadelphia, Pennsylvania},
  author = {Brinton, Laurel J. and Brinton, Donna M.},
  biburl = {https://www.bibsonomy.org/bibtex/2e0dce994b03d87e207a4f364b9368f27/vngudivada},
  edition = {Second},
  interhash = {b73b25a0290638e8d5159d77cd7e4c70},
  intrahash = {e0dce994b03d87e207a4f364b9368f27},
  isbn = {978-9027211729},
  keywords = {Book Linguistics},
  publisher = {John Benjamins Publishing Company},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The Linguistic Structure of Modern English},
  year = 2010
}

@book{koptjevskajatamm2015linguistics,
  abstract = {The volume is the first comprehensive typological study of the conceptualization of temperature in languages as reflected in their systems of central temperature terms (hot, cold, to freeze, etc.). The key issues addressed here include questions such as how languages categorize the temperature domain and what other uses the temperature expressions may have, e.g., when metaphorically referring to emotions (warm words). The volume contains studies of more than 50 genetically, areally and typologically diverse languages and is unique in considering cross-linguistic patterns defined both by lexical and grammatical information. The detailed descriptions of the linguistic and extra-linguistic facts will serve as an important step in teasing apart the role of the different factors in how we speak about temperature - neurophysiology, cognition, environment, social-cultural practices, genetic relations among languages, and linguistic contact. The book is a significant contribution to semantic typology, and will be of interest for linguists, psychologists, anthropologists and philosophers.},
  added-at = {2017-12-29T03:33:43.000+0100},
  address = {Philadelphia, Pennsylvania},
  biburl = {https://www.bibsonomy.org/bibtex/2e63f1cba856d4edffe1b41f7291392c9/vngudivada},
  editor = {Koptjevskaja-Tamm, Maria},
  interhash = {56f8cbb364159eccfe15c0480e83cbbf},
  intrahash = {e63f1cba856d4edffe1b41f7291392c9},
  isbn = {978-9027206886},
  keywords = {Book Linguistics},
  publisher = {John Benjamins Publishing Company},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The Linguistics of Temperature},
  year = 2015
}

@book{mccandless2010lucene,
  abstract = {When Lucene first hit the scene five years ago, it was nothing short ofamazing. By using this open-source, highly scalable, super-fast search engine,developers could integrate search into applications quickly and efficiently.A lot has changed since then-search has grown from a "nice-to-have" featureinto an indispensable part of most enterprise applications. Lucene now powerssearch in diverse companies including Akamai, Netflix, LinkedIn,Technorati, HotJobs, Epiphany, FedEx, Mayo Clinic, MIT, New ScientistMagazine, and many others.

Some things remain the same, though. Lucene still delivers high-performancesearch features in a disarmingly easy-to-use API. Due to its vibrant and diverseopen-source community of developers and users, Lucene is relentlessly improving,with evolutions to APIs, significant new features such as payloads, and ahuge increase (as much as 8x) in indexing speed with Lucene 2.3.

And with clear writing, reusable examples, and unmatched advice on bestpractices, Lucene in Action, Second Edition is still the definitive guide todeveloping with Lucene.

Purchase of the print book comes with an offer of a free PDF, ePub, and Kindle eBook from Manning. Also available is all code from the book.},
  added-at = {2018-01-09T18:23:33.000+0100},
  address = {Shelter Island, New York},
  author = {McCandless, Michael and Hatcher, Erik and Gospodnetic, Otis},
  biburl = {https://www.bibsonomy.org/bibtex/269d79f22fb962d3e3fbf48dbad1e7275/vngudivada},
  edition = {Second},
  interhash = {50acb643e1f868d44d71fd4fc62ce7a0},
  intrahash = {69d79f22fb962d3e3fbf48dbad1e7275},
  isbn = {978-1933988177},
  keywords = {Book IR Lucene},
  publisher = {Manning},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Lucene in Action},
  year = 2010
}

@article{brown2013sound,
  abstract = {An automated sound correspondence-recognition program developed by the authors is applied to a data set consisting of standardized word lists for over half of the world's languages. Online appendices present the results in a compendium of 692 recurrent sound correspondences that contains information about the frequency of occurrence of each correspondence. Applications of the compendium to historical linguistics are proposed. For example, the catalog of correspondences and frequencies facilitates objective assessment of the commonness or rarity of shared phonological innovations cited as evidence for language-family subgrouping. In another analysis, correspondence frequency is used to measure the degree of similarity between different sounds, yielding models for classifying consonants and vowels that substantially agree with articulatory properties. Correspondence-based similarities are also compared with measurements of sound similarity involving factors such as perceptual confusions, speech errors, and cooccurrence patterns in synchronic phonological rules. Sound similarity discerned from both the perception and production of speech is found to correlate to about the same extent with correspondence-based similarities.},
  added-at = {2017-12-28T23:14:22.000+0100},
  author = {Brown, Cecil H. and Holman, Eric W. and Wichmann, Søren},
  biburl = {https://www.bibsonomy.org/bibtex/2878cafc4d53c683f7c9baeeb2acf3cc3/vngudivada},
  comment = {Volume 89, Number 1, March 2013},
  doi = {10.1353/lan.2013.0009},
  interhash = {23ab446da5655ff4a67ccd3378eee467},
  intrahash = {878cafc4d53c683f7c9baeeb2acf3cc3},
  issn = {15350665},
  journal = {Language},
  keywords = {Language Phonetics},
  month = mar,
  number = 1,
  pages = {4--29},
  privnote = {Volume 89, Number 1, March 2013},
  publisher = {Linguistic Society of America},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Sound Correspondences in the World's Languages},
  url = {https://muse.jhu.edu/article/503022},
  volume = 89,
  year = 2013
}

@book{subrahmanyam1974introduction,
  added-at = {2018-01-05T05:11:35.000+0100},
  address = {Annamalaingar},
  author = {Subrahmanyam, P. S.},
  biburl = {https://www.bibsonomy.org/bibtex/2d48235fcfa6e613e82716c7c67fe7126/vngudivada},
  interhash = {140c3a8cf04732afb40d5c3a48bd3b3c},
  intrahash = {d48235fcfa6e613e82716c7c67fe7126},
  keywords = {Book Telugu},
  publisher = {Annamalai University, Department of Linguistics},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {An Introduction to Modern Telugu},
  year = 1974
}

@article{fitch2016sound,
  abstract = {The sounds of words that represent particular meanings are usually thought to vary arbitrarily across languages. However, a large-scale study of languages finds that some associations between sound and meaning are widespread.},
  added-at = {2017-12-28T22:04:22.000+0100},
  author = {Fitch, W. Tecumseh},
  biburl = {https://www.bibsonomy.org/bibtex/2742fee40b9484a5df7cfb9407350d3f6/vngudivada},
  doi = {http://dx.doi.org/10.1038/nature20474},
  interhash = {31d185251a479ed03b8cb4731eec4801},
  intrahash = {742fee40b9484a5df7cfb9407350d3f6},
  journal = {Nature},
  keywords = {Language Linguistics Phonology},
  month = oct,
  pages = {39--40},
  publisher = {Nature Publishing Group, London, United Kingdom},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Sound and meaning in the world's languages},
  volume = 539,
  year = 2016
}

@book{hubbard1998vector,
  abstract = {Using a dual-presentation that is rigorous and comprehensive -- yet exceptionally student-friendly in approach -- this text covers most of the standard topics in multivariate calculus and a substantial part of a standard first course in linear algebra. It focuses on underlying ideas, integrates theory and applications, offers a host of pedagogical aids, and features coverage of differential forms. There is an emphasis on numerical methods to prepare students for modern applications of mathematics.},
  added-at = {2018-01-14T01:35:26.000+0100},
  address = {New York, NY},
  author = {Hubbard, Barbara Burke and Hubbard, John H.},
  biburl = {https://www.bibsonomy.org/bibtex/2ae2b687b3d1ca429366330135daf30b3/vngudivada},
  interhash = {9e59fec014dcd74cb96fabc5a6291923},
  intrahash = {ae2b687b3d1ca429366330135daf30b3},
  isbn = {978-0136574460},
  keywords = {Book VectorCalculus},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Vector Calculus, Linear Algebra and Differential Forms: A Unified Approach},
  year = 1998
}

@book{baayen2008analyzing,
  abstract = {Statistical analysis is a useful skill for linguists and psycholinguists, allowing them to understand the quantitative structure of their data. This textbook provides a straightforward introduction to the statistical analysis of language. Designed for linguists with a non-mathematical background, it clearly introduces the basic principles and methods of statistical analysis, using 'R', the leading computational statistics program. The reader is guided step-by-step through a range of real data sets, allowing them to analyze acoustic data, construct grammatical trees for a variety of languages, quantify register variation in corpus linguistics, and measure experimental data using state-of-the-art models. The visualization of data plays a key role, both in the initial stages of data exploration and later on when the reader is encouraged to criticize various models. Containing over 40 exercises with model answers, this book will be welcomed by all linguists wishing to learn more about working with and presenting quantitative data.},
  added-at = {2017-12-29T15:46:10.000+0100},
  address = {Cambridge, UK},
  author = {Baayen, R. H.},
  biburl = {https://www.bibsonomy.org/bibtex/257e07fda44984d534ca8ef5221202af2/vngudivada},
  interhash = {1b45ef97a7778fa25e86a205dd9e4af1},
  intrahash = {57e07fda44984d534ca8ef5221202af2},
  isbn = {978-0521709187},
  keywords = {Book Linguistics Statistics},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Analyzing Linguistic Data: A Practical Introduction to Statistics using R},
  year = 2008
}

@book{gormley2015elasticsearch,
  abstract = {Whether you need full-text search or real-time analytics of structured data—or both—the Elasticsearch distributed search engine is an ideal way to put your data to work. This practical guide not only shows you how to search, analyze, and explore data with Elasticsearch, but also helps you deal with the complexities of human language, geolocation, and relationships.

If you’re a newcomer to both search and distributed systems, you’ll quickly learn how to integrate Elasticsearch into your application. More experienced users will pick up lots of advanced techniques. Throughout the book, you’ll follow a problem-based approach to learn why, when, and how to use Elasticsearch features.

Understand how Elasticsearch interprets data in your documents
Index and query your data to take advantage of search concepts such as relevance and word proximity
Handle human language through the effective use of analyzers and queries
Summarize and group data to show overall trends, with aggregations and analytics
Use geo-points and geo-shapes—Elasticsearch’s approaches to geolocation
Model your data to take advantage of Elasticsearch’s horizontal scalability
Learn how to configure and monitor your cluster in production},
  added-at = {2018-01-06T23:49:50.000+0100},
  address = {Sebastopol, California},
  author = {Gormley, Clinton and Tong, Zachary},
  biburl = {https://www.bibsonomy.org/bibtex/2f8f6c2af889e337e65d959c16ee62fc4/vngudivada},
  interhash = {99989bc5878a53f9d7a0991a72e401f3},
  intrahash = {f8f6c2af889e337e65d959c16ee62fc4},
  isbn = {978-1449358549},
  keywords = {Elasticsearch IR},
  publisher = {O'Reilly Media},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Elasticsearch: The Definitive Guide. A Distributed Real-Time Search and Analytics Engine},
  year = 2015
}

@book{gries2016quantitative,
  abstract = {As in its first edition, the new edition of Quantitative Corpus Linguistics with R demonstrates how to process corpus-linguistic data with the open-source programming language and environment R. Geared in general towards linguists working with observational data, and particularly corpus linguists, it introduces R programming with emphasis on:

	data processing and manipulation in general;

	text processing with and without regular expressions of large bodies of textual and/or literary data, and;

	basic aspects of statistical analysis and visualization.

	This book is extremely hands-on and leads the reader through dozens of small applications as well as larger case studies. Along with an array of exercise boxes and separate answer keys, the text features a didactic sequential approach in case studies by way of subsections that zoom in to every programming problem. The companion website to the book contains all relevant R code (amounting to approximately 7,000 lines of heavily commented code), most of the data sets as well as pointers to others, and a dedicated Google newsgroup. This new edition is ideal for both researchers in corpus linguistics and instructors who want to promote hands-on approaches to data in corpus linguistics courses.},
  added-at = {2017-12-29T16:05:07.000+0100},
  address = {Abingdon, United Kingdom},
  author = {Gries, Stefan Thomas},
  biburl = {https://www.bibsonomy.org/bibtex/2ccd9a7b15abd4006a78623d10bdaaa69/vngudivada},
  interhash = {955cd32b6210fdb6f55a332219c273c1},
  intrahash = {ccd9a7b15abd4006a78623d10bdaaa69},
  isbn = {978-1138816282},
  keywords = {Book Corpus Linguistics},
  publisher = {Routledge},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Quantitative Corpus Linguistics with R: A Practical Introduction},
  year = 2016
}

@book{thomas2017discovering,
  abstract = {Sketch Engine is an online corpus resource used by students, teachers and language professionals in many parts of the world. The book, Discovering English with Sketch Engine, is intended primarily for students and teachers of English as a foreign language. As the reader undertakes hundreds of guided discovery tasks, many aspects of language and linguistics are taught and experienced. In the process, the reader acquires the practical skills of working with corpus software, and processing and interpreting the data that searches return. This book takes its readers on a voyage of guided discovery that ultimately equips them with the knowledge and skills to formulate, ask and solve language queries about morphology, vocabulary, grammar, discourse, pragmatics and stylistics. This is a skill for life.},
  added-at = {2017-12-31T15:33:47.000+0100},
  address = {New Delhi, India},
  author = {Thomas, James},
  biburl = {https://www.bibsonomy.org/bibtex/263ca7965168e8211477a77742c87ae28/vngudivada},
  edition = {Second},
  interhash = {05818bd4b62597b3daf530c2bb4cae3a},
  intrahash = {63ca7965168e8211477a77742c87ae28},
  isbn = {978-8026083603},
  keywords = {Book Corpus SketchEngine},
  publisher = {Versatile},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Discovering English with Sketch Engine},
  url = {https://www.sketchengine.co.uk/discovering-english-with-sketch-engine/},
  year = 2017
}

@book{newman1998linguistics,
  abstract = {In this collection of papers twelve linguists explore a range of interesting properties of give verbs. The volume offers an in-depth look at many morphological, syntactic, and semantic properties of give verbs, including both literal and figurative senses, across languages. Topics include: an apparent zero-morpheme realisation of give in a Papuan language; noun plus causative-like suffix expressing the give concept in Nahuatl; give and other ditransitive constructions in Zulu; the complex verbal morphologies associated with give verbs in Chipewyan, Cora, and Sochiapan Chinantec; the elaborate classificatory system found with give verbs in Chipewyan and Cora; give, have and take constructions in Slavic languages; the expression of give in American Sign Language; the origin of the German es gibt construction; the extension of give to an adverbial marker in Thai, Khmer, and Vietnamese; the syntax and semantics of Dutch give; first language acquisition of possession terms.},
  added-at = {2017-12-29T03:35:40.000+0100},
  address = {Philadelphia, Pennsylvania},
  biburl = {https://www.bibsonomy.org/bibtex/29caa2d3832245516a0c169769b0c344d/vngudivada},
  editor = {Newman, John},
  interhash = {719f671cfff2e7793d08a8c5b79928bc},
  intrahash = {9caa2d3832245516a0c169769b0c344d},
  isbn = {978-9027229342},
  keywords = {Book Linguistics},
  publisher = {John Benjamins Publishing Company},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {The Linguistics of Giving},
  year = 1998
}

@article{madnani2007getting,
  added-at = {2017-12-29T19:22:26.000+0100},
  address = {New York, NY},
  author = {Madnani, Nitin},
  biburl = {https://www.bibsonomy.org/bibtex/287a90bcb540a2a7501393f7b77a17801/vngudivada},
  doi = {10.1145/1315325.1315330},
  interhash = {4417f36357a5f166692605cf62b0fa1a},
  intrahash = {87a90bcb540a2a7501393f7b77a17801},
  journal = {Crossroads},
  keywords = {NLP NLTK},
  month = sep,
  number = 4,
  pages = {5--5},
  publisher = {ACM},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Getting Started on Natural Language Processing with Python},
  url = {http://doi.acm.org/10.1145/1315325.1315330},
  volume = 13,
  year = 2007
}

@book{fokkink2013distributed,
  abstract = {This book offers students and researchers a guide to distributed algorithms that emphasizes examples and exercises rather than the intricacies of mathematical models. It avoids mathematical argumentation, often a stumbling block for students, teaching algorithmic thought rather than proofs and logic. This approach allows the student to learn a large number of algorithms within a relatively short span of time. Algorithms are explained through brief, informal descriptions, illuminating examples, and practical exercises. The examples and exercises allow readers to understand algorithms intuitively and from different perspectives. Proof sketches, arguing the correctness of an algorithm or explaining the idea behind fundamental results, are also included. An appendix offers pseudocode descriptions of many algorithms.

Distributed algorithms are performed by a collection of computers that send messages to each other or by multiple software threads that use the same shared memory. The algorithms presented in the book are for the most part "classics," selected because they shed light on the algorithmic design of distributed systems or on key issues in distributed computing and concurrent programming.

Distributed Algorithms can be used in courses for upper-level undergraduates or graduate students in computer science, or as a reference for researchers in the field.},
  added-at = {2018-01-07T20:43:44.000+0100},
  address = {Cambridge, Massachusetts},
  author = {Fokkink, Wan},
  biburl = {https://www.bibsonomy.org/bibtex/27fa27bfd44b296ee2d758136cca9e659/vngudivada},
  interhash = {5d7f2400addd541c0d9cbfb9ba972962},
  intrahash = {7fa27bfd44b296ee2d758136cca9e659},
  isbn = {978-0262026772},
  keywords = {Book DistributedComputing},
  publisher = {The MIT Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Distributed Algorithms: An Intuitive Approach},
  year = 2013
}

@book{katz2013phonetics,
  abstract = {The clear and easy way to get a handle on the science of speech.

	The science of how people produce and perceive speech, phonetics has an array of real-world applications, from helping engineers create an authentic sounding Irish or Canadian accent for a GPS voice, to assisting forensics investigators identifying the person whose voice was caught on tape, to helping a film actor make the transition to the stage. Phonetics is a required course among students of speech pathology and linguistics, and it's a popular elective among students of telecommunications and forensics. The first popular guide to this fascinating discipline, Phonetics For Dummies is an excellent overview of the field for students enrolled in introductory phonetics courses and an ideal introduction for anyone with an interest in the field.

	Bonus instructional videos, video quizzes, and other content available online for download on the dummies.com product page for this book.

	William F. Katz, PhD, is Professor of Communication Sciences and Disorders in the School of Behavioral and Brain Sciences at the University of Texas at Dallas, where he teaches and directs research in linguistics, speech science, and language disorders. He has pioneered new treatments for speech loss after stroke, and he studies an unusual disorder known as foreign accent syndrome.},
  added-at = {2017-12-28T22:19:15.000+0100},
  address = {New York, NY},
  author = {Katz, William F.},
  biburl = {https://www.bibsonomy.org/bibtex/2933e83ff659a2624788317f73b6fcc2e/vngudivada},
  interhash = {76f3bc5fe7a6c4d4c97186bb782d98fd},
  intrahash = {933e83ff659a2624788317f73b6fcc2e},
  isbn = {978-1118505083},
  keywords = {Book Phonetics},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Phonetics For Dummies},
  year = 2013
}

@book{hunston2000pattern,
  added-at = {2018-01-02T03:57:57.000+0100},
  address = {Philadelphia, Pennsylvania},
  author = {Hunston, Susan and Francis, Gill},
  biburl = {https://www.bibsonomy.org/bibtex/21d70f5a0a8c68defbdc2636f36c72c55/vngudivada},
  interhash = {528f845eaf8bef9fad626f2b2a5e0f15},
  intrahash = {1d70f5a0a8c68defbdc2636f36c72c55},
  isbn = {978-1556193996},
  keywords = {Book Corpus Grammar Linguistics},
  publisher = {Benjamins Publishing Company},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Pattern Grammar: A Corpus-Driven Approach to the Lexical Grammar of English},
  year = 2000
}

@book{culicover1997principles,
  abstract = {This authoritative survey shows readers how specific methodological assumptions underlie the core analyses on which syntactic theory is based. The author, an internationally respected figure in the field, gives extensive treatment of Government and Binding (GB) theory and summarizes the major proposals and results of Case theory, Theta theory, X' theory, Binding theory, the theory of A- and A' movement, locality conditions, and the theory of Logical Form (LF). He also provides an up-to-date introduction to a number of more recent proposals, including Chomsky's Minminalist Program, Larsonian Shells, and Kaynes's Antisymmetry theory. The most coherent and organized account of syntactic theory currently available, this volume is enhanced by carefully selected and extensive sets of exercises, annotated suggestions for further reading at the end of each chapter, and a comprehensive glossary of technical terms.

	I have used this book, along with other similar sources, for writing a parametric grammar of Persian.The book is coherently organized into ten chapters: 1. Foundations Methods, Arguments, 2. Government and Case, 3. Binding Theory, 4. A-Movement, 5. X'-Theory, 6. A'-Movement, 7. Barriers, 8. LF Representation, 9. Binding and Logical Form. 10. Head Movement and Minimalism. Beginning with the second chapter the author picks up a syntactic problem and critically examines the solutions offered by linguists. For instance,in chapter four he concentrates on the analysis of the passive and its properties, namely, the movement of the underlying object into the surface subject position of the sentence in terms of a lexical analysis, a movement hypothesis and a chain relationship.Moreover, at the end of each chapter there are ample exercises from various languages ( except Persian, of course) for readers / students to ponder over. Crucially,readers/students can familiarize themselves with current linguistic as well as syntactic issues and their proposed solutions. Nevertheless, the book suffers from a couple of drawbacks concerning the ultimate plausible solution for each problem and , more significantly, the definition of principles and parameters as a theoretical foundation/ approach selected by the author,as the title suggests.I consider the latter a critical issue because there seems to be a fair degree of consensus among researchers concerning the principles which describe universal properties of grammatical operations or structures.However, differences of opinions arise regarding the parameters and the components of the language in which they have to be found. According to Chomsky, parameters are associated with the principles of Universal Grammar.Other linguists attribute them to the inflectional system of the language, the fictional categories or lexical categories in general.From this book one cannot understand the author's position on the theory of parametrization.

	Since Chomsky established the GB model in the early 1980's, many introductory books have been published. Culicover's book came out by the end of the GB fashion when most people were attracted by Chomsky's new ideas of the minimalist program (MP). However, this book is not out of date at all, since Culicover illustrated the GB theory in a very critical way providing with quite a number of problems which cannot be solved at ease within the GB framework. All those problems are interesting issues that generative syntax has to manage. Though the author gave some suggestions, the issues remained open both to the GB framework and the MP followers. After reading this book, you may realize that the GB model is not as good as most people believe and that the burden of the MP will be heavier when it has to take over what left insolvable in the GB.

	Peter W. Culicover is at Ohio State University.},
  added-at = {2017-12-27T23:06:04.000+0100},
  address = {Oxford, UK},
  author = {Culicover, Peter W.},
  biburl = {https://www.bibsonomy.org/bibtex/2e3562fff635636548847e3d78e8c2498/vngudivada},
  interhash = {79c13cad35193bb06ecb135b5286f719},
  intrahash = {e3562fff635636548847e3d78e8c2498},
  isbn = {978-0198700142},
  keywords = {Book Syntax},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Principles and Parameters: An Introduction to Syntactic Theory},
  year = 1997
}

@article{gablasova2017exploring,
  abstract = {This article contributes to the debate about the appropriate use of corpus data in language learning research. It focuses on frequencies of linguistic features in language use and their comparison across corpora. The majority of corpus-based second language acquisition studies employ a comparative design in which either one or more second language (L2) corpora are compared to a first language (L1) production corpus or two or more L2 corpora are compared to each other. This article critically examines some of the central tenets of the comparative method related to the interspeaker variation in L1 and L2 use, the representativeness and comparability of corpus data, the interpretation of difference found between corpora and the appropriate use of statistics.  Using and discussing a set of five L1 spoken English corpora and three L2 English corpora (two spoken and one written), we approach these areas empirically exploring different sources of variations and methodological options that corpus-based SLA studies offer.},
  added-at = {2018-01-02T04:54:14.000+0100},
  author = {Gablasova, Dana and Brezina, Vaclav and McEnery, Tony},
  biburl = {https://www.bibsonomy.org/bibtex/228172ced9716c8e7cb1756adcbb203e6/vngudivada},
  doi = {10.1111/lang.12226},
  interhash = {890d72777c6181f55e024271a7a70da1},
  intrahash = {28172ced9716c8e7cb1756adcbb203e6},
  issn = {1467-9922},
  journal = {Language Learning},
  keywords = {Corpus Language Learning},
  number = {S1},
  pages = {130--154},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Exploring Learner Language Through Corpora: Comparing and Interpreting Corpus Frequency Information},
  url = {http://dx.doi.org/10.1111/lang.12226},
  volume = 67,
  year = 2017
}

@book{wierzbicka1997understanding,
  abstract = {This book develops the dual themes that languages can differ widely in their vocabularies, and are also sensitive indices to the cultures to which they belong. Wierzbicka seeks to demonstrate that every language has "key concepts," expressed in "key words," which reflect the core values of a given culture. She shows that cultures can be revealingly studied, compared, and explained to outsiders through their key concepts, and that the analytical framework necessary for this purpose is provided by the "natural semantic metalanguage," based on lexical universals, that the author and colleagues have developed on the basis of wide-ranging cross-linguistic investigations. Appealing to anthropologists, psychologists, and philosophers as well as linguists, this book demonstrates that cultural patterns can be studied in a verifiable, rigorous, and non-speculative way, on the basis of empirical evidence and in a coherent theoretical framework.},
  added-at = {2017-12-29T01:20:29.000+0100},
  author = {Wierzbicka, Anna},
  biburl = {https://www.bibsonomy.org/bibtex/21e977ea02ce1f0df6c2c940b07c70e37/vngudivada},
  interhash = {122cee54cf1573803e34e4e42cf1fa1b},
  intrahash = {1e977ea02ce1f0df6c2c940b07c70e37},
  isbn = {978-0195088366},
  keywords = {Book Linguistics},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Understanding Cultures Through Their Key Words: English, Russian, Polish, German, and Japanese},
  year = 1997
}

@book{bird2009natural,
  abstract = {This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication.

	Packed with examples and exercises, Natural Language Processing with Python will help you:

	Extract information from unstructured text, either to guess the topic or identify named entities

	Analyze linguistic structure in text, including parsing and semantic analysis

	Access popular linguistic databases, including WordNet and treebanks

	Integrate techniques drawn from fields as diverse as linguistics and artificial intelligence

	This book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you are interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you are simply curious to have a programmer's perspective on how human language works -- you will find Natural Language Processing with Python both fascinating and immensely useful.},
  added-at = {2017-12-29T15:17:55.000+0100},
  address = {Sebastopol, California},
  author = {Bird, Steven and Klein, Ewan and Loper, Edward},
  biburl = {https://www.bibsonomy.org/bibtex/2c90dc59441d01c8bef58a947274164d4/vngudivada},
  interhash = {5408d7da097b9cd81239c238da8bfaf4},
  intrahash = {c90dc59441d01c8bef58a947274164d4},
  isbn = {978-0596516499},
  keywords = {Book Linguistics NLP},
  publisher = {O'Reilly Media},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit},
  url = {http://www.nltk.org/book/},
  year = 2009
}

@book{gries2013statistics,
  abstract = {This book is the revised and extended second edition of Statistics for Linguistics with R. The comprehensive revision includes new small sections on programming topics that facilitate statistical analysis, the addition of a variety of statistical functions readers can apply to their own data, and a revision of overview sections on statistical tests and regression modeling. The main revision is a complete rewrite of the chapter on multifactorial approaches, which now contains sections on linear regression, binary and ordinal logistic regression, multinomial and Poisson regression, and repeated-measures ANOVA.The revisions are completed by a new visual tool to identify the right statistical test for a given problem and data set.},
  added-at = {2017-12-29T16:04:44.000+0100},
  address = {Boston, Massachusetts},
  author = {Gries, Stefan Thomas},
  biburl = {https://www.bibsonomy.org/bibtex/2e9617d28a070f95c7e8d4c20549b3448/vngudivada},
  interhash = {85b3c653e3696031cff764f6fcafd2ed},
  intrahash = {e9617d28a070f95c7e8d4c20549b3448},
  isbn = {978-3110307283},
  keywords = {Book Linguistics R Statistics},
  publisher = {De Gruyter Mouton},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Statistics for Linguistics with R},
  year = 2013
}

@book{laney2017infonomics,
  abstract = {The discipline of infonomics takes you beyond thinking and talking about information as an asset to actually valuing and treating it as one. Infonomics provides the foundation and methods for quantifying information asset value and tactics for using information as your competitive edge to drive growth.

Many people have inquired about what compelled me to write a book about infonomics. Moreover, what kind of crazy person decides to write a book in his "spare time" while in a job demanding much more than bankers' hours?

The short answer is that it's been germinating inside me for many years, and kind of bursting to get out. (Yes, much like in the Alien films.) Back in 1999, I casually and kind of tongue-in-cheek coined the "infonomics" term in a META Group research piece, before META was acquired by Gartner, and even before the first incredible Freakonomics book. It took a couple years for me to realize that information isn't formally valued, recognized or reported as an economic asset. From that moment onward, I was on a mission! And for the past couple decades I have been researching, working with clients, developing ideas, and writing articles on this topic among others.

After compiling 3000 articles, papers and ideas in an Evernote folder on the topic of valuing, accounting for, and administering various kinds of assets, and having amassed over 300 examples of organizations monetizing information, it seemed  high time to write a book. Thankfully Gartner has provided me with the ideal platform, surrounding me with dozens of brilliant colleagues and hundreds of enthusiastic clients over the years with whom to further explore and develop the infonomics concept. But being either too busy or too lazy (more likely both) to write three separate books on monetizing, managing, and measuring information as an asset, it's packed it all into a single book. Feel free however to digest it as three separate books, focusing on one part or another, depending upon your role and interests.

What do I hope to achieve from writing the book? In short, it would be great to affect some kind of awakening that provokes further economic expansion at the hands of a growing cadre of infosavvy leaders and their companies. The book isn't about me nor about achieving fame and fortune. (Most Gartner analysts already have all the tech industry notoriety we can handle. And the book is a Gartner book, so no monetization for me no matter how well it sells.) Rather, day in and day out we Gartner analysts speak with clients who are struggling to manage their data better and do more with it. Their organizations put incredible discipline and resources into the handling and leveraging of other business assets, but not so much with their information.

So if there's one question (maybe two) that I would like the book to evoke from business, technology and information leaders, it's this:

Why hasn't our organization been treating information as an asset all along, and how the heck can we hope to thrive, let alone survive, in an increasingly digital world without doing so???

It would be great to see many of you in any of these roles take some of the ideas from the book on just how to do so, implement them, and build on them.

Perhaps the book brings about a revolution of sorts, leading to the recognition of information as an accounting asset, and subject to the same legal treatment as other forms of property. Or maybe the book will compel some bright young economist to expand upon and develop infonomics into a new branch of economics, earning herself a Nobel Prize. But accounting regulations and the field of economics are not really my main concern. More than anything, I'm anxious to see how organizations that internally embrace and execute on these ideas and practices become tomorrow's economic powerhouses.

So I hope you and your organization's other business leaders, technology, data and analytics leaders, and architects (at least) get the book. And that you each find it interesting, informative, and instructive. Or at least one of the above.},
  added-at = {2018-01-21T00:55:48.000+0100},
  address = {New York, NY},
  author = {Laney, Douglas B.},
  biburl = {https://www.bibsonomy.org/bibtex/2fa4716a359bd63f56ce687143099a414/vngudivada},
  interhash = {98317c4f9095f7dfd568a72373e39cdb},
  intrahash = {fa4716a359bd63f56ce687143099a414},
  isbn = {978-1138090385},
  keywords = {Book InformationMonetization},
  publisher = {Routledge},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Infonomics: How to Monetize, Manage, and Measure Information as an Asset for Competitive Advantage},
  year = 2017
}

@article{arora2016linear,
  abstract = {Word embeddings are ubiquitous in NLP and information retrieval, but it's unclear what they represent when the word is polysemous, i.e., has multiple senses. Here it is shown that multiple word senses reside in linear superposition within the word embedding and can be recovered by simple sparse coding.

	The success of the method --- which applies to several embedding methods including word2vec --- is mathematically explained using the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each word sense is also accompanied by one of about 2000 discourse atoms that give a succinct description of which other words co-occur with that word sense. Discourse atoms seem of independent interest, and make the method potentially more useful than the traditional clustering-based approaches to polysemy.},
  added-at = {2018-01-15T03:02:42.000+0100},
  archiveprefix = {arXiv},
  author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  biburl = {https://www.bibsonomy.org/bibtex/2b2ce37e2ca5464e8ccd41b0816a74e4c/vngudivada},
  eprint = {1601.03764},
  interhash = {c206b10fbb99ce6bd59c1c2d9abf10ff},
  intrahash = {b2ce37e2ca5464e8ccd41b0816a74e4c},
  journal = {CoRR},
  keywords = {Polysemy WordSense},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Linear Algebraic Structure of Word Senses, with Applications to Polysemy},
  url = {http://arxiv.org/abs/1601.03764},
  volume = {abs/1601.03764},
  year = 2016
}

@article{zhu2009introduction,
  abstract = {Semi-supervised learning is a learning paradigm concerned with the study of how computers and natural systems such as humans learn in the presence of both labeled and unlabeled data. Traditionally, learning has been studied either in the unsupervised paradigm (e.g., clustering, outlier detection) where all the data are unlabeled, or in the supervised paradigm (e.g., classification, regression) where all the data are labeled. The goal of semi-supervised learning is to understand how combining labeled and unlabeled data may change the learning behavior, and design algorithms that take advantage of such a combination. Semi-supervised learning is of great interest in machine learning and data mining because it can use readily available unlabeled data to improve supervised learning tasks when the labeled data are scarce or expensive. Semi-supervised learning also shows potential as a quantitative tool to understand human category learning, where most of the input is self-evidently unlabeled. In this introductory book, we present some popular semi-supervised learning models, including self-training, mixture models, co-training and multiview learning, graph-based methods, and semi-supervised support vector machines. For each model, we discuss its basic mathematical formulation. The success of semi-supervised learning depends critically on some underlying assumptions. We emphasize the assumptions made by each model and give counterexamples when appropriate to demonstrate the limitations of the different models. In addition, we discuss semi-supervised learning for cognitive psychology. Finally, we give a computational learning theoretic perspective on semi-supervised learning, and we conclude the book with a brief discussion of open questions in the field.},
  added-at = {2018-01-31T02:12:36.000+0100},
  author = {Zhu, Xiaojin and Goldberg, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/2374c27110aa7417a8b81085de4190ca5/vngudivada},
  doi = {10.2200/S00196ED1V01Y200906AIM006},
  eprint = {https://doi.org/10.2200/S00196ED1V01Y200906AIM006},
  interhash = {b7bbcd90364051d64ea01112c64f2513},
  intrahash = {374c27110aa7417a8b81085de4190ca5},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  keywords = {sys:relevantfor:ecu-cc-research SemiSupervisedLearning SynthesisLecture},
  number = 1,
  pages = {1-130},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Introduction to Semi-Supervised Learning},
  volume = 3,
  year = 2009
}

@book{erl2013cloud,
  abstract = {Clouds are distributed technology platforms that leverage sophisticated technology innovations to provide highly scalable and resilient environments that can be remotely utilized by organizations in a multitude of powerful ways. To successfully build upon, integrate with, or even create a cloud environment requires an understanding of its common inner mechanics, architectural layers, and models, as well as an understanding of the business and economic factors that result from the adoption and real-world use of cloud-based services.

	In Cloud Computing: Concepts, Technology & Architecture, Thomas Erl, one of the world’s top-selling IT authors, teams up with cloud computing experts and researchers to break down proven and mature cloud computing technologies and practices into a series of well-defined concepts, models, technology mechanisms, and technology architectures, all from an industry-centric and vendor-neutral point of view. In doing so, the book establishes concrete, academic coverage with a focus on structure, clarity, and well-defined building blocks for mainstream cloud computing platforms and solutions.

	Subsequent to technology-centric coverage, the book proceeds to establish business-centric models and metrics that allow for the financial assessment of cloud-based IT resources and their comparison to those hosted on traditional IT enterprise premises. Also provided are templates and formulas for calculating SLA-related quality-of-service values and numerous explorations of the SaaS, PaaS, and IaaS delivery models.

	With more than 260 figures, 29 architectural models, and 20 mechanisms, this indispensable guide provides a comprehensive education of cloud computing essentials that will never leave your side.},
  added-at = {2018-01-29T20:40:55.000+0100},
  address = {Upper Saddle River, New Jersey},
  author = {Erl, Thomas and Puttini, Ricardo and Mahmood, Zaigham},
  biburl = {https://www.bibsonomy.org/bibtex/2ffd2397e0becfbec4eb47a207730fd91/vngudivada},
  interhash = {ebcc1f77f4e3d6959f676dec43bd1486},
  intrahash = {ffd2397e0becfbec4eb47a207730fd91},
  isbn = {978-0133387520},
  keywords = {Book CloudComputing},
  publisher = {Prentice Hall},
  series = {The Prentice Hall Service Technology Series},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Cloud Computing: Concepts, Technology \& Architecture},
  year = 2013
}

@book{elatia2016mining,
  abstract = {Addresses the impacts of data mining on education and reviews applications in educational research teaching, and learning

This book discusses the insights, challenges, issues, expectations, and practical implementation of data mining (DM) within educational mandates. Initial series of chapters offer a general overview of DM, Learning Analytics (LA), and data collection models in the context of educational research, while also defining and discussing data mining’s four guiding principles— prediction, clustering, rule association, and outlier detection. The next series of chapters showcase the pedagogical applications of Educational Data Mining (EDM) and feature case studies drawn from Business, Humanities, Health Sciences, Linguistics, and Physical Sciences education that serve to highlight the successes and some of the limitations of data mining research applications in educational settings. The remaining chapters focus exclusively on EDM’s emerging role in helping to advance educational research—from identifying at-risk students and closing socioeconomic gaps in achievement to aiding in teacher evaluation and facilitating peer conferencing. This book features contributions from international experts in a variety of fields.

 Includes case studies where data mining techniques have been effectively applied to advance teaching and learning
Addresses applications of data mining in educational research, including: social networking and education; policy and legislation in the classroom; and identification of at-risk students
Explores Massive Open Online Courses (MOOCs) to study the effectiveness of online networks in promoting learning and understanding the communication patterns among users and students
Features supplementary resources including a primer on foundational aspects of educational mining and learning analytics
Data Mining and Learning Analytics: Applications in Educational Research is written for both scientists in EDM and educators interested in using and integrating DM and LA to improve education and advance educational research.},
  added-at = {2018-01-21T02:36:15.000+0100},
  address = {Hoboken, New Jersey},
  author = {ElAtia, Samira and Ipperciel, Donald and Zai\"{a}ne, Osmar R.},
  biburl = {https://www.bibsonomy.org/bibtex/27aef78ffeee475d3a7e7f0edebc11a6f/vngudivada},
  interhash = {4051ebb5d58246782c6f6ac90399b1f7},
  intrahash = {7aef78ffeee475d3a7e7f0edebc11a6f},
  isbn = {978-1118998236},
  keywords = {Book EDM LearningAnalytics},
  publisher = {John Wiley \& Sons},
  series = {Wiley Series on Methods and Applications in Data Mining},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Data Mining and Learning Analytics: Applications in Educational Research},
  year = 2016
}

@article{white2016pluralized,
  abstract = {Understanding the connection between leadership and informal social network structures is important in advancing understanding of the enactment of pluralized leadership. In this article we explore how the enactment of pluralized leadership is shaped by leadership influence and informal (advice and support) networks and the interactions between the two. Building on recent developments in Exponential Random Graph Modeling, we empirically model the cross network effects across three leadership networks and explore different forms of cross network effects and under what conditions they occur. Our findings suggest that patterns of pluralized leadership have important endogenous qualities, as shaped through actors' leadership and informal networks, and are important for understanding the required capability for facing increasingly complex organizational situations.},
  added-at = {2018-02-25T18:06:02.000+0100},
  author = {White, Leroy and Currie, Graeme and Lockett, Andy},
  biburl = {https://www.bibsonomy.org/bibtex/2e8500b7f7b3a20ec8eb3f0614c3ae804/vngudivada},
  doi = {10.1016/j.leaqua.2016.01.004},
  interhash = {8e622426c58440fff2a9dfeee40de20b},
  intrahash = {e8500b7f7b3a20ec8eb3f0614c3ae804},
  journal = {The Leadership Quarterly},
  keywords = {Leadership},
  month = apr,
  number = 2,
  pages = {280--297},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Pluralized leadership in complex organizations: Exploring the cross network effects between formal and informal leadership relations},
  url = {https://doi.org/10.1016%2Fj.leaqua.2016.01.004},
  volume = 27,
  year = 2016
}

@article{marion2016informal,
  abstract = {This study proposes that dynamically changing organizations can achieve stable productive capacity (or environmentally stable states) by adaptively processing internal and external volatility. It tests this proposal with agent network measures rather than with more traditional variables. We examine three such network dynamics that, according to the collective perspectives of complexity theory, influence a network's capacity to perform: informal leadership, interaction among agents, and clique engagement. Data were collected at an elementary school in the southeastern United States; the methodologies include qualitative interviews, network analysis, and response surface methods. Results revealed that informal leadership and engagement in cliques positively affect the productive capacity of organizations, and that cliques can absorb large amounts of information flow (volatility) thus promoting stable productivity levels. That is, collective, information-processing adaptability fosters stable productivity plateaus that absorb unpredictable demands. Suggestions for practitioners are provided.},
  added-at = {2018-02-25T18:02:35.000+0100},
  author = {Marion, Russ and Christiansen, Jon and Klar, Hans W. and Schreiber, Craig and Erdener, Mehmet Akif},
  biburl = {https://www.bibsonomy.org/bibtex/2a61879c594a6e4944197f8cb6c5e41ec/vngudivada},
  doi = {10.1016/j.leaqua.2016.01.003},
  interhash = {fdd74fa168c91fe418e906f4d79c3408},
  intrahash = {a61879c594a6e4944197f8cb6c5e41ec},
  journal = {The Leadership Quarterly},
  keywords = {Leadership},
  month = apr,
  number = 2,
  pages = {242--260},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Informal leadership, interaction, cliques and productive capacity in organizations: A collectivist analysis},
  url = {https://doi.org/10.1016%2Fj.leaqua.2016.01.003},
  volume = 27,
  year = 2016
}

@book{aiden2014uncharted,
  abstract = {One of the most exciting developments from the world of ideas in decades, presented with panache by two frighteningly brilliant, endearingly unpretentious, and endlessly creative young scientists. -- Steven Pinker, author of The Better Angels of Our Nature

Our society has gone from writing snippets of information by hand to generating a vast flood of 1s and 0s that record almost every aspect of our lives: who we know, what we do, where we go, what we buy, and who we love. This year, the world will generate 5 zettabytes of data. (That is a five with twenty-one zeros after it.) Big data is revolutionizing the sciences, transforming the humanities, and renegotiating the boundary between industry and the ivory tower.

What is emerging is a new way of understanding our world, our past, and possibly, our future. In Uncharted, Erez Aiden and Jean-Baptiste Michel tell the story of how they tapped into this sea of information to create a new kind of telescope: a tool that, instead of uncovering the motions of distant stars, charts trends in human history across the centuries. By teaming up with Google, they were able to analyze the text of millions of books. The result was a new field of research and a scientific tool, the Google Ngram Viewer, so groundbreaking that its public release made the front page of The New York Times, The Wall Street Journal, and The Boston Globe, and so addictive that Mother Jones called it the greatest timewaster in the history of the internet.

Using this scope, Aiden and Michel -- and millions of users worldwide -- are beginning to see answers to a dizzying array of once intractable questions. How quickly does technology spread? Do we talk less about God today? When did people start having sex instead of making love? At what age do the most famous people become famous? How fast does grammar change? Which writers had their works most effectively censored by the Nazis? When did the spelling donut start replacing the venerable doughnut? Can we predict the future of human history? Who is better known -- Bill Clinton or the rutabaga?

All over the world, new scopes are popping up, using big data to quantify the human experience at the grandest scales possible. Yet dangers lurk in this ocean of 1s and 0s -- threats to privacy and the specter of ubiquitous government surveillance. Aiden and Michel take readers on a voyage through these uncharted waters.},
  added-at = {2018-01-21T00:36:23.000+0100},
  address = {New York, NY},
  author = {Aiden, Erez and Michel, Jean-Baptiste},
  biburl = {https://www.bibsonomy.org/bibtex/2a098bf7137e5756fbc2b36213c5a8bc5/vngudivada},
  interhash = {fdb99fcb541e141637ef902fb8022491},
  intrahash = {a098bf7137e5756fbc2b36213c5a8bc5},
  isbn = {978-1594632907},
  keywords = {BigDataEthics Book},
  publisher = {Riverhead Books},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Uncharted: Big Data as a Lens on Human Culture},
  year = 2014
}

@article{cullenlester2016collective,
  abstract = {In this introduction to the special issue on collective and network approaches to leadership, we begin by discussing the state of research and practice in this burgeoning area to clarify the need for the empirical articles compiled in this issue. We then describe each article, how it contributes to the goals of the issue, and some common themes across the articles. We close by identifying some important areas for future research on collective and network leadership approaches.},
  added-at = {2018-02-25T17:53:03.000+0100},
  author = {Cullen-Lester, Kristin L. and Yammarino, Francis J.},
  biburl = {https://www.bibsonomy.org/bibtex/2ca64f8a6323c83c9e2b2563b0bc69a07/vngudivada},
  doi = {10.1016/j.leaqua.2016.02.001},
  interhash = {84d0b2281ad1695b6475fb4cb97bf6e0},
  intrahash = {ca64f8a6323c83c9e2b2563b0bc69a07},
  journal = {The Leadership Quarterly},
  keywords = {Leadership},
  month = apr,
  number = 2,
  pages = {173--180},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Collective and network approaches to leadership: Special issue introduction},
  url = {https://doi.org/10.1016%2Fj.leaqua.2016.02.001},
  volume = 27,
  year = 2016
}

@book{epp2010discrete,
  abstract = {Susanna Epp's DISCRETE MATHEMATICS WITH APPLICATIONS, FOURTH EDITION provides a clear introduction to discrete mathematics. Renowned for her lucid, accessible prose, Epp explains complex, abstract concepts with clarity and precision. This book presents not only the major themes of discrete mathematics, but also the reasoning that underlies mathematical thought. Students develop the ability to think abstractly as they study the ideas of logic and proof. While learning about such concepts as logic circuits and computer addition, algorithm analysis, recursive thinking, computability, automata, cryptography, and combinatorics, students discover that the ideas of discrete mathematics underlie and are essential to the science and technology of the computer age. Overall, Epp's emphasis on reasoning provides students with a strong foundation for computer science and upper-level mathematics courses.},
  added-at = {2018-02-02T02:12:00.000+0100},
  address = {Boston, Massachusetts},
  author = {Epp, Susanna S.},
  biburl = {https://www.bibsonomy.org/bibtex/241c11d3a71369280b0d1d7d9718acfac/vngudivada},
  edition = {Fourth},
  interhash = {daa79dc2626a904db591fbba45852744},
  intrahash = {41c11d3a71369280b0d1d7d9718acfac},
  isbn = {978-0495391326},
  keywords = {Book DiscreteMathematics},
  publisher = {Brooks Cole},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Discrete Mathematics with Applications},
  year = 2010
}

@book{harari2015sapiens,
  abstract = {From a renowned historian comes a groundbreaking narrative of humanity’s creation and evolution—a #1 international bestseller—that explores the ways in which biology and history have defined us and enhanced our understanding of what it means to be “human.”

One hundred thousand years ago, at least six different species of humans inhabited Earth. Yet today there is only one—homo sapiens. What happened to the others? And what may happen to us?

Most books about the history of humanity pursue either a historical or a biological approach, but Dr. Yuval Noah Harari breaks the mold with this highly original book that begins about 70,000 years ago with the appearance of modern cognition. From examining the role evolving humans have played in the global ecosystem to charting the rise of empires, Sapiens integrates history and science to reconsider accepted narratives, connect past developments with contemporary concerns, and examine specific events within the context of larger ideas.

Dr. Harari also compels us to look ahead, because over the last few decades humans have begun to bend laws of natural selection that have governed life for the past four billion years. We are acquiring the ability to design not only the world around us, but also ourselves. Where is this leading us, and what do we want to become?

Featuring 27 photographs, 6 maps, and 25 illustrations/diagrams, this provocative and insightful work is sure to spark debate and is essential reading for aficionados of Jared Diamond, James Gleick, Matt Ridley, Robert Wright, and Sharon Moalem.},
  added-at = {2018-03-02T04:51:20.000+0100},
  author = {Harari, Yuval Noah},
  biburl = {https://www.bibsonomy.org/bibtex/21b5b27b206e8f4c78b5cc45151cfa759/vngudivada},
  interhash = {c07cb4612a093dbc88c20d216ff5f936},
  intrahash = {1b5b27b206e8f4c78b5cc45151cfa759},
  keywords = {Book Sapiens},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Sapiens: A Brief History of Humankind},
  year = 2015
}

@article{serban2016exploring,
  abstract = {Leadership research, traditionally focused on the behavior of an appointed/elected leader, is rapidly shifting towards a distributed, group process form of leadership known as “shared leadership”. Since empirical research supporting this approach is limited, we extend prior work exploring antecedents and outcomes of shared leadership and develop a framework examining its role as a mediator between task and team characteristics (internal team environment, task cohesion and task ambiguity) and task and team-level consequences (task satisfaction, team satisfaction and team performance). Analyzing experimental data through a mixed-methods approach (quantitative via regression-based analysis and qualitative using thematic analysis for unstructured data in NVivo 10), our results indicate that, in the context of a creative task, internal team environment and task cohesion predict shared leadership, which, in turn, determines task satisfaction. We discuss implications of these findings and future paths for exploring shared leadership antecedents and outcomes.},
  added-at = {2018-02-25T17:55:42.000+0100},
  author = {Serban, Andra and Roberts, Ashley J.B.},
  biburl = {https://www.bibsonomy.org/bibtex/2b1c57e866258f51c81dc7acaa0978956/vngudivada},
  doi = {10.1016/j.leaqua.2016.01.009},
  interhash = {bbc7c12422a8048430c5604d2e8f37db},
  intrahash = {b1c57e866258f51c81dc7acaa0978956},
  journal = {The Leadership Quarterly},
  keywords = {Leadership},
  month = apr,
  number = 2,
  pages = {181--199},
  publisher = {Elsevier {BV}},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Exploring antecedents and outcomes of shared leadership in a creative context: A mixed-methods approach},
  url = {https://doi.org/10.1016%2Fj.leaqua.2016.01.009},
  volume = 27,
  year = 2016
}

@article{wolpert1992stacked,
  added-at = {2018-02-01T02:07:24.000+0100},
  author = {Wolpert, David H.},
  biburl = {https://www.bibsonomy.org/bibtex/29e96aa910e02c2538a90b67e1f3a2154/vngudivada},
  doi = {10.1016/s0893-6080(05)80023-1},
  interhash = {c2d406b7f1679b8bf46d42632e6cd022},
  intrahash = {9e96aa910e02c2538a90b67e1f3a2154},
  journal = {Neural Networks},
  keywords = {MachineLearning StackedGeneralization},
  month = jan,
  number = 2,
  pages = {241--259},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Stacked generalization},
  volume = 5,
  year = 1992
}

@book{levitt2009freakonomics,
  abstract = {These may not sound like typical questions for an economist to ask. But Steven D. Levitt is not a typical economist. He studies the riddles of everyday life—from cheating and crime to parenting and sports—and reaches conclusions that turn conventional wisdom on its head. Freakonomics is a groundbreaking collaboration between Levitt and Stephen J. Dubner, an award-winning author and journalist. They set out to explore the inner workings of a crack gang, the truth about real estate agents, the secrets of the Ku Klux Klan, and much more. Through forceful storytelling and wry insight, they show that economics is, at root, the study of incentives—how people get what they want or need, especially when other people want or need the same thing.},
  added-at = {2018-01-21T01:11:22.000+0100},
  address = {New York, NY},
  author = {Levitt, Steven D. and Dubner, Stephen J},
  biburl = {https://www.bibsonomy.org/bibtex/22ecd1b36cf7f241b3f9dc806b97ae27d/vngudivada},
  interhash = {7dc6f94a5a327142e08505222f111fbd},
  intrahash = {2ecd1b36cf7f241b3f9dc806b97ae27d},
  isbn = {978-0060731335},
  keywords = {Book Economics Success},
  publisher = {HarperCollins Publishers},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Freakonomics: A Rogue Economist Explores the Hidden Side of Everything},
  year = 2009
}

@book{goldberg2008genetic,
  abstract = {This book, suitable for both course work and self-study, brings together for the first time, in an informal, tutorial fashion, the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields: programmers, scientists, engineers, mathematicians, statisticians and management scientists will all find interesting possibilities here. Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. Chapter concludes with exercises and computer assignments. No prior knowledge of GAs or genetics is assumed.},
  added-at = {2018-01-31T04:43:39.000+0100},
  address = {London, England},
  author = {Goldberg, David E.},
  biburl = {https://www.bibsonomy.org/bibtex/2e5cf64053bac6fc15981ebda32350c61/vngudivada},
  interhash = {d7c8bf5e0c4f90994558d58aea202d08},
  intrahash = {e5cf64053bac6fc15981ebda32350c61},
  isbn = {978-8177588293},
  keywords = {Book GeneticAlgorithms},
  publisher = {Dorling Kindersley Pvt Ltd},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Genetic Algorithms},
  year = 2008
}

@book{seni2010ensemble,
  abstract = {Ensemble methods have been called the most influential development in Data Mining and Machine Learning in the past decade. They combine multiple models into one usually more accurate than the best of its components. Ensembles can provide a critical boost to industrial challenges -- from investment timing to drug discovery, and fraud detection to recommendation systems -- where predictive accuracy is more vital than model interpretability.

	Ensembles are useful with all modeling algorithms, but this book focuses on decision trees to explain them most clearly. After describing trees and their strengths and weaknesses, the authors provide an overview of regularization -- today understood to be a key reason for the superior performance of modern ensembling algorithms. The book continues with a clear description of two recent developments: Importance Sampling (IS) and Rule Ensembles (RE). IS reveals classic ensemble methods -- bagging, random forests, and boosting -- to be special cases of a single algorithm, thereby showing how to improve their accuracy and speed. REs are linear rule models derived from decision tree ensembles. They are the most interpretable version of ensembles, which is essential to applications such as credit scoring and fault diagnosis. Lastly, the authors explain the paradox of how ensembles achieve greater accuracy on new data despite their (apparently much greater) complexity.

	This book is aimed at novice and advanced analytic researchers and practitioners -- especially in Engineering, Statistics, and Computer Science. Those with little exposure to ensembles will learn why and how to employ this breakthrough method, and advanced practitioners will gain insight into building even more powerful models. Throughout, snippets of code in R are provided to illustrate the algorithms described and to encourage the reader to try the techniques.},
  added-at = {2018-01-31T05:46:36.000+0100},
  address = {San Rafael, California},
  author = {Seni, Giovanni and Elder, John F.},
  biburl = {https://www.bibsonomy.org/bibtex/2401a022383d365a0ae1c0fd4b2bcedaa/vngudivada},
  interhash = {21632bba002df477e26c029dfbff34e3},
  intrahash = {401a022383d365a0ae1c0fd4b2bcedaa},
  isbn = {978-1608452842},
  keywords = {EnsembleLearning SynthesisLecture},
  publisher = {Morgan \& Claypool},
  series = {Synthesis Lectures on Data Mining and Knowledge Discovery},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions},
  year = 2010
}

@article{drescher2016shared,
  abstract = {Although research has extensively examined the relationship between shared leadership and performance outcomes, little is known about the interaction with other team variables such as commonality and communication mode. Moreover, nearly all research on shared leadership has adopted a cross-sectional approach. Accordingly, this research examined the effects of shared leadership, commonality, and communication mode on work performance and satisfaction. Using an experimental policy-capturing design, shared leadership, commonality, and communication mode were manipulated. Students (sample 1) and employees (sample 2) evaluated their performance and satisfaction. The results of multilevel analyses revealed that both shared leadership and high commonality had positive effects on team members' intended performance and predicted satisfaction. Moreover, we found that commonality and communication mode had interactive effects. Interestingly, commonality was more important for face-to-face teams than for virtual teams. The results both emphasize the importance of shared leadership and prompt significant recommendations for virtual teamwork.},
  added-at = {2018-02-25T17:58:25.000+0100},
  author = {Drescher, Gesche and Garbers, Yvonne},
  biburl = {https://www.bibsonomy.org/bibtex/26e25eb6de74edd258143de17f2892b5d/vngudivada},
  doi = {10.1016/j.leaqua.2016.02.002},
  interhash = {9cb95f9dcbbd62a6bf50d47a2ee163a0},
  intrahash = {6e25eb6de74edd258143de17f2892b5d},
  journal = {The Leadership Quarterly},
  keywords = {Leadership},
  month = apr,
  number = 2,
  pages = {200--217},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Shared leadership and commonality: A policy-capturing study},
  volume = 27,
  year = 2016
}

@book{gentle2017matrix,
  abstract = {This much-needed work presents, among other things, the relevant aspects of the theory of matrix algebra for applications in statistics. Written in an informal style, it addresses computational issues and places more emphasis on applications than existing texts.},
  added-at = {2018-01-14T15:01:34.000+0100},
  address = {New York, NY},
  author = {Gentle, James E.},
  biburl = {https://www.bibsonomy.org/bibtex/2a2a1b4fa63ec4e1bbbdc979be1ea8cd9/vngudivada},
  edition = {Second},
  interhash = {09c81e1d6f7d43da5c8f584d52e15c35},
  intrahash = {a2a1b4fa63ec4e1bbbdc979be1ea8cd9},
  isbn = {978-3319648668},
  keywords = {Book MatrixAlgebra},
  publisher = {Springer},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Matrix Algebra: Theory, Computations and Applications in Statistics},
  year = 2017
}

@article{rohde2006improved,
  abstract = {The lexical semantic system is an important component of human language and cognitive processing. One approach to modeling semantic knowledge makes use of hand-constructed networks or trees of interconnected word senses (Miller, Beckwith, Fellbaum, Gross, \& Miller, 1990; Jarmasz \& Szpakowicz, 2003). An alternative approach seeks to model word meanings as high-dimensional vectors, which are derived from the cooccurrence of words in unlabeled text corpora (Landauer \& Dumais, 1997; Burgess \& Lund, 1997a). This paper introduces a new vector-space method for deriving word-meanings from large corpora that was inspired by the HAL and LSA models, but which achieves better and more consistent results in predicting human similarity judgments. We explain the new model, known as COALS, and how it relates to prior methods, and then evaluate the various models on a range of tasks, including a novel set of semantic similarity ratings involving both semantically and morphologically related terms.},
  added-at = {2018-01-15T02:29:31.000+0100},
  address = {New York, NY},
  author = {Rohde, Douglas L. and Gonnerman, Laura M. and Plaut, David C.},
  biburl = {https://www.bibsonomy.org/bibtex/2b54f3d2e75632b35d960ed60e12a5124/vngudivada},
  interhash = {78f5d61cd238b7aa586221e1430e702b},
  intrahash = {b54f3d2e75632b35d960ed60e12a5124},
  journal = {Communications of the ACM},
  keywords = {Lexical-Co-occurrence},
  pages = {627--633},
  publisher = {ACM},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {An improved model of semantic similarity based on lexical co-occurence},
  volume = 8,
  year = 2006
}

@book{curzan2011english,
  abstract = {A major introductory language/linguistics textbook written specifically for English and Education majors, this book is an engaging introduction to the structure of English, general theories in linguistics, and important issues in sociolinguistics. This accessible text provides more extensive coverage of issues of particular interest to English and Education majors. Tapping into our natural curiosity about language, it invites all students to connect academic linguistics to everyday use of the English language and to become active participants in the construction of linguistic knowledge.},
  added-at = {2018-01-14T15:04:12.000+0100},
  address = {New York, NY},
  author = {Curzan, Anne and Adams, Michael Patrick},
  biburl = {https://www.bibsonomy.org/bibtex/24a1c3ba95e4728c015d9ddc2c945f2f2/vngudivada},
  edition = {Third},
  interhash = {731124c4d58b4d49f2f4e6d46466fc6b},
  intrahash = {4a1c3ba95e4728c015d9ddc2c945f2f2},
  isbn = {978-0205032280},
  keywords = {Book English Linguistics},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {How English Works: A Linguistic Introduction},
  year = 2011
}

@book{stephensdavidowitz2017everybody,
  abstract = {How many Americans are actually racist?
Is America experiencing a hidden back-alley abortion crisis?
Can you game the stock market?
Does violent entertainment increase the rate of violent crime?
Do parents treat sons differently from daughters?
How many people actually read the books they buy?

In this groundbreaking work, Seth Stephens-Davidowitz, a Harvard-trained economist, former Google data scientist, and New York Times writer, argues that much of what we thought about people has been dead wrong. The reason?  People lie, to friends, lovers, doctors, surveys—and themselves.

However, we no longer need to rely on what people tell us. New data from the internet—the traces of information that billions of people leave on Google, social media, dating, and even pornography sites -- finally reveals the truth. By analyzing this digital goldmine, we can now learn what people really think, what they really want, and what they really do. Sometimes the new data will make you laugh out loud. Sometimes the new data will shock you.  Sometimes the new data will deeply disturb you.  But, always, this new data will make you think.

Everybody Lies combines the informed analysis of Nate Silver's The Signal and the Noise, the storytelling of Malcolm Gladwell's Outliers, and the wit and fun of Steven Levitt and Stephen Dubner's Freakonomics in a book that will change the way you view the world. There is almost no limit to what can be learned about human nature from Big Data -- provided, that is, you ask the right questions.},
  added-at = {2018-01-21T00:48:58.000+0100},
  address = {New York, NY},
  author = {Stephens-Davidowitz, Seth},
  biburl = {https://www.bibsonomy.org/bibtex/2b5c468d372c060a990bc05f2bd45b677/vngudivada},
  interhash = {f6cb092a1dd4b0acb42605b26bd46c83},
  intrahash = {b5c468d372c060a990bc05f2bd45b677},
  isbn = {978-0062390851},
  keywords = {BigData BigDataEthics Book},
  publisher = {Dey Street Books},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Everybody Lies: Big Data, New Data, and What the Internet Can Tell Us About Who We Really Are},
  year = 2017
}

@article{clauset2015systematic,
  abstract = {The faculty job market plays a fundamental role in shaping research priorities, educational outcomes, and career trajectories among scientists and institutions. However, a quantitative understanding of faculty hiring as a system is lacking. Using a simple technique to extract the institutional prestige ranking that best explains an observed faculty hiring network—who hires whose graduates as faculty—we present and analyze comprehensive placement data on nearly 19,000 regular faculty in three disparate disciplines. Across disciplines, we find that faculty hiring follows a common and steeply hierarchical structure that reflects profound social inequality. Furthermore, doctoral prestige alone better predicts ultimate placement than a U.S. News \& World Report rank, women generally place worse than men, and increased institutional prestige leads to increased faculty production, better faculty placement, and a more influential position within the discipline. These results advance our ability to quantify the influence of prestige in academia and shed new light on the academic system.},
  added-at = {2018-02-20T04:57:14.000+0100},
  author = {Clauset, A. and Arbesman, S. and Larremore, D. B.},
  biburl = {https://www.bibsonomy.org/bibtex/26d5b283c6d14aceec1265ca56c975be0/vngudivada},
  doi = {10.1126/sciadv.1400005},
  interhash = {84a0bd564b8ebd2b7c01be5a4ad1e233},
  intrahash = {6d5b283c6d14aceec1265ca56c975be0},
  journal = {Science Advances},
  keywords = {FacultyHiring FacultyNetworks GraphDatabase SNA},
  month = feb,
  number = 1,
  pages = {e1400005--e1400005},
  publisher = {American Association for the Advancement of Science ({AAAS})},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Systematic inequality and hierarchy in faculty hiring networks},
  url = {https://doi.org/10.1126%2Fsciadv.1400005},
  volume = 1,
  year = 2015
}

@article{mchugh2016collective,
  abstract = {This multi-level (individual and collective) study examines collective decision making as it relates to the performance metric of collective decision quality. A collectivistic leadership approach is used, as leaderless collectives engaged in decision making are inherently involved in collective leadership. A multi-level conceptual model for collective decision making is introduced, which incorporates leadership and collective intelligence. Using agent-based simulations and content-coded field study data, results from both methods suggest that there is a positive relationship between individual and collective intelligence, as well as a positive relationship between collective intelligence and collective decision quality. The implications of these and related findings for future collective level research bridging the fields of decision making, leadership, and collective intelligence are discussed.},
  added-at = {2018-02-25T18:01:06.000+0100},
  author = {McHugh, Kristie A. and Yammarino, Francis J. and Dionne, Shelley D. and Serban, Andra and Sayama, Hiroki and Chatterjee, Subimal},
  biburl = {https://www.bibsonomy.org/bibtex/2ea97bec2c0f4c6264a7bb929f4570795/vngudivada},
  doi = {10.1016/j.leaqua.2016.01.001},
  interhash = {43f778be082147689cad1da69de5f29c},
  intrahash = {ea97bec2c0f4c6264a7bb929f4570795},
  journal = {The Leadership Quarterly},
  keywords = {Leadership},
  month = apr,
  number = 2,
  pages = {218--241},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Collective decision making, leadership, and collective intelligence: Tests with agent-based simulations and a Field study},
  url = {https://doi.org/10.1016%2Fj.leaqua.2016.01.001},
  volume = 27,
  year = 2016
}

@article{margolis2016vertical,
  abstract = {This study explores the connection between formal leaders and collective leadership in teams through the examination of how collective strategic vision flows downward in organizations and the function that formal leaders play in the resulting cascade of collective leadership. Building from a sensemaking framework, we propose that a supervisor's perceptions of the collective navigator role (the establishing and enacting of strategic vision among members of a team) in their immediate supervisor-level work group ultimately links to the collective leadership navigator role in the lower-level team he or she leads thereby illustrating the vertical flow of collective leadership across organizational levels. To understand how this cascading process operates, we propose that two key characteristics of supervisors, their job satisfaction and empowering leadership behaviors, mediate the linkage between collective strategic visions at these different levels. We find support for this connection in our study of teams within a large manufacturing company.},
  added-at = {2018-02-25T18:10:54.000+0100},
  author = {Margolis, Jaclyn A. and Ziegert, Jonathan C.},
  biburl = {https://www.bibsonomy.org/bibtex/241345549cdb1bf9e0d27dc20a7813914/vngudivada},
  doi = {10.1016/j.leaqua.2016.01.005},
  interhash = {31a6cc89552ea3094b3abb6cdde60a59},
  intrahash = {41345549cdb1bf9e0d27dc20a7813914},
  journal = {The Leadership Quarterly},
  keywords = {Leadership},
  month = apr,
  number = 2,
  pages = {334--348},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Vertical flow of collectivistic leadership: An examination of the cascade of visionary leadership across levels},
  url = {https://doi.org/10.1016%2Fj.leaqua.2016.01.005},
  volume = 27,
  year = 2016
}

@book{curzan2016fixing,
  abstract = {Over the past 300 years, attempts have been made to prescribe how we should and should not use the English language. The efforts have been institutionalized in places such as usage guides, dictionaries, and school curricula. Such authorities have aspired to 'fix' the language, sometimes by keeping English exactly where it is, but also by trying to improve the current state of the language. Anne Curzan demonstrates the important role prescriptivism plays in the history of the English language, as a sociolinguistic factor in language change and as a vital meta-discourse about language. Starting with a pioneering new definition of prescriptivism as a linguistic phenomenon, she highlights the significant role played by Microsoft's grammar checker, debates about 'real words', non-sexist language reform, and efforts to reappropriate stigmatized terms. Essential reading for anyone interested in the regulation of language, the book is a fascinating re-examination of how we tell language history.},
  added-at = {2018-01-14T15:06:39.000+0100},
  address = {Cambridge, UK},
  author = {Curzan, Anne},
  biburl = {https://www.bibsonomy.org/bibtex/2fb2c989a076d46bb841282fecbb21bf3/vngudivada},
  interhash = {094389daa3254549df8f20fe78cbbb18},
  intrahash = {fb2c989a076d46bb841282fecbb21bf3},
  isbn = {978-1316604885},
  keywords = {Book Linguistics},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Fixing English: Prescriptivism and Language History},
  year = 2016
}

@book{chambers2018spark,
  abstract = {Learn how to use, deploy, and maintain Apache Spark with this comprehensive guide, written by the creators of this open-source cluster-computing framework. With an emphasis on improvements and new features in Spark 2.0, authors Bill Chambers and Matei Zaharia break down Spark topics into distinct sections, each with unique goals.

	You will explore the basic operations and common functions of Spark's structured APIs, as well as Structured Streaming, a new high-level API for building end-to-end streaming applications. Developers and system administrators will learn the fundamentals of monitoring, tuning, and debugging Spark, and explore machine learning techniques and scenarios for employing MLlib, Spark's scalable machine learning library.

	Get a gentle overview of big data and Spark

	Learn about DataFrames, SQL, and Datasets—Spark's core APIs—through worked examples

	Dive into Spark's low-level APIs, RDDs, and execution of SQL and DataFrames

	Understand how Spark runs on a cluster

	Debug, monitor, and tune Spark clusters and applications

	Learn the power of Spark's Structured Streaming and MLlib for machine learning tasks

	Explore the wider Spark ecosystem, including SparkR and Graph Analysis

	Examine Spark deployment, including coverage of Spark in the Cloud},
  added-at = {2018-01-30T00:32:16.000+0100},
  address = {Sebastopol, California},
  author = {Chambers, Bill and Zaharia, Matei},
  biburl = {https://www.bibsonomy.org/bibtex/2aeb2889b0636431fc072140b9882ad28/vngudivada},
  interhash = {af4598a1e2ff776434af8fd54f5a21c2},
  intrahash = {aeb2889b0636431fc072140b9882ad28},
  isbn = {978-1491912218},
  keywords = {Book Spark},
  publisher = {O'Reilly Media},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Spark: The Definitive Guide},
  year = 2018
}

@book{levitt2015think,
  abstract = {With their trademark blend of captivating storytelling and unconventional analysis, Steven D. Levitt and Stephen J. Dubner take us inside their thought process and teach us all how to think a bit more productively, more creatively, more rationally. In Think Like A Freak, they offer a blueprint for an entirely new way to solve problems, whether your interest lies in minor lifehacks or major global reforms. The topics range from business to philanthropy to sports to politics, all with the goal of retraining your brain. Along the way, you will learn the secrets of a Japanese hot-dog-eating champion, the reason an Australian doctor swallowed a batch of dangerous bacteria, and why Nigerian e-mail scammers make a point of saying they are from Nigeria.

Levitt and Dubner plainly see the world like no one else. Now you can too. Never before have such iconoclastic thinkers been so revealing—and so much fun to read.},
  added-at = {2018-01-21T01:27:51.000+0100},
  address = {New York, NY},
  author = {Levitt, Steven D. and Dubner, Stephen J},
  biburl = {https://www.bibsonomy.org/bibtex/246dc368d2627723a8e1d3d89e31e1a6f/vngudivada},
  interhash = {abb791c2c413231653129deb56b7cab9},
  intrahash = {46dc368d2627723a8e1d3d89e31e1a6f},
  isbn = {978-0062218346},
  keywords = {Book Success},
  publisher = {HarperCollins Publishers},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Think Like a Freak: The Authors of Freakonomics Offer to Retrain Your Brain},
  year = 2015
}

@incollection{gudivada2018database,
  abstract = {Special needs of Big Data applications have ushered in several new classes of systems for data storage and retrieval. Each class targets the needs of a category of Big Data application. These systems differ greatly in their data models and system architecture, approaches used for high availability and scalability, query languages and client interfaces provided. This chapter begins with a description of the emergence of Big Data and data management requirements of Big Data applications. Several new classes of database management systems have emerged recently to address the needs of Big Data applications. NoSQL is an umbrella term used to refer to these systems. Next, a taxonomy for NoSQL systems is developed and several NoSQL systems are classified under this taxonomy. Characteristics of representative systems in each class are also discussed. The chapter concludes by indicating the emerging trends of NoSQL systems and research issues.},
  added-at = {2018-02-21T16:26:49.000+0100},
  address = {Boston, MA},
  author = {Gudivada, V. and Apon, A. and Rao, D.},
  biburl = {https://www.bibsonomy.org/bibtex/227deef8ca7c0c29ae8157f1e976a0e64/vngudivada},
  booktitle = {Handbook of Research on Big Data Storage and Visualization Techniques},
  doi = {10.4018/978-1-5225-3142-5.ch003},
  editor = {Segall, Richard and Cook, Jeffrey},
  interhash = {5b64c1d4e4644a2ced505cc01c240618},
  intrahash = {27deef8ca7c0c29ae8157f1e976a0e64},
  isbn = {978-1522531425},
  keywords = {BigData NoSQL},
  mon = {jan},
  pages = {76 -- 100},
  publisher = {IDG Global},
  series = {Advances in Data Mining and Database Management},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Database Systems for Big Data Storage and Retrieval},
  year = 2018
}

@article{deville2014career,
  abstract = {Changing institutions is an integral part of an academic life. Yet little is known about the mobility patterns of scientists at an institutional level and how these career choices affect scientific outcomes. Here, we examine over 420,000 papers, to track the affiliation information of individual scientists, allowing us to reconstruct their career trajectories over decades. We find that career movements are not only temporally and spatially localized, but also characterized by a high degree of stratification in institutional ranking. When cross-group movement occurs, we find that while going from elite to lower-rank institutions on average associates with modest decrease in scientific performance, transitioning into elite institutions does not result in subsequent performance gain. These results offer empirical evidence on institutional level career choices and movements and have potential implications for science policy.},
  added-at = {2018-02-21T00:42:33.000+0100},
  author = {Deville, Pierre and Wang, Dashun and Sinatra, Roberta and Song, Chaoming and Blondel, Vincent D. and Barab\'{a}si, Albert-L\'{a}szl\'{o}},
  biburl = {https://www.bibsonomy.org/bibtex/2f1c2695465b503d744ff8bae6366fe61/vngudivada},
  interhash = {fe0dca3ff71eb59d711b04211d311b4c},
  intrahash = {f1c2695465b503d744ff8bae6366fe61},
  journal = {Scientific Reports},
  keywords = {AcademicCareer SNA},
  month = apr,
  pages = {1 -- 7},
  publisher = {Phys.org},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Career on the Move: Geography, Stratification, and Scientific Impact},
  url = {http://dx.doi.org/10.1038/srep04770},
  volume = 4,
  year = 2014
}

@book{cameron2012verbal,
  abstract = {In Verbal Hygiene, Deborah Cameron takes a serious look at popular attitudes towards language and examines the practices by which people attempt to regulate its use. Instead of dismissing the practice of ‘verbal hygiene’, as a misguided and pernicious exercise, she argues that popular discourse about language values – good and bad, right and wrong – serves an important function for those engaged in it. A series of case studies deal with specific examples of verbal hygiene: the regulation of ‘style’ by editors, the teaching of English grammar in schools, the movements for and against so-called ‘politically correct’ language and the advice given to women on how they can speak more effectively.

This Routledge Linguistics Classic includes a new foreword which looks at how the issues covered in the case studies have developed over time and a new afterword which discusses new concerns which have emerged in the last 15 years, from the regimentation of language in the workplace to panics about immigration and terrorism, which are expressed in linguistic terms.

Addressed to linguists, to professional language-users of all kinds, and to anyone interested in language and culture, Verbal Hygiene calls for legitimate concerns about language and value to be discussed, by experts and lay-speakers alike, in a rational and critical spirit.},
  added-at = {2018-01-14T15:10:24.000+0100},
  address = {Abingdon, UK},
  author = {Cameron, Deborah},
  biburl = {https://www.bibsonomy.org/bibtex/21b0776ca41aeca81f428ef16db5f000a/vngudivada},
  interhash = {dddf0875c5d6fba37b8183d950156b8f},
  intrahash = {1b0776ca41aeca81f428ef16db5f000a},
  isbn = {978-0415696005},
  keywords = {Book LanguageUse Linguistics},
  publisher = {Routledge},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Verbal Hygiene},
  year = 2012
}

@article{karpatne2017machine,
  abstract = {Geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet. As geosciences enters the era of big data, machine learning (ML) -- that has been widely successful in commercial domains -- offers immense potential to contribute to problems in geosciences. However, problems in geosciences have several unique challenges that are seldom found in traditional applications, requiring novel problem formulations and methodologies in machine learning. This article introduces researchers in the machine learning (ML) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences. We first highlight typical sources of geoscience data and describe their properties that make it challenging to use traditional machine learning techniques. We then describe some of the common categories of geoscience problems where machine learning can play a role, and discuss some of the existing efforts and promising directions for methodological development in machine learning. We conclude by discussing some of the emerging research themes in machine learning that are applicable across all problems in the geosciences, and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines.},
  added-at = {2018-01-21T21:54:39.000+0100},
  archiveprefix = {arXiv},
  author = {Karpatne, Anuj and Ebert{-}Uphoff, Imme and Ravela, Sai and Babaie, Hassan Ali and Kumar, Vipin},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {https://www.bibsonomy.org/bibtex/270de9501b9d375071480748ab3d537e1/vngudivada},
  eprint = {1711.04708},
  interhash = {de2c5564d79541815f03edc0cefa32b9},
  intrahash = {70de9501b9d375071480748ab3d537e1},
  journal = {CoRR},
  keywords = {GeoSciences ML},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Machine Learning for the Geosciences: Challenges and Opportunities},
  url = {http://arxiv.org/abs/1711.04708},
  volume = {abs/1711.04708},
  year = 2017
}

@article{shaoul2010exploring,
  abstract = {Hyperspace analog to language (HAL) is a high-dimensional model of semantic space that uses the global co-occurrence frequency of words in a large corpus of text as the basis for a representation of semantic memory. In the original HAL model, many parameters were set without any a priori rationale. We have created and publicly released a computer application, the High Dimensional Explorer (HiDEx), that makes it possible to systematically alter the values of these parameters to examine their effect on the co-occurrence matrix that instantiates the model. We took an empirical approach to understanding the influence of the parameters on the measures produced by the models, looking at how well matrices derived with different parameters could predict human reaction times in lexical decision and semantic decision tasks. New parameter sets give us measures of semantic density that improve the model's ability to predict behavioral measures. Implications for such models are discussed.},
  added-at = {2018-01-15T02:15:40.000+0100},
  author = {Shaoul, Cyrus and Westbury, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/2f815ffc20d21bc5e595f024e114b34a9/vngudivada},
  day = 01,
  doi = {10.3758/BRM.42.2.393},
  interhash = {53ab5a4f82dee46d916b9125c218bd8f},
  intrahash = {f815ffc20d21bc5e595f024e114b34a9},
  journal = {Behavior Research Methods},
  keywords = {Lexical-Co-occurrence},
  month = may,
  number = 2,
  pages = {393--413},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Exploring lexical co-occurrence space using HiDEx},
  volume = 42,
  year = 2010
}

@techreport{sclater2016learning,
  added-at = {2018-02-05T02:34:25.000+0100},
  author = {Sclater, Niall and Peasgood, Alice and Mullan, Joel},
  biburl = {https://www.bibsonomy.org/bibtex/2177b1c3cb45138a3db89917bac060a43/vngudivada},
  institution = {Jisc},
  interhash = {bfeec64930b3cbc2ac90ab95930ed64e},
  intrahash = {177b1c3cb45138a3db89917bac060a43},
  keywords = {LearningAnalytics},
  month = apr,
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Learning Analytics in Higher Education: A review of UK and international practice},
  url = {https://www.jisc.ac.uk/sites/default/files/learning-analytics-in-he-v2_0.pdf},
  year = 2016
}

@book{miner2017mapreduce,
  abstract = {This handy guide brings together a unique collection of valuable MapReduce patterns that will save you time and effort regardless of the domain, language, or development framework you are using. Updated to include new versions of Hadoop, this second edition features several new patterns, such as transformation, join with a secondary sort, external join, and MapReduce over HBase.

    Each pattern is explained in context, with pitfalls and caveats clearly identified to help you avoid common design mistakes when modeling your big data architecture. This book also provides a complete overview of MapReduce that explains its origins and implementations, and why design patterns are so important. All code examples are written for Hadoop.},
  added-at = {2018-02-05T23:42:26.000+0100},
  address = {Sabestopol, California},
  author = {Miner, Donald and Shook, Adam},
  biburl = {https://www.bibsonomy.org/bibtex/28852e6de9ac9d428c58d0d3fbba27557/vngudivada},
  edition = {Second},
  interhash = {0cf6441f3260490bb6617a3637e164ed},
  intrahash = {8852e6de9ac9d428c58d0d3fbba27557},
  isbn = {978-1491927922},
  keywords = {Book MapReduce},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {MapReduce Design Patterns: Building Effective Algorithms and Analytics for Hadoop and Other Systems},
  year = 2017
}

@book{levitt2011superfreakonomics,
  abstract = {Freakonomics lived on the New York Times bestseller list for an astonishing two years. Now authors Steven D. Levitt and Stephen J. Dubner return with more iconoclastic insights and observations in SuperFreakonomics -- the long awaited follow-up to their New York Times Notable blockbuster. Based on revolutionary research and original studies SuperFreakonomics promises to once again challenge our view of the way the world really works.},
  added-at = {2018-01-21T01:17:08.000+0100},
  address = {New York, NY},
  author = {Levitt, Steven D. and Dubner, Stephen J},
  biburl = {https://www.bibsonomy.org/bibtex/2dc26d3bc95168b73778a61cd0debfbdf/vngudivada},
  interhash = {9612b533b278e21dd3388d1086a1a136},
  intrahash = {dc26d3bc95168b73778a61cd0debfbdf},
  keywords = {Book Success},
  publisher = {HarperCollins Publishers},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {SuperFreakonomics: Global Cooling, Patriotic Prostitutes, and Why Suicide Bombers Should Buy Life Insurance },
  year = 2011
}

@article{will2016flock,
  abstract = {This study introduces Flock Leadership, a framework for understanding and influencing emergent collective behavior in the context of human organizing. Collective capacities emerge when interactions between individuals enact divergent and convergent ways of perceiving and responding to reality. An agent-based flocking model is employed to represent these interactive dynamics and emergent processes. This study explicates the model's constructs, translating its algorithms into behavioral norms at the individual level and its outcomes into collective behaviors at the group level. Phenomena-based simulation modeling links two collective states—technical capacity and adaptive capacity—to the specific underlying norm configurations from which they emerge. Flock Leadership provides a unique theoretical framing of emergent collective behavior in organizational settings, a new methodology for analyzing relationships between those emergent behavioral patterns and the interaction norms underlying them, and a useful means for identifying leadership opportunities.},
  added-at = {2018-02-25T18:04:11.000+0100},
  author = {Will, Thomas E.},
  biburl = {https://www.bibsonomy.org/bibtex/2d61d182680198449426122f630e08296/vngudivada},
  doi = {10.1016/j.leaqua.2016.01.002},
  interhash = {97e7c882d9203096c8168813e6ee21a1},
  intrahash = {d61d182680198449426122f630e08296},
  journal = {The Leadership Quarterly},
  keywords = {Leadership},
  month = apr,
  number = 2,
  pages = {261--279},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Flock Leadership: Understanding and influencing emergent collective behavior},
  url = {https://doi.org/10.1016%2Fj.leaqua.2016.01.002},
  volume = 27,
  year = 2016
}

@article{jamelske2009measuring,
  abstract = {In 1997 a medium-size Midwestern public university in the U.S. initiated a first year experience program. The program is designed to infuse added curricular and extracurricular components into core courses in an effort to integrate students into the university community. This article examined the FYE impact on grade point average (GPA) and retention after 1 year for the fall 2006 cohort of entering students. The findings suggest no positive FYE effect on retention, but on average FYE students earned higher GPAs than non-FYE students. Reducing the sample to include only courses identified as goal compatible FYE courses yielded a positive effect on retention and also accentuated the GPA differential. The estimated positive FYE impact on retention was larger for below average students (especially females) and smaller for above average students.},
  added-at = {2018-03-11T20:21:24.000+0100},
  author = {Jamelske, Eric},
  biburl = {https://www.bibsonomy.org/bibtex/26744e3a25859d86faf7e5a042bcd4a9e/vngudivada},
  day = 01,
  doi = {10.1007/s10734-008-9161-1},
  interhash = {455312dd113773902d084e158d26c72e},
  intrahash = {6744e3a25859d86faf7e5a042bcd4a9e},
  issn = {1573-174X},
  journal = {Higher Education},
  keywords = {StudentEngagement StudentSuccess},
  month = mar,
  number = 3,
  pages = {373--391},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Measuring the impact of a university first-year experience program on student GPA and retention},
  volume = 57,
  year = 2009
}

@article{friedrich2016collective,
  abstract = {The focus on non-hierarchical, collectivistic, leadership has been steadily increasing with several different theories emerging (Yammarino, Salas, Serban, Shirreffs, & Shuffler, 2012). While most take the view that collectivistic approaches to leadership (e.g., shared and distributed leadership) are emergent properties of the team, a recent, integrative framework by Friedrich, Vessey, Schuelke, Ruark and Mumford (2009) proposed that collective leadership, defined as the selective utilization of expertise within the network, does not eliminate the role of the focal leader. In the present study, three dimensions of collective leadership behaviors from the Friedrich et al. (2009) framework — Communication, Network Development, and Leader–Team Exchange were tested with regard to how individual differences of leaders (intelligence, experience, and personality), the team's network (size, interconnectedness, and embeddedness), the given problem domain (strategic change or innovation), and problem focus (task or relationship focused) influenced the use of each collective leadership dimension.},
  added-at = {2018-02-25T18:09:20.000+0100},
  author = {Friedrich, Tamara L. and Griffith, Jennifer A. and Mumford, Michael D.},
  biburl = {https://www.bibsonomy.org/bibtex/2006dec5b7096edb7f81a560189357191/vngudivada},
  doi = {10.1016/j.leaqua.2016.02.004},
  interhash = {04e6e77c42228f83e913abeadba6b4ec},
  intrahash = {006dec5b7096edb7f81a560189357191},
  journal = {The Leadership Quarterly},
  keywords = {Leadership},
  month = apr,
  number = 2,
  pages = {312--333},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Collective leadership behaviors: Evaluating the leader, team network, and problem situation characteristics that influence their use},
  url = {https://doi.org/10.1016%2Fj.leaqua.2016.02.004},
  volume = 27,
  year = 2016
}

@book{sterling2017performance,
  abstract = {High Performance Computing: Modern Systems and Practices is a fully comprehensive and easily accessible treatment of high performance computing, covering fundamental concepts and essential knowledge while also providing key skills training. With this book, domain scientists will learn how to use supercomputers as a key tool in their quest for new knowledge. In addition, practicing engineers will discover how supercomputers can employ HPC systems and methods to the design and simulation of innovative products, and students will begin their careers with an understanding of possible directions for future research and development in HPC.

	Those who maintain and administer commodity clusters will find this textbook provides essential coverage of not only what HPC systems do, but how they are used.

	Covers enabling technologies, system architectures and operating systems, parallel programming languages and algorithms, scientific visualization, correctness and performance debugging tools and methods, GPU accelerators and big data problems

	Provides numerous examples that explore the basics of supercomputing, while also providing practical training in the real use of high-end computers

	Helps users with informative and practical examples that build knowledge and skills through incremental steps

	Features sidebars of background and context to present a live history and culture of this unique field

	Includes online resources, such as recorded lectures from the authors' HPC courses},
  added-at = {2018-01-29T20:49:56.000+0100},
  address = {Burlington, Massachusetts},
  author = {Sterling, Thomas and Anderson, Matthew and Brodowicz, Maciej},
  biburl = {https://www.bibsonomy.org/bibtex/2c61229816a0460e10798f39cbbd14d3f/vngudivada},
  interhash = {6e8f304f45029741ab42f32c059faf06},
  intrahash = {c61229816a0460e10798f39cbbd14d3f},
  isbn = {978-0124201583},
  keywords = {Book HPC},
  publisher = {Morgan Kaufmann},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {High Performance Computing: Modern Systems and Practices},
  year = 2017
}

@article{knight2014epistemology,
  abstract = {Learning Analytics is an emerging research field and design discipline that occupies the ``middle space" between the learning sciences/educational research and the use of computational techniques to capture and analyze data (Suthers \& Verbert, 2013). We propose that the literature examining the triadic relationships between epistemology (the nature of knowledge), pedagogy (the nature of learning and teaching), and assessment provide critical considerations for bounding this middle space. We provide examples to illustrate the ways in which the understandings of particular analytics are informed by this triad. As a detailed worked example of how one might design analytics to scaffold a specific form of higher order learning, we focus on the construct of epistemic beliefs: beliefs about the nature of knowledge. We argue that analytics grounded in a pragmatic, socio-cultural perspective are well placed to explore this construct using discourse-centric technologies. The examples provided throughout this paper, through emphasizing the consideration of intentional design issues in the middle space, underscore the ''interpretative flexibility'' (Hamilton \& Feenberg, 2005) of new technologies, including analytics.},
  added-at = {2018-01-29T21:31:53.000+0100},
  author = {Knight, Simon and Shum, Simon Buckingham and Littleton, Karen},
  biburl = {https://www.bibsonomy.org/bibtex/20a53c151ebcc6a502de7d2987d07a8e1/vngudivada},
  doi = {10.18608/jla.2014.12.3},
  interhash = {0835d64822926905d3a0c853b2b229a9},
  intrahash = {0a53c151ebcc6a502de7d2987d07a8e1},
  journal = {Journal of Learning Analytics},
  keywords = {Assessment Epistemology LearningAnalytics Pedagogy},
  number = 2,
  pages = {23 -- 47},
  publisher = {Society for Learning Analytics Research},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Epistemology, Assessment, Pedagogy: Where Learning Meets Analytics in the Middle Space},
  url = {https://doi.org/10.18608%2Fjla.2014.12.3},
  volume = 1,
  year = 2014
}

@book{atkinson2016intelligent,
  added-at = {2018-01-30T01:39:34.000+0100},
  address = {Hauppauge, New York},
  biburl = {https://www.bibsonomy.org/bibtex/26a5199ff41fa14a6f68c2cc8b2e35d9a/vngudivada},
  editor = {Atkinson, Robert Kenneth},
  interhash = {e6424164ef7d125864040609c906cb8c},
  intrahash = {6a5199ff41fa14a6f68c2cc8b2e35d9a},
  isbn = {978-1634851671},
  keywords = {Book ITS},
  publisher = {Nova Science Publishers},
  series = {Education in a Competitive and Globalizing World},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Intelligent Tutoring Systems: Structure, Applications and Challenges},
  year = 2016
}

@incollection{thille2017incorporating,
  abstract = {Gain an overview of learning analytics technologies in higher education, including broad considerations and the barriers to introducing them. This volume features the work of practitioners who led some of the most notable implementations, like:

    the Open Learning Initiative now at Stanford University,

    faculty-led projects at the University of Michigan, including ECoach and SLAM,

    the University of Maryland, Baltimore Countys Check My Activity, and

    Indiana Universitys FLAGS early warning system and e-course advising initiatives.

    Readers will glean from these experiences, as well as from a national project in Australia on innovative approaches for enhancing student experience, an informed description of the role of feedback within these technologies, and a thorough discussion of ethical and social justice issues related to the use of learning analytics, and why higher education institutions should approach such initiatives cautiously, intentionally, and collaboratively.

    This is the 179th volume of the Jossey-Bass quarterly report series New Directions for Higher Education. Addressed to presidents, vice presidents, deans, and other higher education decision makers on all kinds of campuses, it provides timely information and authoritative advice about major issues and administrative problems confronting every institution.

	1. An Overview of Learning Analytics 9
    John Zilvinskis, James Willis, III, Victor M. H. Borden

    2. Incorporating Learning Analytics in the Classroom 19
    Candace Thille, Dawn Zimmaro

    3. Learning Analytics Across a Statewide System 33
    Catherine Buyarski, Jim Murray, Rebecca Torstrick

    4. Learner Analytics and Student Success Interventions 43
    Matthew D. Pistilli

    5. Cultivating Institutional Capacities for Learning Analytics 53
    Steven Lonn, Timothy A. McKay, Stephanie D. Teasley

    6. Using Analytics to Nudge Student Responsibility for Learning 65
    John Fritz

    7. Ethics and Justice in Learning Analytics 77
    Jeffrey Alan Johnson

    8. Learning Analytics as a Counterpart to Surveys of Student Experience 89
    Victor M. H. Borden, Hamish Coates},
  added-at = {2018-01-30T14:12:08.000+0100},
  address = {Hoboken, New Jersey},
  author = {Thille, C. and Zimmaro, D.},
  biburl = {https://www.bibsonomy.org/bibtex/2c354e27b65c3c5669ce082474c36fb6c/vngudivada},
  booktitle = {Learning Analytics in Higher Education: New Directions for Higher Education},
  editor = {Zilvinskis, J. and Borden, V.},
  interhash = {fc1ba781f044d443ef962d46e8e1e533},
  intrahash = {c354e27b65c3c5669ce082474c36fb6c},
  isbn = {978-1-119-44382-7},
  keywords = {Book LearningAnalytics},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Incorporating learning analytics in the classroom},
  year = 2017
}

@book{mirkin2012clustering,
  abstract = {Often considered more of an art than a science, books on clustering have been dominated by learning through example with techniques chosen almost through trial and error. Even the two most popular, and most related, clustering methods  -- K-Means for partitioning and Ward's method for hierarchical clustering -- have lacked the theoretical underpinning required to establish a firm relationship between the two methods and relevant interpretation aids. Other approaches, such as spectral clustering or consensus clustering, are considered absolutely unrelated to each other or to the two above mentioned methods.

	Clustering: A Data Recovery Approach, Second Edition, presents a unified modeling approach for the most popular clustering methods: the K-Means and hierarchical techniques, especially for divisive clustering. It significantly expands coverage of the mathematics of data recovery, and includes a new chapter covering more recent popular network clustering approaches―spectral, modularity and uniform, additive, and consensus―treated within the same data recovery approach. Another added chapter covers cluster validation and interpretation, including recent developments for ontology-driven interpretation of clusters. Altogether, the insertions added a hundred pages to the book, even in spite of the fact that fragments unrelated to the main topics were removed.

	Illustrated using a set of small real-world datasets and more than a hundred examples, the book is oriented towards students, practitioners, and theoreticians of cluster analysis. Covering topics that are beyond the scope of most texts, the author's explanations of data recovery methods, theory-based advice, pre- and post-processing issues and his clear, practical instructions for real-world data mining make this book ideally suited for teaching, self-study, and professional reference.},
  added-at = {2018-01-31T04:27:37.000+0100},
  address = {Boca Raton, Florida},
  author = {Mirkin, Boris},
  biburl = {https://www.bibsonomy.org/bibtex/2a9e07f000362e680532725743e64b5dd/vngudivada},
  edition = {Second},
  interhash = {5bb8f6b406e40f700dce0e5f164db27c},
  intrahash = {a9e07f000362e680532725743e64b5dd},
  isbn = {978-1439838419},
  keywords = {Book Clustering MachineLearning},
  publisher = {Chapman and Hall/CRC},
  series = {Chapman \& Hall/CRC Computer Science \& Data Analysis},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Clustering: A Data Recovery Approach},
  year = 2012
}

@inbook{wang2008automatic,
  abstract = {An approach of automatic question generation from given learning material of medical text is presented in this paper. The main idea is to generate the questions automatically based on question templates which are created by training on many medical articles. In order to provide interesting questions, our research focuses on medical related concepts. This method can be used for evaluation of learner's comprehension after he/she finished a reading material. Different from traditional learning system the articles and questions are all prepared beforehand; participants can learn whatever new input medical articles with the help of automatic question generation.},
  added-at = {2018-01-16T01:39:46.000+0100},
  address = {Berlin, Heidelberg},
  author = {Wang, Weiming and Hao, Tianyong and Liu, Wenyin},
  biburl = {https://www.bibsonomy.org/bibtex/2971c60336b9bc31b549b6c13ca3dc1d9/vngudivada},
  booktitle = {Advances in Web Based Learning -- ICWL 2007: 6th International Conference Edinburgh, UK, August 15-17, 2007 Revised Papers},
  doi = {10.1007/978-3-540-78139-4_22},
  editor = {Leung, Howard and Li, Frederick and Lau, Rynson and Li, Qing},
  interhash = {05fa91694b7f6e901381b2e455dc15eb},
  intrahash = {971c60336b9bc31b549b6c13ca3dc1d9},
  isbn = {978-3-540-78139-4},
  keywords = {QuestionGeneration},
  pages = {242--251},
  publisher = {Springer Berlin Heidelberg},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Automatic Question Generation for Learning Evaluation in Medicine},
  url = {https://doi.org/10.1007/978-3-540-78139-4_22},
  year = 2008
}

@book{chapelle2006semisupervised,
  abstract = {A comprehensive review of an area of machine learning that deals with the use of unlabeled data in classification problems: state-of-the-art algorithms, a taxonomy of the field, applications, benchmark experiments, and directions for future research.

In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research.Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction.},
  added-at = {2018-01-31T02:29:30.000+0100},
  address = {Cambridge, Massachusetts},
  biburl = {https://www.bibsonomy.org/bibtex/2639930cadf7970871348467bcfd51e18/vngudivada},
  editor = {Chapelle, Olivier and Sch\"{o}lkopf, Bernhard and Zien, Alexander},
  interhash = {e4b0e93ee48a0f568f0af181c4a56566},
  intrahash = {639930cadf7970871348467bcfd51e18},
  isbn = {978-0262033589},
  keywords = {Book UnsupervisedLearning},
  publisher = {The MIT Press},
  series = {Adaptive Computation and Machine Learning series},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Semi-Supervised Learning},
  year = 2006
}

@article{chrobotmason2016predicting,
  abstract = {In many organizations, leadership increasingly looks less like a hierarchy of authority. Instead, it is better understood as a network of influence relationships in which multiple people participate, blurring the distinction between leader and follower and raising the question, how do we predict the existence of these leadership relationships? In this study, we examine identification with one's organization and work team to predict the presence or absence of a leadership relationship. Using Exponential Random Graph Models (ERGMs) we find that employees who strongly identify with their company and team are more likely to view others as a source of leadership. We also find that employees who strongly identify with the organization are more likely to be viewed by others as a source of leadership. Implications for enhancing the understanding of plural forms of leadership and leadership development are discussed.},
  added-at = {2018-02-25T18:07:45.000+0100},
  author = {Chrobot-Mason, Donna and Gerbasi, Alexandra and Cullen-Lester, Kristin L.},
  biburl = {https://www.bibsonomy.org/bibtex/2812ed18d0fd361435d39fa7c3ce4eee4/vngudivada},
  doi = {10.1016/j.leaqua.2016.02.003},
  interhash = {6c513f74fffa7b9eb9660cf18fe78244},
  intrahash = {812ed18d0fd361435d39fa7c3ce4eee4},
  journal = {The Leadership Quarterly},
  keywords = {Leadership},
  month = apr,
  number = 2,
  pages = {298--311},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Predicting leadership relationships: The importance of collective identity},
  url = {https://doi.org/10.1016%2Fj.leaqua.2016.02.003},
  volume = 27,
  year = 2016
}

@book{elatia2016mining,
  abstract = {Addresses the impacts of data mining on education and reviews applications in educational research teaching, and learning

This book discusses the insights, challenges, issues, expectations, and practical implementation of data mining (DM) within educational mandates. Initial series of chapters offer a general overview of DM, Learning Analytics (LA), and data collection models in the context of educational research, while also defining and discussing data mining’s four guiding principles— prediction, clustering, rule association, and outlier detection. The next series of chapters showcase the pedagogical applications of Educational Data Mining (EDM) and feature case studies drawn from Business, Humanities, Health Sciences, Linguistics, and Physical Sciences education that serve to highlight the successes and some of the limitations of data mining research applications in educational settings. The remaining chapters focus exclusively on EDM’s emerging role in helping to advance educational research—from identifying at-risk students and closing socioeconomic gaps in achievement to aiding in teacher evaluation and facilitating peer conferencing. This book features contributions from international experts in a variety of fields.

 Includes case studies where data mining techniques have been effectively applied to advance teaching and learning
Addresses applications of data mining in educational research, including: social networking and education; policy and legislation in the classroom; and identification of at-risk students
Explores Massive Open Online Courses (MOOCs) to study the effectiveness of online networks in promoting learning and understanding the communication patterns among users and students
Features supplementary resources including a primer on foundational aspects of educational mining and learning analytics
Data Mining and Learning Analytics: Applications in Educational Research is written for both scientists in EDM and educators interested in using and integrating DM and LA to improve education and advance educational research.},
  added-at = {2018-01-16T00:23:58.000+0100},
  address = {Hoboken, New Jersey},
  biburl = {https://www.bibsonomy.org/bibtex/2238e280e982fdd6da7742bd2f24b1fb3/vngudivada},
  editor = {ElAtia, Samira and Ipperciel, Donald and Za\"{i}ane, Osmar R.},
  interhash = {4051ebb5d58246782c6f6ac90399b1f7},
  intrahash = {238e280e982fdd6da7742bd2f24b1fb3},
  isbn = {978-1118998236},
  keywords = {Book EDM LearningAnalytics},
  publisher = {John Wiley \& Sons},
  series = {Wiley Series on Methods and Applications in Data Mining},
  timestamp = {2019-03-25T17:07:16.000+0100},
  title = {Data Mining and Learning Analytics: Applications in Educational Research },
  year = 2016
}

@article{zhao2015analysis,
  abstract = {Citation analysis—the exploration of reference patterns in the scholarly and scientific literature—has long been applied in a number of social sciences to study research impact, knowledge flows, and knowledge networks. It has important information science applications as well, particularly in knowledge representation and in information retrieval.

Recent years have seen a burgeoning interest in citation analysis to help address research, management, or information service issues such as university rankings, research evaluation, or knowledge domain visualization. This renewed and growing interest stems from significant improvements in the availability and accessibility of digital bibliographic data (both citation and full text) and of relevant computer technologies. The former provides large amounts of data and the latter the necessary tools for researchers to conduct new types of large-scale citation analysis, even without special access to special data collections. Exciting new developments are emerging this way in many aspects of citation analysis.

This book critically examines both theory and practical techniques of citation network analysis and visualization, one of the two main types of citation analysis (the other being evaluative citation analysis). To set the context for its main theme, the book begins with a discussion of the foundations of citation analysis in general, including an overview of what can and what cannot be done with citation analysis (Chapter 1). An in-depth examination of the generally accepted steps and procedures for citation network analysis follows, including the concepts and techniques that are associated with each step (Chapter 2). Individual issues that are particularly important in citation network analysis are then scrutinized, namely: field delineation and data sources for citation analysis (Chapter 3); disambiguation of names and references (Chapter 4); and visualization of citation networks (Chapter 5). Sufficient technical detail is provided in each chapter so the book can serve as a practical how-to guide to conducting citation network analysis and visualization studies.

While the discussion of most of the topics in this book applies to all types of citation analysis, the structure of the text and the details of procedures, examples, and tools covered here are geared to citation network analysis rather than evaluative citation analysis. This conscious choice was based on the authors’ observation that, compared to evaluative citation analysis, citation network analysis has not been covered nearly as well by dedicated books, despite the fact that it has not been subject to nearly as much severe criticism and has been substantially enriched in recent years with new theory and techniques from research areas such as network science, social network analysis, or information visualization.

Table of Contents: Acknowledgment / Dedications /Foundations of Citation Analysis / Conducting Citation Network Analysis: Steps, Concepts, Techniques, and Tools / Field Delineation and Data Sources for Citation Analysis / Disambiguation in Citation Network Analysis / Visualization of Citation Networks / Appendix 3.3 / Appendix 5.4.2 / Bibliography / Author Biographies},
  added-at = {2018-03-19T13:09:05.000+0100},
  author = {Zhao, Dangzhi and Strotmann, Andreas},
  biburl = {https://www.bibsonomy.org/bibtex/274d110fbe49c8644780dc05e45aa7a28/vngudivada},
  doi = {10.2200/S00624ED1V01Y201501ICR039},
  eprint = {https://doi.org/10.2200/S00624ED1V01Y201501ICR039},
  interhash = {b6145d3d72575a3cf23a50f19ff11787},
  intrahash = {74d110fbe49c8644780dc05e45aa7a28},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {SNA SynthesisLecture},
  number = 1,
  pages = {1 -- 207},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Analysis and Visualization of Citation Networks},
  volume = 7,
  year = 2015
}

@article{hlava2014taxobook,
  abstract = {This is the first volume in a series about creating and maintaining taxonomies and their practical applications, especially in search functions.

In Book 1 (The Taxobook: History, Theories, and Concepts of Knowledge Organization), the author introduces the very foundations of classification, starting with the ancient Greek philosophers Plato and Aristotle, as well as Theophrastus and the Roman Pliny the Elder. They were first in a line of distinguished thinkers and philosophers to ponder the organization of the world around them and attempt to apply a structure or framework to that world.

The author continues by discussing the works and theories of several other philosophers from Medieval and Renaissance times, including Saints Aquinas and Augustine, William of Occam, Andrea Cesalpino, Carl Linnaeus, and René Descartes.

In the 17th, 18th, and 19th centuries, John Locke, Immanuel Kant, James Frederick Ferrier, Charles Ammi Cutter, and Melvil Dewey contributed greatly to the theories of classification systems and knowledge organization. Cutter and Dewey, especially, created systems that are still in use today.

Chapter 8 covers the contributions of Shiyali Ramamrita Ranganathan, who is considered by many to be the “father of modern library science.” He created the concept of faceted vocabularies, which are widely used—even if they are not well understood—on many e-commerce websites.

Following the discussions and historical review, the author has included a glossary that covers all three books of this series so that it can be referenced as you work your way through the second and third volumes. The author believes that it is important to understand the history of knowledge organization and the differing viewpoints of various philosophers—even if that understanding is only that the differing viewpoints simply exist. Knowing the differing viewpoints will help answer the fundamental questions: Why do we want to build taxonomies? How do we build them to serve multiple points of view?

Table of Contents: List of Figures / Preface / Acknowledgments / Origins of Knowledge Organization Theory: Early Philosophy of Knowledge / Saints and Traits: Realism and Nominalism / Arranging the glowers… and the Birds, and the Insects, and Everything Else: Early Naturalists and Taxonomies / The Age of Enlightenment Impacts Knowledge Theory / 18th-Century Developments: Knowledge Theory Coming to the Foreground / High Resolution: Classification Sharpens in the 19th and 20th Centuries / Outlining the World and Its Parts / Facets: An Indian Mathematician and Children’s Toys at Selfridge’s / Points of Knowledge / Glossary / End Notes / Author Biography},
  added-at = {2018-03-19T13:16:57.000+0100},
  author = {Hlava, Marjorie M.K.},
  biburl = {https://www.bibsonomy.org/bibtex/2d73030932266969391dcbbcd403af6e9/vngudivada},
  doi = {10.2200/S00602ED1V01Y201410ICR035},
  eprint = {https://doi.org/10.2200/S00602ED1V01Y201410ICR035},
  interhash = {ed4ecbb7ccbf9ba948a25b5d4732f2ed},
  intrahash = {d73030932266969391dcbbcd403af6e9},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR SynthesisLecture Taxonomy},
  number = 3,
  pages = {1 -- 80},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {The Taxobook: History, Theories, and Concepts of Knowledge Organization, Part 1 of a 3-Part Series},
  volume = 6,
  year = 2014
}

@book{zhai2016management,
  abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media such as blog articles, forum posts, product reviews, and tweets. This has led to an increasing demand for powerful software tools to help people analyze and manage vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and are accompanied by semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to analysis and management of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.

This book provides a systematic introduction to all these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. The focus is on text mining applications that can help users analyze patterns in text data to extract and reveal useful knowledge. Information retrieval systems, including search engines and recommender systems, are also covered as supporting technology for text mining applications. The book covers the major concepts, techniques, and ideas in text data mining and information retrieval from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of text mining and information retrieval to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. The book can be used as a textbook for a computer science undergraduate course or a reference book for practitioners working on relevant problems in analyzing and managing text data.},
  added-at = {2018-03-15T01:50:23.000+0100},
  address = {New York, NY},
  author = {Zhai, ChengXiang and Massung, Sean},
  biburl = {https://www.bibsonomy.org/bibtex/2be656f689c5785978d786c62b65a5086/vngudivada},
  interhash = {ceea361995ce890d83c61116c931d195},
  intrahash = {be656f689c5785978d786c62b65a5086},
  isbn = {978-1970001167},
  keywords = {Book IR NLP},
  publisher = {ACM Books},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining},
  year = 2016
}

@book{mayer2010applying,
  abstract = {A concrete guide to the science of learning, instruction, and assessment written in a friendly tone and presented in a dynamic format.



The underlying premise of Applying the Science of Learning is that educators can better help students learn if they understand the processes through which student learning takes place. In this clear and concise first edition text, educational psychology scholar Richard Mayer teaches readers how to apply the science of learning through understanding the reciprocal relationships between learning, instruction, and assessment.



Utilizing the significant advances in scientific learning research over the last 25 years, this introductory text identifies the features of science of learning that are most relevant to education, explores the possible prescriptions of these findings for instructional methods, and highlights the essentials of evaluating instructional effectiveness through assessment. Applying the Science of Learning is also presented in an easy-to-read modular design and with a conversational tone — making it particularly student-friendly, whether it is being used as a supplement to a core textbook or as a standalone course text.},
  added-at = {2018-03-19T21:10:30.000+0100},
  address = {New York, NY},
  author = {Mayer, Richard E.},
  biburl = {https://www.bibsonomy.org/bibtex/24dbffd397d056e0ede85c41d282c9afb/vngudivada},
  interhash = {54b37cef3e71546b9d8abb343297ceb1},
  intrahash = {4dbffd397d056e0ede85c41d282c9afb},
  isbn = {978-0136117575},
  keywords = {Book Learning},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Applying the Science of Learning},
  year = 2010
}

@article{moore2016trustworthy,
  abstract = {A trustworthy repository provides assurance in the form of management documents, event logs, and audit trails that digital objects are being managed correctly. The assurance includes plans for the sustainability of the repository, the accession of digital records, the management of technology evolution, and the mitigation of the risk of data loss. A detailed assessment is provided by the ISO-16363:2012 standard, "Space data and information transfer systems—Audit and certification of trustworthy digital repositories." This book examines whether the ISO specification for trustworthiness can be enforced by computer actionable policies. An implementation of the policies is provided and the policies are sorted into categories for procedures to manage externally generated documents, specify repository parameters, specify preservation metadata attributes, specify audit mechanisms for all preservation actions, specify control of preservation operations, and control preservation properties as technology evolves. An application of the resulting procedures is made to enforce trustworthiness within National Science Foundation data management plans.

Table of Contents: Preface / Acknowledgments / Introduction / Trustworthy Repository Description / ISO 16363 Organization Infrastructure / ISO 16363 Digital Object Management / ISO 16363 Infrastructure and Security Risk Management / Trustworthy Repository Implementation / Summary / References / Author Biographies},
  added-at = {2018-03-19T12:54:30.000+0100},
  author = {Moore, Reagan W. and Xu, Hao and Conway, Mike and Rajasekar, Arcot and Crabtree, Jon and Tibbo, Helen},
  biburl = {https://www.bibsonomy.org/bibtex/2ab840eaddf2e333fa721aa4ae47ca89e/vngudivada},
  doi = {10.2200/S00732ED1V01Y201609ICR051},
  eprint = {https://doi.org/10.2200/S00732ED1V01Y201609ICR051},
  interhash = {e7a04006f379c94d93a1d52b40329f94},
  intrahash = {ab840eaddf2e333fa721aa4ae47ca89e},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR InformationSecurity},
  number = 3,
  pages = {1 -- 133},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Trustworthy Policies for Distributed Repositories},
  volume = 8,
  year = 2016
}

@article{saracevic2016notion,
  abstract = {Everybody knows what relevance is. It is a "ya'know" notion, concept, idea–no need to explain whatsoever. Searching for relevant information using information technology (IT) became a ubiquitous activity in contemporary information society. Relevant information means information that pertains to the matter or problem at hand—it is directly connected with effective communication. The purpose of this book is to trace the evolution and with it the history of thinking and research on relevance in information science and related fields from the human point of view. The objective is to synthesize what we have learned about relevance in several decades of investigation about the notion in information science. This book deals with how people deal with relevance—it does not cover how systems deal with relevance; it does not deal with algorithms. Spurred by advances in information retrieval (IR) and information systems of various kinds in handling of relevance, a number of basic questions are raised: But what is relevance to start with? What are some of its properties and manifestations? How do people treat relevance? What affects relevance assessments? What are the effects of inconsistent human relevance judgments on tests of relative performance of different IR algorithms or approaches? These general questions are discussed in detail.

Table of Contents: Acknowledgments / Preface / Introduction / A Bit of History / Understanding, Manifestations, and Attributes / Models of Relevance / Theories of Relevance / Experimental Studies on Behavior of Relevance / Experimental Studies on Effects of Relevance / Effects of Inconsistent Relevance Judgments on Information Retrieval Test Results / Conclusions / References / Author Biography},
  added-at = {2018-03-19T12:56:33.000+0100},
  author = {Saracevic, Tefko},
  biburl = {https://www.bibsonomy.org/bibtex/22d5f4301e0e35d96f6d685c4b6bee555/vngudivada},
  doi = {10.2200/S00723ED1V01Y201607ICR050},
  eprint = {https://doi.org/10.2200/S00723ED1V01Y201607ICR050},
  interhash = {4dc54befa53174ffab7bfced3a8f8bd9},
  intrahash = {2d5f4301e0e35d96f6d685c4b6bee555},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR Relevance SynthesisLecture},
  number = 3,
  pages = {1 -- 109},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {The Notion of Relevance in Information Science: Everybody knows what relevance is. But, what is it really?},
  volume = 8,
  year = 2016
}

@article{hlava2014taxobook,
  abstract = {This book is the third of a three-part series on taxonomies, and covers putting your taxonomy into use in as many ways as possible to maximize retrieval for your users. Chapter 1 suggests several items to research and consider before you start your implementation and integration process. It explores the different pieces of software that you will need for your system and what features to look for in each.

Chapter 2 launches with a discussion of how taxonomy terms can be used within a workflow, connecting two—or more—taxonomies, and intelligent coordination of platforms and taxonomies. Microsoft SharePoint is a widely used and popular program, and I consider their use of taxonomies in this chapter. Following that is a discussion of taxonomies and semantic integration and then the relationship between indexing and the hierarchy of a taxonomy.

Chapter 3 (“How is a Taxonomy Connected to Search?”) provides discussions and examples of putting taxonomies into use in practical applications. It discusses displaying content based on search, how taxonomy is connected to search, using a taxonomy to guide a searcher, tools for search, including search engines, crawlers and spiders, and search software, the parts of a search-capable system, and then how to assemble that search-capable system. This chapter also examines how to measure quality in search, the different kinds of search, and theories on search from several famous theoreticians—two from the 18th and 19th centuries, and two contemporary. Following that is a section on inverted files, parsing, discovery, and clustering. While you probably don’t need a comprehensive understanding of these concepts to build a solid, workable system, enough information is provided for the reader to see how they fit into the overall scheme. This chapter concludes with a look at faceted search and some possibilities for search interfaces.

Chapter 4, “Implementing a Taxonomy in a Database or on a Website,” starts where many content systems really should—with the authors, or at least the people who create the content. This chapter discusses matching up various groups of related data to form connections, data visualization and text analytics, and mobile and e-commerce applications for taxonomies. Finally, Chapter 5 presents some educated guesses about the future of knowledge organization.

Table of Contents: List of Figures / Preface / Acknowledgments / On Your Mark, Get Ready …. WAIT! Things to Know Before You Start the Implementation Step / Taxonomy and Thesaurus Implementation / How is a Taxonomy Connected to Search? / Implementing a Taxonomy in a Database or on a Website / What Lies Ahead for Knowledge Organization? / Glossary / End Notes / Author Biography},
  added-at = {2018-03-19T13:14:37.000+0100},
  author = {Hlava, Marjorie M.K.},
  biburl = {https://www.bibsonomy.org/bibtex/2f13b93778a94fb7e3c009c7f6d08d710/vngudivada},
  doi = {10.2200/S00604ED1V03Y201410ICR037},
  eprint = {https://doi.org/10.2200/S00604ED1V03Y201410ICR037},
  interhash = {341f0574257a25391a57ff09a40a535a},
  intrahash = {f13b93778a94fb7e3c009c7f6d08d710},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR SynthesisLecture Taxonomy},
  number = 4,
  pages = {1 -- 156},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {The Taxobook: Applications, Implementation, and Integration in Search: Part 3 of a 3-Part Series},
  volume = 6,
  year = 2014
}

@article{yang2016dynamic,
  abstract = {Big data and human-computer information retrieval (HCIR) are changing IR. They capture the dynamic changes in the data and dynamic interactions of users with IR systems. A dynamic system is one which changes or adapts over time or a sequence of events. Many modern IR systems and data exhibit these characteristics which are largely ignored by conventional techniques. What is missing is an ability for the model to change over time and be responsive to stimulus. Documents, relevance, users and tasks all exhibit dynamic behavior that is captured in data sets typically collected over long time spans and models need to respond to these changes. Additionally, the size of modern datasets enforces limits on the amount of learning a system can achieve. Further to this, advances in IR interface, personalization and ad display demand models that can react to users in real time and in an intelligent, contextual way.

In this book we provide a comprehensive and up-to-date introduction to Dynamic Information Retrieval Modeling, the statistical modeling of IR systems that can adapt to change. We define dynamics, what it means within the context of IR and highlight examples of problems where dynamics play an important role. We cover techniques ranging from classic relevance feedback to the latest applications of partially observable Markov decision processes (POMDPs) and a handful of useful algorithms and tools for solving IR problems incorporating dynamics.

The theoretical component is based around the Markov Decision Process (MDP), a mathematical framework taken from the field of Artificial Intelligence (AI) that enables us to construct models that change according to sequential inputs. We define the framework and the algorithms commonly used to optimize over it and generalize it to the case where the inputs aren't reliable. We explore the topic of reinforcement learning more broadly and introduce another tool known as a Multi-Armed Bandit which is useful for cases where exploring model parameters is beneficial. Following this we introduce theories and algorithms which can be used to incorporate dynamics into an IR model before presenting an array of state-of-the-art research that already does, such as in the areas of session search and online advertising.

Change is at the heart of modern Information Retrieval systems and this book will help equip the reader with the tools and knowledge needed to understand Dynamic Information Retrieval Modeling.

Table of Contents: Acknowledgments / Introduction / Information Retrieval Frameworks / Dynamic IR for a Single Query / Dynamic IR for Sessions / Dynamic IR for Recommender Systems / Evaluating Dynamic IR Systems / Conclusion / Bibliography / Authors' Biographies},
  added-at = {2018-03-19T12:58:57.000+0100},
  author = {Yang, Grace Hui and Sloan, Marc and Wang, Jun},
  biburl = {https://www.bibsonomy.org/bibtex/20afe37d7f54412ee868f4c3691bfd88b/vngudivada},
  doi = {10.2200/S00718ED1V01Y201605ICR049},
  eprint = {https://doi.org/10.2200/S00718ED1V01Y201605ICR049},
  interhash = {b391928ddbcdd94eb78d352eea0f176d},
  intrahash = {0afe37d7f54412ee868f4c3691bfd88b},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR SynthesisLecture},
  number = 3,
  pages = {1 -- 144},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Dynamic Information Retrieval Modeling},
  volume = 8,
  year = 2016
}

@article{lalmas2014measuring,
  abstract = {User engagement refers to the quality of the user experience that emphasizes the positive aspects of interacting with an online application and, in particular, the desire to use that application longer and repeatedly. User engagement is a key concept in the design of online applications (whether for desktop, tablet or mobile), motivated by the observation that successful applications are not just used, but are engaged with. Users invest time, attention, and emotion in their use of technology, and seek to satisfy pragmatic and hedonic needs. Measurement is critical for evaluating whether online applications are able to successfully engage users, and may inform the design of and use of applications. User engagement is a multifaceted, complex phenomenon; this gives rise to a number of potential measurement approaches. Common ways to evaluate user engagement include using self-report measures, e.g., questionnaires; observational methods, e.g. facial expression analysis, speech analysis; neuro-physiological signal processing methods, e.g., respiratory and cardiovascular accelerations and decelerations, muscle spasms; and web analytics, e.g., number of site visits, click depth. These methods represent various trade-offs in terms of the setting (laboratory versus ``in the wild''), object of measurement (user behaviour, affect or cognition) and scale of data collected. For instance, small-scale user studies are deep and rich, but limited in terms of generalizability, whereas large-scale web analytic studies are powerful but negate users' motivation and context. The focus of this book is how user engagement is currently being measured and various considerations for its measurement. Our goal is to leave readers with an appreciation of the various ways in which to measure user engagement, and their associated strengths and weaknesses. We emphasize the multifaceted nature of user engagement and the unique contextual constraints that come to bear upon attempts to measure engagement in different settings, and across different user groups and web domains. At the same time, this book advocates for the development of ``good'' measures and good measurement practices that will advance the study of user engagement and improve our understanding of this construct, which has become so vital in our wired world.

Table of Contents: Preface / Acknowledgments / Introduction and Scope / Approaches Based on Self-Report Methods / Approaches Based on Physiological Measurements / Approaches Based on Web Analytics / Beyond Desktop, Single Site, and Single Task / Enhancing the Rigor of User Engagement Methods and Measures / Conclusions and Future Research Directions / Bibliography / Authors' Biographies / Index},
  added-at = {2018-03-19T13:20:12.000+0100},
  author = {Lalmas, Mounia and O'Brien, Heather and Yom-Tov, Elad},
  biburl = {https://www.bibsonomy.org/bibtex/25dfeef8d58b4fb15f91bac60a0b5ba18/vngudivada},
  doi = {10.2200/S00605ED1V01Y201410ICR038},
  eprint = {https://doi.org/10.2200/S00605ED1V01Y201410ICR038},
  interhash = {4fbd3df25c5574dd68a75372ee2ba35f},
  intrahash = {5dfeef8d58b4fb15f91bac60a0b5ba18},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR SynthesisLecture UserModeling},
  number = 4,
  pages = {1 -- 132},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Measuring User Engagement},
  volume = 6,
  year = 2014
}

@article{jones2015building,
  abstract = {Personal Information Management (PIM) is the art of getting things done in our lives through information. How do we - can we better - manage our information at home, at school, at work, at play and "@large" in a global community? How do we use information not only to know but also to represent, communicate and effect useful change in the world around us?

In the study of PIM, does the search for practical methods with practical impact lead to methods that are "massive open on-line"? Can the ancient practice of storytelling help us better to weave our fragmented information together? In the practice of PIM, how can our information best serve as "near knowledge" - close at hand and, through our information tools, serving in practical ways to extend the knowledge that's "in the head"? If attempts to multitask lead to ineffective, even dangerous, instances of task switching and divided attention, can better PIM help us to realize, instead, opportunities for "multi-goaling" where the same time and effort accomplishes not just one but several goals?

These and other questions are addressed in this third and final book to conclude the series on "The Future of Personal Information Management".

Part 1, "Our Information, Always and Forever", covered the fundamentals of PIM and then explored the seismic shift, already well underway, towards a world where our information is always at hand (thanks to our devices) and "forever" on the web.

Part 2, "Transforming Technologies to Manage Our Information", provided a more focused look at technologies for managing information. The opening chapter discussed "natural interface" technologies of input/output to free us from keyboard, screen and mouse. Successive chapters then explored technologies to save, search and structure our information. A concluding chapter introduced the possibility that we may see dramatic reductions in the "clerical tax" we pay as we work with our information.

Now in Part 3, "Building a Better World with Our Information", focus shifts to the practical present and to the near future. Part 3 is in three chapters:

• Group information management and the social fabric in PIM. How do we preserve and promote our PIM practices as we interact with others at home, at work, at play and in wider, even global, communities? (Chapter 10).

• Designing for PIM in the development of tools and in the selection of teachable (learnable) "better practices" of PIM. (Chapter 11).

• To each of us, our own concludes with an exploration of the ways each of us, individually, can develop better practices for the management of our information in service of the lives we wish to live and towards a better world we all must share. (Chapter 12).

Table of Contents: Preface / Acknowledgments / Group Information Management and the Social Fabric of PIM / PIM by Design / To Each of Us, Our Own / References / Author Biography

Reviews:

William Jones has worked intently on this topic for many years. His coverage is very broad, and he has thought carefully and deeply about the issues, pursuing them to the point where knowledge passes into wisdom. Because each of us is different, and each of us has many different relationships to different categories of information, this is a complex topic with no silver bullet. Yet this book will remain an invaluable resource for a very long time, whether as a foundation for research or as a basis for action, because no one is likely to come along with such a monumental grasp of the subject.-Jonathan T. Grudin (Snippet from Amazon.com Review)},
  added-at = {2018-03-19T13:07:31.000+0100},
  author = {Jones, William},
  biburl = {https://www.bibsonomy.org/bibtex/2698cc3a192428ca5c78727e4a70215ef/vngudivada},
  doi = {10.2200/S00653ED1V01Y201506ICR042},
  eprint = {https://doi.org/10.2200/S00653ED1V01Y201506ICR042},
  interhash = {7b2ba2b4bc8ebf7b46027ce03211a1c5},
  intrahash = {698cc3a192428ca5c78727e4a70215ef},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR PersonalInformationManagement SynthesisLecture},
  number = 4,
  pages = {1 -- 203},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Building a Better World with Our Information: The Future of Personal Information Management, Part 3},
  volume = 7,
  year = 2015
}

@article{robertson1997overview,
  abstract = {This paper gives a brief description of the Okapi projects and the work of the Centre for Interactive Systems research, as an introduction to this special issue of the Journal of Documentation. Okapi is the name given to an experimental text retrieval system (or rather, family of systems, as will be discussed below), based at City University, London. The current systems and their predecessors have been used as the basis for a series of projects, generally addressing aspects of user information‐seeking behaviour and user‐system interaction, as well as system design. The projects have been supported extensively by the British Library, and to some degree by a number of other funders. They have been at City since 1989; for the previous seven years they were based at the Polytechnic of Central London (now the University of Westminster). In order to give a picture of the system(s) that now constitute Okapi, it is appropriate to describe one version containing some of the features that have become central to the Okapi projects, and then to indicate the variety of systems now implemented or implementable within the present setup, as well as the directions it may go in the future. In what follows, papers in this issue are referred to by brief titles.},
  added-at = {2018-03-19T03:47:10.000+0100},
  author = {Robertson, S.E.},
  biburl = {https://www.bibsonomy.org/bibtex/2e40bdf15e6ddb400caae357298cfd323/vngudivada},
  doi = {10.1108/EUM0000000007186},
  interhash = {68a217852d513adf447bc7f478a25dac},
  intrahash = {e40bdf15e6ddb400caae357298cfd323},
  journal = {Journal of Documentation},
  keywords = {Okapi ProbabilisticRanking},
  number = 1,
  pages = {3-7},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Overview of the Okapi projects},
  volume = 53,
  year = 1997
}

@article{ding2017information,
  abstract = {Information Architecture is about organizing and simplifying information, designing and integrating information spaces/systems, and creating ways for people to find and interact with information content. Its goal is to help people understand and manage information and make the right decisions accordingly. This updated and revised edition of the book looks at integrated information spaces in the web context and beyond, with a focus on putting theories and principles into practice.

In the ever-changing social, organizational, and technological contexts, information architects not only design individual information spaces (e.g., websites, software applications, and mobile devices), but also tackle strategic aggregation and integration of multiple information spaces across websites, channels, modalities, and platforms. Not only do they create predetermined navigation pathways, but they also provide tools and rules for people to organize information on their own and get connected with others.

Information architects work with multi-disciplinary teams to determine the user experience strategy based on user needs and business goals, and make sure the strategy gets carried out by following the user-centered design (UCD) process via close collaboration with others. Drawing on the authors’ extensive experience as HCI researchers, User Experience Design practitioners, and Information Architecture instructors, this book provides a balanced view of the IA discipline by applying theories, design principles, and guidelines to IA and UX practices. It also covers advanced topics such as iterative design, UX decision support, and global and mobile IA considerations. Major revisions include moving away from a web-centric view toward multi-channel, multi-device experiences. Concepts such as responsive design, emerging design principles, and user-centered methods such as Agile, Lean UX, and Design Thinking are discussed and related to IA processes and practices.

Table of Contents: Preface / Information Architecture: Definitions and Scopes / Information Architecture and Evolving Information Spaces / IA and User-centered Design / IA Research and Evaluation / Information Organization and Navigation Design / User Information Behavior and Design Implications / Interaction Design / Design Patterns, Emerging Principles, and Mobile Considerations / IA in Practice / The Future of IA / Bibliography / Author Biographies},
  added-at = {2018-03-19T12:50:33.000+0100},
  author = {Ding, Wei and Lin, Xia and Zarro, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2598228355eb962b4c6fc6ba13b91add1/vngudivada},
  doi = {10.2200/S00755ED2V01Y201701ICR056},
  eprint = {https://doi.org/10.2200/S00755ED2V01Y201701ICR056},
  interhash = {fc43723105d9288f529932b670cbb68b},
  intrahash = {598228355eb962b4c6fc6ba13b91add1},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR IRArchitecture SynthesisLecture},
  number = 2,
  pages = {1--152},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Information Architecture: The Design and Integration of Information Spaces, Second Edition},
  volume = 9,
  year = 2017
}

@article{kraft2017fuzzy,
  abstract = {Information retrieval used to mean looking through thousands of strings of texts to find words or symbols that matched a user's query. Today, there are many models that help index and search more effectively so retrieval takes a lot less time. Information retrieval (IR) is often seen as a subfield of computer science and shares some modeling, applications, storage applications and techniques, as do other disciplines like artificial intelligence, database management, and parallel computing. This book introduces the topic of IR and how it differs from other computer science disciplines. A discussion of the history of modern IR is briefly presented, and the notation of IR as used in this book is defined. The complex notation of relevance is discussed. Some applications of IR is noted as well since IR has many practical uses today. Using information retrieval with fuzzy logic to search for software terms can help find software components and ultimately help increase the reuse of software. This is just one practical application of IR that is covered in this book.

Some of the classical models of IR is presented as a contrast to extending the Boolean model. This includes a brief mention of the source of weights for the various models. In a typical retrieval environment, answers are either yes or no, i.e., on or off. On the other hand, fuzzy logic can bring in a "degree of" match, vs. a crisp, i.e., strict match. This, too, is looked at and explored in much detail, showing how it can be applied to information retrieval. Fuzzy logic is often times considered a soft computing application and this book explores how IR with fuzzy logic and its membership functions as weights can help indexing, querying, and matching. Since fuzzy set theory and logic is explored in IR systems, the explanation of where the fuzz is ensues.

The concept of relevance feedback, including pseudorelevance feedback is explored for the various models of IR. For the extended Boolean model, the use of genetic algorithms for relevance feedback is delved into.

The concept of query expansion is explored using rough set theory. Various term relationships is modeled and presented, and the model extended for fuzzy retrieval. An example using the UMLS terms is also presented. The model is also extended for term relationships beyond synonyms.

Finally, this book looks at clustering, both crisp and fuzzy, to see how that can improve retrieval performance. An example is presented to illustrate the concepts.

Table of Contents: Preface / Acknowledgments / Introduction to Information Retrieval / Modeling / Source of Weights / Relevance Feedback and Query Expansion / Clustering for Retrieval / Uses of Information Retrieval Today / Bibliography / Author Biographies},
  added-at = {2018-03-19T12:51:56.000+0100},
  author = {Kraft, Donald H. and Colvin, Erin},
  biburl = {https://www.bibsonomy.org/bibtex/29dbab39a8e5e96bf98add1dc8568c0f3/vngudivada},
  doi = {10.2200/S00752ED1V01Y201701ICR055},
  eprint = {https://doi.org/10.2200/S00752ED1V01Y201701ICR055},
  interhash = {1a64a470d593168369f820addf6a4668},
  intrahash = {9dbab39a8e5e96bf98add1dc8568c0f3},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {FuzzyIR IR SynthesisLecture},
  number = 1,
  pages = {1 -- 63},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Fuzzy Information Retrieval},
  volume = 9,
  year = 2017
}

@article{he2016scholarly,
  abstract = {Collaboration among scholars has always been recognized as a fundamental feature of scientific discovery. The ever-increasing diversity among disciplines and complexity of research problems makes it even more compelling to collaborate in order to keep up with the fast pace of innovation and advance knowledge. Along with the rapidly developing Internet communication technologies and the increasing popularity of the social web, we have observed many important developments of scholarly collaboration on the academic social web.

In this book, we review the rapid transformation of scholarly collaboration on various academic social web platforms and examine how these platforms have facilitated academics throughout their research lifecycle—from forming ideas, collecting data, and authoring articles to disseminating findings. We refer to the term "academic social web platforms" in this book as a category of Web 2.0 tools or online platforms (such as CiteULike, Mendeley, Academia.edu, and ResearchGate) that enable and facilitate scholarly information exchange and participation. We will also examine scholarly collaboration behaviors including sharing academic resources, exchanging opinions, following each other's research, keeping up with current research trends, and, most importantly, building up their professional networks.

Inspired by the model developed Olson et al. [2000] on factors for successful scientific collaboration, our examination of the status of scholarly collaboration on the academic social web has four emphases: technology readiness, coupling work, building common ground, and collaboration readiness. Finally, we talk about the insights and challenges of all these online scholarly collaboration activities imposed on the research communities who are engaging in supporting online scholarly collaboration.

This book aims to help researchers and practitioners understand the development of scholarly collaboration on the academic social web, and to build up an active community of scholars who are interested in this topic.

Table of Contents: Acknowledgments / Scholarship in Networked Participatory Environment / Technology Readiness for Social Scholarly Collaboration / Coupling Work for Social Scholarly Collaboration / Common Ground for Social Scholarly Collaboration / Collaboration Readiness for Social Scholarly Collaboration / Discussions and Conclusions / Bibliography / Authors' Biographies},
  added-at = {2018-03-19T13:02:44.000+0100},
  author = {He, Daqing and Jeng, Wei},
  biburl = {https://www.bibsonomy.org/bibtex/2a5bbf1bd79214cf25af38148eea4c574/vngudivada},
  doi = {10.2200/S00698ED1V01Y201601ICR047},
  eprint = {https://doi.org/10.2200/S00698ED1V01Y201601ICR047},
  interhash = {19a771176b7185ff4fbf89239054aea0},
  intrahash = {a5bbf1bd79214cf25af38148eea4c574},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR SocialNetworks SynthesisLecture},
  number = 1,
  pages = {1 -- 106},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Scholarly Collaboration on the Academic Social Web},
  volume = 8,
  year = 2016
}

@article{hlava2014taxobook,
  abstract = {This book outlines the basic principles of creation and maintenance of taxonomies and thesauri. It also provides step by step instructions for building a taxonomy or thesaurus and discusses the various ways to get started on a taxonomy construction project.

Often, the first step is to get management and budgetary approval, so I start this book with a discussion of reasons to embark on the taxonomy journey. From there I move on to a discussion of metadata and how taxonomies and metadata are related, and then consider how, where, and why taxonomies are used.

Information architecture has its cornerstone in taxonomies and metadata. While a good discussion of information architecture is beyond the scope of this work, I do provide a brief discussion of the interrelationships among taxonomies, metadata, and information architecture.

Moving on to the central focus of this book, I introduce the basics of taxonomies, including a definition of vocabulary control and why it is so important, how indexing and tagging relate to taxonomies, a few of the types of tagging, and a definition and discussion of post- and pre-coordinate indexing. After that I present the concept of a hierarchical structure for vocabularies and discuss the differences among various kinds of controlled vocabularies, such as taxonomies, thesauri, authority files, and ontologies.

Once you have a green light for your project, what is the next step? Here I present a few options for the first phase of taxonomy construction and then a more detailed discussion of metadata and markup languages. I believe that it is important to understand the markup languages (SGML and XML specifically, and HTML to a lesser extent) in relation to information structure, and how taxonomies and metadata feed into that structure. After that, I present the steps required to build a taxonomy, from defining the focus, collecting and organizing terms, analyzing your vocabulary for even coverage over subject areas, filling in gaps, creating relationships between terms, and applying those terms to your content. Here I offer a cautionary note: don’t believe that your taxonomy is “done!” Regular, scheduled maintenance is an important—critical, really—component of taxonomy construction projects.

After you’ve worked through the steps in this book, you will be ready to move on to integrating your taxonomy into the workflow of your organization. This is covered in Book 3 of this series.

Table of Contents: List of Figures / Preface / Acknowledgments / Building a Case for Building a Taxonomy / Taxonomy Basics / Getting Started / Terms: The Building Blocks of a Taxonomy / Building the Structure of Your Taxonomy / Evaluation and Maintenance / Standards and Taxonomies / Glossary / End Notes / Author Biography},
  added-at = {2018-03-19T13:12:04.000+0100},
  author = {Hlava, Marjorie M.K.},
  biburl = {https://www.bibsonomy.org/bibtex/270a25de53188724881ff4020216cd95c/vngudivada},
  doi = {10.2200/S00603ED1V02Y201410ICR036},
  eprint = {https://doi.org/10.2200/S00603ED1V02Y201410ICR036},
  interhash = {d4eae188e3ad9a254d193def131b3359},
  intrahash = {70a25de53188724881ff4020216cd95c},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR SynthesisLecture Taxonomy},
  number = 4,
  pages = {1--164},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {The Taxobook: Principles and Practices of Building Taxonomies, Part 2 of a 3-Part Series},
  volume = 6,
  year = 2014
}

@inproceedings{arguello2006infomagnets,
  abstract = {We introduce a new interactive corpus exploration tool called InfoMagnets. InfoMagnets aims at making exploratory corpus analysis accessible to researchers who are not experts in text mining. As evidence of its usefulness and usability, it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains: tutorial dialogue (Kumar et al., submitted) and on-line communities (Arguello et al., 2006). As an educational tool, it has been used as part of a unit on protocol analysis in an Educational Research Methods course.},
  added-at = {2018-06-13T02:29:19.000+0200},
  address = {Stroudsburg, Pennsylvania},
  author = {Arguello, Jaime and Ros{\'e}, Carolyn},
  biburl = {https://www.bibsonomy.org/bibtex/288630ea3e9883c5b93dd5ffd13756f12/vngudivada},
  booktitle = {Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Companion Volume: Demonstrations},
  doi = {10.3115/1225785.1225786},
  interhash = {e91ff181fd8a6fc97f9c672877b91c9e},
  intrahash = {88630ea3e9883c5b93dd5ffd13756f12},
  keywords = {Corpus CorpusLinguistics},
  location = {New York, NY},
  pages = {253--256},
  publisher = {Association for Computational Linguistics},
  series = {NAACL-Demonstrations '06},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {InfoMagnets: Making Sense of Corpus Data},
  year = 2006
}

@article{white2009exploratory,
  abstract = {As information becomes more ubiquitous and the demands that searchers have on search systems grow, there is a need to support search behaviors beyond simple lookup. Information seeking is the process or activity of attempting to obtain information in both human and technological contexts. Exploratory search describes an information-seeking problem context that is open-ended, persistent, and multifaceted, and information-seeking processes that are opportunistic, iterative, and multitactical. Exploratory searchers aim to solve complex problems and develop enhanced mental capacities. Exploratory search systems support this through symbiotic human-machine relationships that provide guidance in exploring unfamiliar information landscapes.

Exploratory search has gained prominence in recent years. There is an increased interest from the information retrieval, information science, and human-computer interaction communities in moving beyond the traditional turn-taking interaction model supported by major Web search engines, and toward support for human intelligence amplification and information use. In this lecture, we introduce exploratory search, relate it to relevant extant research, outline the features of exploratory search systems, discuss the evaluation of these systems, and suggest some future directions for supporting exploratory search. Exploratory search is a new frontier in the search domain and is becoming increasingly important in shaping our future world.

Table of Contents: Introduction / Defining Exploratory Search / Related Work / Features of Exploratory Search Systems / Evaluation of Exploratory Search Systems / Future Directions and concluding Remarks

Reviews

White and Roth's book is a concise introduction to the emerging area of exploratory search. It is very easy to read, and it works very well as an introduction to the area, with over 250 references. This book is a recommended starting point for developers, academics, and advanced undergraduates who are interested in this area},
  added-at = {2018-03-19T21:27:40.000+0100},
  author = {White, Ryen W. and Roth, Resa A.},
  biburl = {https://www.bibsonomy.org/bibtex/21953768e2f551a29d60120b71c794b87/vngudivada},
  doi = {10.2200/S00174ED1V01Y200901ICR003},
  eprint = {https://doi.org/10.2200/S00174ED1V01Y200901ICR003},
  interhash = {85ea13f16ba2f0dc2d8a7bd264a7caf8},
  intrahash = {1953768e2f551a29d60120b71c794b87},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {ExploratorySearch IR SynthesisLecture},
  number = 1,
  pages = {1 -- 98},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Exploratory Search: Beyond the Query-Response Paradigm},
  volume = 1,
  year = 2009
}

@article{lalmas2009retrieval,
  abstract = {Documents usually have a content and a structure. The content refers to the text of the document, whereas the structure refers to how a document is logically organized. An increasingly common way to encode the structure is through the use of a mark-up language. Nowadays, the most widely used mark-up language for representing structure is the eXtensible Mark-up Language (XML). XML can be used to provide a focused access to documents, i.e. returning XML elements, such as sections and paragraphs, instead of whole documents in response to a query. Such focused strategies are of particular benefit for information repositories containing long documents, or documents covering a wide variety of topics, where users are directed to the most relevant content within a document. The increased adoption of XML to represent a document structure requires the development of tools to effectively access documents marked-up in XML. This book provides a detailed description of query languages, indexing strategies, ranking algorithms, presentation scenarios developed to access XML documents. Major advances in XML retrieval were seen from 2002 as a result of INEX, the Initiative for Evaluation of XML Retrieval. INEX, also described in this book, provided test sets for evaluating XML retrieval effectiveness. Many of the developments and results described in this book were investigated within INEX.

Table of Contents: Introduction / Basic XML Concepts / Historical Perspectives / Query Languages / Indexing Strategies / Ranking Strategies / Presentation Strategies / Evaluating XML Retrieval Effectiveness / Conclusions},
  added-at = {2018-03-19T21:40:35.000+0100},
  author = {Lalmas, Mounia},
  biburl = {https://www.bibsonomy.org/bibtex/238078854fa0f5d2a244145cf8f60bad3/vngudivada},
  doi = {10.2200/S00203ED1V01Y200907ICR007},
  eprint = {https://doi.org/10.2200/S00203ED1V01Y200907ICR007},
  interhash = {1bbdd50d7138263029e3f4d58b66efcb},
  intrahash = {38078854fa0f5d2a244145cf8f60bad3},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR SynthesisLecture XML},
  number = 1,
  pages = {1 -- 111},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {XML Retrieval},
  volume = 1,
  year = 2009
}

@inproceedings{robertson1994simple,
  abstract = {The 2-Poisson model for term frequencies is used to suggest ways of incorporating certain variables in probabilistic models for information retrieval. The variables concerned are within-document term frequency, document length, and within-query term frequency. Simple weighting functions are developed, and tested on the TREC test collection. Considerable performance improvements (over simple inverse collection frequency weighting) are demonstrated.},
  added-at = {2018-05-12T19:01:34.000+0200},
  address = {New York, NY},
  author = {Robertson, S. E. and Walker, S.},
  biburl = {https://www.bibsonomy.org/bibtex/24a164b027e9e62d11da11ffb36c5900e/vngudivada},
  booktitle = {Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  interhash = {04ce4735292d17b36af7be32578db1f5},
  intrahash = {4a164b027e9e62d11da11ffb36c5900e},
  isbn = {0-387-19889-X},
  keywords = {IR ProbabilisticRanking},
  pages = {232--241},
  publisher = {Springer-Verlag},
  series = {SIGIR '94},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval},
  year = 1994
}

@book{grossman2004information,
  abstract = {Interested in how an efficient search engine works? Want to know what algorithms are used to rank resulting documents in response to user requests? The authors answer these and other key information retrieval design and implementation questions.

  This book is not yet another high level text. Instead, algorithms are thoroughly described, making this book ideally suited for both computer science students and practitioners who work on search-related applications. As stated in the foreword, this book provides a current, broad, and detailed overview of the field and is the only one that does so. Examples are used throughout to illustrate the algorithms.

  The authors explain how a query is ranked against a document collection using either a single or a combination of retrieval strategies, and how an assortment of utilities are integrated into the query processing scheme to improve these rankings. Methods for building and compressing text indexes, querying and retrieving documents in multiple languages, and using parallel or distributed processing to expedite the search are likewise described.

  This edition is a major expansion of the one published in 1998. Besides updating the entire book with current techniques, it includes new sections on language models, cross-language information retrieval, peer-to-peer processing, XML search, mediators, and duplicate document detection.},
  added-at = {2018-03-31T15:33:36.000+0200},
  address = {New York, NY},
  author = {Grossman, David A. and Frieder, Ophir},
  biburl = {https://www.bibsonomy.org/bibtex/23ca2a56d0401c525f3fa3db3dd39945f/vngudivada},
  edition = {Second},
  interhash = {55db330f9b8da03de5f7b7c088a0b1de},
  intrahash = {3ca2a56d0401c525f3fa3db3dd39945f},
  isbn = {978-1402030048},
  keywords = {Book IR},
  publisher = {Springer},
  series = {The Information Retrieval Series},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Information Retrieval: Algorithms and Heuristics},
  year = 2004
}

@book{baezayates2011modern,
  abstract = {This is a rigorous and complete textbook for a first course on information retrieval from the computer science perspective. It provides an up-to-date student oriented treatment of information retrieval including extensive coverage of new topics such as web retrieval, web crawling, open source search engines and user interfaces.

  From parsing to indexing, clustering to classification, retrieval to ranking, and user feedback to retrieval evaluation, all of the most important concepts are carefully introduced and exemplified. The contents and structure of the book have been carefully designed by the two main authors, with individual contributions coming from leading international authorities in the field, including Yoelle Maarek, Senior Director of Yahoo! Research Israel; Dulce Poncele on IBM Research; and Malcolm Slaney, Yahoo Research USA.

  This completely reorganized, revised and enlarged second edition of Modern Information Retrieval contains many new chapters and double the number of pages and bibliographic references of the first edition, and a companion website www.mir2ed.org with teaching material. It will prove invaluable to students, professors, researchers, practitioners, and scholars of this fascinating field of information retrieval.},
  added-at = {2018-03-31T15:25:50.000+0200},
  address = {Boston, Massachusetts},
  author = {Baeza-Yates, Ricardo and Ribeiro-Neto, ‎Berthier},
  biburl = {https://www.bibsonomy.org/bibtex/2e81779becadffa3fcd83ad137381171b/vngudivada},
  edition = {Second},
  interhash = {545e1948d8efdc990071dcccc0d9b1ec},
  intrahash = {e81779becadffa3fcd83ad137381171b},
  isbn = {978-0321416919},
  keywords = {Book IR},
  publisher = {Addison-Wesley Professional},
  series = {ACM Press Books},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Modern Information Retrieval: The Concepts and Technology behind Search},
  year = 2011
}

@inproceedings{pinto2016comparing,
  abstract = {Nowadays, there are many toolkits available for performing common natural language processing tasks, which enable the development of more powerful applications without having to start from scratch. In fact, for English, there is no need to develop tools such as tokenizers, part-of-speech (POS) taggers, chunkers or named entity recognizers (NER). The current challenge is to select which one to use, out of the range of available tools. This choice may depend on several aspects, including the kind and source of text, where the level, formal or informal, may influence the performance of such tools. In this paper, we assess a range of natural language processing toolkits with their default configuration, while performing a set of standard tasks (e.g. tokenization, POS tagging, chunking and NER), in popular datasets that cover newspaper and social network text. The obtained results are analyzed and, while we could not decide on a single toolkit, this exercise was very helpful to narrow our choice.},
  added-at = {2018-06-20T00:34:02.000+0200},
  address = {Dagstuhl, Germany},
  author = {Pinto, Alexandre and Oliveira, Hugo Gon{\c{c}}alo and Alves, Ana Oliveira},
  biburl = {https://www.bibsonomy.org/bibtex/24ac6d1f01cee60cca9d736ff15481cfe/vngudivada},
  booktitle = {5th Symposium on Languages, Applications and Technologies (SLATE'16)},
  doi = {10.4230/OASIcs.SLATE.2016.3},
  editor = {Mernik, Marjan and Leal, Jos{\'e} Paulo and Oliveira, Hugo Gon{\c{c}}alo},
  interhash = {57a02909a4613c5e4fa4522e7f8ad84d},
  intrahash = {4ac6d1f01cee60cca9d736ff15481cfe},
  isbn = {978-3-95977-006-4},
  keywords = {Framework NLP Toolkit},
  pages = {3:1--3:16},
  publisher = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  series = {OpenAccess Series in Informatics (OASIcs)},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {{Comparing the Performance of Different NLP Toolkits in Formal and Social Media Text}},
  volume = 51,
  year = 2016
}

@article{wu2018mobile,
  abstract = {With the rapid development of mobile Internet and smart personal devices in recent years, mobile search has gradually emerged as a key method with which users seek online information. In addition, cross-device search also has been regarded recently as an important research topic. As more mobile applications (APPs) integrate search functions, a user's mobile search behavior on different APPs becomes more significant. This book provides a systematic review of current mobile search analysis and studies user mobile search behavior from several perspectives, including mobile search context, APP usage, and different devices. Two different user experiments to collect user behavior data were conducted. Then, through the data from user mobile phone usage logs in natural settings, we analyze the mobile search strategies employed and offer a context-based mobile search task collection, which then can be used to evaluate the mobile search engine. In addition, we combine mobile search with APP usage to give more in-depth analysis, such as APP transition in mobile search and follow-up actions triggered by mobile search. The study, combining the mobile search with APP usage, can contribute to the interaction design of APPs, such as the search recommendation and APP recommendation. Addressing the phenomenon of users owning more smart devices today than ever before, we focus on user cross-device search behavior. We model the information preparation behavior and information resumption behavior in cross-device search and evaluate the search performance in cross-device search. Research on mobile search behaviors across different devices can help to understand online user information behavior comprehensively and help users resume their search tasks on different devices.

Table of Contents: Preface / Acknowledgments / Information Search in Mobile Context / Context-based Mobile Search Behaviors / Mobile Search Behaviors and APP Usage / Mobile Search Behavior Across Different Devices / Discussion and Conclusions / References / Author Biographies},
  added-at = {2018-05-01T04:40:51.000+0200},
  author = {Wu, Dan and Liang, Shaobo},
  biburl = {https://www.bibsonomy.org/bibtex/24d54cc89c85a38063fded5766bbe2960/vngudivada},
  doi = {10.2200/S00831ED1V01Y201802ICR063},
  interhash = {dffa4b4c6967004b3c049219d227e0d0},
  intrahash = {4d54cc89c85a38063fded5766bbe2960},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {MobileSearch SynthesisLecture},
  number = 2,
  pages = {i-159},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Mobile Search Behaviors: An In-depth Analysis Based on Contexts, APPs, and Devices},
  volume = 10,
  year = 2018
}

@book{kshemkalyani2011distributed,
  abstract = {Designing distributed computing systems is a complex process requiring a solid understanding of the design problems and the theoretical and practical aspects of their solutions. This comprehensive textbook covers the fundamental principles and models underlying the theory, algorithms and systems aspects of distributed computing. Broad and detailed coverage of the theory is balanced with practical systems-related issues such as mutual exclusion, deadlock detection, authentication, and failure recovery. Algorithms are carefully selected, lucidly presented, and described without complex proofs. Simple explanations and illustrations are used to elucidate the algorithms. Important emerging topics such as peer-to-peer networks and network security are also considered. With vital algorithms, numerous illustrations, examples and homework problems, this textbook is suitable for advanced undergraduate and graduate students of electrical and computer engineering and computer science. Practitioners in data networking and sensor networks will also find this a valuable resource. Additional resources are available online at www.cambridge.org/9780521876346.},
  added-at = {2018-03-26T18:33:09.000+0200},
  address = {Cambridge, UK},
  author = {Kshemkalyani, Ajay D. and Singhal, Mukesh},
  biburl = {https://www.bibsonomy.org/bibtex/2ea7ec8e4b399f6c3b8fe9f03971ff80e/vngudivada},
  interhash = {8aa7e079de11bb09c79f85887d3eabc6},
  intrahash = {ea7ec8e4b399f6c3b8fe9f03971ff80e},
  isbn = {978-0521189842},
  keywords = {Book DistributedSystems},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Distributed Computing: Principles, Algorithms, and Systems},
  year = 2011
}

@article{williams2016syntaxbased,
  abstract = {This unique book provides a comprehensive introduction to the most popular syntax-based statistical machine translation models, filling a gap in the current literature for researchers and developers in human language technologies. While phrase-based models have previously dominated the field, syntax-based approaches have proved a popular alternative, as they elegantly solve many of the shortcomings of phrase-based models. The heart of this book is a detailed introduction to decoding for syntax-based models.

The book begins with an overview of synchronous-context free grammar (SCFG) and synchronous tree-substitution grammar (STSG) along with their associated statistical models. It also describes how three popular instantiations (Hiero, SAMT, and GHKM) are learned from parallel corpora. It introduces and details hypergraphs and associated general algorithms, as well as algorithms for decoding with both tree and string input. Special attention is given to efficiency, including search approximations such as beam search and cube pruning, data structures, and parsing algorithms. The book consistently highlights the strengths (and limitations) of syntax-based approaches, including their ability to generalize phrase-based translation units, their modeling of specific linguistic phenomena, and their function of structuring the search space.

Table of Contents: Preface / Acknowledgments / Models / Learning from Parallel Text / Decoding I: Preliminaries / Decoding II: Tree Decoding / Decoding III: String Decoding / Selected Topics / Closing Remarks / Bibliography / Authors' Biographies / Author Index / Index},
  added-at = {2018-06-28T20:44:48.000+0200},
  author = {Williams, Philip and Sennrich, Rico and Post, Matt and Koehn, Philipp},
  biburl = {https://www.bibsonomy.org/bibtex/24bdffd8eec54ea105ed9b183d13dd443/vngudivada},
  doi = {10.2200/S00716ED1V04Y201604HLT033},
  eprint = {https://doi.org/10.2200/S00716ED1V04Y201604HLT033},
  interhash = {1ef97dd3dddcd3af52e9242656632ec3},
  intrahash = {4bdffd8eec54ea105ed9b183d13dd443},
  journal = {Synthesis Lectures on Human Language Technologies},
  keywords = {MT MachineTranslation NLP SynthesisLecture},
  number = 4,
  pages = {1-208},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Syntax-based Statistical Machine Translation},
  volume = 9,
  year = 2016
}

@book{rijsbergen1979information,
  abstract = {In recent years, there have been several attempts to define a logic for information retrieval (IR). The aim was to provide a rich and uniform representation of information and its semantics with the goal of improving retrieval effectiveness. The basis of a logical model for IR is the assumption that queries and documents can be represented effectively by logical formulae. To retrieve a document, an IR system has to infer the formula representing the query from the formula representing the document. This logical interpretation of query and document emphasizes that relevance in IR is an inference process.

  The use of logic to build IR models enables one to obtain models that are more general than earlier well-known IR models. Indeed, some logical models are able to represent within a uniform framework various features of IR systems such as hypermedia links, multimedia data, and user's knowledge. Logic also provides a common approach to the integration of IR systems with logical database systems. Finally, logic makes it possible to reason about an IR model and its properties. This latter possibility is becoming increasingly more important since conventional evaluation methods, although good indicators of the effectiveness of IR systems, often give results which cannot be predicted, or for that matter satisfactorily explained.

  However, logic by itself cannot fully model IR. The success or the failure of the inference of the query formula from the document formula is not enough to model relevance in IR. It is necessary to take into account the uncertainty inherent in such an inference process. In 1986, Van Rijsbergen proposed the uncertainty logical principle to model relevance as an uncertain inference process. When proposing the principle, Van Rijsbergen was not specific about which logic and which uncertainty theory to use. As a consequence, various logics and uncertainty theories have been proposed and investigated. The choice of an appropriate logic and uncertainty mechanism has been a main research theme in logical IR modeling leading to a number of logical IR models over the years.

  Information Retrieval: Uncertainty and Logics contains a collection of exciting papers proposing, developing and implementing logical IR models. This book is appropriate for use as a text for a graduate-level course on Information Retrieval or Database Systems, and as a reference for researchers and practitioners in industry.},
  added-at = {2018-03-31T15:59:13.000+0200},
  address = {Oxford, United Kingdom},
  author = {Rijsbergen, C. J. Van},
  biburl = {https://www.bibsonomy.org/bibtex/20bc6015eaa45ecb0c5c9186f68f77562/vngudivada},
  edition = {Second},
  interhash = {0edccdac9af024f458911b82f61686ab},
  intrahash = {0bc6015eaa45ecb0c5c9186f68f77562},
  isbn = {978-0408709293},
  keywords = {Book IR},
  publisher = {Butterworth-Heinemann},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Information Retrieval},
  year = 1979
}

@article{ibm2018content,
  abstract = {IBM Content Analytics provides a UIMA compliant analytics engine and rich platform for deploying text analytic solutions.

  Content Analytics is a platform to derive rapid insight. It can transform raw information into business insight quickly without building models or deploying complex systems enabling all knowledge workers to derive insight in hours or days ... not weeks or months. Flexible and extensible for deeper insights, Content Analytics enables better decision making by deriving valuable insights from your enterprise content regardless of source or format. It allows deep, rich text analysis of your information. Solutions built on Content Analytics can help organizations surface undetected problems, fix content-centric process inefficiencies, improve customer service and corporate accountability, reduce operating costs and risks and discover new revenue opportunities.

  Unstructured Information Management Architecture (UIMA) is an open framework for building analytic applications - to find latent meaning, relationships and relevant facts hidden in unstructured text. UIMA defines a common, standard interface that enables text analytics components from multiple vendors to work together. It provides tools for either creating new interoperable text analytics modules or enabling existing text analytics investments to operate within the framework.

  Although UIMA originated at IBM, it has now moved on to be an Open Source project at the Apache Software Foundation. UIMA is the only recognized standard for semantic search and content analytics, by OASIS (Organization for the Advancement of Structured Information Standards).

  Content Analytics can analyze documents, comment and note fields, problem reports, e-mail, web sites and other text-based information sources. Sample Applications

  Product defect detection. Analysis of service and maintenance records provides early insight into product defects and service issues before they become widespread, thus enabling quicker resolution and lower after market service and recall costs.

  Insurance fraud analysis. Analysis of claims documentation, policies, and other customer information allows organizations to identify patterns and hidden relationships in claims activities to reduce incidents of fraud and unnecessary payouts.

  Advanced intelligence for anti-terrorism and law enforcement. Analysts can uncover hidden patterns and identify potential criminal or terrorist activity by better analysis of various information sources such as field analyst reports, surveillance transcripts, public records and financial transactions.

  Customer support and self-service. Analysis of call center logs, support e-mails, and other support documentation provides more accurate problem identification to better identify appropriate resolutions.

  e-Commerce and product finders. Customers can find both target and complementary products online, and sales reps can make the right cross-sell offer by identifying relationships and concepts from analysis of product content, customer profiles and sales notes.},
  added-at = {2018-04-01T02:23:57.000+0200},
  author = {IBM},
  biburl = {https://www.bibsonomy.org/bibtex/2c4e774d89172afd61d2ea2e88b0aaa8c/vngudivada},
  interhash = {47fbdf60861eedecdd33319e17488816},
  intrahash = {c4e774d89172afd61d2ea2e88b0aaa8c},
  keywords = {IBM IR UIMA},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Content Analytics with Enterprise Search: UIMA framework},
  url = {https://www-01.ibm.com/software/ecm/content-analytics/uima.html},
  urldate = {2018-31-03},
  year = 2018
}

@article{turney2010frequency,
  abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
  added-at = {2018-04-05T02:47:53.000+0200},
  address = {USA},
  author = {Turney, Peter D. and Pantel, Patrick},
  biburl = {https://www.bibsonomy.org/bibtex/2689abb6e7fb2e7d327209536316e2278/vngudivada},
  interhash = {397ead0766aba687b471395729a263d1},
  intrahash = {689abb6e7fb2e7d327209536316e2278},
  issn = {1076-9757},
  issue_date = {January 2010},
  journal = {J. Artif. Int. Res.},
  keywords = {IR VSM},
  month = jan,
  number = 1,
  pages = {141 -- 188},
  publisher = {AI Access Foundation},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {From Frequency to Meaning: Vector Space Models of Semantics},
  url = {http://dl.acm.org/citation.cfm?id=1861751.1861756},
  volume = 37,
  year = 2010
}

@article{piantadosi2014zipfs,
  abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf's law. This article first shows that human language has a highly complex, reliable structure in the frequency distribution over and above this classic law, although prior data visualization methods have obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf's law and are then used to evaluate many of the theoretical explanations of Zipf's law in language. No prior account straightforwardly explains all the basic facts or is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf's law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
  added-at = {2018-05-14T13:31:03.000+0200},
  author = {Piantadosi, Steven T.},
  biburl = {https://www.bibsonomy.org/bibtex/24a8e50191082c1e567cc9ed18d390a82/vngudivada},
  day = 01,
  doi = {10.3758/s13423-014-0585-6},
  interhash = {951878cef924b5d3e7577ba7ea9f9c3a},
  intrahash = {4a8e50191082c1e567cc9ed18d390a82},
  issn = {1531-5320},
  journal = {Psychonomic Bulletin {\&} Review},
  keywords = {IR NLP ZipsLaw},
  month = oct,
  number = 5,
  pages = {1112--1130},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Zipf's word frequency law in natural language: A critical review and future directions},
  volume = 21,
  year = 2014
}

@book{witten1999managing,
  abstract = {In this fully updated second edition of the highly acclaimed Managing Gigabytes, authors Witten, Moffat, and Bell continue to provide unparalleled coverage of state-of-the-art techniques for compressing and indexing data. Whatever your field, if you work with large quantities of information, this book is essential reading--an authoritative theoretical resource and a practical guide to meeting the toughest storage and access challenges. It covers the latest developments in compression and indexing and their application on the Web and in digital libraries. It also details dozens of powerful techniques supported by mg, the authors' own system for compressing, storing, and retrieving text, images, and textual images. mg's source code is freely available on the Web.

  Up-to-date coverage of new text compression algorithms such as block sorting, approximate arithmetic coding, and fat Huffman coding

  New sections on content-based index compression and distributed querying, with 2 new data structures for fast indexing

  New coverage of image coding, including descriptions of de facto standards in use on the Web (GIF and PNG), information on CALIC, the new proposed JPEG Lossless standard, and JBIG2

  New information on the Internet and WWW, digital libraries, web search engines, and agent-based retrieval

  Accompanied by a public domain system called MG which is a fully worked-out operational example of the advanced techniques developed and explained in the book

  New appendix on an existing digital library system that uses the MG software},
  added-at = {2018-03-31T16:12:38.000+0200},
  address = {Burlington, Massachusetts},
  author = {Witten, Ian H. and Moffat, Alistair and Bell, Timothy C.},
  biburl = {https://www.bibsonomy.org/bibtex/24d6c6e19ce3ed9be3a7c10e21392f8e1/vngudivada},
  edition = {Second},
  interhash = {f45eb6d5616b79fd1cd0548ca18c7320},
  intrahash = {4d6c6e19ce3ed9be3a7c10e21392f8e1},
  isbn = {978-1558605701},
  keywords = {Book IR},
  publisher = {Morgan Kaufmann},
  series = {The Morgan Kaufmann Series in Multimedia Information and Systems},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Managing Gigabytes: Compressing and Indexing Documents and Images},
  year = 1999
}

@article{wilson2011search,
  abstract = {Search User Interfaces (SUIs) represent the gateway between people who have a task to complete, and the repositories of information and data stored around the world. Not surprisingly, therefore, there are many communities who have a vested interest in the way SUIs are designed. There are people who study how humans search for information, and people who study how humans use computers. There are people who study good user interface design, and people who design aesthetically pleasing user interfaces. There are also people who curate and manage valuable information resources, and people who design effective algorithms to retrieve results from them. While it would be easy for one community to reject another for their limited ability to design a good SUI, the truth is that they all can, and they all have made valuable contributions. Fundamentally, therefore, we must accept that designing a great SUI means leveraging the knowledge and skills from all of these communities.

The aim of this book is to at least acknowledge, if not integrate, all of these perspectives to bring the reader into a multidisciplinary mindset for how we should think about SUI design. Further, this book aims to provide the reader with a framework for thinking about how different innovations each contribute to the overall design of a SUI. With this framework and a multidisciplinary perspective in hand, the book then continues by reviewing: early, successful, established, and experimental concepts for SUI design. The book then concludes by discussing how we can analyse and evaluate the on-going developments in SUI design, as this multidisciplinary area of research moves forwards. Finally, in reviewing these many SUIs and SUI features, the book finishes by extracting a series of 20 SUI design recommendations that are listed in the conclusions.

Table of Contents: Introduction / Searcher-Computer Interaction / Early Search User Interfaces / Modern Search User Interfaces / Experimental Search User Interfaces / Evaluating Search User Interfaces / Conclusions},
  added-at = {2018-03-19T22:03:10.000+0100},
  author = {Wilson, Max L.},
  biburl = {https://www.bibsonomy.org/bibtex/2a4cada5244a2aeab08cf56c3017c2eae/vngudivada},
  doi = {10.2200/S00371ED1V01Y201111ICR020},
  eprint = {https://doi.org/10.2200/S00371ED1V01Y201111ICR020},
  interhash = {6c7deea64986964f061090c7cc0af6e3},
  intrahash = {a4cada5244a2aeab08cf56c3017c2eae},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR InterfaceDesign SynthesisLecture},
  number = 3,
  pages = {1 -- 143},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Search User Interface Design},
  volume = 3,
  year = 2011
}

@article{saggion2017automatic,
  abstract = {Thanks to the availability of texts on the Web in recent years, increased knowledge and information have been made available to broader audiences. However, the way in which a text is written—its vocabulary, its syntax—can be difficult to read and understand for many people, especially those with poor literacy, cognitive or linguistic impairment, or those with limited knowledge of the language of the text. Texts containing uncommon words or long and complicated sentences can be difficult to read and understand by people as well as difficult to analyze by machines. Automatic text simplification is the process of transforming a text into another text which, ideally conveying the same message, will be easier to read and understand by a broader audience. The process usually involves the replacement of difficult or unknown phrases with simpler equivalents and the transformation of long and syntactically complex sentences into shorter and less complex ones. Automatic text simplification, a research topic which started 20 years ago, now has taken on a central role in natural language processing research not only because of the interesting challenges it posesses but also because of its social implications. This book presents past and current research in text simplification, exploring key issues including automatic readability assessment, lexical simplification, and syntactic simplification. It also provides a detailed account of machine learning techniques currently used in simplification, describes full systems designed for specific languages and target audiences, and offers available resources for research and development together with text simplification evaluation techniques.

Table of Contents: Acknowledgments / Introduction / Readability and Text Simplification / Lexical Simplification / Syntactic Simplification / Learning to Simplify / Full Text Simplification Systems / Applications of Automatic Text Simplification / Text Simplification Resources and Evaluation / Conclusion / Bibliography / Author's Biography},
  added-at = {2018-06-02T05:39:40.000+0200},
  author = {Saggion, Horacio},
  biburl = {https://www.bibsonomy.org/bibtex/2464850b8597417e715b84d365b40e61a/vngudivada},
  doi = {10.2200/S00700ED1V01Y201602HLT032},
  eprint = {https://doi.org/10.2200/S00700ED1V01Y201602HLT032},
  interhash = {29029104b3ee80bb248088e4a70744ef},
  intrahash = {464850b8597417e715b84d365b40e61a},
  journal = {Synthesis Lectures on Human Language Technologies},
  keywords = {IR NLP SynthesisLecture TextSimplification},
  number = 1,
  pages = {1-137},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Automatic Text Simplification},
  volume = 10,
  year = 2017
}

@article{amati2002probabilistic,
  abstract = {We introduce and create a framework for deriving probabilistic models of Information Retrieval. The models are nonparametric models of IR obtained in the language model approach. We derive term-weighting models by measuring the divergence of the actual term distribution from that obtained under a random process. Among the random processes we study the binomial distribution and Bose--Einstein statistics. We define two types of term frequency normalization for tuning term weights in the document--query matching process. The first normalization assumes that documents have the same length and measures the information gain with the observed term once it has been accepted as a good descriptor of the observed document. The second normalization is related to the document length and to other statistics. These two normalization methods are applied to the basic models in succession to obtain weighting formulae. Results show that our framework produces different nonparametric models forming baseline alternatives to the standard tf-idf model.},
  acmid = {582416},
  added-at = {2018-05-13T17:58:52.000+0200},
  address = {New York, NY},
  author = {Amati, Gianni and Van Rijsbergen, Cornelis Joost},
  biburl = {https://www.bibsonomy.org/bibtex/2071ce3540f207adc5eb57b4c318b83e3/vngudivada},
  doi = {10.1145/582415.582416},
  interhash = {d0a4443db29ed41c5c681b216c3216e1},
  intrahash = {071ce3540f207adc5eb57b4c318b83e3},
  journal = {ACM Trans. Inf. Syst.},
  keywords = {DFR IR},
  month = oct,
  number = 4,
  pages = {357--389},
  publisher = {ACM},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Probabilistic Models of Information Retrieval Based on Measuring the Divergence from Randomness},
  volume = 20,
  year = 2002
}

@article{agarwal2017exploring,
  abstract = {The field of human information behavior runs the gamut of processes from the realization of a need or gap in understanding, to the search for information from one or more sources to fill that gap, to the use of that information to complete a task at hand or to satisfy a curiosity, as well as other behaviors such as avoiding information or finding information serendipitously. Designers of mechanisms, tools, and computer-based systems to facilitate this seeking and search process often lack a full knowledge of the context surrounding the search. This context may vary depending on the job or role of the person; individual characteristics such as personality, domain knowledge, age, gender, perception of self, etc.; the task at hand; the source and the channel and their degree of accessibility and usability; and the relationship that the seeker shares with the source. Yet researchers have yet to agree on what context really means. While there have been various research studies incorporating context, and biennial conferences on context in information behavior, there lacks a clear definition of what context is, what its boundaries are, and what elements and variables comprise context.

In this book, we look at the many definitions of and the theoretical and empirical studies on context, and I attempt to map the conceptual space of context in information behavior. I propose theoretical frameworks to map the boundaries, elements, and variables of context. I then discuss how to incorporate these frameworks and variables in the design of research studies on context. We then arrive at a unified definition of context. This book should provide designers of search systems a better understanding of context as they seek to meet the needs and demands of information seekers. It will be an important resource for researchers in Library and Information Science, especially doctoral students looking for one resource that covers an exhaustive range of the most current literature related to context, the best selection of classics, and a synthesis of these into theoretical frameworks and a unified definition. The book should help to move forward research in the field by clarifying the elements, variables, and views that are pertinent. In particular, the list of elements to be considered, and the variables associated with each element will be extremely useful to researchers wanting to include the influences of context in their studies.

Table of Contents: Preface / Acknowledgments / Introduction: Why Context? / Literature Review: The Influence of Context on Information Behavior / Mapping the Conceptual Space of Context / Discussion / Definition and Conclusions / Further Reading / Bibliography / Author's Biography},
  added-at = {2018-05-01T04:55:12.000+0200},
  author = {Agarwal, Naresh Kumar},
  biburl = {https://www.bibsonomy.org/bibtex/2de71a776aa373b188a4b46d87c03f620/vngudivada},
  doi = {10.2200/S00807ED1V01Y201710ICR061},
  interhash = {19492b67b03195483ff98cab275463f4},
  intrahash = {de71a776aa373b188a4b46d87c03f620},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {SynthesisLecture UserModeling},
  number = 7,
  pages = {i-163},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Exploring Context in Information Behavior: Seeker, Situation, Surroundings, and Shared Identities},
  volume = 9,
  year = 2017
}

@book{salton1968automatic,
  added-at = {2018-03-31T18:39:32.000+0200},
  address = {New York, NY},
  author = {Salton, Gerard},
  biburl = {https://www.bibsonomy.org/bibtex/28617afa7ea9f25a27cbd58380db34450/vngudivada},
  interhash = {b6c1a5f4ed3e0f18d0aa3ab49c5afd82},
  intrahash = {8617afa7ea9f25a27cbd58380db34450},
  isbn = {978-0070544857},
  keywords = {Book IR},
  publisher = {McGraw-Hill},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Automatic Information and Retrieval},
  year = 1968
}

@book{voorhees2005experiment,
  abstract = {Results from twelve years of the Text REtrieval Conference (TREC), documenting test collections, evaluation standards, and current best practices.

  The Text REtrieval Conference (TREC), a yearly workshop hosted by the US government's National Institute of Standards and Technology, provides the infrastructure necessary for large-scale evaluation of text retrieval methodologies. With the goal of accelerating research in this area, TREC created the first large test collections of full-text documents and standardized retrieval evaluation. The impact has been significant; since TREC's beginning in 1992, retrieval effectiveness has approximately doubled. TREC has built a variety of large test collections, including collections for such specialized retrieval tasks as cross-language retrieval and retrieval of speech. Moreover, TREC has accelerated the transfer of research ideas into commercial systems, as demonstrated in the number of retrieval techniques developed in TREC that are now used in Web search engines.This book provides a comprehensive review of TREC research, summarizing the variety of TREC results, documenting the best practices in experimental information retrieval, and suggesting areas for further research.

  The first part of the book describes TREC's history, test collections, and retrieval methodology. Next, the book provides "track" reports―describing the evaluations of specific tasks, including routing and filtering, interactive retrieval, and retrieving noisy text. The final part of the book offers perspectives on TREC from such participants as Microsoft Research, University of Massachusetts, Cornell University, University of Waterloo, City University of New York, and IBM. The book will be of interest to researchers in information retrieval and related technologies, including natural language processing.},
  added-at = {2018-05-10T05:20:08.000+0200},
  address = {Cambridge, Massachusetts},
  biburl = {https://www.bibsonomy.org/bibtex/23a1722823a457b0c1ad904bcce6e59d1/vngudivada},
  editor = {Voorhees, Ellen and Harman, Donna},
  interhash = {6343ff00c48695a72283c1234e2023cb},
  intrahash = {3a1722823a457b0c1ad904bcce6e59d1},
  isbn = {978-0262220736},
  keywords = {Evaluation IR TREC},
  publisher = {The MIT Press},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {TREC: Experiment and Evaluation in Information Retrieval},
  year = 2005
}

@phdthesis{amati2003probability,
  abstract = {This thesis devises a novel methodology based on probability theory, suitable for the construction of term-weighting models of Information Retrieval. Our term-weighting functions are created within a general framework made up of three components. Each of the three components is built independently from the others. We obtain the term-weighting functions from the general model in a purely theoretic way instantiating each component with different probability distribution forms. The thesis begins with investigating the nature of the statistical inference involved in Information Retrieval. We explore the estimation problem underlying the process of sampling. De Finetti’s theorem is used to show how to convert the frequentist approach into Bayesian inference and we display and employ the derived estimation techniques in the context of Information Retrieval. We initially pay a great attention to the construction of the basic sample spaces of Information Retrieval. The notion of single or multiple sampling from different populations in the context of Information Retrieval is extensively discussed and used through-out the thesis. The language modelling approach and the standard probabilistic model are studied under the same foundational view and are experimentally compared to the divergence-from-randomness approach. In revisiting the main information retrieval models in the literature, we show that even language modelling approach can be exploited to assign term-frequency normalization to the models of divergence from randomness. We finally introduce a novel framework for the query expansion. This framework is based on the models of divergence-from-randomness and it can be applied to arbitrary models of IR, divergence-based, language modelling and probabilistic models included. We have done a very large number of experiment and results show that the framework generates highly effective Information Retrieval models.},
  added-at = {2018-05-13T18:06:45.000+0200},
  address = {Glasgow, Scotland},
  author = {Amati, Giambattista},
  biburl = {https://www.bibsonomy.org/bibtex/2b554024efd6905b3350a3c985852ae3a/vngudivada},
  interhash = {21bf0093a26c0e199acb00904c623e6f},
  intrahash = {b554024efd6905b3350a3c985852ae3a},
  keywords = {DFR IR},
  school = {The University of Glasgow},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Probability models for information retrieval based on divergence from randomness},
  url = {http://theses.gla.ac.uk/1570/},
  year = 2003
}

@book{salton1988automatic,
  abstract = {It is a pity that most of the author's books seem to be out of print, since he's a very clear writer. This book isn't necessarily accessible to the complete beginner--the intended audience is advanced computer science students, computational linguists/natural language processing people, and library/information science students--but if you fall into one of those categories, you should find this a very readable book. The coverage of topics is heavily slanted towards practical applications--editting and formatting, compression, encryption, file access, information retrieval, indexing, abstracting, spell checking, syntax and style checking--rather than towards theoretical background. That's not a bad thing, though--for many of those topics, this might be the most accessible resource you'll find.

  The book was published in the late 80's, and hence is a bit dated by now -- for instance, the statistical revolution in NLP pretty much is not covered (Bayes does not even show up in the index). However, that in no way detracts from the value of what IS covered.},
  added-at = {2018-03-31T15:53:45.000+0200},
  address = {Boston, Massachusetts},
  author = {Salton, Gerard},
  biburl = {https://www.bibsonomy.org/bibtex/2b58f98dbaeb3dd9071e7fe70931bac88/vngudivada},
  interhash = {f2b1a341a1859bdedc8b1f7f88b252e7},
  intrahash = {b58f98dbaeb3dd9071e7fe70931bac88},
  isbn = {978-0201122275},
  keywords = {Book IR},
  publisher = {Addison-Wesley},
  series = {Addison-Wesley series in Computer Science},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Automatic Text Processing: The Transformation Analysis and Retrieval of Information by Computer},
  year = 1988
}

@inproceedings{ding2017building,
  abstract = {Providing an easily accessible data service with high quality data is important for building big data applications. In this paper, we introduce a big data service for managing and accessing massive-scale biomedical image data. The service includes three major components: a NoSQL database for storing images and data analytics results, a client consisting of a group of query scripts for data access and management, and a data quality enhancement component for improving the performance of data analytics. Low-quality data can result in incorrect analytics results and may lead to no value even harmful conclusions. Therefore, it is important to provide an effective mechanism for ensuring data quality improvement in a big data service. We describe the implement ion of a deep learning classifier to automatically filter low quality data in datasets. To improve the effectiveness of data separation, the classifier is rigorously validated with synthetic data generated by a collection of scientific tools. Design of big data services with data quality improvement as an integral component, along with the best practices collected from this experimental study, will help researchers and practitioners to develop strategies for improving the quality of big data services, building big data applications, and designing machine learning classifiers.},
  added-at = {2018-03-25T18:49:19.000+0200},
  address = {Honolulu, Hawaii},
  author = {Ding, J. and Kang, X. and Hu, X. and Gudivada, V.},
  biburl = {https://www.bibsonomy.org/bibtex/21275f3b1961bd954b3af75a5633dc4b1/vngudivada},
  booktitle = {IEEE International Conference on Services Computing (SCC)},
  interhash = {049b5afd6b087cf2cbc703b3c16641bd},
  intrahash = {1275f3b1961bd954b3af75a5633dc4b1},
  keywords = {BigData ServicesComputing},
  month = jun,
  pages = {140 -- 147},
  publisher = {IEEE},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Building a Deep Learning Classifier for Enhancing a Biomedical Big Data Service},
  url = {/brokenurl# http://doi.ieeecomputersociety.org/10.1109/SCC.2017.25},
  year = 2017
}

@inproceedings{cao2008selecting,
  abstract = {Pseudo-relevance feedback assumes that most frequent terms in the pseudo-feedback documents are useful for the retrieval. In this study, we re-examine this assumption and show that it does not hold in reality - many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. We also show that good expansion terms cannot be distinguished from bad ones merely on their distributions in the feedback documents and in the whole collection. We then propose to integrate a term classification process to predict the usefulness of expansion terms. Multiple additional features can be integrated in this process. Our experiments on three TREC collections show that retrieval effectiveness can be much improved when term classification is used. In addition, we also demonstrate that good terms should be identified directly according to their possible impact on the retrieval effectiveness, i.e. using supervised learning, instead of unsupervised learning.},
  added-at = {2018-06-02T13:37:50.000+0200},
  address = {New York, NY},
  author = {Cao, Guihong and Nie, Jian-Yun and Gao, Jianfeng and Robertson, Stephen},
  biburl = {https://www.bibsonomy.org/bibtex/28295b40afd40245e41988b0313db2dcb/vngudivada},
  booktitle = {Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/1390334.1390377},
  interhash = {0ca1dd390c212157ed62b128cbc20ed2},
  intrahash = {8295b40afd40245e41988b0313db2dcb},
  isbn = {978-1-60558-164-4},
  keywords = {IR RelevanceFeedback SVM},
  pages = {243--250},
  publisher = {ACM},
  series = {SIGIR '08},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Selecting Good Expansion Terms for Pseudo-relevance Feedback},
  year = 2008
}

@techreport{allan2012frontiers,
  abstract = {During a three-day workshop in February 2012, 45 Information Retrieval researchers met to discuss long-range challenges and opportunities within the field. The result of the workshop is a diverse set of research directions, project ideas, and challenge areas. This report describes the workshop format, provides summaries of broad themes that emerged, includes brief descriptions of all the ideas, and provides detailed discussion of six proposals that were voted ``most interesting" by the participants. Key themes include the need to: move beyond ranked lists of documents to support richer dialog and presentation, represent the context of search and searchers, provide richer support for information seeking, enable retrieval of a wide range of structured and unstructured content, and develop new evaluation methodologies.},
  added-at = {2018-05-19T15:22:35.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/224a50031fad42c634d89b0e18aac68a4/vngudivada},
  editor = {Allan, James and Croft, Bruce and Moffat, Alistair and Sanderson, Mark},
  institution = {The Second Strategic Workshop on Information Retrieval in Lorne},
  interhash = {5f0b8f48603e7ee8b8d1627ba4b4db55},
  intrahash = {24a50031fad42c634d89b0e18aac68a4},
  keywords = {IR},
  month = feb,
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Frontiers, Challenges, and Opportunities for Information Retrieval},
  year = 2012
}

@book{turnbull2016relevant,
  abstract = {Relevant Search demystifies relevance work. Using Elasticsearch, it teaches you how to return engaging search results to your users, helping you understand and leverage the internals of Lucene-based search engines.

  Users are accustomed to and expect instant, relevant search results. To achieve this, you must master the search engine. Yet for many developers, relevance ranking is mysterious or confusing.

  Relevant Search demystifies the subject and shows you that a search engine is a programmable relevance framework. You will learn how to apply Elasticsearch or Solr to your business's unique ranking problems. The book demonstrates how to program relevance and how to incorporate secondary data sources, taxonomies, text analytics, and personalization. In practice, a relevance framework requires softer skills as well, such as collaborating with stakeholders to discover the right relevance requirements for your business. By the end, you will be able to achieve a virtuous cycle of provable, measurable relevance improvements over a search product's lifetime.

  What is Inside

  Techniques for debugging relevance?
  Applying search engine features to real problems?
  Using the user interface to guide searchers?
  A systematic approach to relevance?
  A business culture focused on improving search
  About the Reader

  For developers trying to build smarter search with Elasticsearch or Solr.

  Doug Turnbull is lead relevance consultant at OpenSource Connections, where he frequently speaks and blogs. John Berryman is a data engineer at Eventbrite, where he specializes in recommendations and search.

  Foreword author, Trey Grainger, is a director of engineering at CareerBuilder and author of Solr in Action.

  Table of Contents

  The search relevance problem
  Search under the hood
  Debugging your first relevance problem
  Taming tokens
  Basic multifield search
  Term-centric search
  Shaping the relevance function
  Providing relevance feedback
  Designing a relevance-focused search application
  The relevance-centered enterprise
  Semantic and personalized search},
  added-at = {2018-04-01T01:21:18.000+0200},
  address = {Shelter Island, New York},
  author = {Turnbull, Doug and Berryman, John},
  biburl = {https://www.bibsonomy.org/bibtex/2787f73ebfaf6796cb76e69798ec3bfb9/vngudivada},
  interhash = {878f39ddb271e94b338ae5c9506af897},
  intrahash = {787f73ebfaf6796cb76e69798ec3bfb9},
  isbn = {978-1617292774},
  keywords = {Book IR Relevance},
  publisher = {Manning},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Relevant Search: With applications for Solr and Elasticsearch},
  year = 2016
}

@article{chuklin2015click,
  abstract = {With the rapid growth of web search in recent years the problem of modeling its users has started to attract more and more attention of the information retrieval community. This has several motivations. By building a model of user behavior we are essentially developing a better understanding of a user, which ultimately helps us to deliver a better search experience. A model of user behavior can also be used as a predictive device for non-observed items such as document relevance, which makes it useful for improving search result ranking. Finally, in many situations experimenting with real users is just infeasible and hence user simulations based on accurate models play an essential role in understanding the implications of algorithmic changes to search engine results or presentation changes to the search engine result page.

In this survey we summarize advances in modeling user click behavior on a web search engine result page. We present simple click models as well as more complex models aimed at capturing non-trivial user behavior patterns on modern search engine result pages. We discuss how these models compare to each other, what challenges they have, and what ways there are to address these challenges. We also study the problem of evaluating click models and discuss the main applications of click models.

Table of Contents: Acknowledgments / Introduction / Terminology / Basic Click Models / Parameter Estimation / Evaluation / Data and Tools / Experimental Comparison / Advanced Click Models / Applications / Discussion and Directions for Future Work / Authors' Biographies / Index},
  added-at = {2018-05-01T04:31:47.000+0200},
  author = {Chuklin, Aleksandr and Markov, Ilya and de Rijke, Maarten},
  biburl = {https://www.bibsonomy.org/bibtex/2323fdcbb0d8cad87f52f09e684ec171d/vngudivada},
  doi = {10.2200/S00654ED1V01Y201507ICR043},
  interhash = {1852411e756611987964c40778c2d38f},
  intrahash = {323fdcbb0d8cad87f52f09e684ec171d},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {ClickModels SynthesisLecture},
  number = 3,
  pages = {1-115},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Click Models for Web Search},
  volume = 7,
  year = 2015
}

@phdthesis{hiemstra2001using,
  added-at = {2018-05-07T06:24:51.000+0200},
  address = {Enschede, The Netherlands},
  author = {Hiemstra, Djoerd},
  biburl = {https://www.bibsonomy.org/bibtex/286044009894a65f77639e053474cb666/vngudivada},
  interhash = {4161da17dd7c84c7a663ebddbd02be4f},
  intrahash = {86044009894a65f77639e053474cb666},
  issn = {1381-3617},
  keywords = {IR LanguageModel},
  month = jun,
  school = {University of Twente},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Using language models for information retrieval},
  year = 2001
}

@article{heinz2015grammatical,
  abstract = {This book provides a thorough introduction to the subfield of theoretical computer science known as grammatical inference from a computational linguistic perspective. Grammatical inference provides principled methods for developing computationally sound algorithms that learn structure from strings of symbols. The relationship to computational linguistics is natural because many research problems in computational linguistics are learning problems on words, phrases, and sentences: What algorithm can take as input some finite amount of data (for instance a corpus, annotated or otherwise) and output a system that behaves "correctly" on specific tasks?

Throughout the text, the key concepts of grammatical inference are interleaved with illustrative examples drawn from problems in computational linguistics. Special attention is paid to the notion of "learning bias." In the context of computational linguistics, such bias can be thought to reflect common (ideally universal) properties of natural languages. This bias can be incorporated either by identifying a learnable class of languages which contains the language to be learned or by using particular strategies for optimizing parameter values. Examples are drawn largely from two linguistic domains (phonology and syntax) which span major regions of the Chomsky Hierarchy (from regular to context-sensitive classes). The conclusion summarizes the major lessons and open questions that grammatical inference brings to computational linguistics.

Table of Contents: List of Figures / List of Tables / Preface / Studying Learning / Formal Learning / Learning Regular Languages / Learning Non-Regular Languages / Lessons Learned and Open Problems / Bibliography / Author Biographies},
  added-at = {2018-06-28T20:41:45.000+0200},
  author = {Heinz, Jeffrey and de la Higuera, Colin and van Zaanen, Menno},
  biburl = {https://www.bibsonomy.org/bibtex/251a081f0f2fbbdde8c524eb6f072aabd/vngudivada},
  doi = {10.2200/S00643ED1V01Y201504HLT028},
  eprint = {https://doi.org/10.2200/S00643ED1V01Y201504HLT028},
  interhash = {2ddf74f92a6704610922f85fdc0513f1},
  intrahash = {51a081f0f2fbbdde8c524eb6f072aabd},
  journal = {Synthesis Lectures on Human Language Technologies},
  keywords = {sys:relevantfor:ecu-cc-research Grammar GrammaticalInference NLP SynthesisLecture},
  number = 4,
  pages = {1-139},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Grammatical Inference for Computational Linguistics},
  volume = 8,
  year = 2015
}

@article{belkin1992information,
  added-at = {2018-05-07T03:22:00.000+0200},
  address = {New York, NY},
  author = {Belkin, Nicholas J. and Croft, W. Bruce},
  biburl = {https://www.bibsonomy.org/bibtex/2b50bebc8a5c685c1aa0697504f1a8494/vngudivada},
  doi = {10.1145/138859.138861},
  interhash = {d248a4e14c0b809a4e914eb0e3ea05c9},
  intrahash = {b50bebc8a5c685c1aa0697504f1a8494},
  journal = {Commun. ACM},
  keywords = {IR InformationFiltering},
  month = dec,
  number = 12,
  pages = {29--38},
  publisher = {ACM},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Information Filtering and Information Retrieval: Two Sides of the Same Coin?},
  volume = 35,
  year = 1992
}

@inproceedings{singhal1996pivoted,
  abstract = {Automatic information retrieval systems have to deal with documents of varying lengths in a text collection. Document length normalization is used to fairly retrieve documents of all lengths. In this study, we observe that a normalization scheme that retrieves documents of all lengths with similar chances as their likelihood of relevance will outperform another scheme which retrieves documents with chances very different from their likelihood of relevance. We show that the retrieval probabilities for a particular normalization method deviate systematically from the relevance probabilities across different collections. We present pivoted normalization, a technique that can be used to modify any normalization function thereby reducing the gap between the relevance and the retrieval probabilities. Training pivoted normalization on one collection, we can successfully use it on other (new) text collections, yielding a robust, collection-independent normalization technique. We use the idea of pivoting with the well known cosine normalization function. We point out some shortcomings of the cosine function and present two new normalization functions -- pivoted unique normalization and pivoted byte size normalization.},
  added-at = {2018-05-12T18:52:58.000+0200},
  address = {New York, NY},
  author = {Singhal, Amit and Buckley, Chris and Mitra, Mandar},
  biburl = {https://www.bibsonomy.org/bibtex/2ad982f80a96d15f586b8aea6c4c3e426/vngudivada},
  booktitle = {Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/243199.243206},
  interhash = {706d19d11915e65e4cafa824f695fc61},
  intrahash = {ad982f80a96d15f586b8aea6c4c3e426},
  isbn = {0-89791-792-8},
  keywords = {IR LengthNormalization},
  numpages = {9},
  pages = {21--29},
  publisher = {ACM},
  series = {SIGIR '96},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Pivoted Document Length Normalization},
  year = 1996
}

@book{grainger2015action,
  abstract = {Solr in Action is a comprehensive guide to implementing scalable search using Apache Solr. This clearly written book walks you through well-documented examples ranging from basic keyword searching to scaling a system for billions of documents and queries. It will give you a deep understanding of how to implement core Solr capabilities.

  Whether you are handling big (or small) data, managing documents, or building a website, it is important to be able to quickly search through your content and discover meaning in it. Apache Solr is your tool: a ready-to-deploy, Lucene-based, open source, full-text search engine. Solr can scale across many servers to enable real-time queries and data analytics across billions of documents.

  Solr in Action teaches you to implement scalable search using Apache Solr. This easy-to-read guide balances conceptual discussions with practical examples to show you how to implement all of Solr's core capabilities. You'll master topics like text analysis, faceted search, hit highlighting, result grouping, query suggestions, multilingual search, advanced geospatial and data operations, and relevancy tuning.

  This book assumes basic knowledge of Java and standard database technology. No prior knowledge of Solr or Lucene is required.

  How to scale Solr for big data
  Rich real-world examples
  Solr as a NoSQL data store
  Advanced multilingual, data, and relevancy tricks
  Coverage of versions through Solr 4.7
  About the Authors

  Trey Grainger is a director of engineering at CareerBuilder. Timothy Potter is a senior member of the engineering team at LucidWorks. The authors work on the scalability and reliability of Solr, as well as on recommendation engine and big data analytics technologies.

  Table of Contents

  PART 1 MEET SOLR
  Introduction to Solr
  Getting to know Solr
  Key Solr concepts
  Configuring Solr
  Indexing
  Text analysis

  PART 2 CORE SOLR CAPABILITIES

  Performing queries and handling results
  Faceted search
  Hit highlighting
  Query suggestions
  Result grouping/field collapsing
  Taking Solr to production

  PART 3 TAKING SOLR TO THE NEXT LEVEL
  SolrCloud
  Multilingual search
  Complex query operations
  Mastering relevancy},
  added-at = {2018-04-01T01:09:27.000+0200},
  address = {Shelter Island, New York},
  author = {Grainger, Trey and Potter, Timothy},
  biburl = {https://www.bibsonomy.org/bibtex/26a77ebc11bb0fe81e4aa5d5bb9b82aaf/vngudivada},
  interhash = {460e02b26829c4017f40ac208652ff77},
  intrahash = {6a77ebc11bb0fe81e4aa5d5bb9b82aaf},
  isbn = {978-1617291029},
  keywords = {Book IR Solr},
  publisher = {Manning},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Solr in Action},
  year = 2015
}

@article{shokouhi2007using,
  abstract = {Users of search engines express their needs as queries, typically consisting of a small number of terms. The resulting search engine query logs are valuable resources that can be used to predict how people interact with the search system. In this paper, we introduce two novel applications of query logs, in the context of distributed information retrieval. First, we use query log terms to guide sampling from uncooperative distributed collections. We show that while our sampling strategy is at least as efficient as current methods, it consistently performs better. Second, we propose and evaluate a pruning strategy that uses query log information to eliminate terms. Our experiments show that our proposed pruning method maintains the accuracy achieved by complete indexes, while decreasing the index size by up to 60%. While such pruning may not always be desirable in practice, it provides a useful benchmark against which other pruning strategies can be measured.},
  added-at = {2018-06-08T20:12:20.000+0200},
  author = {Shokouhi, Milad and Zobel, Justin and Tahaghoghi, Saied and Scholer, Falk},
  biburl = {https://www.bibsonomy.org/bibtex/2e65bfa1590b88309b26d8aa9244df3c9/vngudivada},
  doi = {10.1016/j.ipm.2006.04.003},
  interhash = {2cb4d13119d299ac19f83e432f0ab4b1},
  intrahash = {e65bfa1590b88309b26d8aa9244df3c9},
  journal = {Information Processing {\&} Management},
  keywords = {DistributedSystems IR QueryLogs},
  month = jan,
  number = 1,
  pages = {169--180},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Using query logs to establish vocabularies in distributed information retrieval},
  volume = 43,
  year = 2007
}

@article{wong1995modeling,
  abstract = {This article examines and extends the logical models of information retrieval in the context of probability theory. The fundamental notions of term weights and relevance are given probabilistic interpretations. A unified framework is developed for modeling the retrieval process with probabilistic inference. This new approach provides a common conceptual and mathematical basis for many retrieval models, such as the Boolean, fuzzy set, vector space, and conventional probabilistic models. Within this framework, the underlying assumptions employed by each model are identified, and the inherent relationships between these models are analyzed. Although this article is mainly a theoretical analysis of probabilistic inference for information retrieval, practical methods for estimating the required probabilities are provided by simple examples.},
  added-at = {2018-05-16T02:26:56.000+0200},
  address = {New York, NY},
  author = {Wong, S. K. M. and Yao, Y. Y.},
  biburl = {https://www.bibsonomy.org/bibtex/286e4e636f566c65f18af4861377aad12/vngudivada},
  doi = {10.1145/195705.195713},
  interhash = {249732d021b81bb95cf9521e4348bc07},
  intrahash = {86e4e636f566c65f18af4861377aad12},
  journal = {ACM Trans. Inf. Syst.},
  keywords = {IR Inference ProbabilisticInference},
  month = jan,
  number = 1,
  numpages = {31},
  pages = {38--68},
  publisher = {ACM},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {On Modeling Information Retrieval with Probabilistic Inference},
  volume = 13,
  year = 1995
}

@inproceedings{wong1985generalized,
  abstract = {In information retrieval, it is common to model index terms and documents as vectors in a suitably defined vector space. The main difficulty with this approach is that the explicit representation of term vectors is not known a priori. For this reason, the vector space model adopted by Salton for the SMART system treats the terms as a set of orthogonal vectors. In such a model it is often necessary to adopt a separate, corrective procedure to take into account the correlations between terms. In this paper, we propose a systematic method (the generalized vector space model) to compute term correlations directly from automatic indexing scheme. We also demonstrate how such correlations can be included with minimal modification in the existing vector based information retrieval systems. The preliminary experimental results obtained from the new model are very encouraging.},
  added-at = {2018-05-16T14:16:22.000+0200},
  address = {New York, NY},
  author = {Wong, S. K. M. and Ziarko, Wojciech and Wong, Patrick C. N.},
  biburl = {https://www.bibsonomy.org/bibtex/2b29d756f92dc76456fd9586c3bddd6a3/vngudivada},
  booktitle = {Proceedings of the 8th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/253495.253506},
  interhash = {2142b88ca07eb462ab45e62f73427de7},
  intrahash = {b29d756f92dc76456fd9586c3bddd6a3},
  keywords = {GVSM IRModel VectorSpaceModel},
  pages = {18--25},
  publisher = {ACM},
  series = {SIGIR '85},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Generalized Vector Spaces Model in Information Retrieval},
  year = 1985
}

@article{tunkelang2009faceted,
  abstract = {We live in an information age that requires us, more than ever, to represent, access, and use information. Over the last several decades, we have developed a modern science and technology for information retrieval, relentlessly pursuing the vision of a "memex" that Vannevar Bush proposed in his seminal article, "As We May Think."

Faceted search plays a key role in this program. Faceted search addresses weaknesses of conventional search approaches and has emerged as a foundation for interactive information retrieval. User studies demonstrate that faceted search provides more effective information-seeking support to users than best-first search. Indeed, faceted search has become increasingly prevalent in online information access systems, particularly for e-commerce and site search.

In this lecture, we explore the history, theory, and practice of faceted search. Although we cannot hope to be exhaustive, our aim is to provide sufficient depth and breadth to offer a useful resource to both researchers and practitioners. Because faceted search is an area of interest to computer scientists, information scientists, interface designers, and usability researchers, we do not assume that the reader is a specialist in any of these fields. Rather, we offer a self-contained treatment of the topic, with an extensive bibliography for those who would like to pursue particular aspects in more depth.

Table of Contents: I. Key Concepts / Introduction: What Are Facets? / Information Retrieval / Faceted Information Retrieval / II. Research and Practice / Academic Research / Commercial Applications / III. Practical Concerns / Back-End Concerns / Front-End Concerns / Conclusion / Glossary},
  added-at = {2018-03-19T21:38:53.000+0100},
  author = {Tunkelang, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/28cab24da6556dbfc42059b03d23b26cf/vngudivada},
  doi = {10.2200/S00190ED1V01Y200904ICR005},
  eprint = {https://doi.org/10.2200/S00190ED1V01Y200904ICR005},
  interhash = {85d96bac0efdf2b84d3fca101f3b1432},
  intrahash = {8cab24da6556dbfc42059b03d23b26cf},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {FacetedSearch IR SynthesisLecture},
  number = 1,
  pages = {1 -- 80},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Faceted Search},
  volume = 1,
  year = 2009
}

@article{faerber2017memory,
  abstract = {This article provides an overview of recent developments in mainmemory database systems. With growing memory sizes and memory prices dropping by a factor of 10 every 5 years, data having a “primary home” in memory is now a reality. Main-memory databases eschew many of the traditional architectural pillars of relational database systems that optimized for disk-resident data. The result of these memory-optimized designs are systems that feature several innovative approaches to fundamental issues (e.g., concurrency control, query processing) that achieve orders of magnitude performance improvements over traditional designs. Our survey covers five main issues and architectural choices that need to be made when building a high performance main-memory optimized database: data organization and storage, indexing, concurrency control, durability and recovery techniques, and query processing and compilation. We focus our survey on four commercial and research systems: H-Store/VoltDB, Hekaton, HyPer, and SAP HANA. These systems are diverse in their design choices and form a representative sample of the state of the art in main-memory database systems. We also cover other commercial and academic systems, along with current and future research trends.},
  added-at = {2018-03-30T14:46:41.000+0200},
  author = {Faerber, Franz and Kemper, Alfons and \r{A}ke Larson, Per and Levandoski, Justin and Neumann, Thomas and Pavlo, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/26e3351241e8ab458cb1196fbc30501b7/vngudivada},
  doi = {10.1561/1900000058},
  interhash = {40a2d2ef0b72a6a9ce94590301d8d81e},
  intrahash = {6e3351241e8ab458cb1196fbc30501b7},
  issn = {1931-7883},
  journal = {Foundations and Trends\textregistered in Databases},
  keywords = {DBMS NoSQL},
  number = {1-2},
  pages = {1-130},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Main Memory Database Systems},
  volume = 8,
  year = 2017
}

@article{robertson2010probabilistic,
  abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970 -- 1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modeling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimization for models with free parameters.},
  added-at = {2018-03-29T00:48:27.000+0200},
  author = {Robertson, Stephen},
  biburl = {https://www.bibsonomy.org/bibtex/23000ed9bbe28635b9607b7981fbda07b/vngudivada},
  doi = {10.1561/1500000019},
  interhash = {550af29c4d0083c52afe2de3c8070f76},
  intrahash = {3000ed9bbe28635b9607b7981fbda07b},
  journal = {Foundations and Trends{\textregistered} in Information Retrieval},
  keywords = {BM25 IR},
  number = 4,
  pages = {333- -- 389},
  publisher = {Now Publishers},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {The Probabilistic Relevance Framework: {BM}25 and Beyond},
  volume = 3,
  year = 2010
}

@inproceedings{zhai2001modelbased,
  abstract = {The language modeling approach to retrieval has been shown to perform well empirically. One advantage of this new approach is its statistical foundations. However, feedback, as one important component in a retrieval system, has only been dealt with heuristically in this new retrieval approach: the original query is usually literally expanded by adding additional terms to it. Such expansion-based feedback creates an inconsistent interpretation of the original and the expanded query. In this paper, we present a more principled approach to feedback in the language modeling approach. Specifically, we treat feedback as updating the query language model based on the extra evidence carried by the feedback documents. Such a model-based feedback strategy easily fits into an extension of the language modeling approach. We propose and evaluate two different approaches to updating a query language model based on feedback documents, one based on a generative probabilistic model of feedback documents and one based on minimization of the KL-divergence over feedback documents. Experimental results show that both approaches are effective and outperform the Rocchio feedback approach.},
  added-at = {2018-05-28T23:35:45.000+0200},
  address = {New York, NY},
  author = {Zhai, Chengxiang and Lafferty, John},
  biburl = {https://www.bibsonomy.org/bibtex/2eea0a92e9051330c245dd38b046a7dea/vngudivada},
  booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
  doi = {10.1145/502585.502654},
  interhash = {743d907deb209cc46c998a1d575b886b},
  intrahash = {eea0a92e9051330c245dd38b046a7dea},
  isbn = {1-58113-436-3},
  keywords = {IR RelevanceFeedback},
  pages = {403--410},
  publisher = {ACM},
  series = {CIKM '01},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Model-based Feedback in the Language Modeling Approach to Information Retrieval},
  year = 2001
}

@inproceedings{cui2002probabilistic,
  abstract = {Query expansion has long been suggested as an effective way to resolve the short query and word mismatching problems. A number of query expansion methods have been proposed in traditional information retrieval. However, these previous methods do not take into account the specific characteristics of web searching; in particular, of the availability of large amount of user interaction information recorded in the web query logs. In this study, we propose a new method for query expansion based on query logs. The central idea is to extract probabilistic correlations between query terms and document terms by analyzing query logs. These correlations are then used to select high-quality expansion terms for new queries. The experimental results show that our log-based probabilistic query expansion method can greatly improve the search performance and has several advantages over other existing methods.},
  acmid = {511489},
  added-at = {2018-06-08T20:48:01.000+0200},
  address = {New York, NY},
  author = {Cui, Hang and Wen, Ji-Rong and Nie, Jian-Yun and Ma, Wei-Ying},
  biburl = {https://www.bibsonomy.org/bibtex/2e99c53efcf5b7b906c53d0e7e41f699c/vngudivada},
  booktitle = {Proceedings of the 11th International Conference on World Wide Web},
  doi = {10.1145/511446.511489},
  interhash = {1d29adfcce62c3cd9332d27eaa481578},
  intrahash = {e99c53efcf5b7b906c53d0e7e41f699c},
  isbn = {1-58113-449-5},
  keywords = {IR QueryLogs RelevanceFeedback},
  pages = {325--332},
  publisher = {ACM},
  series = {WWW '02},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Probabilistic Query Expansion Using Query Logs},
  year = 2002
}

@article{zaharia2016apache,
  abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
  acmid = {2934664},
  added-at = {2018-04-08T18:04:49.000+0200},
  address = {New York, NY},
  author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
  biburl = {https://www.bibsonomy.org/bibtex/2472ecf9d5b35733ac9735a7c4509f9a0/vngudivada},
  doi = {10.1145/2934664},
  interhash = {b738cb49f24ffce7c52974ab49a09b7f},
  intrahash = {472ecf9d5b35733ac9735a7c4509f9a0},
  issn = {0001-0782},
  issue_date = {November 2016},
  journal = {Commun. ACM},
  keywords = {BigData NoSQL},
  month = oct,
  number = 11,
  numpages = {10},
  pages = {56 -- 65},
  publisher = {ACM},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Apache Spark: A Unified Engine for Big Data Processing},
  volume = 59,
  year = 2016
}

@book{urma2014action,
  abstract = {Java 8 in Action is a clearly written guide to the new features of Java 8. The book covers lambdas, streams, and functional-style programming. With Java 8's functional features you can now write more concise code in less time, and also automatically benefit from multicore architectures. It's time to dig in!

Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.

About the Book

Every new version of Java is important, but Java 8 is a game changer. Java 8 in Action is a clearly written guide to the new features of Java 8. It begins with a practical introduction to lambdas, using real-world Java code. Next, it covers the new Streams API and shows how you can use it to make collection-based code radically easier to understand and maintain. It also explains other major Java 8 features including default methods, Optional, CompletableFuture, and the new Date and Time API.

This book is written for programmers familiar with Java and basic OO programming.

What's Inside

How to use Java 8's powerful new features
Writing effective multicore-ready applications
Refactoring, testing, and debugging
Adopting functional-style programming
Quizzes and quick-check questions
About the Authors

Raoul-Gabriel Urma is a software engineer, speaker, trainer, and PhD candidate at the University of Cambridge. Mario Fusco is an engineer at Red Hat and creator of the lambdaj library. Alan Mycroft is a professor at Cambridge and cofounder of the Raspberry Pi Foundation.

Table of Contents

PART 1 FUNDAMENTALS
Java 8: why should you care?
Passing code with behavior parameterization
Lambda expressions
PART 2 FUNCTIONAL-STYLE DATA PROCESSING
Introducing streams
Working with streams
Collecting data with streams
Parallel data processing and performance
PART 3 EFFECTIVE JAVA 8 PROGRAMMING
Refactoring, testing, and debugging
Default methods
Using Optional as a better alternative to null
CompletableFuture: composable asynchronousprogramming
New Date and Time API
PART 4 BEYOND JAVA 8
Thinking functionally
Functional programming techniques
Blending OOP and FP: comparing Java 8 and Scala
Conclusions and where next for Java
APPENDIXES
Miscellaneous language updates
Miscellaneous library updates
Performing multiple operations in parallelon a stream
Lambdas and JVM bytecode},
  added-at = {2018-04-15T14:20:25.000+0200},
  author = {Urma, Raoul-Gabriel and Fusco, Mario and Mycroft, Alan},
  biburl = {https://www.bibsonomy.org/bibtex/2d37dd6acf7313aef1acbe4dd2f8a2c69/vngudivada},
  interhash = {56dbdfcd6672e43efdfd446fd619a9a6},
  intrahash = {d37dd6acf7313aef1acbe4dd2f8a2c69},
  keywords = {Book FunctionalProgramming Java},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Java 8 in Action: Lambdas, Streams, and functional-style programming},
  year = 2014
}

@article{lux2013visual,
  abstract = {Visual information retrieval (VIR) is an active and vibrant research area, which attempts at providing means for organizing, indexing, annotating, and retrieving visual information (images and videos) from large, unstructured repositories.

The goal of VIR is to retrieve matches ranked by their relevance to a given query, which is often expressed as an example image and/or a series of keywords. During its early years (1995-2000), the research efforts were dominated by content-based approaches contributed primarily by the image and video processing community. During the past decade, it was widely recognized that the challenges imposed by the lack of coincidence between an image's visual contents and its semantic interpretation, also known as semantic gap, required a clever use of textual metadata (in addition to information extracted from the image's pixel contents) to make image and video retrieval solutions efficient and effective. The need to bridge (or at least narrow) the semantic gap has been one of the driving forces behind current VIR research. Additionally, other related research problems and market opportunities have started to emerge, offering a broad range of exciting problems for computer scientists and engineers to work on.

In this introductory book, we focus on a subset of VIR problems where the media consists of images, and the indexing and retrieval methods are based on the pixel contents of those images -- an approach known as content-based image retrieval (CBIR). We present an implementation-oriented overview of CBIR concepts, techniques, algorithms, and figures of merit. Most chapters are supported by examples written in Java, using Lucene (an open-source Java-based indexing and search implementation) and LIRE (Lucene Image REtrieval), an open-source Java-based library for CBIR.

Table of Contents: Introduction / Information Retrieval: Selected Concepts and Techniques / Visual Features / Indexing Visual Features / LIRE: An Extensible Java CBIR Library / Concluding Remarks

"This well-written, well-illustrated and low-cost book is particularly useful and provides a valuable resource for undergraduate and postgraduate students as well as for young researchers in the field of image processing, pattern recognition and image retrieval. It is enjoyable to read and I strongly recommend it!" -Savvas A. Chatzichristofis},
  added-at = {2018-03-19T22:12:27.000+0100},
  author = {Lux, Mathias and Marques, Oge},
  biburl = {https://www.bibsonomy.org/bibtex/2adb02b7f6630adddcb958dd4922553a4/vngudivada},
  doi = {10.2200/S00468ED1V01Y201301ICR025},
  eprint = {https://doi.org/10.2200/S00468ED1V01Y201301ICR025},
  interhash = {ba932313a2ab2034988c2228e08cd9ba},
  intrahash = {adb02b7f6630adddcb958dd4922553a4},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR SynthesisLecture},
  number = 1,
  pages = {1 -- 112},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Visual Information Retrieval using Java and LIRE},
  volume = 5,
  year = 2013
}

@article{chang2011libsvm,
  abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems, theoretical convergence, multiclass classification probability estimates, and parameter selection are discussed in detail.},
  added-at = {2018-06-09T23:08:34.000+0200},
  address = {New York, NY},
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  biburl = {https://www.bibsonomy.org/bibtex/240736763acad00c36d6d3aefe60bb85d/vngudivada},
  doi = {10.1145/1961189.1961199},
  interhash = {b6d3cf7743c5d218d5bc5f789e3a9984},
  intrahash = {40736763acad00c36d6d3aefe60bb85d},
  issn = {2157-6904},
  journal = {ACM Trans. Intell. Syst. Technol.},
  keywords = {DependencyParsing LIBSVM ML NLP},
  month = may,
  number = 3,
  pages = {27:1--27:27},
  publisher = {ACM},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {{LIBSVM}: A Library for Support Vector Machines},
  volume = 2,
  year = 2011
}

@article{sharma2014brief,
  abstract = {Today, science is passing through an era of transformation, where the inundation of data, dubbed data deluge is influencing the decision making process. The science is driven by the data and is being termed as data science. In this internet age, the volume of the data has grown up to petabytes, and this large, complex, structured or unstructured, and heterogeneous data in the form of “Big Data” has gained significant attention. The rapid pace of data growth through various disparate sources, especially social media such as Facebook, has seriously challenged the data analytic capabilities of traditional relational databases. The velocity of the expansion of the amount of data gives rise to a complete paradigm shift in how new age data is processed. Confidence in the data engineering of the existing data processing systems is gradually fading whereas the capabilities of the new techniques for capturing, storing, visualizing, and analyzing data are evolving. In this review paper, we discuss some of the modern Big Data models that are leading contributors in the NoSQL era and claim to address Big Data challenges in reliable and efficient ways. Also, we take the potential of Big Data into consideration and try to reshape the original operational oriented definition of “Big Science” (Furner, 2003) into a new data-driven definition and rephrase it as “The science that deals with Big Data is Big Science.”},
  added-at = {2018-03-26T02:07:24.000+0200},
  author = {Sharma, Sugam and Tim, Udoyara S and Wong, Johnny and Gadia, Shashi and Sharma, Subhash},
  biburl = {https://www.bibsonomy.org/bibtex/2b80a3ba4fd0b01940db6300ea41e3429/vngudivada},
  doi = {10.2481/dsj.14-041},
  interhash = {de687dc289646e5f1c8cb4352584dd0b},
  intrahash = {b80a3ba4fd0b01940db6300ea41e3429},
  journal = {Data Science Journal},
  keywords = {BigData DataModel NoSQL},
  number = 0,
  pages = {138 -- 157},
  publisher = {Ubiquity Press, Ltd.},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {A Brief Review on Leading Big Data Models},
  volume = 13,
  year = 2014
}

@article{harman2011information,
  abstract = {Evaluation has always played a major role in information retrieval, with the early pioneers such as Cyril Cleverdon and Gerard Salton laying the foundations for most of the evaluation methodologies in use today. The retrieval community has been extremely fortunate to have such a well-grounded evaluation paradigm during a period when most of the human language technologies were just developing. This lecture has the goal of explaining where these evaluation methodologies came from and how they have continued to adapt to the vastly changed environment in the search engine world today.

The lecture starts with a discussion of the early evaluation of information retrieval systems, starting with the Cranfield testing in the early 1960s, continuing with the Lancaster "user" study for MEDLARS, and presenting the various test collection investigations by the SMART project and by groups in Britain. The emphasis in this chapter is on the how and the why of the various methodologies developed. The second chapter covers the more recent batch evaluations, examining the methodologies used in the various open evaluation campaigns such as TREC, NTCIR (emphasis on Asian languages), CLEF (emphasis on European languages), INEX (emphasis on semi-structured data), etc. Here again the focus is on the how and why, and in particular on the evolving of the older evaluation methodologies to handle new information access techniques. This includes how the test collection techniques were modified and how the metrics were changed to better reflect operational environments. The final chapters look at evaluation issues in user studies -- the interactive part of information retrieval, including a look at the search log studies mainly done by the commercial search engines. Here the goal is to show, via case studies, how the high-level issues of experimental design affect the final evaluations.

Table of Contents: Introduction and Early History / "Batch" Evaluation Since 1992 / Interactive Evaluation / Conclusion},
  added-at = {2018-03-19T22:01:04.000+0100},
  author = {Harman, Donna},
  biburl = {https://www.bibsonomy.org/bibtex/2c72c87f884797a0091021b57f646f6a9/vngudivada},
  doi = {10.2200/S00368ED1V01Y201105ICR019},
  eprint = {https://doi.org/10.2200/S00368ED1V01Y201105ICR019},
  interhash = {c331b00a2d16e56767a770cc573eaa77},
  intrahash = {c72c87f884797a0091021b57f646f6a9},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {Evaluation IR SynthesisLecture},
  number = 2,
  pages = {1 -- 119},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Information Retrieval Evaluation},
  volume = 3,
  year = 2011
}

@inproceedings{itakura2010framework,
  abstract = {We evaluate a framework for BM25F-based XML element retrieval. The framework gathers contextual information associated with each XML element into an associated field, which we call a characteristic field. The contents of the element and the contents of the characteristic field are then treated as distinct fields for BM25F weighting purposes. Evidence supporting this framework is drawn from both our own experiments and experiments reported in related work.},
  added-at = {2018-05-12T21:11:30.000+0200},
  address = {New York, NY},
  author = {Itakura, Kelly Y. and Clarke, Charles L.A.},
  biburl = {https://www.bibsonomy.org/bibtex/2274f7d31f3d4c811630ff36ef0251984/vngudivada},
  booktitle = {Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/1835449.1835644},
  interhash = {fb0a551a31c6c9c257e383e624f007af},
  intrahash = {274f7d31f3d4c811630ff36ef0251984},
  isbn = {978-1-4503-0153-4},
  keywords = {BM25 BM25F XMLRetrieval},
  pages = {843--844},
  publisher = {ACM},
  series = {SIGIR '10},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {A Framework for BM25F-based XML Retrieval},
  year = 2010
}

@inproceedings{duartetorres2010query,
  abstract = {In this paper we analyze queries and sessions intended to satisfy children's information needs using a large-scale query log. The aim of this analysis is twofold: i) To identify differences between such queries and sessions, and general queries and sessions; ii) To enhance the query log by including annotations of queries, sessions, and actions for future research on information retrieval for children. We found statistically significant differences between the set of general purpose and queries seeking for content intended for children. We show that our findings are consistent with previous studies on the physical behavior of children using Web search engines.},
  added-at = {2018-06-08T20:24:16.000+0200},
  address = {New York, NY},
  author = {Duarte Torres, Sergio and Hiemstra, Djoerd and Serdyukov, Pavel},
  biburl = {https://www.bibsonomy.org/bibtex/278eaff4059548597066eed8bd975dd37/vngudivada},
  booktitle = {Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/1835449.1835646},
  interhash = {7f7b20d62653771d1f1f647c14d796fe},
  intrahash = {78eaff4059548597066eed8bd975dd37},
  isbn = {978-1-4503-0153-4},
  keywords = {IR QueryLogs},
  pages = {847--848},
  publisher = {ACM},
  series = {SIGIR '10},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Query Log Analysis in the Context of Information Retrieval for Children},
  year = 2010
}

@book{carpenter2011processing,
  added-at = {2018-04-14T03:17:50.000+0200},
  author = {Carpenter, Bob and Morris, Mitzi and Baldwin, Breck},
  biburl = {https://www.bibsonomy.org/bibtex/21130eb6f3045e83a3becfbe637c28089/vngudivada},
  interhash = {f587c0bc9935e1218f41b2e5438454b5},
  intrahash = {1130eb6f3045e83a3becfbe637c28089},
  keywords = {Book NLP TextProcessing},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Text Processing with Java 6},
  year = 2011
}

@article{grefenstette2010searchbased,
  abstract = {We are poised at a major turning point in the history of information management via computers. Recent evolutions in computing, communications, and commerce are fundamentally reshaping the ways in which we humans interact with information, and generating enormous volumes of electronic data along the way. As a result of these forces, what will data management technologies, and their supporting software and system architectures, look like in ten years? It is difficult to say, but we can see the future taking shape now in a new generation of information access platforms that combine strategies and structures of two familiar -- and previously quite distinct -- technologies, search engines and databases, and in a new model for software applications, the Search-Based Application (SBA), which offers a pragmatic way to solve both well-known and emerging information management challenges as of now. Search engines are the world's most familiar and widely deployed information access tool, used by hundreds of millions of people every day to locate information on the Web, but few are aware they can now also be used to provide precise, multidimensional information access and analysis that is hard to distinguish from current database applications, yet endowed with the usability and massive scalability of Web search. In this book, we hope to introduce Search Based Applications to a wider audience, using real case studies to show how this flexible technology can be used to intelligently aggregate large volumes of unstructured data (like Web pages) and structured data (like database content), and to make that data available in a highly contextual, quasi real-time manner to a wide base of users for a varied range of purposes. We also hope to shed light on the general convergences underway in search and database disciplines, convergences that make SBAs possible, and which serve as harbingers of information management paradigms and technologies to come.

Table of Contents: Search Based Applications / Evolving Business Information Access Needs / Origins and Histories / Data Models and Storage / Data Collection/Population / Data Processing / Data Retrieval / Data Security, Usability, Performance, Cost / Summary Evolutions and Convergences / SBA Platforms / SBA Uses and Preconditions / Anatomy of a Search Based Application / Case Study: GEFCO / Case Study: Urbanizer / Case Study: National Postal Agency / Future Directions},
  added-at = {2018-03-19T21:48:15.000+0100},
  author = {Grefenstette, Gregory and Wilber, Laura},
  biburl = {https://www.bibsonomy.org/bibtex/202755793a1bdb7c022294051d3af02c6/vngudivada},
  doi = {10.2200/S00320ED1V01Y201012ICR017},
  eprint = {https://doi.org/10.2200/S00320ED1V01Y201012ICR017},
  interhash = {213bad55eadd6e5902493d649b654802},
  intrahash = {02755793a1bdb7c022294051d3af02c6},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {DBMS IR SynthesisLecture},
  number = 1,
  pages = {1 -- 141},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Search-Based Applications: At the Confluence of Search and Database Technologies},
  volume = 2,
  year = 2010
}

@inproceedings{roelleke2008tfidf,
  abstract = {Interpretations of TF-IDF are based on binary independence retrieval, Poisson, information theory, and language modelling. This paper contributes a review of existing interpretations, and then, TF-IDF is systematically related to the probabilities P(q|d) and P(d|q). Two approaches are explored: a space of independent, and a space of disjoint terms. For independent terms, an "extreme" query/non-query term assumption uncovers TF-IDF, and an analogy of P(d|q) and the probabilistic odds O(r|d, q) mirrors relevance feedback. For disjoint terms, a relationship between probability theory and TF-IDF is established through the integral + 1/x dx = log x. This study uncovers components such as divergence from randomness and pivoted document length to be inherent parts of a document-query independence (DQI) measure, and interestingly, an integral of the DQI over the term occurrence probability leads to TF-IDF.},
  added-at = {2018-06-01T13:10:33.000+0200},
  address = {New York, NY},
  author = {Roelleke, Thomas and Wang, Jun},
  biburl = {https://www.bibsonomy.org/bibtex/25f7c0740ac32cfabe3df5e1127e330d0/vngudivada},
  booktitle = {Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/1390334.1390409},
  interhash = {7230ef2ea93c9abab65d09d20d812523},
  intrahash = {5f7c0740ac32cfabe3df5e1127e330d0},
  isbn = {978-1-60558-164-4},
  keywords = {Probability TF-IDF},
  pages = {435--442},
  publisher = {ACM},
  series = {SIGIR '08},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {TF-IDF Uncovered: A Study of Theories and Probabilities},
  year = 2008
}

@article{liu2017creating,
  abstract = {This book is the first technical overview of autonomous vehicles written for a general computing and engineering audience. The authors share their practical experiences of creating autonomous vehicle systems. These systems are complex, consisting of three major subsystems: (1) algorithms for localization, perception, and planning and control; (2) client systems, such as the robotics operating system and hardware platform; and (3) the cloud platform, which includes data storage, simulation, high-definition (HD) mapping, and deep learning model training. The algorithm subsystem extracts meaningful information from sensor raw data to understand its environment and make decisions about its actions. The client subsystem integrates these algorithms to meet real-time and reliability requirements. The cloud platform provides offline computing and storage capabilities for autonomous vehicles. Using the cloud platform, we are able to test new algorithms and update the HD map—plus, train better recognition, tracking, and decision models.

This book consists of nine chapters. Chapter 1 provides an overview of autonomous vehicle systems; Chapter 2 focuses on localization technologies; Chapter 3 discusses traditional techniques used for perception; Chapter 4 discusses deep learning based techniques for perception; Chapter 5 introduces the planning and control sub-system, especially prediction and routing technologies; Chapter 6 focuses on motion planning and feedback control of the planning and control subsystem; Chapter 7 introduces reinforcement learning-based planning and control; Chapter 8 delves into the details of client systems design; and Chapter 9 provides the details of cloud platforms for autonomous driving.

This book should be useful to students, researchers, and practitioners alike. Whether you are an undergraduate or a graduate student interested in autonomous driving, you will find herein a comprehensive overview of the whole autonomous vehicle technology stack. If you are an autonomous driving practitioner, the many practical techniques introduced in this book will be of interest to you. Researchers will also find plenty of references for an effective, deeper exploration of the various technologies.

Table of Contents: Preface / Introduction to Autonomous Driving / Autonomous Vehicle Localization / Perception in Autonomous Driving / Deep Learning in Autonomous Driving Perception / Prediction and Routing / Decision, Planning, and Control / Reinforcement Learning-based Planning and Control / Client Systems for Autonomous Driving / Cloud Platform for Autonomous Driving / Author Biographies},
  added-at = {2018-03-30T20:55:15.000+0200},
  author = {Liu, Shaoshan and Li, Liyun and Tang, Jie and Wu, Shuang and Gaudiot, Jean-Luc},
  biburl = {https://www.bibsonomy.org/bibtex/20a89241cc9e577201d1f4e40bed6f600/vngudivada},
  doi = {10.2200/S00787ED1V01Y201707CSL009},
  eprint = {https://doi.org/10.2200/S00787ED1V01Y201707CSL009},
  interhash = {b32796d9442d0dca48ef852517e5ad6d},
  intrahash = {0a89241cc9e577201d1f4e40bed6f600},
  journal = {Synthesis Lectures on Computer Science},
  keywords = {AV SelfDriving SynthesisLecture},
  number = 1,
  pages = {1 --186},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Creating Autonomous Vehicle Systems},
  volume = 6,
  year = 2017
}

@book{kleinberg2005algorithm,
  abstract = {Algorithm Design introduces algorithms by looking at the real-world problems that motivate them. The book teaches students a range of design and analysis techniques for problems that arise in computing applications. The text encourages an understanding of the algorithm design process and an appreciation of the role of algorithms in the broader field of computer science.},
  added-at = {2018-05-18T05:47:08.000+0200},
  address = {Boston, Massachusetts},
  author = {Kleinberg, Jon and \'{E}va Tardos},
  biburl = {https://www.bibsonomy.org/bibtex/2ef5f561c20363ae2863c69263c98fb52/vngudivada},
  interhash = {4b144383490ec9accc1c3b497813bdd6},
  intrahash = {ef5f561c20363ae2863c69263c98fb52},
  isbn = {978-0321295354},
  keywords = {Algorithms Book},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Algorithm Design},
  year = 2005
}

@article{wu2008interpreting,
  abstract = {A novel probabilistic retrieval model is presented. It forms a basis to interpret the TF-IDF term weights as making relevance decisions. It simulates the local relevance decision-making for every location of a document, and combines all of these “local” relevance decisions as the ``document-wide'' relevance decision for the document. The significance of interpreting TF-IDF in this way is the potential to: (1) establish a unifying perspective about information retrieval as relevance decision-making; and (2) develop advanced TF-IDF-related term weights for future elaborate retrieval models. Our novel retrieval model is simplified to a basic ranking formula that directly corresponds to the TF-IDF term weights. In general, we show that the term-frequency factor of the ranking formula can be rendered into different term-frequency factors of existing retrieval systems. In the basic ranking formula, the remaining quantity - $\log p(\bar{r} \, \mid \, t \in d)$ is interpreted as the probability of randomly picking a nonrelevant usage (denoted by $\bar{r}$) of term $t$. Mathematically, we show that this quantity can be approximated by the inverse document-frequency (IDF). Empirically, we show that this quantity is related to IDF, using four reference TREC ad hoc retrieval data collections.},
  added-at = {2018-04-23T00:09:49.000+0200},
  address = {New York, NY},
  author = {Wu, Ho Chung and Luk, Robert Wing Pong and Wong, Kam Fai and Kwok, Kui Lam},
  biburl = {https://www.bibsonomy.org/bibtex/2b69703a963a77b1ffa61cd02b2b8c5bf/vngudivada},
  doi = {10.1145/1361684.1361686},
  interhash = {d1af629bb16abac2e3c0b0480be11121},
  intrahash = {b69703a963a77b1ffa61cd02b2b8c5bf},
  issn = {1046-8188},
  journal = {ACM Trans. Inf. Syst.},
  keywords = {IR Relevance TermWeighting},
  month = jun,
  number = 3,
  pages = {13:1--13:37},
  publisher = {ACM},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Interpreting TF-IDF Term Weights As Making Relevance Decisions},
  volume = 26,
  year = 2008
}

@article{jagadish2002timber,
  abstract = {This paper describes the overall design and architecture of the Timber XML database system currently being implemented at the University of Michigan. The system is based upon a bulk algebra for manipulating trees, and natively stores XML. New access methods have been developed to evaluate queries in the XML context, and new cost estimation and query optimization techniques have also been developed. We present performance numbers to support some of our design decisions. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.},
  added-at = {2018-03-26T02:00:35.000+0200},
  author = {Jagadish, H.V. and Al-Khalifa, S. and Chapman, A. and Lakshmanan, L.V.S. and Nierman, A. and Paparizos, S. and Patel, J.M. and Srivastava, D. and Wiwatwattana, N. and Wu, Y. and Yu, C.},
  biburl = {https://www.bibsonomy.org/bibtex/24842080b2c72768ffa08981b77fd80aa/vngudivada},
  day = 01,
  doi = {10.1007/s00778-002-0081-x},
  interhash = {7282275fac360ef2b3ae5c928cfa04f9},
  intrahash = {4842080b2c72768ffa08981b77fd80aa},
  issn = {0949-877X},
  journal = {The VLDB Journal},
  keywords = {NoSQL XML},
  month = dec,
  number = 4,
  pages = {274 -- 291},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {TIMBER: A native XML database},
  volume = 11,
  year = 2002
}

@book{salton1983introduction,
  added-at = {2018-03-31T15:53:14.000+0200},
  address = {New York, NY},
  author = {Salton, Gerard},
  biburl = {https://www.bibsonomy.org/bibtex/262c46a7e19b9915111475c2d1e751f10/vngudivada},
  interhash = {b00328632b65900b40f36c2410d41b69},
  intrahash = {62c46a7e19b9915111475c2d1e751f10},
  isbn = {978-0070544840},
  keywords = {Book IR},
  publisher = {McGraw-Hill},
  series = {McGraw Hill Computer Science Series},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Introduction to Modern Information Retrieval},
  year = 1983
}

@article{carmel2010estimating,
  abstract = {Many information retrieval (IR) systems suffer from a radical variance in performance when responding to users' queries. Even for systems that succeed very well on average, the quality of results returned for some of the queries is poor. Thus, it is desirable that IR systems will be able to identify "difficult" queries so they can be handled properly. Understanding why some queries are inherently more difficult than others is essential for IR, and a good answer to this important question will help search engines to reduce the variance in performance, hence better servicing their customer needs. Estimating the query difficulty is an attempt to quantify the quality of search results retrieved for a query from a given collection of documents. This book discusses the reasons that cause search engines to fail for some of the queries, and then reviews recent approaches for estimating query difficulty in the IR field. It then describes a common methodology for evaluating the prediction quality of those estimators, and experiments with some of the predictors applied by various IR methods over several TREC benchmarks. Finally, it discusses potential applications that can utilize query difficulty estimators by handling each query individually and selectively, based upon its estimated difficulty.

Table of Contents: Introduction - The Robustness Problem of Information Retrieval / Basic Concepts / Query Performance Prediction Methods / Pre-Retrieval Prediction Methods / Post-Retrieval Prediction Methods / Combining Predictors / A General Model for Query Difficulty / Applications of Query Difficulty Estimation / Summary and Conclusions},
  added-at = {2018-03-19T21:45:00.000+0100},
  author = {Carmel, David and Yom-Tov, Elad},
  biburl = {https://www.bibsonomy.org/bibtex/2afdc03911742f5b47426f75185391de1/vngudivada},
  doi = {10.2200/S00235ED1V01Y201004ICR015},
  eprint = {https://doi.org/10.2200/S00235ED1V01Y201004ICR015},
  interhash = {72748faa1a3e0b09fe9154a271bcab62},
  intrahash = {afdc03911742f5b47426f75185391de1},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR SynthesisLecture},
  number = 1,
  pages = {1 -- 89},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Estimating the Query Difficulty for Information Retrieval},
  volume = 2,
  year = 2010
}

@inproceedings{choi2015depends,
  abstract = {The last few years have seen a surge in the number of accurate, fast, publicly available dependency parsers. At the same time, the use of dependency parsing in NLP applications has increased. It can be difficult for a non-expert to select a good ``off-the-shelf" parser. We present a comparative analysis of ten leading statistical dependency parsers on a multi-genre corpus of English. For our analysis, we developed a new web-based tool that gives a convenient way of comparing dependency parser outputs. Our analysis will help practitioners choose a parser to optimize their desired speed/accuracy tradeoff, and our tool will help practitioners examine and compare parser output.},
  added-at = {2018-06-20T00:31:36.000+0200},
  address = {Stroudsburg, Pennsylvania},
  author = {Choi, Jinho D and Tetreault, Joel R and Stent, Amanda},
  biburl = {https://www.bibsonomy.org/bibtex/2532673876d8530cb2c81e36e746c6707/vngudivada},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
  interhash = {ad23159f859dd87f6aea137035f7ace0},
  intrahash = {532673876d8530cb2c81e36e746c6707},
  keywords = {DependencyParsing NLP},
  pages = {387--396},
  publisher = {The Association for Computer Linguistics},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {It Depends: Dependency Parser Comparison Using A Web-based Evaluation Tool},
  year = 2015
}

@inproceedings{hersh1994ohsumed,
  added-at = {2018-05-10T05:51:42.000+0200},
  address = {New York, NY},
  author = {Hersh, William and Buckley, Chris and Leone, T.J. and Hickam, David},
  biburl = {https://www.bibsonomy.org/bibtex/2eb6abe0e732556c71c129a1413fddc3a/vngudivada},
  booktitle = {Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  interhash = {a8dde79aedeb459a2f8dfce8ccc4270d},
  intrahash = {eb6abe0e732556c71c129a1413fddc3a},
  isbn = {0-387-19889-X},
  keywords = {Evaluation IR OHSUMED},
  pages = {192--201},
  publisher = {Springer-Verlag},
  series = {SIGIR '94},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {OHSUMED: An Interactive Retrieval Evaluation and New Large Test Collection for Research},
  year = 1994
}

@article{lampa2016towards,
  abstract = {Predictive modelling in drug discovery is challenging to automate as it often contains multiple analysis steps and might involve cross-validation and parameter tuning that create complex dependencies between tasks. With large-scale data or when using computationally demanding modelling methods, e-infrastructures such as high-performance or cloud computing are required, adding to the existing challenges of fault-tolerant automation. Workflow management systems can aid in many of these challenges, but the currently available systems are lacking in the functionality needed to enable agile and flexible predictive modelling. We here present an approach inspired by elements of the flow-based programming paradigm, implemented as an extension of the Luigi system which we name SciLuigi. We also discuss the experiences from using the approach when modellng a large set of biochemical interactions using a shared computer cluster.},
  added-at = {2018-06-09T23:17:27.000+0200},
  author = {Lampa, Samuel and Alvarsson, Jonathan and Spjuth, Ola},
  biburl = {https://www.bibsonomy.org/bibtex/2621faf692bf754820456de94b8bca437/vngudivada},
  doi = {10.1186/s13321-016-0179-6},
  interhash = {01693b164287a83db4c0474ad3870eb3},
  intrahash = {621faf692bf754820456de94b8bca437},
  journal = {Journal of Cheminformatics},
  keywords = {DrugDiscovery ML PredictiveModeling},
  month = nov,
  number = 1,
  publisher = {Springer Nature},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Towards agile large-scale predictive modelling in drug discovery with flow-based programming design principles},
  volume = 8,
  year = 2016
}

@article{chen2013information,
  abstract = {Research on social networks has exploded over the last decade. To a large extent, this has been fueled by the spectacular growth of social media and online social networking sites, which continue growing at a very fast pace, as well as by the increasing availability of very large social network datasets for purposes of research. A rich body of this research has been devoted to the analysis of the propagation of information, influence, innovations, infections, practices and customs through networks. Can we build models to explain the way these propagations occur? How can we validate our models against any available real datasets consisting of a social network and propagation traces that occurred in the past? These are just some questions studied by researchers in this area. Information propagation models find applications in viral marketing, outbreak detection, finding key blog posts to read in order to catch important stories, finding leaders or trendsetters, information feed ranking, etc. A number of algorithmic problems arising in these applications have been abstracted and studied extensively by researchers under the garb of influence maximization.

This book starts with a detailed description of well-established diffusion models, including the independent cascade model and the linear threshold model, that have been successful at explaining propagation phenomena. We describe their properties as well as numerous extensions to them, introducing aspects such as competition, budget, and time-criticality, among many others. We delve deep into the key problem of influence maximization, which selects key individuals to activate in order to influence a large fraction of a network. Influence maximization in classic diffusion models including both the independent cascade and the linear threshold models is computationally intractable, more precisely NP-hard, and we describe several approximation algorithms and scalable heuristics that have been proposed in the literature. Finally, we also deal with key issues that need to be tackled in order to turn this research into practice, such as learning the strength with which individuals in a network influence each other, as well as the practical aspects of this research including the availability of datasets and software tools for facilitating research. We conclude with a discussion of various research problems that remain open, both from a technical perspective and from the viewpoint of transferring the results of research into industry strength applications.

Table of Contents: Acknowledgments / Introduction / Stochastic Diffusion Models / Influence Maximization / Extensions to Diffusion Modeling and Influence Maximization / Learning Propagation Models / Data and Software for Information/Influence: Propagation Research / Conclusion and Challenges / Bibliography / Authors' Biographies / Index},
  added-at = {2018-03-19T23:45:55.000+0100},
  author = {Chen, Wei and Lakshmanan, Laks V.S. and Castillo, Carlos},
  biburl = {https://www.bibsonomy.org/bibtex/2e2fbc26fb5cbaffa39d17cc69f700fbd/vngudivada},
  doi = {10.2200/S00527ED1V01Y201308DTM037},
  eprint = {https://doi.org/10.2200/S00527ED1V01Y201308DTM037},
  interhash = {884a0521abec94a6022ae1578f2810c5},
  intrahash = {e2fbc26fb5cbaffa39d17cc69f700fbd},
  journal = {Synthesis Lectures on Data Management},
  keywords = {SNA SynthesisLecture},
  number = 4,
  pages = {1 -- 177},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Information and Influence Propagation in Social Networks},
  volume = 5,
  year = 2013
}

@article{bing2018learning,
  abstract = {In the procedure of Web search, a user first comes up with an information need and a query is issued with the need as guidance. After that, some URLs are clicked and other queries may be issued if those URLs do not meet his need well. We advocate that Web search is governed by a unified hidden space, and each involved element such as query and URL has its inborn position, i.e., projected as a vector, in this space. Each of above actions in the search procedure, i.e. issuing queries or clicking URLs, is an interaction result of those elements in the space. In this paper, we aim at uncovering such a unified hidden space of Web search that uniformly captures the hidden semantics of search queries, URLs and other involved elements in Web search. We learn the semantic space with search session data, because a search session can be regarded as an instantiation of users’ information need on a particular semantic topic and it keeps the interaction information of queries and URLs. We use a set of session graphs to represent search sessions, and the space learning task is cast as a vector learning problem for the graph vertices by maximizing the log-likelihood of a training session data set. Specifically, we developed the well-known Word2vec to perform the learning procedure. Experiments on the query log data of a commercial search engine are conducted to examine the efficacy of learnt vectors, and the results show that our framework is helpful for different finer tasks in Web search.},
  added-at = {2018-06-08T20:22:08.000+0200},
  author = {Bing, Lidong and Niu, Zheng-Yu and Li, Piji and Lam, Wai and Wang, Haifeng},
  biburl = {https://www.bibsonomy.org/bibtex/26a83242d13a94a0139e707c0a8e035fb/vngudivada},
  doi = {10.1016/j.knosys.2018.02.037},
  interhash = {7e5965b296234f75a6698ff7acb8863a},
  intrahash = {6a83242d13a94a0139e707c0a8e035fb},
  issn = {0950-7051},
  journal = {Knowledge-Based Systems},
  keywords = {IR QueryLogs},
  pages = {38 - 48},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Learning a unified embedding space of web search from large-scale query log},
  volume = 150,
  year = 2018
}

@inproceedings{blanco2012extending,
  abstract = {Traditional probabilistic relevance frameworks for informational retrieval refrain from taking positional information into account, due to the hurdles of developing a sound model while avoiding an explosion in the number of parameters. Nonetheless, the well-known BM25F extension of the successful Okapi ranking function can be seen as an embryonic attempt in that direction. In this paper, we proceed along the same line, defining the notion of virtual region: a virtual region is a part of the document that, like a BM25F-field, can provide a (larger or smaller, depending on a tunable weighting parameter) evidence of relevance of the document; differently from BM25F fields, though, virtual regions are generated implicitly by applying suitable (usually, but not necessarily, positional-aware) operators to the query. This technique fits nicely in the eliteness model behind BM25 and provides a principled explanation to BM25F; it specializes to BM25(F) for some trivial operators, but has a much more general appeal. Our experiments (both on standard collections, such as TREC, and on Web-like repertoires) show that the use of virtual regions is beneficial for retrieval effectiveness.},
  added-at = {2018-05-12T21:02:23.000+0200},
  address = {New York, NY},
  author = {Blanco, Roi and Boldi, Paolo},
  biburl = {https://www.bibsonomy.org/bibtex/2ff4f5b4f2a8f6127bcb4dda381806545/vngudivada},
  booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/2348283.2348406},
  interhash = {b910ffb108d9da9562deca85c844b7e9},
  intrahash = {ff4f5b4f2a8f6127bcb4dda381806545},
  isbn = {978-1-4503-1472-5},
  keywords = {BM25 IR},
  pages = {921--930},
  publisher = {ACM},
  series = {SIGIR '12},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Extending BM25 with Multiple Query Operators},
  year = 2012
}

@article{jones2013transforming,
  abstract = {With its theme, "Our Information, Always and Forever," Part I of this book covers the basics of personal information management (PIM) including six essential activities of PIM and six (different) ways in which information can be personal to us. Part I then goes on to explore key issues that arise in the "great migration" of our information onto the Web and into a myriad of mobile devices.

Part II provides a more focused look at technologies for managing information that promise to profoundly alter our practices of PIM and, through these practices, the way we lead our lives.

Part II is in five chapters:

- Chapter 5. Technologies of Input and Output. Technologies in support of gesture, touch, voice, and even eye movements combine to support a more natural user interface (NUI). Technologies of output include glasses and "watch" watches. Output will also increasingly be animated with options to "zoom".

- Chapter 6. Technologies to Save Our Information. We can opt for "life logs" to record our experiences with increasing fidelity. What will we use these logs for? And what isn’t recorded that should be?

- Chapter 7. Technologies to Search Our Information. The potential for personalized search is enormous and mostly yet to be realized. Persistent searches, situated in our information landscape, will allow us to maintain a diversity of projects and areas of interest without a need to continually switch from one to another to handle incoming information.

- Chapter 8. Technologies to Structure Our Information. Structure is key if we are to keep, find, and make effective use of our information. But how best to structure? And how best to share structured information between the applications we use, with other people, and also with ourselves over time? What lessons can we draw from the failures and successes in web-based efforts to share structure?

- Chapter 9. PIM Transformed and Transforming: Stories from the Past, Present and Future. Part II concludes with a comparison between Licklider’s world of information in 1957 and our own world of information today. And then we consider what the world of information is likely to look like in 2057. Licklider estimated that he spent 85% of his "thinking time" in activities that were clerical and mechanical and might (someday) be delegated to the computer. What percentage of our own time is spent with the clerical and mechanical? What about in 2057?

Table of Contents: Technologies of Input and Output / Technologies to Save Our Information / Technologies to Search Our Information / Technologies to Structure Our Information / PIM Transformed and Transforming: Stories from the Past, Present, and Future

Snippet from a review on Amazon.com by John V. Levy:

"Prof. Jones' style & tone is very accessible and pleasant. The content is excellent and should find a wide audience among non-specialists as well as would-be specialists. And I hope that this book helps to encourage more specialists to enter this important new field."},
  added-at = {2018-03-19T22:21:34.000+0100},
  author = {Jones, William},
  biburl = {https://www.bibsonomy.org/bibtex/2130d92a967595cd9e042de8259021b3c/vngudivada},
  doi = {10.2200/S00532ED1V01Y201308ICR028},
  eprint = {https://doi.org/10.2200/S00532ED1V01Y201308ICR028},
  interhash = {78250b2571e65e2a732acb56e61b03a5},
  intrahash = {130d92a967595cd9e042de8259021b3c},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR PersonalInformationManagement SynthesisLecture},
  number = 4,
  pages = {1 -- 179},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Transforming Technologies to Manage Our Information: The Future of Personal Information Management, Part II},
  volume = 5,
  year = 2013
}

@article{li2014learning,
  abstract = {Learning to rank refers to machine learning techniques for training a model in a ranking task. Learning to rank is useful for many applications in information retrieval, natural language processing, and data mining. Intensive studies have been conducted on its problems recently, and significant progress has been made. This lecture gives an introduction to the area including the fundamental problems, major approaches, theories, applications, and future work.

The author begins by showing that various ranking problems in information retrieval and natural language processing can be formalized as two basic ranking tasks, namely ranking creation (or simply ranking) and ranking aggregation. In ranking creation, given a request, one wants to generate a ranking list of offerings based on the features derived from the request and the offerings. In ranking aggregation, given a request, as well as a number of ranking lists of offerings, one wants to generate a new ranking list of the offerings.

Ranking creation (or ranking) is the major problem in learning to rank. It is usually formalized as a supervised learning task. The author gives detailed explanations on learning for ranking creation and ranking aggregation, including training and testing, evaluation, feature creation, and major approaches. Many methods have been proposed for ranking creation. The methods can be categorized as the pointwise, pairwise, and listwise approaches according to the loss functions they employ. They can also be categorized according to the techniques they employ, such as the SVM based, Boosting based, and Neural Network based approaches.

The author also introduces some popular learning to rank methods in details. These include: PRank, OC SVM, McRank, Ranking SVM, IR SVM, GBRank, RankNet, ListNet & ListMLE, AdaRank, SVM MAP, SoftRank, LambdaRank, LambdaMART, Borda Count, Markov Chain, and CRanking.

The author explains several example applications of learning to rank including web search, collaborative filtering, definition search, keyphrase extraction, query dependent summarization, and re-ranking in machine translation.

A formulation of learning for ranking creation is given in the statistical learning framework. Ongoing and future research directions for learning to rank are also discussed.

Table of Contents: Learning to Rank / Learning for Ranking Creation / Learning for Ranking Aggregation / Methods of Learning to Rank / Applications of Learning to Rank / Theory of Learning to Rank / Ongoing and Future Work},
  added-at = {2018-03-30T19:07:21.000+0200},
  author = {Li, Hang},
  biburl = {https://www.bibsonomy.org/bibtex/2b9dbbeab271cea1bff37e3c615e829c5/vngudivada},
  doi = {10.2200/S00607ED2V01Y201410HLT026},
  eprint = {https://doi.org/10.2200/S00607ED2V01Y201410HLT026},
  interhash = {b3fac0b636d53973a93afa4d9e89af6c},
  intrahash = {b9dbbeab271cea1bff37e3c615e829c5},
  journal = {Synthesis Lectures on Human Language Technologies},
  keywords = {IR Ranking SynthesisLecture},
  number = 3,
  pages = {1 -- 121},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Learning to Rank for Information Retrieval and Natural Language Processing, Second Edition},
  volume = 7,
  year = 2014
}

@book{perkins2018seven,
  abstract = {Data is getting bigger and more complex by the day, and so are your choices in handling it. Explore some of the most cutting-edge databases available - from traditional relational databases to newer NoSQL approaches - and make informed decisions about challenging data storage problems. This is the only comprehensive guide to the world of NoSQL databases, with in-depth practical and conceptual introductions to seven different technologies: Redis, Neo4J, CouchDB, MongoDB, HBase, Postgres, and DynamoDB. This second edition includes a new chapter on DynamoDB and updated content for each chapter.

While relational databases such as MySQL remain as relevant as ever, the alternative, NoSQL paradigm has opened up new horizons in performance and scalability and changed the way we approach data-centric problems. This book presents the essential concepts behind each database alongside hands-on examples that make each technology come alive.

With each database, tackle a real-world problem that highlights the concepts and features that make it shine. Along the way, explore five database models - relational, key/value, columnar, document, and graph - from the perspective of challenges faced by real applications. Learn how MongoDB and CouchDB are strikingly different, make your applications faster with Redis and more connected with Neo4J, build a cluster of HBase servers using cloud services such as Amazon's Elastic MapReduce, and more. This new edition brings a brand new chapter on DynamoDB, updated code samples and exercises, and a more up-to-date account of each database's feature set.

Whether you're a programmer building the next big thing, a data scientist seeking solutions to thorny problems, or a technology enthusiast venturing into new territory, you will find something to inspire you in this book.

What You Need:

You'll need a *nix shell (Mac OS or Linux preferred, Windows users will need Cygwin), Java 6 (or greater), and Ruby 1.8.7 (or greater). Each chapter will list the downloads required for that database.},
  added-at = {2018-03-26T18:38:59.000+0200},
  address = {Raleigh, North Carolina},
  author = {Perkins, Luc and Redmond, Eric and Wilson, Jim},
  biburl = {https://www.bibsonomy.org/bibtex/22c01c6c7df1aed5db5eb05a77a45fc80/vngudivada},
  edition = {Second},
  interhash = {e523aadb66335a81c3d51db7e73ba2bf},
  intrahash = {2c01c6c7df1aed5db5eb05a77a45fc80},
  isbn = {978-1680502534},
  keywords = {Book NoSQL ParExcellence},
  publisher = {Pragmatic Bookshelf},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Seven Databases in Seven Weeks: A Guide to Modern Databases and the NoSQL Movement},
  year = 2018
}

@article{raynal2010communication,
  abstract = {Understanding distributed computing is not an easy task. This is due to the many facets of uncertainty one has to cope with and master in order to produce correct distributed software. Considering the uncertainty created by asynchrony and process crash failures in the context of message-passing systems, the book focuses on the main abstractions that one has to understand and master in order to be able to produce software with guaranteed properties. These fundamental abstractions are communication abstractions that allow the processes to communicate consistently (namely the register abstraction and the reliable broadcast abstraction), and the consensus agreement abstractions that allows them to cooperate despite failures. As they give a precise meaning to the words "communicate" and "agree" despite asynchrony and failures, these abstractions allow distributed programs to be designed with properties that can be stated and proved.

Impossibility results are associated with these abstractions. Hence, in order to circumvent these impossibilities, the book relies on the failure detector approach, and, consequently, that approach to fault-tolerance is central to the book.

Table of Contents: List of Figures / The Atomic Register Abstraction / Implementing an Atomic Register in a Crash-Prone Asynchronous System / The Uniform Reliable Broadcast Abstraction / Uniform Reliable Broadcast Abstraction Despite Unreliable Channels / The Consensus Abstraction / Consensus Algorithms for Asynchronous Systems Enriched with Various Failure Detectors / Constructing Failure Detectors},
  added-at = {2018-03-26T16:33:28.000+0200},
  author = {Raynal, Michel},
  biburl = {https://www.bibsonomy.org/bibtex/245fb8830f7411ceec6f9008a6e795d37/vngudivada},
  doi = {10.2200/S00236ED1V01Y201004DCT002},
  eprint = {https://doi.org/10.2200/S00236ED1V01Y201004DCT002},
  interhash = {0735fdeb2b9cbe26d3ce646bfe124154},
  intrahash = {45fb8830f7411ceec6f9008a6e795d37},
  journal = {Synthesis Lectures on Distributed Computing Theory},
  keywords = {DistributedComputing},
  number = 1,
  pages = {1 -- 273},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Communication and Agreement Abstractions for Fault-Tolerant Asynchronous Distributed Systems},
  volume = 1,
  year = 2010
}

@article{strohmaier2012acquiring,
  abstract = {A better understanding of what motivates humans to perform certain actions is relevant for a range of research challenges including generating action sequences that implement goals (planning). A first step in this direction is the task of acquiring knowledge about human goals. In this work, we investigate whether Search Query Logs are a viable source for extracting expressions of human goals. For this purpose, we devise an algorithm that automatically identifies queries containing explicit goals such as find home to rent in Florida. Evaluation results of our algorithm achieve useful precision/recall values. We apply the classification algorithm to two large Search Query Logs, recorded by AOL and Microsoft Research in 2006, and obtain a set of ∼110,000 queries containing explicit goals. To study the nature of human goals in Search Query Logs, we conduct qualitative, quantitative and comparative analyses. Our findings suggest that Search Query Logs (i) represent a viable source for extracting human goals, (ii) contain a great variety of human goals and (iii) contain human goals that can be employed to complement existing commonsense knowledge bases. Finally, we illustrate the potential of goal knowledge for addressing following application scenario: to refine and extend commonsense knowledge with human goals from Search Query Logs. This work is relevant for (i) knowledge engineers interested in acquiring human goals from textual corpora and constructing knowledge bases of human goals (ii) researchers interested in studying characteristics of human goals in Search Query Logs.},
  added-at = {2018-06-08T20:43:45.000+0200},
  author = {Strohmaier, Markus and Kr\"{o}ll, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/2b31361f686786c427203d2b82afb7757/vngudivada},
  doi = {10.1016/j.ipm.2011.03.010},
  interhash = {c28e180728dced912a1eace0d954d2a2},
  intrahash = {b31361f686786c427203d2b82afb7757},
  issn = {0306-4573},
  journal = {Information Processing \& Management},
  keywords = {IR QueryLogs},
  number = 1,
  pages = {63 - 82},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Acquiring knowledge about human goals from Search Query Logs},
  volume = 48,
  year = 2012
}

@inproceedings{lam2015combining,
  abstract = {Bug localization refers to the automated process of locating the potential buggy files for a given bug report. To help developers focus their attention to those files is crucial. Several existing automated approaches for bug localization from a bug report face a key challenge, called lexical mismatch, in which the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. This paper presents a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files and documentation if they appear frequently enough in the pairs of reports and buggy files. Our empirical evaluation on real-world projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, HyLoc, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it is correct with just a single suggested file. Two out of three cases, a correct buggy file is in the list of three suggested files.},
  added-at = {2018-06-03T01:30:08.000+0200},
  author = {Lam, A. N. and Nguyen, A. T. and Nguyen, H. A. and Nguyen, T. N.},
  biburl = {https://www.bibsonomy.org/bibtex/21fb98db617e8d18f76119d83dfcc880a/vngudivada},
  booktitle = {30th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  doi = {10.1109/ASE.2015.73},
  interhash = {b68f05426d5c1aeb642f04d7fabb91d9},
  intrahash = {1fb98db617e8d18f76119d83dfcc880a},
  keywords = {DeepLearning IR},
  month = nov,
  pages = {476--481},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Combining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports (N)},
  year = 2015
}

@article{salton1988termweighting,
  abstract = {The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective term weighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.},
  added-at = {2018-05-20T05:49:47.000+0200},
  author = {Salton, G. and Buckley, C.},
  biburl = {https://www.bibsonomy.org/bibtex/208a9c2eb7210091eb3c198142025d232/vngudivada},
  interhash = {4605fc5d206bd5cbc617c560584406bb},
  intrahash = {08a9c2eb7210091eb3c198142025d232},
  journal = {Information Processing \& Management},
  keywords = {IR TermWeighting},
  number = 5,
  pages = {513--523},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Term-weighting approaches in automatic text retrieval},
  volume = 24,
  year = 1988
}

@book{karwin2010antipatterns,
  abstract = {Bill Karwin has helped thousands of people write better SQL and build stronger relational databases. Now he's sharing his collection of antipatterns--the most common errors he's identified in those thousands of requests for help.

Most developers aren't SQL experts, and most of the SQL that gets used is inefficient, hard to maintain, and sometimes just plain wrong. This book shows you all the common mistakes, and then leads you through the best fixes. What's more, it shows you what's behind these fixes, so you'll learn a lot about relational databases along the way.

Each chapter in this book helps you identify, explain, and correct a unique and dangerous antipattern. The four parts of the book group the anti​patterns in terms of logical database design, physical database design, queries, and application development.

The chances are good that your application's database layer already contains problems such as Index Shotgun, Keyless Entry, Fear of the Unknown, and Spaghetti Query. This book will help you and your team find them. Even better, it will also show you how to fix them, and how to avoid these and other problems in the future.

SQL Antipatterns gives you a rare glimpse into an SQL expert's playbook. Now you can stamp out these common database errors once and for all.

Whatever platform or programming language you use, whether you're a junior programmer or a Ph.D., SQL Antipatterns will show you how to design and build databases, how to write better database queries, and how to integrate SQL programming with your application like an expert. You'll also learn the best and most current technology for full-text search, how to design code that is resistant to SQL injection attacks, and other techniques for success.},
  added-at = {2018-03-26T18:42:48.000+0200},
  address = {Raleigh, North Carolina},
  author = {Karwin, Bill},
  biburl = {https://www.bibsonomy.org/bibtex/27a895ab22eda11b5ee8fd6c0e3c3d8f2/vngudivada},
  interhash = {eef20df1d0d5ce4f99d3a4cb97dc63a3},
  intrahash = {7a895ab22eda11b5ee8fd6c0e3c3d8f2},
  isbn = {978-1934356555},
  keywords = {Book SQL},
  publisher = {Pragmatic Bookshelf},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {SQL Antipatterns: Avoiding the Pitfalls of Database Programming},
  year = 2010
}

@inproceedings{amigo2017axiomatic,
  abstract = {This is the first workshop on the emerging interdisciplinary research area of applying axiomatic thinking to information retrieval (IR) and related tasks. The workshop aims to help foster collaboration of researchers working on different perspectives of axiomatic thinking and encourage discussion and research on general methodological issues related to applying axiomatic thinking to IR and related tasks.},
  acmid = {3084369},
  added-at = {2018-05-19T14:53:50.000+0200},
  address = {New York, NY},
  author = {Amigo, Enrique and Fang, Hui and Mizzaro, Stefano and Zhai, ChengXiang},
  biburl = {https://www.bibsonomy.org/bibtex/20b753d48e72fa1321475c962ff6aafbc/vngudivada},
  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/3077136.3084369},
  interhash = {d40c0d4121888d5ac492a479bf7c3cc0},
  intrahash = {0b753d48e72fa1321475c962ff6aafbc},
  isbn = {978-1-4503-5022-8},
  keywords = {AxiomaticThinking IR IRModel},
  location = {Shinjuku, Tokyo, Japan},
  numpages = {2},
  pages = {1419--1420},
  publisher = {ACM},
  series = {SIGIR '17},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Axiomatic Thinking for Information Retrieval: And Related Tasks},
  year = 2017
}

@inproceedings{perezaguera2010using,
  abstract = {Information Retrieval (IR) approaches for semantic web search engines have become very populars in the last years. Popularization of different IR libraries, like Lucene, that allows IR implementations almost out-of-the-box have make easier IR integration in Semantic Web search engines. However, one of the most important features of Semantic Web documents is the structure, since this structure allow us to represent semantic in a machine readable format. In this paper we analyze the specific problems of structured IR and how to adapt weighting schemas for semantic document retrieval.},
  added-at = {2018-05-12T21:07:10.000+0200},
  address = {New York, NY},
  author = {P\'{e}rez-Ag\"{u}era, Jos\'{e} R. and Arroyo, Javier and Greenberg, Jane and Iglesias, Joaquin Perez and Fresno, Victor},
  biburl = {https://www.bibsonomy.org/bibtex/29770fc6e004c4f4ec0b931978956a7ef/vngudivada},
  booktitle = {Proceedings of the 3rd International Semantic Search Workshop},
  doi = {10.1145/1863879.1863881},
  interhash = {2c11ceaf4fd520196fa3817aca10a278},
  intrahash = {9770fc6e004c4f4ec0b931978956a7ef},
  isbn = {978-1-4503-0130-5},
  keywords = {BM25F IR SemanticSearch},
  pages = {2:1--2:8},
  publisher = {ACM},
  series = {SEMSEARCH '10},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Using BM25F for Semantic Search},
  year = 2010
}

@article{qiao2017finding,
  abstract = {This study has proposed a topic based competitive keywords suggestion method called TCK to enhance search engine advertising. On the basis of query logs, the method explores the indirect associations between keywords and extracts the hidden topic information to identify competitive keywords. It can help advertisers not only broaden the choices of keywords but also carry out a competitive strategy for search engine advertising. Extensive experiments have been conducted to demonstrate the effectiveness of the proposed method. Results prove that the proposed method performs better than existing keyword suggestion methods, contributing greatly to the keyword suggestion advertising market.},
  added-at = {2018-06-08T20:34:31.000+0200},
  author = {Qiao, Dandan and Zhang, Jin and Wei, Qiang and Chen, Guoqing},
  biburl = {https://www.bibsonomy.org/bibtex/2dbe395e8bdec0e148085d0fc43fd3a24/vngudivada},
  doi = {h10.1016/j.im.2016.11.003},
  interhash = {0d150b1c6283aef7b25ef6e27bd766eb},
  intrahash = {dbe395e8bdec0e148085d0fc43fd3a24},
  issn = {0378-7206},
  journal = {Information \& Management},
  keywords = {IR QueryLogs},
  number = 4,
  pages = {531 - 543},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Finding competitive keywords from query logs to enhance search engine advertising},
  volume = 54,
  year = 2017
}

@inproceedings{manning2014stanford,
  abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
  added-at = {2018-06-09T14:34:44.000+0200},
  address = {Stroudsburg, Pennsylvania},
  author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
  biburl = {https://www.bibsonomy.org/bibtex/2be8b61313ff90c0434f668f5431d3349/vngudivada},
  booktitle = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  doi = {10.3115/v1/p14-5010},
  interhash = {7393490b9aff4e7db81441b792a2bbde},
  intrahash = {be8b61313ff90c0434f668f5431d3349},
  keywords = {CoreNLP NLP},
  pages = {55--60},
  publisher = {Association for Computational Linguistics},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {The Stanford {CoreNLP} Natural Language Processing Toolkit},
  year = 2014
}

@article{lewis2004benchmark,
  abstract = {Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.},
  added-at = {2018-05-09T23:14:41.000+0200},
  author = {Lewis, David D. and Yang, Yiming and Rose, Tony G. and Li, Fan},
  biburl = {https://www.bibsonomy.org/bibtex/21ae643731985eb49e6c29ab18ef6fcc2/vngudivada},
  interhash = {ff940c50e028cb53fc10f99ddd39fe3e},
  intrahash = {1ae643731985eb49e6c29ab18ef6fcc2},
  issn = {1532-4435},
  journal = {J. Mach. Learn. Res.},
  keywords = {Evaluation IR ML RCV1},
  month = dec,
  numpages = {37},
  pages = {361--397},
  publisher = {JMLR.org},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {{RCV1}: A New Benchmark Collection for Text Categorization Research},
  volume = 5,
  year = 2004
}

@inproceedings{fang2004formal,
  abstract = {Empirical studies of information retrieval methods show that good retrieval performance is closely related to the use of various retrieval heuristics, such as TF-IDF weighting. One basic research question is thus what exactly are these "necessary" heuristics that seem to cause good retrieval performance. In this paper, we present a formal study of retrieval heuristics. We formally define a set of basic desirable constraints that any reasonable retrieval function should satisfy, and check these constraints on a variety of representative retrieval functions. We find that none of these retrieval functions satisfies all the constraints unconditionally. Empirical results show that when a constraint is not satisfied, it often indicates non-optimality of the method, and when a constraint is satisfied only for a certain range of parameter values, its performance tends to be poor when the parameter is out of the range. In general, we find that the empirical performance of a retrieval formula is tightly related to how well it satisfies these constraints. Thus the proposed constraints provide a good explanation of many empirical observations and make it possible to evaluate any existing or new retrieval formula analytically.},
  added-at = {2018-05-07T02:41:00.000+0200},
  address = {New York, NY},
  author = {Fang, Hui and Tao, Tao and Zhai, ChengXiang},
  biburl = {https://www.bibsonomy.org/bibtex/28d4ba1abd45e45e420280e4c1e7dbe85/vngudivada},
  booktitle = {Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/1008992.1009004},
  interhash = {05b9696434dde7f259a2fbeaada45c5b},
  intrahash = {8d4ba1abd45e45e420280e4c1e7dbe85},
  isbn = {1-58113-881-4},
  keywords = {IR RetrievalHeuristics TF-IDF},
  pages = {49--56},
  publisher = {ACM},
  series = {SIGIR '04},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {A Formal Study of Information Retrieval Heuristics},
  year = 2004
}

@inproceedings{angeli2015leveraging,
  abstract = {Relation triples produced by open domain information extraction (open IE) systems are useful for question answering, inference, and other IE tasks. Traditionally these are extracted using a large set of patterns; however, this approach is brittle on out-of-domain text and long-range dependencies, and gives no insight into the substructure of the arguments. We replace this large pattern set with a few patterns for canonically structured sentences, and shift the focus to a classifier which learns to extract self-contained clauses from longer sentences. We then run natural logic inference over these short clauses to determine the maximally specific arguments for each candidate triple. We show that our approach outperforms a state-of-The-Art open IE system on the end-to-end TAC-KBP 2013 Slot Filling task.},
  added-at = {2018-06-09T14:19:57.000+0200},
  author = {Angeli, Gabor and Premkumar, Melvin Jose Johnson and Manning, Christopher D.},
  biburl = {https://www.bibsonomy.org/bibtex/2faa32c289dab9df110abfc1ffc8509ba/vngudivada},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  doi = {10.3115/v1/p15-1034},
  interhash = {6ca269c5ddbe19032cc95c857cc3b423},
  intrahash = {faa32c289dab9df110abfc1ffc8509ba},
  keywords = {IE IR InformationExtraction},
  publisher = {Association for Computational Linguistics},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Leveraging Linguistic Structure For Open Domain Information Extraction},
  year = 2015
}

@article{manasse2012efficient,
  abstract = { Abstract The time-worn aphorism "close only counts in horseshoes and hand-grenades" is clearly inadequate. Close also counts in golf, shuffleboard, archery, darts, curling, and other games of accuracy in which hitting the precise center of the target isn't to be expected every time, or in which we can expect to be driven from the target by skilled opponents. This lecture is not devoted to sports discussions, but to efficient algorithms for determining pairs of closely related web pages -- and a few other situations in which we have found that inexact matching is good enough; where proximity suffices. We will not, however, attempt to be comprehensive in the investigation of probabilistic algorithms, approximation algorithms, or even techniques for organizing the discovery of nearest neighbors. We are more concerned with finding nearby neighbors; if they are not particularly close by, we are not particularly interested. In thinking of when approximation is sufficient, remember the oft-told joke about two campers sitting around after dinner. They hear noises coming towards them. One of them reaches for a pair of running shoes, and starts to don them. The second then notes that even with running shoes, they cannot hope to outrun a bear, to which the first notes that most likely the bear will be satiated after catching the slower of them. We seek problems in which we don't need to be faster than the bear, just faster than the others fleeing the bear.

Critical Acclaim for On the Efficient Determination of Most Near Neighbors "The material in this book grew from a simple question: "We know how to easily determine whether two files are identical, but what do we know about determining whether two files are similar?" The answer was "Not much," but when a theorist gives this answer, good things often happen. Such was the case here. This book will be important to practitioners interested in this and similar questions. It contains two intertwined threads; a mathematical treatment of the problem and an engineering thread that provides extremely efficient code for obtaining the solution at scale. I recommend it highly." -- Charles P. (Chuck) Thacker, Microsoft Research, 2009 Turing Award Winner

"From de-duplication to search, billion dollar industries rely on the ability to search for keys that are "close" to a specified key. The book by Mark Manasse provides a beautiful exposition of the field. Manasse is a well-known expert who has written some of the fundamental theoretical papers in the field; better still, he has worked on real products such as AltaVista and Windows file de-duplication. Mark has the rare ability to take theoretical ideas and convert them to sound engineering. The book will appeal to developers working in the web milieu because it illuminates the details that are often missing using code snippets. It will also appeal to researchers and students because of the uniform and insightful exposition of an important area." -- George Varghese, Professor, University of California, San Diego

"Mark Manasse, the father of micropayments, provides insight, techniques and theory behind search -- on getting not too large, not too small, but just right results. This 'horseshoes' mini-treatise comes right from the horse's mouth as an AltaVista -- he shows how the game was constructed by high dimensionality mapping into tractable space and time to find ringers and good outliers." -- Gordon Bell, Microsoft Research

Table of Contents: Introduction / Comparing Web Pages for Similarity: An Overview / A Personal History of Web Search / Uniform Sampling after Alta Vista / Why Weight (and How)? / A Few Applications },
  added-at = {2018-05-01T04:23:11.000+0200},
  author = {Manasse, Mark S.},
  biburl = {https://www.bibsonomy.org/bibtex/291f3384f3f290f7dba0e250309e9d325/vngudivada},
  doi = {10.2200/S00444ED1V01Y201208ICR024},
  interhash = {6ff6a65ac9a8f53c56459efd6ac4d475},
  intrahash = {91f3384f3f290f7dba0e250309e9d325},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {NearestNeighbor SynthesisLecture},
  number = 4,
  pages = {1-88},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {On the Efficient Determination of Most Near Neighbors: Horseshoes, Hand Grenades, Web Search and Other Situations When Close is Close Enough},
  volume = 4,
  year = 2012
}

@article{nomura2018programming,
  abstract = {This book is aimed at those in engineering/scientific fields who have never learned programming before but are eager to master the C language quickly so as to immediately apply it to problem solving in numerical analysis. The book skips unnecessary formality but explains all the important aspects of C essential for numerical analysis. Topics covered in numerical analysis include single and simultaneous equations, differential equations, numerical integration, and simulations by random numbers. In the Appendices, quick tutorials for gnuplot, Octave/MATLAB, and FORTRAN for C users are provided.

Table of Contents: Preface / Acknowledgments / First Steps to Run a C Program / Components of C Language / Note on Numerical Errors / Roots of f(x) = 0 / Numerical Differentiation / Numerical Integration / Solving Simultaneous Equations / Differential Equations / Author's Biography / Index},
  added-at = {2018-03-30T20:57:27.000+0200},
  author = {Nomura, Seiichi},
  biburl = {https://www.bibsonomy.org/bibtex/2b15b378de599150a197a1ac9a3502741/vngudivada},
  doi = {10.2200/S00835ED1V01Y201802MEC013},
  eprint = {https://doi.org/10.2200/S00835ED1V01Y201802MEC013},
  interhash = {5a723079c97894c8f84710c03b7866bf},
  intrahash = {b15b378de599150a197a1ac9a3502741},
  journal = {Synthesis Lectures on Mechanical Engineering},
  keywords = {C NumericalAnalysis SynthesisLecture},
  number = 2,
  pages = {1 -- 198},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {C Programming and Numerical Analysis: An Introduction},
  volume = 2,
  year = 2018
}

@article{goodman2001progress,
  abstract = {In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n-grams, skipping, interpolated Kneser-Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38 and 50\% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9\%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline.},
  added-at = {2018-05-13T13:31:50.000+0200},
  author = {Goodman, Joshua T.},
  biburl = {https://www.bibsonomy.org/bibtex/22e70e5b950eff464fcd3a868faad14af/vngudivada},
  doi = {10.1006/csla.2001.0174},
  interhash = {3d85d9f1be8379da8aebc92245e8605b},
  intrahash = {2e70e5b950eff464fcd3a868faad14af},
  journal = {Comput. Speech Lang.},
  keywords = {LanguageModeling NLP},
  month = oct,
  number = 4,
  pages = {403--434},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {A Bit of Progress in Language Modeling},
  volume = 15,
  year = 2001
}

@article{jones2012future,
  abstract = {We are well into a second age of digital information. Our information is moving from the desktop to the laptop to the "palmtop" and up into an amorphous cloud on the Web. How can one manage both the challenges and opportunities of this new world of digital information? What does the future hold? This book provides an important update on the rapidly expanding field of personal information management (PIM). Part I (Always and Forever) introduces the essentials of PIM. Information is personal for many reasons. It's the information on our hard drives we couldn't bear to lose. It's the information about us that we don't want to share. It's the distracting information demanding our attention even as we try to do something else. It's the information we don't know about but need to. Through PIM, we control personal information. We integrate information into our lives in useful ways. We make it "ours." With basics established, Part I proceeds to explore a critical interplay between personal information "always" at hand through mobile devices and "forever" on the Web. How does information stay "ours" in such a world?

Part II (Building Places of Our Own for Digital Information) will be available in the Summer of 2012, and will consist of the following chapters:

Chapter 5. Technologies to eliminate PIM?: We have seen astonishing advances in the technologies of information management -- in particular, to aid in the storing, structuring and searching of information. These technologies will certainly change the way we do PIM; will they eliminate the need for PIM altogether?

Chapter 6. GIM and the social fabric of PIM: We don't (and shouldn't) manage our information in isolation. Group information management (GIM) -- especially the kind practiced more informally in households and smaller project teams -- goes hand in glove with good PIM.

Chapter 7. PIM by design: Methodologies, principles, questions and considerations as we seek to understand PIM better and to build PIM into our tools, techniques and training.

Chapter 8. To each of us, our own.: Just as we must each be a student of our own practice of PIM, we must also be a designer of this practice. This concluding chapter looks at tips, traps and tradeoffs as we work to build a practice of PIM and "places" of our own for personal information.

Table of Contents: A New Age of Information / The Basics of PIM / Our Information, Always at Hand / Our Information, Forever on the Web},
  added-at = {2018-03-19T22:24:26.000+0100},
  author = {Jones, William},
  biburl = {https://www.bibsonomy.org/bibtex/21925352ce670cd17e85446614c73d928/vngudivada},
  doi = {10.2200/S00411ED1V01Y201203ICR021},
  eprint = {https://doi.org/10.2200/S00411ED1V01Y201203ICR021},
  interhash = {1500142787d87b2004d195a91775b6a5},
  intrahash = {1925352ce670cd17e85446614c73d928},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR PersonalInformationManagement SynthesisLecture},
  number = 1,
  pages = {1 -- 125},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {The Future of Personal Information Management, Part I: Our Information, Always and Forever},
  volume = 4,
  year = 2012
}

@article{feldman2012answer,
  abstract = {The Answer Machine is a practical, non-technical guide to the technologies behind information seeking and analysis. It introduces search and content analytics to software buyers, knowledge managers, and searchers who want to understand and design effective online environments. The book describes how search evolved from an expert-only to an end user tool. It provides an overview of search engines, categorization and clustering, natural language processing, content analytics, and visualization technologies. Detailed profiles for Web search, eCommerce search, eDiscovery, and enterprise search contrast the types of users, uses, tasks, technologies, and interaction designs for each. These variables shape each application, although the underlying technologies are the same. Types of information tasks and the trade-offs between precision and recall, time, volume and precision, and privacy vs. personalization are discussed within this context. The book examines trends toward convenient, context-aware computing, big data and analytics technologies, conversational systems, and answer machines. The Answer Machine explores IBM Watson's DeepQA technology and describes how it is used to answer health care and Jeopardy questions. The book concludes by discussing the implications of these advances: how they will change the way we run our businesses, practice medicine, govern, or conduct our lives in the digital age.

Table of Contents: Introduction / The Query Process and Barriers to Finding Information Online / Online Search: An Evolution / Search and Discovery Technologies: An Overview / Information Access: A Spectrum of Needs and Uses / Future Tense: The Next Era in Information Access and Discovery / Answer Machines

Snippets of reviews from Amazon.com:

"I highly recommend The Answer Machine for all of the folks involved in search, SEO and everyone else who want to understand how computer search systems and 'virtual assistants' are going to transform our lives over the next 10-20 years."

"Everyone in the search and technology field should read this book, as well as anyone who is curious about how technology like IBM's Watson works and the challenges that lie ahead."},
  added-at = {2018-03-19T22:10:41.000+0100},
  author = {Feldman, Susan E.},
  biburl = {https://www.bibsonomy.org/bibtex/2b4bfb2088d4a9d15fd548b922f5929a8/vngudivada},
  doi = {10.2200/S00442ED1V01Y201208ICR023},
  eprint = {https://doi.org/10.2200/S00442ED1V01Y201208ICR023},
  interhash = {a1ac721710dfe1a3da0fdc4d52ccef22},
  intrahash = {b4bfb2088d4a9d15fd548b922f5929a8},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
  keywords = {IR SynthesisLecture},
  number = 3,
  pages = {1 -- 137},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {The Answer Machine},
  volume = 4,
  year = 2012
}

@article{fan2012foundations,
  abstract = {Data quality is one of the most important problems in data management. A database system typically aims to support the creation, maintenance, and use of large amount of data, focusing on the quantity of data. However, real-life data are often dirty: inconsistent, duplicated, inaccurate, incomplete, or stale. Dirty data in a database routinely generate misleading or biased analytical results and decisions, and lead to loss of revenues, credibility and customers. With this comes the need for data quality management. In contrast to traditional data management tasks, data quality management enables the detection and correction of errors in the data, syntactic or semantic, in order to improve the quality of the data and hence, add value to business processes. While data quality has been a longstanding problem for decades, the prevalent use of the Web has increased the risks, on an unprecedented scale, of creating and propagating dirty data. This monograph gives an overview of fundamental issues underlying central aspects of data quality, namely, data consistency, data deduplication, data accuracy, data currency, and information completeness. We promote a uniform logical framework for dealing with these issues, based on data quality rules.

The text is organized into seven chapters, focusing on relational data. Chapter One introduces data quality issues. A conditional dependency theory is developed in Chapter Two, for capturing data inconsistencies. It is followed by practical techniques in Chapter 2b for discovering conditional dependencies, and for detecting inconsistencies and repairing data based on conditional dependencies. Matching dependencies are introduced in Chapter Three, as matching rules for data deduplication. A theory of relative information completeness is studied in Chapter Four, revising the classical Closed World Assumption and the Open World Assumption, to characterize incomplete information in the real world. A data currency model is presented in Chapter Five, to identify the current values of entities in a database and to answer queries with the current values, in the absence of reliable timestamps. Finally, interactions between these data quality issues are explored in Chapter Six. Important theoretical results and practical algorithms are covered, but formal proofs are omitted. The bibliographical notes contain pointers to papers in which the results were presented and proven, as well as references to materials for further reading.

This text is intended for a seminar course at the graduate level. It is also to serve as a useful resource for researchers and practitioners who are interested in the study of data quality. The fundamental research on data quality draws on several areas, including mathematical logic, computational complexity and database theory. It has raised as many questions as it has answered, and is a rich source of questions and vitality.

Table of Contents: Data Quality: An Overview / Conditional Dependencies / Cleaning Data with Conditional Dependencies / Data Deduplication / Information Completeness / Data Currency / Interactions between Data Quality Issues},
  added-at = {2018-03-19T23:56:21.000+0100},
  author = {Fan, Wenfei and Geerts, Floris},
  biburl = {https://www.bibsonomy.org/bibtex/24999df773da20ec36419f639592bc045/vngudivada},
  doi = {10.2200/S00439ED1V01Y201207DTM030},
  eprint = {https://doi.org/10.2200/S00439ED1V01Y201207DTM030},
  interhash = {28301cacff06c9239471a86c563b0529},
  intrahash = {4999df773da20ec36419f639592bc045},
  journal = {Synthesis Lectures on Data Management},
  keywords = {DataQuality SynthesisLecture},
  number = 5,
  pages = {1--217},
  timestamp = {2019-03-25T17:06:11.000+0100},
  title = {Foundations of Data Quality Management},
  volume = 4,
  year = 2012
}

@article{jager2012formal,
  abstract = {The first part of this article gives a brief overview of the four levels of the Chomsky hierarchy, with a special emphasis on context-free and regular languages. It then recapitulates the arguments why neither regular nor context-free grammar is sufficiently expressive to capture all phenomena in the natural language syntax. In the second part, two refinements of the Chomsky hierarchy are reviewed, which are both relevant to the extant research in cognitive science: the mildly context-sensitive languages (which are located between context-free and context-sensitive languages), and the sub-regular hierarchy (which distinguishes several levels of complexity within the class of regular languages).},
  added-at = {2018-06-29T13:34:36.000+0200},
  author = {J{\"a}ger, Gerhard and Rogers, James},
  biburl = {https://www.bibsonomy.org/bibtex/2c4cdef8746de87e1f6566fc45b450602/vngudivada},
  doi = {10.1098/rstb.2012.0077},
  eprint = {http://rstb.royalsocietypublishing.org/content/367/1598/1956.full.pdf},
  interhash = {e636e09ed0263f77c8d203c7632f2b58},
  intrahash = {c4cdef8746de87e1f6566fc45b450602},
  issn = {0962-8436},
  journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
  keywords = {ChomskyHierarchy FormalLanguages NLP},
  number = 1598,
  pages = {1956--1970},
  publisher = {The Royal Society},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Formal language theory: refining the Chomsky hierarchy},
  volume = 367,
  year = 2012
}

@article{leacock2014automated,
  abstract = {It has been estimated that over a billion people are using or learning English as a second or foreign language, and the numbers are growing not only for English but for other languages as well. These language learners provide a burgeoning market for tools that help identify and correct learners' writing errors. Unfortunately, the errors targeted by typical commercial proofreading tools do not include those aspects of a second language that are hardest to learn. This volume describes the types of constructions English language learners find most difficult: constructions containing prepositions, articles, and collocations. It provides an overview of the automated approaches that have been developed to identify and correct these and other classes of learner errors in a number of languages.

Error annotation and system evaluation are particularly important topics in grammatical error detection because there are no commonly accepted standards. Chapters in the book describe the options available to researchers, recommend best practices for reporting results, and present annotation and evaluation schemes.

The final chapters explore recent innovative work that opens new directions for research. It is the authors' hope that this volume will continue to contribute to the growing interest in grammatical error detection by encouraging researchers to take a closer look at the field and its many challenging problems.

Table of Contents: Acknowledgments / Introduction / Background / Special Problems of Language Learners / Evaluating Error Detection Systems / Data-Driven Approaches to Articles and Prepositions / Collocation Errors / Different Errors and Different Approaches / Annotating Learner Errors / Emerging Directions / Conclusion / Bibliography / Authors' Biographies},
  added-at = {2018-06-28T20:50:07.000+0200},
  author = {Leacock, Claudia and Chodorow, Martin and Gamon, Michael and Tetreault, Joel},
  biburl = {https://www.bibsonomy.org/bibtex/2feeb751e382c21bd4689f0340e4b6428/vngudivada},
  doi = {10.2200/S00562ED1V01Y201401HLT025},
  eprint = {https://doi.org/10.2200/S00562ED1V01Y201401HLT025},
  interhash = {1af2ad915e7471fdbbe0a00f0e91d9be},
  intrahash = {feeb751e382c21bd4689f0340e4b6428},
  journal = {Synthesis Lectures on Human Language Technologies},
  keywords = {Grammar NLP},
  number = 1,
  pages = {1-170},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Automated Grammatical Error Detection for Language Learners, Second Edition},
  url = {https://doi.org/10.2200/S00562ED1V01Y201401HLT025},
  volume = 7,
  year = 2014
}

@article{flaounas2013research,
  abstract = {News content analysis is usually preceded by a labour-intensive coding phase, where experts extract key information from news items. The cost of this phase imposes limitations on the sample sizes that can be processed, and therefore to the kind of questions that can be addressed. In this paper we describe an approach that incorporates text-analysis technologies for the automation of some of these tasks, enabling us to analyse data sets that are many orders of magnitude larger than those normally used. The patterns detected by our method include: (1) similarities in writing style among several outlets, which reflect reader demographics; (2) gender imbalance in media content and its relation with topic; (3) the relationship between topic and popularity of articles.},
  added-at = {2018-07-15T04:51:56.000+0200},
  author = {Flaounas, Ilias and Ali, Omar and Lansdall-Welfare, Thomas and Bie, Tijl De and Mosdell, Nick and Lewis, Justin and Cristianini, Nello},
  biburl = {https://www.bibsonomy.org/bibtex/285dc61bb94d252c9d47c5516b01cec8a/vngudivada},
  doi = {10.1080/21670811.2012.714928},
  interhash = {8aab2d2eaaefed8c912493813206ab9c},
  intrahash = {85dc61bb94d252c9d47c5516b01cec8a},
  journal = {Digital Journalism},
  keywords = {DigitalJournalism IR NLP TopicModel},
  number = 1,
  pages = {102 -- 116},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Research Methods in the Age of Digital Journalism},
  volume = 1,
  year = 2013
}

@inproceedings{wallach2006topic,
  abstract = {Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the ``bag-of-words'' assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful.},
  acmid = {1143967},
  added-at = {2018-07-15T05:15:10.000+0200},
  address = {New York, NY},
  author = {Wallach, Hanna M.},
  biburl = {https://www.bibsonomy.org/bibtex/2756c6ba41a64160950bc823259359c8c/vngudivada},
  booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
  doi = {10.1145/1143844.1143967},
  interhash = {18c05dbba12fd90426e1a0e3e5b3d0bd},
  intrahash = {756c6ba41a64160950bc823259359c8c},
  isbn = {1-59593-383-2},
  keywords = {IR NLP TopicModel},
  pages = {977--984},
  publisher = {ACM},
  series = {ICML '06},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Topic Modeling: Beyond Bag-of-words},
  year = 2006
}

@inproceedings{ford2010availability,
  abstract = {Highly available cloud storage is often implemented with complex, multi-tiered distributed systems built on top of clusters of commodity servers and disk drives. Sophisticated management, load balancing and recovery techniques are needed to achieve high performance and availability amidst an abundance of failure sources that include software, hardware, network connectivity, and power issues. While there is a relative wealth of failure studies of individual components of storage systems, such as disk drives, relatively little has been reported so far on the overall availability behavior of large cloud-based storage services. We characterize the availability properties of cloud storage systems based on an extensive one year study of Google's main storage infrastructure and present statistical models that enable further insight into the impact of multiple design choices, such as data placement and replication strategies. With these models we compare data availability under a variety of system parameters given the real patterns of failures observed in our fleet.},
  added-at = {2019-01-14T18:13:19.000+0100},
  author = {Ford, Daniel and Labelle, François and Popovici, Florentina I. and Stokely, Murray and Truong, Van-Anh and Barroso, Luiz and Grimes, Carrie and Quinlan, Sean},
  biburl = {https://www.bibsonomy.org/bibtex/24fe778fbf46ce2b51a2af2a51dd3ac39/vngudivada},
  booktitle = {OSDI},
  crossref = {conf/osdi/2010},
  editor = {Arpaci-Dusseau, Remzi H. and Chen, Brad},
  ee = {http://www.usenix.org/events/osdi10/tech/full_papers/Ford.pdf},
  interhash = {a08ac94c9c88e993d6903ae880bc426c},
  intrahash = {4fe778fbf46ce2b51a2af2a51dd3ac39},
  isbn = {978-1-931971-79-9},
  keywords = {NoSQL},
  pages = {61-74},
  publisher = {USENIX Association},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Availability in Globally Distributed Storage Systems.},
  url = {http://dblp.uni-trier.de/db/conf/osdi/osdi2010.html#FordLPSTBGQ10},
  year = 2010
}

@book{efthimiadis2011teaching,
  abstract = {Information Retrieval has become a very active research field in the 21st century. Many from academia and industry present their innovations in the field in a wide variety of conferences and journals. Companies transfer this new knowledge directly to the general public via services such as web search engines in order to improve their information seeking experience.

In parallel, teaching IR is turning into an important aspect of IR generally, not only because it is necessary to impart effective search techniques to make the most of the IR tools available, but also because we must provide a good foundation for those students who will become the driving force of future IR technologies.

There are very few resources for teaching and learning in IR, the major problem which this book is designed to solve. The objective is to provide ideas and practical experience of teaching and learning IR, for those whose job requires them to teach in one form or another, and where delivering IR courses is a major part of their working lives.

In this context of providing a higher profile for teaching and  learning as applied to IR, the co-editor of this book, Efthimis Efthimiathis, had maintained a leading role in teaching and learning within the domain of IR for a number of years. This book represents a posthumous example of his efforts in the area, as he passed away in April 2011. This book, his book, is dedicated to his memory.},
  added-at = {2018-08-07T04:18:48.000+0200},
  author = {Efthimiadis, Efthimis and Fernández-Luna, Juan M. and Huete, Juan F. and MacFarlane, Andrew and GmbH, Springer-Verlag},
  biburl = {https://www.bibsonomy.org/bibtex/2079a5932940927e9b239588da4f7fcf7/vngudivada},
  interhash = {26287b55d8bae89f4515c1ac9a7607ed},
  intrahash = {079a5932940927e9b239588da4f7fcf7},
  isbn = {9783662506776},
  keywords = {IR},
  refid = {954241563},
  series = {The Information Retrieval Series},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Teaching and Learning in Information Retrieval},
  year = 2011
}

@article{dean2013scale,
  abstract = {Software techniques that tolerate latency variability are vital to building responsive large-scale Web services.},
  added-at = {2019-01-14T18:38:30.000+0100},
  author = {Dean, Jeffrey and Barroso, Luiz Andr{\'e}},
  biburl = {https://www.bibsonomy.org/bibtex/2d53738f7d4ec8865f03804311de81549/vngudivada},
  doi = {10.1145/2408776.2408794},
  file = {ACM Digital Library:2013/DeanBarroso13cacm.pdf:PDF},
  groups = {public},
  interhash = {b1f117b981de508deb5a9635f5dabce6},
  intrahash = {d53738f7d4ec8865f03804311de81549},
  issn = {0001-0782},
  journal = {Communications of the ACM},
  keywords = {NoSQL},
  month = {#feb#},
  number = 2,
  pages = {74-80},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {The Tail at Scale},
  username = {flint63},
  volume = 56,
  year = 2013
}

@incollection{geman2001probabilistic,
  abstract = {Formal grammars are widely used in speech recognition, language translation, and language understanding systems. Grammars rich enough to accommodate natural language generate multiple interpretations of typical sentences. These ambiguities are a fundamental challenge to practical application. Grammars can be equipped with probability distributions, and the various parameters of these distributions can be estimated from data (e.g., acoustic representations of spoken words or a corpus of hand-parsed sentences). The resulting probabilistic grammars help to interpret spoken or written language unambiguously. This article reviews the main classes of probabilistic grammars and points to some active areas of research.},
  added-at = {2018-06-30T23:24:35.000+0200},
  author = {Geman, S. and Johnson, M.},
  biburl = {https://www.bibsonomy.org/bibtex/268d36f117cca07466583f8e8753902a4/vngudivada},
  booktitle = {International Encyclopedia of the Social {\&} Behavioral Sciences},
  doi = {10.1016/b0-08-043076-7/00489-7},
  interhash = {c2ae1c49d6cd9581d30dce68c3d090ad},
  intrahash = {68d36f117cca07466583f8e8753902a4},
  keywords = {Grammar},
  pages = {12075--12082},
  publisher = {Elsevier},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Probabilistic Grammars and their Applications},
  year = 2001
}

@article{mays1991context,
  abstract = {Some mistakes in spelling and typing produce correct words, such as typing “fig” when “fog” was intended. These errors are undetectable by traditional spelling correction techniques. In this paper we present a statistical technique capable of detecting and correcting some of these errors when they occur in sentences. Experimental results show that this technique is capable of detecting 76% of simple spelling errors and correcting 73\%.},
  added-at = {2018-07-04T20:14:31.000+0200},
  author = {Mays, Eric and Damerau, Fred J. and Mercer, Robert L.},
  biburl = {https://www.bibsonomy.org/bibtex/2e320c5664d17812d94d0e28894d1c347/vngudivada},
  doi = {10.1016/0306-4573(91)90066-u},
  interhash = {74e4ce6c5d41b2fdf30bdde7fd394f62},
  intrahash = {e320c5664d17812d94d0e28894d1c347},
  journal = {Information Processing {\&} Management},
  keywords = {SpellCorrector},
  month = jan,
  number = 5,
  pages = {517--522},
  publisher = {Elsevier},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Context based spelling correction},
  url = {https://doi.org/10.1016%2F0306-4573%2891%2990066-u},
  volume = 27,
  year = 1991
}

@article{decandia2007dynamo,
  abstract = {Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. The Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems. This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon's core services use to provide an "always-on" experience. To achieve this level of availability, Dynamo sacrifices consistency under certain failure scenarios. It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.},
  added-at = {2019-01-14T18:31:33.000+0100},
  address = {New York, NY, USA},
  author = {DeCandia, Giuseppe and Hastorun, Deniz and Jampani, Madan and Kakulapati, Gunavardhan and Lakshman, Avinash and Pilchin, Alex and Sivasubramanian, Swaminathan and Vosshall, Peter and Vogels, Werner},
  biburl = {https://www.bibsonomy.org/bibtex/208b15a451aa015197fee0c03beb803e1/vngudivada},
  doi = {http://doi.acm.org/10.1145/1323293.1294281},
  interhash = {5a38a3a7830286733636ae732d0b01cb},
  intrahash = {08b15a451aa015197fee0c03beb803e1},
  issn = {0163-5980},
  journal = {SIGOPS Oper. Syst. Rev.},
  keywords = {NoSQL},
  number = 6,
  pages = {205--220},
  publisher = {ACM},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Dynamo: Amazon's Highly Available key-value Store},
  volume = 41,
  year = 2007
}

@book{rodger2006jflapan,
  abstract = {JFLAP: An Interactive Formal Languages and Automata Package is a hands-on supplemental guide through formal languages and automata theory. JFLAP guides students interactively through many of the concepts in an automata theory course or the early topics in a compiler course, including the descriptions of algorithms JFLAP has implemented. Students can experiment with the concepts in the text and receive immediate feedback when applying these concepts with the accompanying software. The text describes each area of JFLAP and reinforces concepts with end-of-chapter exercises. In addition to JFLAP, this guide incorporates two other automata theory tools into JFLAP: JellRap and Pate.},
  added-at = {2018-07-02T06:30:43.000+0200},
  address = {Sudbury, Massachusetts},
  author = {Rodger, Susan H. and Finley, Thomas W.},
  biburl = {https://www.bibsonomy.org/bibtex/24fc4becad4f98ed397b8a8c8a61ae2db/vngudivada},
  interhash = {c063dfb209a257bbc7ff56136d29182e},
  intrahash = {4fc4becad4f98ed397b8a8c8a61ae2db},
  isbn = {9780763738341},
  keywords = {Automata FormalLanguages JFLAP},
  publisher = {Jones and Bartlett},
  refid = {938386959},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {JFLAP-an interactive formal languages and automata package},
  year = 2006
}

@article{cook2000complex,
  abstract = {1) Complex systems are intrinsically hazardous systems. All of the interesting systems (e.g. transportation, healthcare, power generation) are inherently and unavoidably hazardous by the own nature. The frequency of hazard exposure can sometimes be changed but the processes involved in the system are themselves intrinsically and irreducibly hazardous. It is the presence of these hazards that drives the creation of defenses against hazard that characterize these systems. 2) Complex systems are heavily and successfully defended against failure. The high consequences of failure lead over time to the construction of multiple layers of defense against failure. These defenses include obvious technical components (e.g. backup systems, 'safety' features of equipment) and human components (e.g. training, knowledge) but also a variety of organizational, institutional, and regulatory defenses (e.g. policies and procedures, certification, work rules, team training). The effect of these measures is to provide a series of shields that normally divert operations away from accidents. 3) Catastrophe requires multiple failures – single point failures are not enough..},
  added-at = {2019-01-14T18:21:54.000+0100},
  author = {Cook, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/2d2ab17d11e502125ad6b150fac0079a2/vngudivada},
  interhash = {e60a4501a3aec43b4e358e4b546397bd},
  intrahash = {d2ab17d11e502125ad6b150fac0079a2},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {How Complex Systems Fail},
  year = 2000
}

@inbook{fox2011pedagogical,
  abstract = {Information retrieval graduate courses have been offered in the Department of Computer Science at Virginia Tech since 1973. Since the early 1990s, the Information Storage and Retrieval course has been improved through a variety of pedagogical enhancements, many of which are reported in this chapter. The teaching and learning philosophy is based on team- and project-based learning, concept mapping, use of open source software, and more recently, use of virtual platforms, such as Second Life. In this chapter, we report on these approaches and the tools employed. Also, we describe three course offerings as case studies, which made use of the aforementioned methods. We hope that our experiences may be of interest to others involved in IR education.},
  added-at = {2018-08-07T03:38:32.000+0200},
  address = {Berlin, Heidelberg},
  author = {Fox, Edward and Murthy, Uma and Yang, Seungwon and Torres, Ricardo da S. and Velasco-Martin, Javier and Marchionini, Gary},
  biburl = {https://www.bibsonomy.org/bibtex/2c383c3d20bc7a38a5ea50c312e8c6112/vngudivada},
  booktitle = {Teaching and Learning in Information Retrieval},
  doi = {10.1007/978-3-642-22511-6_4},
  editor = {Efthimiadis, Efthimis and Fern{\'a}ndez-Luna, Juan M. and Huete, Juan F. and MacFarlane, Andrew},
  interhash = {8bf7b578da5652fb48aad1a74c2d00e5},
  intrahash = {c383c3d20bc7a38a5ea50c312e8c6112},
  isbn = {978-3-642-22511-6},
  keywords = {IR},
  pages = {47--60},
  publisher = {Springer Berlin Heidelberg},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Pedagogical Enhancements for Information Retrieval Courses},
  url = {https://doi.org/10.1007/978-3-642-22511-6_4},
  year = 2011
}

@book{horobin2018english,
  abstract = {The English language is spoken by more than a billion people throughout the world. But where did English come from? And how has it evolved into the language used today?

In this Very Short Introduction Simon Horobin investigates how we have arrived at the English we know today, and celebrates the way new speakers and new uses mean that it continues to adapt. Engaging with contemporary concerns about correctness, Horobin considers whether such changes are improvements, or evidence of slipping standards. What is the future for the English language? Will Standard English continue to hold sway, or we are witnessing its replacement by newly emerging Englishes?

ABOUT THE SERIES: The Very Short Introductions series from Oxford University Press contains hundreds of titles in almost every subject area. These pocket-sized books are the perfect way to get ahead in a new subject quickly. Our expert authors combine facts, analysis, perspective, new ideas, and enthusiasm to make interesting and challenging topics highly readable.},
  added-at = {2018-07-03T14:09:13.000+0200},
  address = {Oxford, UK},
  author = {Horobin, Simon},
  biburl = {https://www.bibsonomy.org/bibtex/21276fe0be4b3a010f1eab446fd0780e9/vngudivada},
  interhash = {6a8cf9e74e5b06ec3e6c849818827329},
  intrahash = {1276fe0be4b3a010f1eab446fd0780e9},
  isbn = {978-0198709251},
  keywords = {Book IntroductionsSeries Linguistics Short Very},
  publisher = {Oxford University Press},
  refid = {1039175105},
  series = {Very Short Introductions},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {The English language: a very short introduction},
  year = 2018
}

@article{lansdallwelfare2017content,
  abstract = {The use of large datasets has revolutionized the natural sciences and is widely believed to have the potential to do so with the social and human sciences. Many digitization efforts are underway, but the high-throughput methods of data production have not yet led to a comparable output in analysis. A notable exception has been the previous statistical analysis of the content of historical books, which started a debate about the limitations of using big data in this context. This study moves the debate forward using a large corpus of historical British newspapers and tools from artificial intelligence to extract macroscopic trends in history and culture, including gender bias, geographical focus, technology, and politics, along with accurate dates for specific events.Previous studies have shown that it is possible to detect macroscopic patterns of cultural change over periods of centuries by analyzing large textual time series, specifically digitized books. This method promises to empower scholars with a quantitative and data-driven tool to study culture and society, but its power has been limited by the use of data from books and simple analytics based essentially on word counts. This study addresses these problems by assembling a vast corpus of regional newspapers from the United Kingdom, incorporating very fine-grained geographical and temporal information that is not available for books. The corpus spans 150 years and is formed by millions of articles, representing 14\% of all British regional outlets of the period. Simple content analysis of this corpus allowed us to detect specific events, like wars, epidemics, coronations, or conclaves, with high accuracy, whereas the use of more refined techniques from artificial intelligence enabled us to move beyond counting words by detecting references to named entities. These techniques allowed us to observe both a systematic underrepresentation and a steady increase of women in the news during the 20th century and the change of geographic focus for various concepts. We also estimate the dates when electricity overtook steam and trains overtook horses as a means of transportation, both around the year 1900, along with observing other cultural transitions. We believe that these data-driven approaches can complement the traditional method of close reading in detecting trends of continuity and change in historical corpora.},
  added-at = {2018-07-15T05:02:57.000+0200},
  author = {Lansdall-Welfare, Thomas and Sudhahar, Saatviga and Thompson, James and Lewis, Justin and Cristianini, Nello},
  biburl = {https://www.bibsonomy.org/bibtex/2722ac1a5a8b11db844a7e46d82bf1552/vngudivada},
  doi = {10.1073/pnas.1606380114},
  editor = {Gregor, Amy and Low, Boon and Atkin-Wright, Toby and Dobson, Malcolm and Callison, Richard},
  interhash = {e4460d12819ce1ee0e3b534f20e5bcc0},
  intrahash = {722ac1a5a8b11db844a7e46d82bf1552},
  issn = {0027-8424},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {ContentAnalysis DataScience DigitalHumanities TopicModel},
  number = 4,
  pages = {E457--E465},
  publisher = {National Academy of Sciences},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Content analysis of 150 years of British periodicals},
  volume = 114,
  year = 2017
}

@inproceedings{qiao2013brewing,
  abstract = {Espresso is a document-oriented distributed data serving platform that has been built to address LinkedIn's requirements for a scalable, performant, source-of-truth primary store. It provides a hierarchical document model, transactional support for modifications to related documents, real-time secondary indexing, on-the-fly schema evolution and provides a timeline consistent change capture stream. This paper describes the motivation and design principles involved in building Espresso, the data model and capabilities exposed to clients, details of the replication and secondary indexing implementation and presents a set of experimental results that characterize the performance of the system along various dimensions. When we set out to build Espresso, we chose to apply best practices in industry, already published works in research and our own internal experience with different consistency models. Along the way, we built a novel generic distributed cluster management framework, a partition-aware change- capture pipeline and a high-performance inverted index implementation.},
  added-at = {2019-01-16T17:38:07.000+0100},
  author = {Qiao, Lin and Surlaker, Kapil and Das, Shirshanka and Quiggle, Tom and Schulman, Bob and Ghosh, Bhaskar and Curtis, Antony and Seeliger, Oliver and Zhang, Zhen and Auradkar, Aditya and Beaver, Chris and Brandt, Gregory and Gandhi, Mihir and Gopalakrishna, Kishore and Ip, Wai and Jagadish, Swaroop and Lu, Shi and Pachev, Alexander and Ramesh, Aditya and Sebastian, Abraham and Shanbhag, Rupa and Subramaniam, Subbu and Sun, Yun and Topiwala, Sajid and Tran, Cuong and Westerman, Jemiah and Zhang, David},
  biburl = {https://www.bibsonomy.org/bibtex/298c08451ca2cc3566512ab9a98e644c5/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2013},
  editor = {Ross, Kenneth A. and Srivastava, Divesh and Papadias, Dimitris},
  ee = {https://doi.org/10.1145/2463676.2465298},
  interhash = {93390e2334207a710d3307f3d886ea4d},
  intrahash = {98c08451ca2cc3566512ab9a98e644c5},
  isbn = {978-1-4503-2037-5},
  keywords = {NoSQL},
  pages = {1135-1146},
  publisher = {ACM},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {On brewing fresh espresso: LinkedIn's distributed data serving platform.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2013.html#QiaoSDQSGCSZABBGGIJLPRSSSSTTWZ13},
  year = 2013
}

@book{linz2017introduction,
  abstract = {n Introduction to Formal Languages and Automata, Sixth Edition provides an accessible, student-friendly presentation of all material essential to an introductory Theory of Computation course. Written to address the fundamentals of formal languages, automata, and computability, the text is designed to familiarize students with the foundations and principles of computer science and to strengthen the students' ability to carry out formal and rigorous mathematical arguments. The author, Peter Linz, continues to offer a straightforward, uncomplicated treatment of formal languages and automata and avoids excessive mathematical detail allowing students to focus on and understand the key underlying principles.

New and Key Features:

An accessible approach allows students to clearly understand key content while retaining the appropriate mathematical notations and theorems required for the course

New Chapter Synopsis, found at the end of each chapter, recap important concepts found in the text

Every major idea is preceded by a motivating example, drawn from applications, that introduces the concept and illustrates its usefulness

The exercise section has been restructured and improved with accessible exercises more closely tied to examples in the text

An enhanced Instructor's Manual includes more detailed solutions to problems found in the text},
  added-at = {2018-06-29T14:09:55.000+0200},
  address = {Sudbury, Massachusetts},
  author = {Linz, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/2e795cc98edfdedf036e4df6a1845dba7/vngudivada},
  interhash = {2a8317dfde70127b49c44335daee6465},
  intrahash = {e795cc98edfdedf036e4df6a1845dba7},
  isbn = {9781284077247},
  keywords = {Automata Book FormalLanguages},
  publisher = {Jones and Bartlett Learning},
  refid = {956468372},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {An introduction to formal languages and automata},
  year = 2017
}

@book{mack2018write,
  abstract = {Many scientists and engineers consider themselves poor writers or find the writing process difficult. The good news is that you do not have to be a talented writer to produce a good scientific paper, but you do have to be a careful writer. In particular, writing for a peer-reviewed scientific or engineering journal requires learning and executing a specific formula for presenting scientific work. This book is all about teaching the style and conventions of writing for a peer-reviewed scientific journal. From structure to style, titles to tables, abstracts to author lists, this book gives practical advice about the process of writing a paper and getting it published.},
  added-at = {2018-08-07T02:59:52.000+0200},
  address = {Bellingham, Washington, USA},
  author = {Mack, Chris A.},
  biburl = {https://www.bibsonomy.org/bibtex/2fac6f84eef103311ff61d20af2339058/vngudivada},
  interhash = {2eabbf4dc54ff83271b4b2b918ecb272},
  intrahash = {fac6f84eef103311ff61d20af2339058},
  isbn = {978-1510619135},
  keywords = {Research},
  publisher = {SPIE Press},
  series = {SPIE Press Monographs},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {How to Write a Good Scientific Paper},
  year = 2018
}

@article{faff2015simple,
  abstract = {In this article, I propose a simple new research tool - a template designed for pitching research. The two‐page pitching template begins with four `preliminaries': working title, research question, key papers and motivation. Following this is the core of the template based on a `3‐2‐1 countdown,' namely THREE elements - idea, data and tools; TWO questions – What's new? and So what?; and ONE bottom line - contribution. The template ends with `other' considerations. Finance and accounting examples are given to illustrate application of the template.},
  added-at = {2018-08-06T22:00:17.000+0200},
  author = {Faff, Robert W.},
  biburl = {https://www.bibsonomy.org/bibtex/279cc43e8b193b80c95fd7d61cf951448/vngudivada},
  doi = {10.1111/acfi.12116},
  interhash = {9ceef1afbd8bd336e138b2bd5905182f},
  intrahash = {79cc43e8b193b80c95fd7d61cf951448},
  journal = {Accounting \& Finance},
  keywords = {REU Research ResearchMethod},
  month = apr,
  number = 2,
  pages = {311 -- 336},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {A simple template for pitching research},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/acfi.12116},
  volume = 55,
  year = 2015
}

@article{goyal2016natural,
  abstract = {Scientific community across many disciplines is exploring new ways to extract knowledge from all available sources. Historically, written manuscripts have been the media of choice for recording experimental findings. Many disciplines such as social science, medical science are exploring ways to automate knowledge discovery from a vast repository of published scientific work. This work attempts to accelerate the process of information extraction by extending Kepler, a graphical workflow management tool. Kepler provides a simple way of designing and executing complex workflows in the form of directed graphs. This work presents a scalable approach to convert published research as PDF documents into indexable XML documents using Kepler. This conversion is a critical step in the Natural Language Processing pipeline. Kepler's distributed data processing capability enables scientists to scale this critical computation by simply adding more computing resources over the cloud.},
  added-at = {2018-07-05T00:17:02.000+0200},
  author = {Goyal, Ankit and Singh, Alok and Bhargava, Shitij and Crawl, Daniel and Altintas, Ilkay and Hsu, Chun-Nan},
  biburl = {https://www.bibsonomy.org/bibtex/289c2e2e842d796f1f751841897b5efc6/vngudivada},
  doi = {10.1016/j.procs.2016.05.358},
  interhash = {df90a6695dfacc23c2550c36b33626aa},
  intrahash = {89c2e2e842d796f1f751841897b5efc6},
  journal = {Procedia Computer Science},
  keywords = {NLP Workflow},
  pages = {712--721},
  publisher = {Elsevier},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Natural Language Processing Using Kepler Workflow System: First Steps},
  url = {https://doi.org/10.1016%2Fj.procs.2016.05.358},
  volume = 80,
  year = 2016
}

@book{cook2001chomskys,
  abstract = {Chomsky's Universal Grammar introduces both the general concepts of the theory, particularly its goals of describing the knowledge of language and of accounting for how it is acquired, and the main areas of syntax such as X-bar theory, movement and government.},
  added-at = {2018-07-04T02:53:43.000+0200},
  address = {Oxford, England},
  author = {Cook, Vivian and Newson, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/23176b350ba268069e4a6002956fb8ee0/vngudivada},
  edition = {Second},
  interhash = {80406ebc1c0e8be442281cb1665acf4b},
  intrahash = {3176b350ba268069e4a6002956fb8ee0},
  isbn = {978-0631195566},
  keywords = {Book Chomsky Linguistics UniversalGrammar},
  publisher = {Blackwell},
  refid = {748999011},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Chomsky's universal grammar: an introduction},
  year = 2001
}

@inproceedings{hofmann1999probabilistic,
  abstract = {Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.
},
  added-at = {2018-07-15T05:17:39.000+0200},
  author = {Hofmann, Thomas},
  biburl = {https://www.bibsonomy.org/bibtex/2bd954dbd9f44377209adc991aaa0840d/vngudivada},
  booktitle = {Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence},
  interhash = {170e69d01faca56cc5422cf2f272ad5e},
  intrahash = {bd954dbd9f44377209adc991aaa0840d},
  keywords = {IR LSA NLP},
  organization = {Morgan Kaufmann Publishers Inc.},
  pages = {289--296},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Probabilistic latent semantic analysis},
  year = 1999
}

@article{marz2013principles,
  added-at = {2019-01-14T18:29:31.000+0100},
  author = {Marz, Nathan},
  biburl = {https://www.bibsonomy.org/bibtex/2f4de1e92febf7f7631e207e7e57e661e/vngudivada},
  interhash = {11a99ed23fe01c4490925d1a02425e5c},
  intrahash = {f4de1e92febf7f7631e207e7e57e661e},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Principles of Software Engineering, Part 1},
  year = 2013
}

@article{herlihy1990linearizability,
  abstract = {A concurrent object is a data object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract data types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object's operations can be given by pre- and post-conditions. This paper defines linearizability, compares it to other correctness conditions, presents and demonstrates a method for proving the correctness of implementations, and shows how to reason about concurrent objects, given they are linearizable.},
  added-at = {2019-01-22T01:09:41.000+0100},
  author = {Herlihy, M.P. and Wing, J.M.},
  biburl = {https://www.bibsonomy.org/bibtex/2221819795fde66efb0d97613a3e5dcfe/vngudivada},
  interhash = {0e74a082f4427e6b45a6a2274470c791},
  intrahash = {221819795fde66efb0d97613a3e5dcfe},
  journal = {ACM Transactions on Programming Languages and Systems},
  keywords = {NoSQL},
  month = {July},
  number = 3,
  pages = {463--492},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Linearizability: A Correctness Condition for Concurrent Objects},
  volume = 12,
  year = 1990
}

@article{terry2011replicated,
  abstract = {Some cloud storage services, like Windows Azure, replicate data while providing strong consistency to their clients while others, like Amazon, have chosen eventual consistency in order to obtain better performance and availability. A broader class of consistency guarantees can, and perhaps should, be offered to clients that read shared data. During a baseball game, for example, different participants (the scorekeeper, umpire, sportswriter, and so on) benefit from six different consistency guarantees when reading the current score. Eventual consistency is insufficient for most of the participants, but strong consistency is not needed either.},
  added-at = {2019-01-17T22:42:50.000+0100},
  author = {Terry, Doug},
  biburl = {https://www.bibsonomy.org/bibtex/2b01d4e6b9477760db477e3bb05be933e/vngudivada},
  ee = {https://doi.org/10.1145/2500500},
  interhash = {0b5258b7a99b4a4a2caab0ca2e6a8f10},
  intrahash = {b01d4e6b9477760db477e3bb05be933e},
  journal = {Commun. ACM},
  keywords = {NoSQL},
  number = 12,
  pages = {82-89},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Replicated data consistency explained through baseball.},
  url = {http://dblp.uni-trier.de/db/journals/cacm/cacm56.html#Terry13},
  volume = 56,
  year = 2011
}

@article{cook2009versus,
  added-at = {2019-01-21T22:17:13.000+0100},
  author = {Cook, John D.},
  biburl = {https://www.bibsonomy.org/bibtex/2ead9f014eb10629f71a1613155955979/vngudivada},
  interhash = {ce3c61fea442101d8e9e50c5a8ed9a19},
  intrahash = {ead9f014eb10629f71a1613155955979},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {ACID Versus BASE for Database Transactions},
  year = 2009
}

@article{msgpackorg2019messagepack,
  added-at = {2019-01-17T21:55:43.000+0100},
  author = {msgpack.org},
  biburl = {https://www.bibsonomy.org/bibtex/233629c03823796106277e86d9cecb6e9/vngudivada},
  interhash = {7f9b17c4702ea1f66ecbff14c0728096},
  intrahash = {33629c03823796106277e86d9cecb6e9},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {MessagePack Specification},
  year = 2019
}

@article{brown2013bluffers,
  added-at = {2019-01-21T19:31:45.000+0100},
  author = {Brown, Russel},
  biburl = {https://www.bibsonomy.org/bibtex/2942108f20ca5fa5d67945b45b4a55649/vngudivada},
  interhash = {f1cb040460efd6b55f5668ec9bca1176},
  intrahash = {942108f20ca5fa5d67945b45b4a55649},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {A Bluffers Guide to CRDT's in Risk},
  year = 2013
}

@article{scotti2015adventures,
  added-at = {2019-01-22T01:06:54.000+0100},
  author = {Scotti, Alex},
  biburl = {https://www.bibsonomy.org/bibtex/264696e8248194c9bf7df1d4e4486bed2/vngudivada},
  interhash = {65e92fab6edd9c65205b96e5c86767ec},
  intrahash = {64696e8248194c9bf7df1d4e4486bed2},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Adventures in Building Your Own Database},
  year = 2015
}

@article{kleppmann2016conflictfree,
  abstract = {Many applications model their data in a general-purpose storage format such as JSON. This data structure is modified by the application as a result of user input. Such modifications are well understood if performed sequentially on a single copy of the data, but if the data is replicated and modified concurrently on multiple devices, it is unclear what the semantics should be. In this paper we present an algorithm and formal semantics for a JSON data structure that automatically resolves concurrent modifications such that no updates are lost, and such that all replicas converge towards the same state (a conflict-free replicated datatype or CRDT). It supports arbitrarily nested list and map types, which can be modified by insertion, deletion and assignment. The algorithm performs all merging client-side and does not depend on ordering guarantees from the network, making it suitable for deployment on mobile devices with poor network connectivity, in peer-to-peer networks, and in messaging systems with end-to-end encryption.},
  added-at = {2019-01-21T19:03:09.000+0100},
  author = {Kleppmann, Martin and Beresford, Alastair R.},
  biburl = {https://www.bibsonomy.org/bibtex/2a31f525002864c85b5a7ed815ee23e3c/vngudivada},
  ee = {http://arxiv.org/abs/1608.03960},
  interhash = {e3a324c9663276ecb7311bf9450826e9},
  intrahash = {a31f525002864c85b5a7ed815ee23e3c},
  journal = {CoRR},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {A Conflict-Free Replicated JSON Datatype.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1608.html#KleppmannB16},
  volume = {abs/1608.03960},
  year = 2016
}

@article{babu2013massively,
  abstract = {Timely and cost-effective analytics over "big data" has emerged as a key ingredient for success in many businesses, scientific and engineering disciplines, and government endeavors. Web clicks, social media, scientific experiments, and datacenter monitoring are among data sources that generate vast amounts of raw data every day. The need to convert this raw data into useful information has spawned considerable innovation in systems for large-scale data analytics, especially over the last decade. Massively Parallel Databases and MapReduce Systems addresses the design principles and core features of systems for analyzing very large datasets using massively-parallel computation and storage techniques on large clusters of nodes. It first discusses how the requirements of data analytics have evolved since the early work on parallel database systems. It then describes some of the major technological innovations that have each spawned a distinct category of systems for data analytics. Each unique system category is described along a number of dimensions including data model and query interface, storage layer, execution engine, query optimization, scheduling, resource management, and fault tolerance. It concludes with a summary of present trends in large-scale data analytics. Massively Parallel Databases and MapReduce Systems is an ideal reference for anyone with a research or professional interest in large-scale data analytics.},
  added-at = {2019-01-21T22:09:48.000+0100},
  author = {Babu, Shivnath and Herodotou, Herodotos},
  biburl = {https://www.bibsonomy.org/bibtex/24084724c9bcc2d59cd8d0a5419df8073/vngudivada},
  ee = {https://www.wikidata.org/entity/Q57964275},
  interhash = {b345ad377b6b77106770083a0cb23b29},
  intrahash = {4084724c9bcc2d59cd8d0a5419df8073},
  journal = {Foundations and Trends in Databases},
  keywords = {NoSQL},
  number = 1,
  pages = {1-104},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Massively Parallel Databases and MapReduce Systems.},
  url = {http://dblp.uni-trier.de/db/journals/ftdb/ftdb5.html#BabuH13},
  volume = 5,
  year = 2013
}

@article{graefe2011modern,
  abstract = {Invented about 40 years ago and called ubiquitous less than 10 years later, B-tree indexes have been used in a wide variety of computing systems from handheld devices to mainframes and server farms. Over the years, many techniques have been added to the basic design in order to improve efficiency or to add functionality. Examples include separation of updates to structure or contents, utility operations such as non-logged yet transactional index creation, and robust query processing such as graceful degradation during index-to-index navigation. Modern B-Tree Techniques reviews the basics of B-trees and of B-tree indexes in databases, transactional techniques and query processing techniques related to B-trees, B-tree utilities essential for database operations, and many optimizations and improvements. It is intended both as a tutorial and as a reference, enabling researchers to compare index innovations with advanced B-tree techniques and enabling professionals to select features, functions, and tradeoffs most appropriate for their data management challenges.},
  added-at = {2019-01-17T20:26:08.000+0100},
  author = {Graefe, Goetz},
  biburl = {https://www.bibsonomy.org/bibtex/2265c90349d8b5ef4915a99224bffee5b/vngudivada},
  ee = {http://dx.doi.org/10.1561/1900000028},
  interhash = {7ecf9e9be5cc82ff09840833c25c39d7},
  intrahash = {265c90349d8b5ef4915a99224bffee5b},
  journal = {Foundations and Trends in Databases},
  keywords = {NoSQL},
  number = 4,
  pages = {203-402},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Modern B-Tree Techniques.},
  url = {http://dblp.uni-trier.de/db/journals/ftdb/ftdb3.html#Graefe11},
  volume = 3,
  year = 2011
}

@inproceedings{karger1997consistent,
  abstract = {We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/IP, and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes minimally as the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network. We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems.},
  added-at = {2019-01-21T21:18:40.000+0100},
  author = {Karger, David R. and Lehman, Eric and Leighton, Frank Thomson and Panigrahy, Rina and Levine, Matthew S. and Lewin, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/2a5e6ac8a5a2489febad7e746ff7c49b8/vngudivada},
  booktitle = {STOC},
  description = {dblp},
  ee = {http://doi.acm.org/10.1145/258533.258660},
  interhash = {e30b29cb51068f8cfb116c0031919f7b},
  intrahash = {a5e6ac8a5a2489febad7e746ff7c49b8},
  keywords = {NoSQL},
  pages = {654-663},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web.},
  url = {http://dblp.uni-trier.de/db/conf/stoc/stoc1997.html#KargerLLPLL97},
  year = 1997
}

@inproceedings{howard2016flexible,
  abstract = {Distributed consensus is integral to modern distributed systems. The widely adopted Paxos algorithm uses two phases, each requiring majority agreement, to reliably reach consensus. In this paper, we demonstrate that Paxos, which lies at the foundation of many production systems, is conservative. Specifically, we observe that each of the phases of Paxos may use non-intersecting quorums. Majority quorums are not necessary as intersection is required only across phases. Using this weakening of the requirements made in the original formulation, we propose Flexible Paxos, which generalizes over the Paxos algorithm to provide flexible quorums. We show that Flexible Paxos is safe, efficient and easy to utilize in existing distributed systems. We discuss far reaching implications of this result. For example, improved availability results from reducing the size of second phase quorums by one when the system size is even, while keeping majority quorums in the first phase. Another example is improved throughput of replication by using much smaller phase 2 quorums, while increasing the leader election (phase 1) quorums. Finally, non intersecting quorums in either first or second phases may enhance the efficiency of both.},
  added-at = {2019-01-21T19:36:52.000+0100},
  author = {Howard, Heidi and Malkhi, Dahlia and Spiegelman, Alexander},
  biburl = {https://www.bibsonomy.org/bibtex/2dfa2dcc34f6e3e7055fcb553b259115f/vngudivada},
  booktitle = {OPODIS},
  crossref = {conf/opodis/2016},
  editor = {Fatourou, Panagiota and Jiménez, Ernesto and Pedone, Fernando},
  ee = {https://doi.org/10.4230/LIPIcs.OPODIS.2016.25},
  interhash = {160096a929619398dc59a1bc33ee59f0},
  intrahash = {dfa2dcc34f6e3e7055fcb553b259115f},
  isbn = {978-3-95977-031-6},
  keywords = {NoSQL},
  pages = {25:1-25:14},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik},
  series = {LIPIcs},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Flexible Paxos: Quorum Intersection Revisited.},
  url = {http://dblp.uni-trier.de/db/conf/opodis/opodis2016.html#HowardMS16},
  volume = 70,
  year = 2016
}

@article{hodges2012deploy,
  added-at = {2019-01-21T18:59:00.000+0100},
  author = {Hodges, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/21652ef537e4d39b7f9d6d47aaf1c1a0d/vngudivada},
  interhash = {25ae79b0f562a96751f071573df4c72c},
  intrahash = {1652ef537e4d39b7f9d6d47aaf1c1a0d},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {If You *Must* Deploy Multi-Master Replication, Read This First},
  year = 2012
}

@inproceedings{athanassoulis2016designing,
  abstract = {The database research community has been building methods to store, access, and update data for more than four decades. Throughout the evolution of the structures and techniques used to access data, access methods adapt to the ever-changing hardware and workload requirements. Today, even small changes in the workload or the hardware lead to a redesign of access methods. The need for new designs has been increasing as data generation and workload diversification grow exponentially, and hardware advances introduce increased complexity. New workload requirements are introduced by the emergence of new applications, and data is managed by large systems composed of more and more complex and heterogeneous hardware. As a result, it is increasingly important to develop application-aware and hardware-aware access methods. The fundamental challenges that every researcher, systems architect, or designer faces when designing a new access method are how to minimize, i) read times (R), ii) update cost (U), and iii) memory (or storage) overhead (M). In this paper, we conjecture that when optimizing the read-update-memory overheads, optimizing in any two areas negatively impacts the third. We present a simple model of the RUM overheads, and we articulate the RUM Conjecture. We show how the RUM Conjecture manifests in stateof-the-art access methods, and we envision a trend toward RUMaware access methods for future data systems.},
  added-at = {2019-01-17T21:08:49.000+0100},
  author = {Athanassoulis, Manos and Kester, Michael S. and Maas, Lukas M. and Stoica, Radu and Idreos, Stratos and Ailamaki, Anastasia and Callaghan, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/2a856b8a7d657489d9c7e4c774b507a88/vngudivada},
  booktitle = {EDBT},
  crossref = {conf/edbt/2016},
  editor = {Pitoura, Evaggelia and Maabout, Sofian and Koutrika, Georgia and Marian, Amélie and Tanca, Letizia and Manolescu, Ioana and Stefanidis, Kostas},
  ee = {http://dx.doi.org/10.5441/002/edbt.2016.42},
  interhash = {fe1c3c2906ddbd63c2fee0d23a33c29e},
  intrahash = {a856b8a7d657489d9c7e4c774b507a88},
  isbn = {978-3-89318-070-7},
  keywords = {NoSQL},
  pages = {461-466},
  publisher = {OpenProceedings.org},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Designing Access Methods: The RUM Conjecture.},
  url = {http://dblp.uni-trier.de/db/conf/edbt/edbt2016.html#AthanassoulisKM16},
  year = 2016
}

@inproceedings{arulraj2015about,
  abstract = {The advent of non-volatile memory (NVM) will fundamentally change the dichotomy between memory and durable storage in database management systems (DBMSs). These new NVM devices are almost as fast as DRAM, but all writes to it are potentially persistent even after power loss. Existing DBMSs are unable to take full advantage of this technology because their internal architectures are predicated on the assumption that memory is volatile. With NVM, many of the components of legacy DBMSs are unnecessary and will degrade the performance of data intensive applications. To better understand these issues, we implemented three engines in a modular DBMS testbed that are based on different storage management architectures: (1) in-place updates, (2) copy-on-write updates, and (3) log-structured updates. We then present NVM-aware variants of these architectures that leverage the persistence and byte-addressability properties of NVM in their storage and recovery methods. Our experimental evaluation on an NVM hardware emulator shows that these engines achieve up to 5.5X higher throughput than their traditional counterparts while reducing the amount of wear due to write operations by up to 2X. We also demonstrate that our NVM-aware recovery protocols allow these engines to recover almost instantaneously after the DBMS restarts.},
  added-at = {2019-01-17T21:42:21.000+0100},
  author = {Arulraj, Joy and Pavlo, Andrew and Dulloor, Subramanya},
  biburl = {https://www.bibsonomy.org/bibtex/214e6fcba84669f7eb6314cd3f75fb6e4/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2015},
  editor = {Sellis, Timos K. and Davidson, Susan B. and Ives, Zachary G.},
  ee = {https://doi.org/10.1145/2723372.2749441},
  interhash = {f8cec4fa9ca2dd6392b135c045105e72},
  intrahash = {14e6fcba84669f7eb6314cd3f75fb6e4},
  isbn = {978-1-4503-2758-9},
  keywords = {NoSQL},
  pages = {707-722},
  publisher = {ACM},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Let's Talk About Storage & Recovery Methods for Non-Volatile Memory Database Systems.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2015.html#ArulrajPD15},
  year = 2015
}

@book{bernstein1987concurrency,
  added-at = {2019-01-22T01:26:58.000+0100},
  author = {Bernstein, P.A. and Hadzilacos, V. and Goodman, N.},
  biburl = {https://www.bibsonomy.org/bibtex/22d67c62a020570685be4a28fa1e331ce/vngudivada},
  interhash = {23b756d10dad62ff39603435d81fc114},
  intrahash = {2d67c62a020570685be4a28fa1e331ce},
  keywords = {NoSQL},
  publisher = {Addison-Wesley},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Concurrency Control and Recovery in Database Systems},
  year = 1987
}

@article{daily2013clocks,
  added-at = {2019-01-21T19:12:04.000+0100},
  author = {Daily, John},
  biburl = {https://www.bibsonomy.org/bibtex/2c62de0d42ae76aaf10790f53b8ed2852/vngudivada},
  interhash = {e39ed933b8c622e4e87656292e262181},
  intrahash = {c62de0d42ae76aaf10790f53b8ed2852},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Clocks Are Bad, or, Welcome to the Wonderful World of Distributed Systems},
  year = 2013
}

@article{rosenblum1992design,
  abstract = {This paper presents a new technique for disk storage management called a log-structured file system. A logstructured file system writes all modifications to disk sequentially in a log-like structure, thereby speeding up both file writing and crash recovery. The log is the only structure on disk; it contains indexing information so that files can be read back from the log efficiently. In order to maintain large free areas on disk for fast writing, we divide the log into segments and use a segment cleaner to compress the live information from heavily fragmented segments. We present a series of simulations that demonstrate the efficiency of a simple cleaning policy based on cost and benefit. We have implemented a prototype logstructured file system called Sprite LFS; it outperforms current Unix file systems by an order of magnitude for small-file writes while matching or exceeding Unix performance for reads and large writes. Even when the overhead for cleaning is included, Sprite LFS can use 70% of the disk bandwidth for writing, whereas Unix file systems typically can use only 5-10%.},
  added-at = {2019-01-17T20:54:37.000+0100},
  author = {Rosenblum, Mendel and Ousterhout, John K.},
  biburl = {https://www.bibsonomy.org/bibtex/2f4f0ecf50aeb54cee8212959e64f5975/vngudivada},
  interhash = {9fe09c90f555ab40cf4e57d34a043089},
  intrahash = {f4f0ecf50aeb54cee8212959e64f5975},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {The Design and Implentation of a Log-Structured File System},
  year = 1992
}

@article{lamport1978clocks,
  abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
  acmid = {359563},
  added-at = {2019-01-21T19:50:34.000+0100},
  author = {Lamport, Leslie},
  biburl = {https://www.bibsonomy.org/bibtex/279d43af2473ad220b2056fb9017cd7c6/vngudivada},
  description = {Time, clocks, and the ordering of events in a distributed system},
  doi = {10.1145/359545.359563},
  interhash = {48bd55c0789290052bce4c28d34c4949},
  intrahash = {79d43af2473ad220b2056fb9017cd7c6},
  issn = {0001-0782},
  issue_date = {July 1978},
  journal = {Commun. ACM},
  keywords = {NoSQL},
  month = jul,
  number = 7,
  numpages = {8},
  pages = {558--565},
  publisher = {ACM},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Time, Clocks, and the Ordering of Events in a Distributed System},
  volume = 21,
  year = 1978
}

@article{irwin2013mongodbconfessions,
  added-at = {2019-01-17T19:37:10.000+0100},
  author = {Irwin, Conrad},
  biburl = {https://www.bibsonomy.org/bibtex/25f5ab5923d15694b38272523abeda26a/vngudivada},
  interhash = {cb19a5cd696ffc9025545bb8fb712194},
  intrahash = {5f5ab5923d15694b38272523abeda26a},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {MongoDB-Confessions of a PostgreSQL Lover},
  year = 2013
}

@article{aerospike2014support,
  added-at = {2019-01-22T00:12:36.000+0100},
  author = {Aerospike, Inc.},
  biburl = {https://www.bibsonomy.org/bibtex/2d8d135a8a2f14bf05b77c9664a49c22d/vngudivada},
  interhash = {838f42cd4d12a100be24a15a09b2c4dc},
  intrahash = {d8d135a8a2f14bf05b77c9664a49c22d},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {ACID Support in Aerospike},
  year = 2014
}

@inproceedings{ongaro2014search,
  abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.},
  added-at = {2019-01-22T01:33:43.000+0100},
  author = {Ongaro, Diego and Ousterhout, John K.},
  biburl = {https://www.bibsonomy.org/bibtex/25c211b33ed41ba32bb936291addebef8/vngudivada},
  booktitle = {USENIX Annual Technical Conference},
  crossref = {conf/usenix/2014},
  editor = {Gibson, Garth and Zeldovich, Nickolai},
  ee = {https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro},
  interhash = {9c0894bad72bfff91b1eaa3f2edf8d2a},
  intrahash = {5c211b33ed41ba32bb936291addebef8},
  keywords = {NoSQL},
  pages = {305-319},
  publisher = {USENIX Association},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {In Search of an Understandable Consensus Algorithm.},
  url = {http://dblp.uni-trier.de/db/conf/usenix/usenix2014.html#OngaroO14},
  year = 2014
}

@article{ellis2013cassandra,
  added-at = {2019-01-21T19:46:43.000+0100},
  author = {Ellis, Jonathan},
  biburl = {https://www.bibsonomy.org/bibtex/23ef10b634018f5983959f3dfff469713/vngudivada},
  interhash = {6be6b4ce571fed2dbd25142163f58661},
  intrahash = {3ef10b634018f5983959f3dfff469713},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Why Cassandra Doesn't Need Vector Clocks},
  year = 2013
}

@article{brown2015vector,
  added-at = {2019-01-21T20:19:32.000+0100},
  author = {Brown, Russell},
  biburl = {https://www.bibsonomy.org/bibtex/210e1153adb5ea0e569b6c8b288542a29/vngudivada},
  interhash = {fdeb6d5175ff8e1b10e36aaabe6e4b7f},
  intrahash = {10e1153adb5ea0e569b6c8b288542a29},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Vector Clocks Revisited Part 2: Dotted Version Vectors},
  year = 2015
}

@book{cormen2009introduction,
  abstract = {Some books on algorithms are rigorous but incomplete; others cover masses of material but lack rigor. Introduction to Algorithms uniquely combines rigor and comprehensiveness. The book covers a broad range of algorithms in depth, yet makes their design and analysis accessible to all levels of readers. Each chapter is relatively self-contained and can be used as a unit of study. The algorithms are described in English and in a pseudocode designed to be readable by anyone who has done a little programming. The explanations have been kept elementary without sacrificing depth of coverage or mathematical rigor.The first edition became a widely used text in universities worldwide as well as the standard reference for professionals. The second edition featured new chapters on the role of algorithms, probabilistic analysis and randomized algorithms, and linear programming. The third edition has been revised and updated throughout. It includes two completely new chapters, on van Emde Boas trees and multithreaded algorithms, substantial additions to the chapter on recurrence (now called "Divide-and-Conquer"), and an appendix on matrices. It features improved treatment of dynamic programming and greedy algorithms and a new notion of edge-based flow in the material on flow networks. Many new exercises and problems have been added for this edition. As of the third edition, this textbook is published exclusively by the MIT Press.},
  added-at = {2019-01-17T20:23:03.000+0100},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
  biburl = {https://www.bibsonomy.org/bibtex/28b6e267f62d8e23bbb127307ff7a4a0f/vngudivada},
  edition = {3rd},
  interhash = {a5e3e9cb3fe0f62575df69d504e53809},
  intrahash = {8b6e267f62d8e23bbb127307ff7a4a0f},
  keywords = {NoSQL},
  publisher = {MIT Press},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Introduction to Algorithms},
  url = {http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=11866},
  year = 2009
}

@article{dewitt1992parallel,
  abstract = {Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems.},
  added-at = {2019-01-21T20:28:55.000+0100},
  author = {DeWitt, David J. and Gray, Jim},
  biburl = {https://www.bibsonomy.org/bibtex/2bf501d91f9a4811521f0668c422544d9/vngudivada},
  cdrom = {CACMs1/CACM3506/P0085.pdf},
  cite = {conf/vldb/ZellerG90},
  ee = {https://www.wikidata.org/entity/Q29394166},
  interhash = {74f65bef1b1670706dd786ac539994ca},
  intrahash = {bf501d91f9a4811521f0668c422544d9},
  journal = {Commun. ACM},
  keywords = {NoSQL},
  number = 6,
  pages = {85-98},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Parallel Database Systems: The Future of High Performance Database Systems.},
  url = {http://dblp.uni-trier.de/db/journals/cacm/cacm35.html#DeWittG92},
  volume = 35,
  year = 1992
}

@article{clement2011eventual,
  added-at = {2019-01-21T19:08:29.000+0100},
  author = {Clement, Frazer},
  biburl = {https://www.bibsonomy.org/bibtex/22cab943c595484da84e1892e7f32a9ef/vngudivada},
  interhash = {4fec8c0915a7f2f3b5ac3a0c3d4712c8},
  intrahash = {2cab943c595484da84e1892e7f32a9ef},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Eventual Consistency - Detecting Conflicts},
  year = 2011
}

@article{berenson1995critique,
  abstract = {ANSI SQL-92 defines Isolation Levels in terms of phenomena: Dirty Reads, Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and the ANSI SQL definitions fail to characterize several popular isolation levels, including the standard locking implementations of the levels. Investigating the ambiguities of the phenomena leads to clearer definitions; in addition new phenomena that better characterize isolation types are introduced. An important multiversion isolation type, Snapshot Isolation, is defined.},
  added-at = {2019-01-22T00:26:16.000+0100},
  author = {Berenson, H. and Bernstein, P. and Gray, J. and Melton, J. and O'Neil, E. and O'Neil, P.},
  biburl = {https://www.bibsonomy.org/bibtex/2e56d965d34d5537bfe9168aa38b7df25/vngudivada},
  interhash = {9eaf5402d47537440f13b559ac417b11},
  intrahash = {e56d965d34d5537bfe9168aa38b7df25},
  journal = {ACM SIGMOD Record},
  keywords = {NoSQL},
  number = 2,
  pages = {1--10},
  publisher = {ACM},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {A Critique of ANSI SQL Isolation Levels.},
  url = {http://scholar.google.com/scholar.bib?q=info:RPxI-4mKQyEJ:scholar.google.com/&output=citation&hl=en&as_sdt=0,5&ct=citation&cd=0},
  volume = 24,
  year = 1995
}

@article{baquero2011version,
  added-at = {2019-01-21T20:22:58.000+0100},
  author = {Baquero, Carlos},
  biburl = {https://www.bibsonomy.org/bibtex/2824ad3a7c929e44a44fb15d6f6a6b7ec/vngudivada},
  interhash = {d9d22305420263f10e54b45cccdb781c},
  intrahash = {824ad3a7c929e44a44fb15d6f6a6b7ec},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Version Vectors Are Not Vector Clocks},
  year = 2011
}

@article{group2015documentation,
  added-at = {2019-01-17T22:45:51.000+0100},
  author = {Group, The PostgreSQL Global Development},
  biburl = {https://www.bibsonomy.org/bibtex/2234fe2a54dbde1bfacedd7ec2970ad8c/vngudivada},
  interhash = {47197cf1b1287b1d6ce84ad5894af0c4},
  intrahash = {234fe2a54dbde1bfacedd7ec2970ad8c},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {BDR 0.10.0 Documentation},
  year = 2015
}

@article{ports2012serializable,
  abstract = {This paper describes our experience implementing PostgreSQL’s new serializable isolation level. It is based on the recently-developed Serializable Snapshot Isolation (SSI) technique. This is the first implementation of SSI in a production database release as well as the first in a database that did not previously have a lock-based serializable isolation level. We reflect on our experience and describe how we overcame some of the resulting challenges, including the implementation of a new lock manager, a technique for ensuring memory usage is bounded, and integration with other PostgreSQL features. We also introduce an extension to SSI that improves performance for read-only transactions. We evaluate PostgreSQL’s serializable isolation level using several benchmarks and show that it achieves performance only slightly below that of snapshot isolation, and significantly outperforms the traditional two-phase locking approach on read-intensive workloads.},
  added-at = {2019-01-22T00:40:25.000+0100},
  author = {Ports, Dan R. K. and Grittner, Kevin},
  biburl = {https://www.bibsonomy.org/bibtex/2d73bffa86ff117f1fd033aca5d68370c/vngudivada},
  ee = {https://www.wikidata.org/entity/Q56059291},
  interhash = {9c10c30e7e599006bcb704f0f5594193},
  intrahash = {d73bffa86ff117f1fd033aca5d68370c},
  journal = {PVLDB},
  keywords = {NoSQL},
  number = 12,
  pages = {1850-1861},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Serializable Snapshot Isolation in PostgreSQL.},
  url = {http://dblp.uni-trier.de/db/journals/pvldb/pvldb5.html#PortsG12},
  volume = 5,
  year = 2012
}

@article{mahajan2011consistency,
  abstract = {We examine the limits of consistency in fault-tolerant distributed storage systems. In particular, we identify fundamental tradeoffs among properties of consistency, availability, and convergence, and we close the gap between what is known to be impossible (i.e. CAP) and known systems that are highly-available but that provide weaker consistency such as causal. Specifically, in the asynchronous model with omission-failures and unreliable networks, we show the following tight bound: No consistency stronger than Real Time Causal Consistency (RTC) can be provided in an always-available, one-way convergent system and RTC can be provided in an always-available, one-way convergent system. In the asynchronous, Byzantine-failure model, we show that it is impossible to implement many of the recently introduced fork-based consistency semantics without sacrificing either availability or convergence; notably, proposed systems allow Byzantine nodes to permanently partition correct nodes from one another. To address this limitation, we introduce bounded fork join causal semantics that extends causal consistency to Byzantine environments while retaining availability and convergence.},
  added-at = {2019-01-22T00:54:06.000+0100},
  author = {Mahajan, Prince and Alvisi, Lorenzo and Dahlin, Mike},
  biburl = {https://www.bibsonomy.org/bibtex/2f8bce50a3bcc369689ff7dad22e5426c/vngudivada},
  interhash = {ec6354689597c0fa233f477af7febaaa},
  intrahash = {f8bce50a3bcc369689ff7dad22e5426c},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Consistency, Availability, and Convergence},
  year = 2011
}

@article{rosenthal2014databases,
  added-at = {2019-01-22T00:45:08.000+0100},
  author = {Rosenthal, Dave},
  biburl = {https://www.bibsonomy.org/bibtex/2e699fcba899ca2256e625eb343130766/vngudivada},
  interhash = {aa7a35c3d50f8b19809ffdb7c14f886b},
  intrahash = {e699fcba899ca2256e625eb343130766},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Databases at 14.4MHz},
  year = 2014
}

@article{kleppmann2015please,
  added-at = {2019-01-22T01:14:34.000+0100},
  author = {Kleppmann, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/290931beaf453acca36264750457ed691/vngudivada},
  interhash = {2b9427c71b0ad66141d06408899c5fbe},
  intrahash = {90931beaf453acca36264750457ed691},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Please Stop Calling Databases CP or AP},
  year = 2015
}

@article{schulz2002string,
  abstract = {The Levenshtein-distance between two words is the minimal number of insertions, deletions or substitutions that are needed to transform one word into the other. Levenshtein-automata of degree n for a word W are dened as nite state automata that regognize the set of all words V where the Levenshtein-distance between V and W does not exceed n. We show how to compute, for any xed bound n and any input word W , a deterministic Levenshtein-automaton of degree n for W in time linear in the length of W . Given an electronic dictionary that is implemented in the form of a trie or a nite state automaton, the Levenshtein-automaton for W can be used to control search in the lexicon in such a way that exactly the lexical words V are generated where the Levenshtein-distance between V and W does not exceed the given bound. This leads to a very fast method for correcting corrupted input words of unrestricted text using large electronic dictionaries. We then introduce a second method that avoids the explicit computation of Levenshtein-automata and leads to even improved eciency. We also describe how to extend both methods to variants of the Levenshtein-distance where further primitive edit operations (transpositions, merges and splits) may be used. Keywords: Spelling correction, Levenshtein-distance, optical character recognition, electronic dictionaries.},
  added-at = {2019-01-17T21:24:26.000+0100},
  author = {Schulz, Klaus U. and Mihov, Stoyan},
  biburl = {https://www.bibsonomy.org/bibtex/279e41e3691781a17b9b7462ae3a664e3/vngudivada},
  ee = {https://www.wikidata.org/entity/Q30053406},
  interhash = {f80b86d6d553a36293ac8452299448e9},
  intrahash = {79e41e3691781a17b9b7462ae3a664e3},
  journal = {IJDAR},
  keywords = {NoSQL},
  number = 1,
  pages = {67-85},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Fast string correction with Levenshtein automata.},
  url = {http://dblp.uni-trier.de/db/journals/ijdar/ijdar5.html#SchulzM02},
  volume = 5,
  year = 2002
}

@article{richardson2014microservices,
  added-at = {2019-01-17T22:12:52.000+0100},
  author = {Richardson, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/23a8e616a52b2e43c26453fbc1449a5bd/vngudivada},
  interhash = {a4c7d60bd46d92561936470ca049f0a4},
  intrahash = {3a8e616a52b2e43c26453fbc1449a5bd},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Microservices: Decomposing Applications for Deployability and Scalability},
  year = 2014
}

@article{bloom1970spacetime,
  abstract = {In this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash-coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency.The new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods.In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to “catch” the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods.Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.},
  added-at = {2019-01-17T21:01:48.000+0100},
  address = {New York, NY, USA},
  author = {Bloom, Burton H.},
  biburl = {https://www.bibsonomy.org/bibtex/255e7953962fa81936928a55488e66afe/vngudivada},
  doi = {http://doi.acm.org/10.1145/362686.362692},
  interhash = {c8539667ad35bdc7779d7a8bf9276053},
  intrahash = {55e7953962fa81936928a55488e66afe},
  issn = {0001-0782},
  journal = {Commun. ACM},
  keywords = {NoSQL},
  number = 7,
  owner = {Christian},
  pages = {422--426},
  publisher = {ACM},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Space/Time Trade-offs in Hash Coding with Allowable Errors.},
  volume = 13,
  year = 1970
}

@article{kingsbury2014computational,
  added-at = {2019-01-22T01:18:30.000+0100},
  author = {Kingsbury, Kyle},
  biburl = {https://www.bibsonomy.org/bibtex/2ce05ad4cbb9241776955832ca01abd0e/vngudivada},
  interhash = {5e6c4bdb3ac05d65ced3f649cb0261a1},
  intrahash = {ce05ad4cbb9241776955832ca01abd0e},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Computational Techniques in Knossos},
  year = 2014
}

@book{newman2015building,
  abstract = {Distributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, the second edition of this practical book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures.

Microservice technologies are moving quickly, and this revised edition gets you up to date with a new chapter on serverless and cloud-native applications, expanded coverage of user interfaces, more hands-on code examples, and other additions throughout the book.

Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You'll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain.},
  added-at = {2019-01-17T22:07:04.000+0100},
  author = {Newman, Sam},
  biburl = {https://www.bibsonomy.org/bibtex/261a28f122c347b555b88a0d835080b6a/vngudivada},
  description = {Building Microservices: Sam Newman: 9781491950357: Amazon.com: Books},
  edition = {1st},
  interhash = {cd274e4310da91cad9f07dfbed64cb9f},
  intrahash = {61a28f122c347b555b88a0d835080b6a},
  isbn = {978-1491950357},
  keywords = {NoSQL},
  month = {February},
  pages = 280,
  publisher = {O'Reilly Media},
  refid = {903609686},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Building Microservices: Designing Fine-Grained Systems},
  year = 2015
}

@article{debrabant2013anticaching,
  abstract = {The traditional wisdom for building disk-based relational database management systems (DBMS) is to organize data in heavily-encoded blocks stored on disk, with a main memory block cache. In order to improve performance given high disk latency, these systems use a multi-threaded architecture with dynamic record-level locking that allows multiple transactions to access the database at the same time. Previous research has shown that this results in substantial overhead for on-line transaction processing (OLTP) applications [15]. The next generation DBMSs seek to overcome these limitations with architecture based on main memory resident data. To overcome the restriction that all data fit in main memory, we propose a new technique, called anti-caching, where cold data is moved to disk in a transactionally-safe manner as the database grows in size. Because data initially resides in memory, an anti-caching architecture reverses the traditional storage hierarchy of disk-based systems. Main memory is now the primary storage device. We implemented a prototype of our anti-caching proposal in a high-performance, main memory OLTP DBMS and performed a series of experiments across a range of database sizes, workload skews, and read/write mixes. We compared its performance with an open-source, disk-based DBMS optionally fronted by a distributed main memory cache. Our results show that for higher skewed workloads the anti-caching architecture has a performance advantage over either of the other architectures tested of up to 9× for a data size 8× larger than memory.},
  added-at = {2019-01-17T21:36:22.000+0100},
  author = {DeBrabant, Justin and Pavlo, Andrew and Tu, Stephen and Stonebraker, Michael and Zdonik, Stanley B.},
  biburl = {https://www.bibsonomy.org/bibtex/248357117bf065b8017cac7364a2e686e/vngudivada},
  ee = {https://doi.org/10.14778/2556549.2556575},
  interhash = {efd6c2daaad53a84181ae0e205e00bb7},
  intrahash = {48357117bf065b8017cac7364a2e686e},
  journal = {PVLDB},
  keywords = {NoSQL},
  number = 14,
  pages = {1942-1953},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Anti-Caching: A New Approach to Database Management System Architecture.},
  url = {http://dblp.uni-trier.de/db/journals/pvldb/pvldb6.html#DeBrabantPTSZ13},
  volume = 6,
  year = 2013
}

@article{auradkar2015introducing,
  added-at = {2019-01-17T22:02:35.000+0100},
  author = {Auradkar, Aditya and Quiggle, Tom},
  biburl = {https://www.bibsonomy.org/bibtex/2573613810e5cef3a95323117df706976/vngudivada},
  interhash = {77d6e553d3f7707e2b3d7ee25582aff1},
  intrahash = {573613810e5cef3a95323117df706976},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Introducing Espresso-LinkedIn's Hot New Distributed Document Store},
  year = 2015
}

@article{galiegue2013schema,
  abstract = {JSON Schema defines the media type "application/schema+json", a JSON based format for defining the structure of JSON data.  JSON Schema provides a contract for what JSON data is required for a given application and how to interact with it.  JSON Schema is intended to define validation, documentation, hyperlink navigation, and interaction control of JSON data.},
  added-at = {2019-01-17T21:53:14.000+0100},
  author = {Galiegue, Francis},
  biburl = {https://www.bibsonomy.org/bibtex/2e3d4614568e6e6f24369b8dbd0b100d6/vngudivada},
  interhash = {6d5f82d6c172edcac2e1ec10beb152cc},
  intrahash = {e3d4614568e6e6f24369b8dbd0b100d6},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {JSON Schema},
  year = 2013
}

@article{borthakur2013history,
  added-at = {2019-01-17T20:35:58.000+0100},
  author = {Borthakur, Dhruba},
  biburl = {https://www.bibsonomy.org/bibtex/2ddc87fadee6986de2eb8c5f86ed95a3c/vngudivada},
  interhash = {d0fd2733584decc67fcf86d64cad362f},
  intrahash = {ddc87fadee6986de2eb8c5f86ed95a3c},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {The History of RocksDB},
  year = 2013
}

@article{foundationdb2013transactions,
  added-at = {2019-01-21T22:13:45.000+0100},
  author = {FoundationDB, LLC},
  biburl = {https://www.bibsonomy.org/bibtex/29512f43e30e9190b8d60fd9d1038beaa/vngudivada},
  interhash = {484c9957af1e515905e7c477bd4b1478},
  intrahash = {9512f43e30e9190b8d60fd9d1038beaa},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {ACID Transactions Are Incredibly Helpful},
  year = 2013
}

@article{bailis2014quantifying,
  abstract = {Data replication results in a fundamental trade-off between operation latency and consistency. At the weak end of the spectrum of possible consistency models is eventual consistency, which provides no limit to the staleness of data returned. However, anecdotally, eventual consistency is often "good enough" for practitioners given its latency and availability benefits. In this work, we explain this phenomenon and demonstrate that, despite their weak guarantees, eventually consistent systems regularly return consistent data while providing lower latency than their strongly consistent counterparts. To quantify the behavior of eventually consistent stores, we introduce Probabilistically Bounded Staleness (PBS), a consistency model that provides expected bounds on data staleness with respect to both versions and wall clock time. We derive a closed-form solution for version-based staleness and model real-time staleness for a large class of quorum replicated, Dynamo-style stores. Using PBS, we measure the trade-off between latency and consistency for partial, non-overlapping quorum systems under Internet production workloads. We quantitatively demonstrate how and why eventually consistent systems frequently return consistent data within tens of milliseconds while offering large latency benefits.},
  added-at = {2019-01-21T19:41:49.000+0100},
  author = {Bailis, Peter and Venkataraman, Shivaram and Franklin, Michael J. and Hellerstein, Joseph M. and Stoica, Ion},
  biburl = {https://www.bibsonomy.org/bibtex/2edc0b90d06c0cfd4bfd82a0729ad5c7e/vngudivada},
  doi = {10.1145/2632792},
  file = {ACM Digital Library:2014/BailisVenkataramanEtAl14cacm.pdf:PDF},
  groups = {public},
  interhash = {0aca56d5c16d08011871aa927d8971fb},
  intrahash = {edc0b90d06c0cfd4bfd82a0729ad5c7e},
  issn = {0001-0782},
  journal = {Communications of the ACM},
  keywords = {NoSQL},
  month = {#aug#},
  number = 8,
  pages = {93--102},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Quantifying Eventual Consistency with PBS},
  username = {flint63},
  volume = 57,
  year = 2014
}

@article{kerstiens2013javascript,
  abstract = {The world of application development is rapidly changing delivering new tools every day to make you more productive. Postgres and the database world are no different, now with JavaScript and JSON support. This powerful functionality is now available on all Heroku Postgres production tier databases – run CREATE EXTENSION plv8; on your database to get started today.},
  added-at = {2019-01-17T19:40:26.000+0100},
  author = {Kerstiens, Craig},
  biburl = {https://www.bibsonomy.org/bibtex/2d4b7e7217d8ff01c6be81505cea9e839/vngudivada},
  interhash = {748e36428b0457d069fc1ab8b2588b50},
  intrahash = {d4b7e7217d8ff01c6be81505cea9e839},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {JavaScript in Your Postgres},
  year = 2013
}

@article{dean2019leveldb,
  added-at = {2019-01-17T20:32:49.000+0100},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  biburl = {https://www.bibsonomy.org/bibtex/289cca81e091430eb42c825fdb2c6ba52/vngudivada},
  interhash = {458a095796351c8d7147fcfa7cbfea4d},
  intrahash = {89cca81e091430eb42c825fdb2c6ba52},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {LevelDB Implementation Notes},
  year = 2019
}

@article{oneil1996logstructured,
  abstract = {High-performance transaction system applications typically insert rows in a History table to provide an activity trace; at the same time the transaction system generates log records for purposes of system recovery. Both types of generated information can benefit from efficient indexing. An example in a well-known setting is the TPC-A benchmark application, modified to support efficient queries on the history for account activity for specific accounts. This requires an index by account-id on the fast-growing History table. Unfortunately, standard disk-based index structures such as the B-tree will effectively double the I/O cost of the transaction to maintain an index such as this in real time, increasing the total system cost up to fifty percent. Clearly a method for maintaining a real-time index at low cost is desirable. The log-structured mergetree (LSM-tree) is a disk-based data structure designed to provide low-cost indexing for a file experiencing a high rate of record inserts (and deletes) over an extended period. The LSM-tree uses an algorithm that defers and batches index changes, cascading the changes from a memory-based component through one or more disk components in an efficient manner reminiscent of merge sort. During this process all index values are continuously accessible to retrievals (aside from very short locking periods), either through the memory component or one of the disk components. The algorithm has greatly reduced disk arm movements compared to a traditional access methods such as B-trees, and will improve cost-performance in domains where disk arm costs for inserts with traditional access methods overwhelm storage media costs. The LSM-tree approach also generalizes to operations other than insert and delete. However, indexed finds requiring immediate response will lose I/O efficiency in some cases, so the LSM-tree is most useful in applications where index inserts are more common than finds that retrieve the entries. This seems to be a common property for history tables and log files, for example. The conclusions of Sect. 6 compare the hybrid use of memory and disk components in the LSM-tree access method with the commonly understood advantage of the hybrid method to buffer disk pages in memory.},
  added-at = {2019-01-17T20:49:49.000+0100},
  author = {O'Neil, Patrick E. and Cheng, Edward and Gawlick, Dieter and O'Neil, Elizabeth J.},
  biburl = {https://www.bibsonomy.org/bibtex/2abbf18cdcb987b613931c4ffb1641a7e/vngudivada},
  ee = {https://www.wikidata.org/entity/Q56600462},
  interhash = {d4f6f75ab32e104ecd6304b62b81d0a4},
  intrahash = {abbf18cdcb987b613931c4ffb1641a7e},
  journal = {Acta Inf.},
  keywords = {NoSQL},
  number = 4,
  pages = {351-385},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {The Log-Structured Merge-Tree (LSM-Tree).},
  url = {http://dblp.uni-trier.de/db/journals/acta/acta33.html#ONeilCGO96},
  volume = 33,
  year = 1996
}

@article{bailis2014linearizability,
  added-at = {2019-01-22T01:22:41.000+0100},
  author = {Bailis, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/237afb3a07cdb2cac6dbbb024e99302d1/vngudivada},
  interhash = {567e4021827633e389327b8223f13291},
  intrahash = {37afb3a07cdb2cac6dbbb024e99302d1},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Linearizability Versus Serializability},
  year = 2014
}

@article{mccandless2011visualizing,
  added-at = {2019-01-17T20:58:32.000+0100},
  author = {McCandless, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2e1a4bf74cd25c56d522335e641137ef3/vngudivada},
  interhash = {1badb3ffd2a41999d87848f118473cc2},
  intrahash = {e1a4bf74cd25c56d522335e641137ef3},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Visualizing Lucene's Segment Merges},
  year = 2011
}

@article{clarke2012nosqls,
  added-at = {2019-01-21T22:20:27.000+0100},
  author = {Clarke, Gavin},
  biburl = {https://www.bibsonomy.org/bibtex/2c1801ff2d1e96d1b731c45724953961c/vngudivada},
  interhash = {d9ccbf684f568e258bb3e1d156e0016e},
  intrahash = {c1801ff2d1e96d1b731c45724953961c},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {NoSQL's CAP Theorem Busters: We Don't Drop ACID},
  year = 2012
}

@article{evans2012rethinking,
  abstract = {A content-addressable system works by creating a deterministic mapping between unique key and a name-space partition. How these partitions are distributed throughout a cluster of machines is key to a well functioning system.

The most naive approach to a one-hop DHT, uses a single unique token from within the key name-space assigned to each node. The range beginning with the next lowest sorting token and ending in the nodes token, makes up that nodes partition. This single partition per node distribution works, but suffers from a number of problems, not least of which is the small number of localized nodes involved in fail-over and repair, and the operational complexity of expanding the cluster without creating imbalance.

In the paper entitled Dynamo: Amazon's Highly Available Key-Value Store, Amazon's solution to partition distribution was to randomly assign many randomly allocated tokens to each machine in the cluster. This approach, referred to as "virtual nodes", permits cluster-wide parallelism during fail-over and repair, and requires no manual intervention in maintaining a high degree of balance.

Despite the inspiration that Dynamo provided for Cassandra, an early design decision eschewed virtual nodes in favor of a simpler implementation. This talk will compare and contrast the differing approaches to distribution, the properties they provide, and will discuss the transition of Cassandra from 1-token-per-node, to a complete virtual nodes implementation.},
  added-at = {2019-01-21T22:07:08.000+0100},
  author = {Evans, Eric},
  biburl = {https://www.bibsonomy.org/bibtex/2cd8f9ff57d4aa08d02b6cfabe23aacbf/vngudivada},
  interhash = {f36eb8164e92b10a38f1ea51b26939fe},
  intrahash = {cd8f9ff57d4aa08d02b6cfabe23aacbf},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Rethinking Topology in Cassandra},
  year = 2012
}

@article{lakshman2010cassandra,
  abstract = {Cassandra is a distributed storage system for managing very large amounts of structured data spread out across many commodity servers, while providing highly available service with no single point of failure. Cassandra aims to run on top of an infrastructure of hundreds of nodes (possibly spread across different data centers). At this scale, small and large components fail continuously. The way Cassandra manages the persistent state in the face of these failures drives the reliability and scalability of the software systems relying on this service. While in many ways Cassandra resembles a database and shares many design and implementation strategies therewith, Cassandra does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format. Cassandra system was designed to run on cheap commodity hardware and handle high write throughput while not sacrificing read efficiency.},
  added-at = {2019-01-21T22:03:11.000+0100},
  author = {Lakshman, Avinash and Malik, Prashant},
  biburl = {https://www.bibsonomy.org/bibtex/233b0bde165a028da3ab752baac319ddf/vngudivada},
  ee = {http://doi.acm.org/10.1145/1773912.1773922},
  interhash = {8d90a233678aa931aee0bccb68c7bf04},
  intrahash = {33b0bde165a028da3ab752baac319ddf},
  journal = {Operating Systems Review},
  keywords = {NoSQL},
  number = 2,
  pages = {35-40},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Cassandra: a decentralized structured storage system},
  url = {http://dblp.uni-trier.de/db/journals/sigops/sigops44.html#LakshmanM10},
  volume = 44,
  year = 2010
}

@inproceedings{calder2011windows,
  abstract = {Windows Azure Storage (WAS) is a cloud storage system that provides customers the ability to store seemingly limitless amounts of data for any duration of time. WAS customers have access to their data from anywhere at any time and only pay for what they use and store. In WAS, data is stored durably using both local and geographic replication to facilitate disaster recovery. Currently, WAS storage comes in the form of Blobs (files), Tables (structured storage), and Queues (message delivery). In this paper, we describe the WAS architecture, global namespace, and data model, as well as its resource provisioning, load balancing, and replication systems.},
  added-at = {2019-01-17T22:30:16.000+0100},
  author = {Calder, Brad and Wang, Ju and Ogus, Aaron and Nilakantan, Niranjan and Skjolsvold, Arild and McKelvie, Sam and Xu, Yikang and Srivastav, Shashwat and Wu, Jiesheng and Simitci, Huseyin and Haridas, Jaidev and Uddaraju, Chakravarthy and Khatri, Hemal and Edwards, Andrew and Bedekar, Vaman and Mainali, Shane and Abbasi, Rafay and Agarwal, Arpit and ul Haq, Mian Fahim and ul Haq, Muhammad Ikram and Bhardwaj, Deepali and Dayanand, Sowmya and Adusumilli, Anitha and McNett, Marvin and Sankaran, Sriram and Manivannan, Kavitha and Rigas, Leonidas},
  biburl = {https://www.bibsonomy.org/bibtex/2faa6eb1b3ef8e6625413641f808c292b/vngudivada},
  booktitle = {SOSP},
  crossref = {conf/sosp/2011},
  editor = {Wobber, Ted and Druschel, Peter},
  ee = {https://doi.org/10.1145/2043556.2043571},
  interhash = {c11ebec0e182fe67dc6c71f67151a577},
  intrahash = {faa6eb1b3ef8e6625413641f808c292b},
  isbn = {978-1-4503-0977-6},
  keywords = {NoSQL},
  pages = {143-157},
  publisher = {ACM},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Windows Azure Storage: a highly available cloud storage service with strong consistency.},
  url = {http://dblp.uni-trier.de/db/conf/sosp/sosp2011.html#CalderWONSMXSWSHUKEBMAAHHBDAMSMR11},
  year = 2011
}

@article{lamping2014minimal,
  abstract = {We present jump consistent hash, a fast, minimal memory, consistent hash algorithm that can be expressed in about 5 lines of code. In comparison to the algorithm of Karger et al., jump consistent hash requires no storage, is faster, and does a better job of evenly dividing the key space among the buckets and of evenly dividing the workload when the number of buckets changes. Its main limitation is that the buckets must be numbered sequentially, which makes it more suitable for data storage applications than for distributed web caching.},
  added-at = {2019-01-21T21:28:42.000+0100},
  author = {Lamping, John and Veach, Eric},
  biburl = {https://www.bibsonomy.org/bibtex/27eb1d94d4a2ee9e57ac3b3bfe9289c3e/vngudivada},
  ee = {http://arxiv.org/abs/1406.2294},
  interhash = {807d959790c5ac1068f0dd272df45d91},
  intrahash = {7eb1d94d4a2ee9e57ac3b3bfe9289c3e},
  journal = {CoRR},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {A Fast, Minimal Memory, Consistent Hash Algorithm.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1406.html#LampingV14},
  volume = {abs/1406.2294},
  year = 2014
}

@article{swaggerio2014openapi,
  added-at = {2019-01-17T22:16:12.000+0100},
  author = {swagger.io},
  biburl = {https://www.bibsonomy.org/bibtex/2ff41a4c1e6d25476abe96f66a9381068/vngudivada},
  interhash = {255f09e1fa5bf80e57a91b34adf5aabf},
  intrahash = {ff41a4c1e6d25476abe96f66a9381068},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {OpenAPI Specification (fka Swagger RESTful API Documentation Specification) Version 2.0},
  year = 2014
}

@article{parikh2013schema,
  added-at = {2019-01-16T17:43:17.000+0100},
  author = {Parikh, Sandeep and Stirman, Kelly},
  biburl = {https://www.bibsonomy.org/bibtex/27a12b947d2da231085ee86033e8d2148/vngudivada},
  interhash = {ad93e6728a6fa3cd12e722f0f380d550},
  intrahash = {7a12b947d2da231085ee86033e8d2148},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Schema Design for Time Series Data in MongoDB},
  year = 2013
}

@inproceedings{bailis2013towards,
  abstract = {While the CAP Theorem is often interpreted to preclude the availability of transactions in a partition-prone environment, we show that highly available systems can provide useful transactional semantics, often matching those of today’s ACID databases. We propose Highly Available Transactions (HATs) that are available in the presence of partitions. HATs support many desirable ACID guarantees for arbitrary transactional sequences of read and write operations and permit low-latency operation.},
  added-at = {2019-01-21T22:22:33.000+0100},
  author = {Bailis, Peter and Fekete, Alan and Ghodsi, Ali and Hellerstein, Joseph M. and Stoica, Ion},
  biburl = {https://www.bibsonomy.org/bibtex/2e40f23fb994ea5efa7bbda43195b8135/vngudivada},
  booktitle = {HotOS},
  crossref = {conf/hotos/2013},
  editor = {Maniatis, Petros},
  ee = {https://www.usenix.org/conference/hotos13/session/bailis},
  interhash = {a1a320aa1c1b9acc611826d10464197f},
  intrahash = {e40f23fb994ea5efa7bbda43195b8135},
  keywords = {NoSQL},
  publisher = {USENIX Association},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {HAT, Not CAP: Towards Highly Available Transactions.},
  url = {http://dblp.uni-trier.de/db/conf/hotos/hotos2013.html#BailisFGHS13},
  year = 2013
}

@article{preguica2010dotted,
  abstract = {In cloud computing environments, a large number of users access data stored in highly available storage systems. To provide good performance to geographically disperse users and allow operation even in the presence of failures or network partitions, these systems often rely on optimistic replication solutions that guarantee only eventual consistency. In this scenario, it is important to be able to accurately and efficiently identify updates executed concurrently. In this paper, first we review, and expose problems with current approaches to causality tracking in optimistic replication: these either lose information about causality or do not scale, as they require replicas to maintain information that grows linearly with the number of clients or updates. Then, we propose a novel solution that fully captures causality while being very concise in that it maintains information that grows linearly only with the number of servers that register updates for a given data element, bounded by the degree of replication. Comment: Preprint, submitted for publication.},
  added-at = {2019-01-21T20:16:58.000+0100},
  author = {Preguiça, Nuno M. and Baquero, Carlos and Almeida, Paulo Sérgio and Fonte, Victor and Gonçalves, Ricardo},
  biburl = {https://www.bibsonomy.org/bibtex/2241a6a3b495e0b4d98cb6680cdd20b62/vngudivada},
  ee = {http://arxiv.org/abs/1011.5808},
  interhash = {7189cb636b40bca0444dd3dcb4c8ea36},
  intrahash = {241a6a3b495e0b4d98cb6680cdd20b62},
  journal = {CoRR},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Dotted Version Vectors: Logical Clocks for Optimistic Replication},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1011.html#abs-1011-5808},
  volume = {abs/1011.5808},
  year = 2010
}

@article{bailis2013highly,
  abstract = {To minimize network latency and remain online during server failures and network partitions, many modern distributed data storage systems eschew transactional functionality, which provides strong semantic guarantees for groups of multiple operations over multiple data items. In this work, we consider the problem of providing Highly Available Transactions (HATs): transactional guarantees that do not suffer unavailability during system partitions or incur high network latency. We introduce a taxonomy of highly available systems and analyze existing ACID isolation and distributed data consistency guarantees to identify which can and cannot be achieved in HAT systems. This unifies the literature on weak transactional isolation, replica consistency, and highly available systems. We analytically and experimentally quantify the availability and performance benefits of HATs—often two to three orders of magnitude over wide-area networks—and discuss their necessary semantic compromises.},
  added-at = {2019-01-22T00:34:18.000+0100},
  author = {Bailis, Peter and Davidson, Aaron and Fekete, Alan and Ghodsi, Ali and Hellerstein, Joseph M. and Stoica, Ion},
  biburl = {https://www.bibsonomy.org/bibtex/28055b5fbe42188b2e787508cae2c9ce2/vngudivada},
  ee = {https://doi.org/10.14778/2732232.2732237},
  interhash = {20e8399bb6f4d2337c7f76b22463ca4f},
  intrahash = {8055b5fbe42188b2e787508cae2c9ce2},
  journal = {PVLDB},
  keywords = {NoSQL},
  number = 3,
  pages = {181-192},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Highly Available Transactions: Virtues and Limitations.},
  url = {http://dblp.uni-trier.de/db/journals/pvldb/pvldb7.html#BailisDFGHS13},
  volume = 7,
  year = 2013
}

@inproceedings{chang2006bigtable,
  abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.},
  added-at = {2019-01-17T20:39:07.000+0100},
  address = {Berkeley, CA, USA},
  author = {Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C. and Wallach, Deborah A. and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E.},
  biburl = {https://www.bibsonomy.org/bibtex/274ddc6aa8bde86c6669fe3545fcbb4d8/vngudivada},
  booktitle = {OSDI '06: Proceedings of the 7th USENIX Symposium on Operating Systems	Design and Implementation},
  interhash = {badf4db5d31f7ee7318ca619a589f2e1},
  intrahash = {74ddc6aa8bde86c6669fe3545fcbb4d8},
  keywords = {NoSQL},
  location = {Seattle, WA},
  pages = {15--15},
  publisher = {USENIX Association},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Bigtable: A Distributed Storage System for Structured Data},
  year = 2006
}

@article{hale2010sacrifice,
  added-at = {2019-01-22T00:47:27.000+0100},
  author = {Hale, Coda},
  biburl = {https://www.bibsonomy.org/bibtex/2485f2931fc79ee21b38a305016e25938/vngudivada},
  interhash = {6d0bf00524a7f64626211d29d7a19a77},
  intrahash = {485f2931fc79ee21b38a305016e25938},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {You Can't Sacrifice Partition Tolerance},
  year = 2010
}

@inproceedings{harizopoulos2008through,
  abstract = {Online Transaction Processing (OLTP) databases include a suite of features---disk-resident B-trees and heap files, locking-based concurrency control, support for multi-threading---that were optimized for computer technology of the late 1970's. Advances in modern processors, memories, and networks mean that today's computers are vastly different from those of 30 years ago, such that many OLTP databases will now fit in main memory, and most OLTP transactions can be processed in milliseconds or less. Yet database architecture has changed little. Based on this observation, we look at some interesting variants of conventional database systems that one might build that exploit recent hardware trends, and speculate on their performance through a detailed instruction-level breakdown of the major components involved in a transaction processing database system (Shore) running a subset of TPC-C. Rather than simply profiling Shore, we progressively modified it so that after every feature removal or optimization, we had a (faster) working system that fully ran our workload. Overall, we identify overheads and optimizations that explain a total difference of about a factor of 20x in raw performance. We also show that there is no single "high pole in the tent" in modern (memory resident) database systems, but that substantial time is spent in logging, latching, locking, B-tree, and buffer management operations.},
  added-at = {2019-01-17T21:32:13.000+0100},
  author = {Harizopoulos, Stavros and Abadi, Daniel J. and Madden, Samuel and Stonebraker, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/246d1083ce00e235a73475231eef34a74/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2008},
  editor = {Wang, Jason Tsong-Li},
  ee = {https://doi.org/10.1145/1376616.1376713},
  interhash = {734f3356625e77252f93698b5da60daf},
  intrahash = {46d1083ce00e235a73475231eef34a74},
  isbn = {978-1-60558-102-6},
  keywords = {NoSQL},
  pages = {981-992},
  publisher = {ACM},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {OLTP through the looking glass, and what we found there.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2008.html#HarizopoulosAMS08},
  year = 2008
}

@article{fredericks2015postgres,
  added-at = {2019-01-22T00:43:02.000+0100},
  author = {Fredericks, Gary},
  biburl = {https://www.bibsonomy.org/bibtex/228b9cd6e6075c9c1e3ad8dd48eeef8d0/vngudivada},
  interhash = {b12a0b7be2919592d21600dabda91044},
  intrahash = {28b9cd6e6075c9c1e3ad8dd48eeef8d0},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Postgres Serializability Bug},
  year = 2015
}

@article{kapila2012internals,
  added-at = {2019-01-17T22:33:48.000+0100},
  author = {Kapila, Amit},
  biburl = {https://www.bibsonomy.org/bibtex/2b39ef0000739c9f099565623573fb501/vngudivada},
  interhash = {7bdf1d37be8d58e41a7e1b961a1117ea},
  intrahash = {b39ef0000739c9f099565623573fb501},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {WAL Internals of PostgreSQL},
  year = 2012
}

@article{momjian2014unmasked,
  added-at = {2019-01-22T00:37:15.000+0100},
  author = {Momjian, Bruce},
  biburl = {https://www.bibsonomy.org/bibtex/2cc5fc7d5cd8c7cb39e006614ce873f56/vngudivada},
  interhash = {3edee73c5efccf48be8528c4d2822849},
  intrahash = {cc5fc7d5cd8c7cb39e006614ce873f56},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {MVCC Unmasked},
  year = 2014
}

@article{jr1983detection,
  abstract = {Many distributed systems are now being developed to provide users with convenient access to data via some kind of communications network. In many cases it is desirable to keep the system functioning even when it is partitioned by network failures. A serious problem in this context is how one can support redundant copies of resources such as files (for the sake of reliability) while simultaneously monitoring their mutual consistency (the equality of multiple copies). This is difficult since network faiures can lead to inconsistency, and disrupt attempts at maintaining consistency. In fact, even the detection of inconsistent copies is a nontrivial problem. Naive methods either 1) compare the multiple copies entirely or 2) perform simple tests which will diagnose some consistent copies as inconsistent. Here a new approach, involving version vectors and origin points, is presented and shown to detect single file, multiple copy mutual inconsistency effectively. The approach has been used in the design of LOCUS, a local network operating system at UCLA.},
  added-at = {2019-01-21T19:57:28.000+0100},
  author = {Jr., Douglas Stott Parker and Popek, Gerald J. and Rudisin, Gerard and Stoughton, Allen and Walker, Bruce J. and Walton, Evelyn and Chow, Johanna M. and Edwards, David A. and Kiser, Stephen and Kline, Charles S.},
  biburl = {https://www.bibsonomy.org/bibtex/260a71457fec17b3d47ce07b65f45cc74/vngudivada},
  ee = {http://doi.ieeecomputersociety.org/10.1109/TSE.1983.236733},
  interhash = {10674afe3f06078e783089e7a9e1dbb2},
  intrahash = {60a71457fec17b3d47ce07b65f45cc74},
  journal = {IEEE Trans. Software Eng.},
  keywords = {NoSQL},
  number = 3,
  pages = {240-247},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Detection of Mutual Inconsistency in Distributed Systems.},
  url = {http://dblp.uni-trier.de/db/journals/tse/tse9.html#ParkerPRSWWCEKK83},
  volume = 9,
  year = 1983
}

@article{vogels2008eventually,
  abstract = {At the foundation of Amazon's cloud computing are infrastructure services such as Amazon's S3 (Simple Storage Service), SimpleDB, and EC2 (Elastic Compute Cloud) that provide the resources for constructing Internet-scale computing platforms and a great variety of applications. Under the covers these services are massive distributed systems that operate on a worldwide scale. This scale creates additional challenges, because when a system processes trillions and trillions of requests, events that normally have a low probability of occurrence are now guaranteed to happen and must be accounted for upfront in the design and architecture of the system. When designing these large-scale systems at Amazon, systems designers use a set of guiding principles and abstractions related to large-scale data replication and focus on the trade-offs between high availability and data consistency. This article presents some of the relevant background that has informed the designers' approach to delivering reliable distributed systems that must operate on a global scale.},
  added-at = {2019-01-17T22:39:04.000+0100},
  author = {Vogels, Werner},
  biburl = {https://www.bibsonomy.org/bibtex/22c1219c0d001a7f226efbdad8b779159/vngudivada},
  ee = {https://doi.org/10.1145/1466443.1466448},
  interhash = {078277d6775622d3261475a6b04c4c42},
  intrahash = {2c1219c0d001a7f226efbdad8b779159},
  journal = {ACM Queue},
  keywords = {NoSQL},
  number = 6,
  pages = {14-19},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Eventually Consistent},
  url = {http://dblp.uni-trier.de/db/journals/queue/queue6.html#Vogels08a},
  volume = 6,
  year = 2008
}

@article{adya1999consistency,
  abstract = {Current commercial databases allow application programmers to trade off consistency for performance. However, existing definitions of weak consistency levels are either imprecise or they disallow efficient implementation techniques such as optimism. Ruling out these techniques is especially unfortunate because commercial databases support optimistic mechanisms. Furthermore, optimism is likely to be the implementation technique of choice in the geographically distributed and mobile systems of the future. This thesis presents the first implementation-independent specifications of existing ANSI isolation levels and a number of levels that are widely used in commercial systems, e.g., Cursor Stability, Snapshot Isolation. It also specifies a variety of guarantees for predicate-based operations in an implementation-independent manner. Two new levels are defined that provide useful consistency guarantees to application writers; one is the weakest level that ensures consistent reads, while the other captures some useful consistency properties provided by pessimistic implementations. We use a graph-based approach to define different isolation levels in a simple and intuitive manner. The thesis describes new implementation techniques for supporting different weak consistency levels in distributed client-server environments. The mechanisms are based on optimism and make use of multipart timestamps. A new technique is presented that allows multipart timestamps to scale well with the number of clients and servers in our system; the technique takes advantage of loosely synchronized clocks for removing old information in multipart timestamps. This thesis also presents the results of a simulation study to evaluate the performance of our optimistic schemes in data-shipping client-server systems The results show that the cost of providing serializability relative to mechanisms that provide lower consistency guarantees is negligible for low-contention workloads; furthermore, even for workloads with moderate to high-contention workloads, the cost of serializability is low. The simulation study also shows that our mechanisms based on multipart timestamps impose very low CPU, memory, and network costs while providing strong consistency guarantees to read-only and executing transactions.},
  added-at = {2019-01-22T00:30:53.000+0100},
  author = {Adya, Atul},
  biburl = {https://www.bibsonomy.org/bibtex/29ef0f40631cfe1e0b55e57665b7ecaed/vngudivada},
  interhash = {cd69e72c868d9a04ee939aa02e0fea8c},
  intrahash = {9ef0f40631cfe1e0b55e57665b7ecaed},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Weak Consistency: A Generalized Theory and Optimistic Implementations for Distibuted Transactions},
  year = 1999
}

@article{elliott2013crdts,
  added-at = {2019-01-21T19:27:31.000+0100},
  author = {Elliott, Sam},
  biburl = {https://www.bibsonomy.org/bibtex/250d4ceb6348107965319ee164238277b/vngudivada},
  interhash = {a7c92e5b865928b8f50b3a85f6f57ede},
  intrahash = {50d4ceb6348107965319ee164238277b},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {CRDTs: An UPDATE (or Maybe Just a PUT)},
  year = 2013
}

@article{schwarz1994detecting,
  abstract = {The paper shows that characterizing the causal relationship between significant events is an important but non-trivial aspect for understanding the behavior of distributed programs. An introduction to the notion of causality and its relation to logical time is given; some fundamental results concerning the characterization of causality are presented. Recent work on the detection of causal relationships in distributed computations is surveyed. The issue of observing distributed computations in a causally consistent way and the basic problems of detecting global predicates are discussed. To illustrate the major difficulties, some typical monitoring and debugging approaches are assessed, and it is demonstrated how their feasibility is severely limited by the fundamental problem to master the complexity of causal relationships.},
  added-at = {2019-01-21T20:25:19.000+0100},
  author = {Schwarz, Reinhard and Mattern, Friedemann},
  biburl = {https://www.bibsonomy.org/bibtex/2ae84037d65526f1b02ed3f36996e5a55/vngudivada},
  ee = {http://dx.doi.org/10.1007/BF02277859},
  interhash = {31aaaa830cf7edfaf05a089c0638c1f8},
  intrahash = {ae84037d65526f1b02ed3f36996e5a55},
  journal = {Distributed Computing},
  keywords = {NoSQL},
  number = 3,
  pages = {149-174},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Detecting Causal Relationships in Distributed Computations: In Search of the Holy Grail.},
  url = {http://dblp.uni-trier.de/db/journals/dc/dc7.html#SchwarzM94},
  volume = 7,
  year = 1994
}

@book{abiteboul1995foundations,
  abstract = {Over the past two decades, the theory concerning the logical level of database management systems has matured and become an elegant and robust piece of science. Foundations of Databases presents indepth coverage of this theory and surveys several emerging topics. Written by three leading researchers, this advanced text presents a unifying and contemporary perspective on the field. A major effort in writing the book has been to highlight the intuitions behind the theoretical development.},
  added-at = {2019-01-17T20:05:02.000+0100},
  author = {Abiteboul, Serge and Hull, Richard and Vianu, Victor},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  biburl = {https://www.bibsonomy.org/bibtex/260db61639969d6316ba76ff38f960845/vngudivada},
  groups = {public},
  interhash = {b9c9908e606f0bac2b7b02f65f2853f0},
  intrahash = {60db61639969d6316ba76ff38f960845},
  isbn = {0-201-53771-0},
  keywords = {NoSQL},
  publisher = {Addison-Wesley},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Foundations of Databases},
  username = {jpcik},
  year = 1995
}

@article{scherer2013those,
  added-at = {2019-01-22T00:07:41.000+0100},
  author = {Scherer, Dave},
  biburl = {https://www.bibsonomy.org/bibtex/25f39eeb33ca705160e04e22ef0731afa/vngudivada},
  interhash = {8177771ce1deb3705ae47214f3798881},
  intrahash = {5f39eeb33ca705160e04e22ef0731afa},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Those Are Not Transactions (Cassandra 2.0)},
  year = 2013
}

@article{george2009hbase,
  added-at = {2019-01-21T21:16:07.000+0100},
  author = {George, Lars},
  biburl = {https://www.bibsonomy.org/bibtex/2b29fdbf38f1c9f98579724452cb3a9d0/vngudivada},
  interhash = {357b8492367197283b3bccf232cfedab},
  intrahash = {b29fdbf38f1c9f98579724452cb3a9d0},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {HBase vs. BigTable Comparison},
  year = 2009
}

@article{kingsbury2013maybe,
  added-at = {2019-01-22T00:09:42.000+0100},
  author = {Kingsbury, Kyle},
  biburl = {https://www.bibsonomy.org/bibtex/253e6ddf318aff33d8614d1e7aa6531d4/vngudivada},
  interhash = {14423983bbcdfafff202c5e13dac4421},
  intrahash = {53e6ddf318aff33d8614d1e7aa6531d4},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Call Me Maybe: Cassandra},
  year = 2013
}

@article{kingsbury2014maybe,
  added-at = {2019-01-22T01:29:46.000+0100},
  author = {Kingsbury, Kyle},
  biburl = {https://www.bibsonomy.org/bibtex/24e5b7acc41f65635d517bb31d3762219/vngudivada},
  interhash = {5c932c222c14397d90e55f7a7a6428a5},
  intrahash = {4e5b7acc41f65635d517bb31d3762219},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Call Me Maybe: etcd and Consul},
  year = 2014
}

@article{berton2016bidirectional,
  added-at = {2019-01-21T19:21:13.000+0100},
  author = {Berton, Riley},
  biburl = {https://www.bibsonomy.org/bibtex/2ba88c214706d79d2e4e88b5636b2d715/vngudivada},
  interhash = {3f53fedd59f36ea595b04e858756970d},
  intrahash = {ba88c214706d79d2e4e88b5636b2d715},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {Is Bi-Directional Replication (BDR) in Postgres Transactional?},
  year = 2016
}

@article{hofhansl2013hbase7709,
  added-at = {2019-01-21T19:34:38.000+0100},
  author = {Hofhansl, Lars},
  biburl = {https://www.bibsonomy.org/bibtex/2540b02f4b29132dd61a25d727304e5f8/vngudivada},
  interhash = {a83e00005c90f9104c011d47737cadad},
  intrahash = {540b02f4b29132dd61a25d727304e5f8},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:13:59.000+0100},
  title = {HBASE-7709: Infinite Loop Possible in Master/Master Replication},
  year = 2013
}

@inproceedings{gaffney2014building,
  abstract = {With the growth of data in science and engineering fields and the I/O intense technologies used to carry out research with these massive datasets, it has become clear new solutions to support data research is required. In support of this, the Texas Advanced Computing Center presents Wrangler, the first open science research platform built from the ground up in support of data. Wrangler features a replicated 10 PB Lustre based parallel file system, compute capacity of 120 Intel Haswell nodes and 15 TB of RAM. In addition to the base system, Wrangler features a unique NAND flash-based storage system from DSSD, providing users with 0.5 PB of storage 1 TB/s bandwidth and 250 million IOP/s across the cluster. Supporting Hadoop, but not just Hadoop, Wrangler will provide current and future researchers with an environment supporting the most I/O intensive workflows in fields from astronomy to paleontology. With data at the forefront of Wrangler's mission, support for ETL workflows, data curation, and data publication will enable users as they both discover new results and publish their own research. Support for both SQL and noSQL databases and GIS based extensions will also be provided, allowing users to leverage these tools for both data cataloging and cross-study integration. Wrangler will allow users to focus more on what is most important to them, the data and knowledge gained from its analysis, and less on the details of curation and I/O optimization.},
  added-at = {2019-01-27T22:38:43.000+0100},
  author = {Gaffney, Niall and Jordan, Christopher and Minyard, Tommy and Stanzione, Dan},
  biburl = {https://www.bibsonomy.org/bibtex/2c7a9c2f75c820b5908e09d45d9340fd5/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2014},
  editor = {Lin, Jimmy J. and Pei, Jian and Hu, Xiaohua and Chang, Wo and Nambiar, Raghunath and Aggarwal, Charu C. and Cercone, Nick and Honavar, Vasant G. and Huan, Jun and Mobasher, Bamshad and Pyne, Saumyadipta},
  ee = {https://doi.org/10.1109/BigData.2014.7004480},
  interhash = {d49bef8d4fdcbad3fd2ec43ba3e4e79f},
  intrahash = {c7a9c2f75c820b5908e09d45d9340fd5},
  isbn = {978-1-4799-5665-4},
  keywords = {NoSQL OpenSciencePlatform},
  pages = {20-22},
  publisher = {IEEE},
  timestamp = {2019-03-19T15:46:28.000+0100},
  title = {Building Wrangler: A transformational data-intensive resource for the open science community},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2014.html#GaffneyJMS14},
  year = 2014
}

@article{lamport1998parttime,
  abstract = {Recent archaeological discoveries on the island of Paxos reveal that the parliament functioned despite the peripatetic propensity of its part-time legislators. The legislators maintained consistent copies of the parliamentary record, despite their frequent forays from the chamber and the forgetfulness of their messengers. The Paxon parliament's protocol provides a new way of implementing the state machine approach to the design of distributed systems.},
  added-at = {2019-03-11T17:21:57.000+0100},
  author = {Lamport, Leslie},
  biburl = {https://www.bibsonomy.org/bibtex/2450872488c36b979e72e4aad44355d21/vngudivada},
  ee = {https://www.wikidata.org/entity/Q55895338},
  interhash = {f33374573d370994feadb44d44f6c99b},
  intrahash = {450872488c36b979e72e4aad44355d21},
  journal = {ACM Trans. Comput. Syst.},
  keywords = {DistributedSystems Paxos StateMachine},
  number = 2,
  pages = {133-169},
  timestamp = {2019-03-19T15:43:21.000+0100},
  title = {The Part-Time Parliament},
  url = {http://dblp.uni-trier.de/db/journals/tocs/tocs16.html#Lamport98},
  volume = 16,
  year = 1998
}

@book{raynal2010communication,
  abstract = {Understanding distributed computing is not an easy task. This is due to the many facets of uncertainty one has to cope with and master in order to produce correct distributed software. Considering the uncertainty created by asynchrony and process crash failures in the context of message-passing systems, the book focuses on the main abstractions that one has to understand and master in order to be able to produce software with guaranteed properties. These fundamental abstractions are communication abstractions that allow the processes to communicate consistently (namely the register abstraction and the reliable broadcast abstraction), and the consensus agreement abstractions that allows them to cooperate despite failures. As they give a precise meaning to the words "communicate" and "agree" despite asynchrony and failures, these abstractions allow distributed programs to be designed with properties that can be stated and proved.

Impossibility results are associated with these abstractions. Hence, in order to circumvent these impossibilities, the book relies on the failure detector approach, and, consequently, that approach to fault-tolerance is central to the book.},
  added-at = {2019-02-04T19:34:17.000+0100},
  author = {Raynal, Michel},
  biburl = {https://www.bibsonomy.org/bibtex/217fd28aba8969016911160f79ede7b91/vngudivada},
  ee = {http://dx.doi.org/10.2200/S00236ED1V01Y201004DCT002},
  interhash = {0735fdeb2b9cbe26d3ce646bfe124154},
  intrahash = {17fd28aba8969016911160f79ede7b91},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Communication and Agreement Abstractions for Fault-Tolerant Asynchronous Distributed Systems},
  url = {http://dx.doi.org/10.2200/S00236ED1V01Y201004DCT002},
  year = 2010
}

@inproceedings{reniers2017schema,
  abstract = {Contemporary storage systems increasingly offer schema flexibility and support for semi-structured data models. This is the case for document-oriented databases, which as such allow ingestion of data from heterogeneous sources (IoT, sensors, monitoring). The increased influx of data further emphasizes the necessity for horizontal and elastic scalability, which are attained in NoSQL document stores through simplifying query functionality and relaxing transactional properties, e.g. through eventual consistency. The most compelling benefits of document stores are attained when data is stored in a denormalized form (De-NF). For example, one can decide to store relationships as an embedded copy to increase read query performance and as such avoid costly cross-node consultations. In comparison to the normalized form (NF), such designs come at a cost of additional data duplication, consistency and decreased write- and update performance. Determining the most appropriate data model for an application however depends on many factors, and the application developer is faced with the complexity of designing document data models that are optimized in terms of performance, scalability, storage and memory size, all requiring in-depth knowledge on the technology, the data meta-model, query plans and expected workloads. In this paper, we first discuss factors that impact the data schema design in document stores, such as the nature of the document and its attributes, horizontal partitioning, index selection, workload variability, and data uniformity. Although some data model design support tools are in existence, there are none that systematically take into account all these factors. Then, we outline our vision and roadmap towards systematic schema design support and tooling that involves (i) leveraging heuristics and common tactics to generate a finite number of candidate data models and (ii) ranking these candidate data models by means of cost functions that express their costeffectiveness.},
  added-at = {2019-01-25T20:21:09.000+0100},
  author = {Reniers, Vincent and Landuyt, Dimitri Van and Rafique, Ansar and Joosen, Wouter},
  biburl = {https://www.bibsonomy.org/bibtex/2c5378f2ac1cbbf9f769b1ecbced32487/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258261},
  interhash = {d96fe5c6c9a20f3611f850d503d3b680},
  intrahash = {c5378f2ac1cbbf9f769b1ecbced32487},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {2921-2930},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Schema design support for semi-structured data: Finding the sweet spot between NF and De-NF.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#ReniersLRJ17},
  year = 2017
}

@inproceedings{yang2018distributed,
  abstract = {We introduce a simple data model to process non-relational data for relational operations, and SHC (Apache Spark - Apache HBase Connector), an implementation of this model in the cluster computing framework, Spark. SHC leverages optimization techniques of relational data processing over the distributed and column-oriented key-value store (i.e., HBase). Compared to existing systems, SHC makes two major contributions. At first, SHC offers a much tighter integration between optimizations of relational data processing and non-relational data store, through a plug-in implementation that integrates with Spark SQL, a distributed in-memory computing engine for relational data. The design makes the system maintenance relatively easy, and enables users to perform complex data analytics on top of key-value store. Second, SHC leverages the Spark SQL Catalyst engine for high performance query optimizations and processing, e.g., data partitions pruning, columns pruning, predicates pushdown and data locality. SHC has been deployed and used in multiple production environments with hundreds of nodes, and provides OLAP query processing on petabytes of data efficiently.},
  added-at = {2019-01-24T19:44:02.000+0100},
  author = {Yang, Weiqing and Tang, MingJie and Yu, Yongyang and Liang, Yanbo and Saha, Bikas},
  biburl = {https://www.bibsonomy.org/bibtex/25b4fc0887e5f8de17806df04a73d21a1/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2018},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2018.00165},
  interhash = {ccd301c862e2d629348c8683847ed43d},
  intrahash = {5b4fc0887e5f8de17806df04a73d21a1},
  isbn = {978-1-5386-5520-7},
  keywords = {NoSQL},
  pages = {1465-1476},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {SHC: Distributed Query Processing for Non-Relational Data Store.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2018.html#YangTYLS18},
  year = 2018
}

@inproceedings{discala2016automatic,
  abstract = {Self-describing key-value data formats such as JSON are becoming increasingly popular as application developers choose to avoid the rigidity imposed by the relational model. Database systems designed for these self-describing formats, such as MongoDB, encourage users to use denormalized, heavily nested data models so that relationships across records and other schema information need not be predefined or standardized. Such data models contribute to long-term development complexity, as their lack of explicit entity and relationship tracking burdens new developers unfamiliar with the dataset. Furthermore, the large amount of data repetition present in such data layouts can introduce update anomalies and poor scan performance, which reduce both the quality and performance of analytics over the data.

In this paper we present an algorithm that automatically transforms the denormalized, nested data commonly found in NoSQL systems into traditional relational data that can be stored in a standard RDBMS. This process includes a schema generation algorithm that discovers relationships across the attributes of the denormalized datasets in order to organize those attributes into relational tables. It further includes a matching algorithm that discovers sets of attributes that represent overlapping entities and merges those sets together. These algorithms reduce data repetition, allow the use of data analysis tools targeted at relational data, accelerate scan-intensive algorithms over the data, and help users gain a semantic understanding of complex, nested datasets.},
  added-at = {2019-01-24T19:35:11.000+0100},
  author = {DiScala, Michael and Abadi, Daniel J.},
  biburl = {https://www.bibsonomy.org/bibtex/21097f1e410b0197881f0b2638df4f039/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2016},
  editor = {Özcan, Fatma and Koutrika, Georgia and Madden, Sam},
  ee = {https://doi.org/10.1145/2882903.2882924},
  interhash = {cf63042e5e81bb6a10be685f9e8cc520},
  intrahash = {1097f1e410b0197881f0b2638df4f039},
  isbn = {978-1-4503-3531-7},
  keywords = {NoSQL},
  pages = {295-310},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Automatic Generation of Normalized Relational Schemas from Nested Key-Value Data.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2016.html#DiScalaA16},
  year = 2016
}

@book{kranakis2010mobile,
  abstract = {Mobile agent computing is being used in fields as diverse as artificial intelligence, computational economics and robotics. Agents' ability to adapt dynamically and execute asynchronously and autonomously brings potential advantages in terms of fault-tolerance, flexibility and simplicity. This monograph focuses on studying mobile agents as modelled in distributed systems research and in particular within the framework of research performed in the distributed algorithms community. It studies the fundamental question of how to achieve rendezvous, the gathering of two or more agents at the same node of a network. Like leader election, such an operation is a useful subroutine in more general computations that may require the agents to synchronize, share information, divide up chores, etc.

The work provides an introduction to the algorithmic issues raised by the rendezvous problem in the distributed computing setting. For the most part our investigation concentrates on the simplest case of two agents attempting to rendezvous on a ring network. Other situations including multiple agents, faulty nodes and other topologies are also examined. An extensive bibliography provides many pointers to related work not covered in the text.

The presentation has a distinctly algorithmic, rigorous, distributed computing flavor and most results should be easily accessible to advanced undergraduate and graduate students in computer science and mathematics departments.},
  added-at = {2019-02-04T19:39:57.000+0100},
  author = {Kranakis, Evangelos and Krizanc, Danny and Markou, Euripides},
  biburl = {https://www.bibsonomy.org/bibtex/2a6abf80d1e2ef219b8224f89400ca949/vngudivada},
  ee = {http://dx.doi.org/10.2200/S00278ED1V01Y201004DCT001},
  interhash = {93179ee91b19c9583109dcb792c7143a},
  intrahash = {a6abf80d1e2ef219b8224f89400ca949},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {The Mobile Agent Rendezvous Problem in the Ring},
  url = {http://dx.doi.org/10.2200/S00278ED1V01Y201004DCT001},
  year = 2010
}

@inproceedings{flores2017invariant,
  abstract = {Due to the requirements imposed by data-intensive applications, NoSQL and NewSQL databases are becoming more present in the IT Market. These products provide alternative data models to the relational databases, and most of them are intrinsically distributed. These database management systems (DBMSs) relax consistency to favor availability and performance. However, applications that use NoSQL/NewSQL databases in distributed environments have to perform consistency control to avoid anomalies, such as inconsistent data and behavior. New approaches suggest the use of replicated data types (RDTs) to control conflicting updates. Another strategy is the use of different consistencies models for each type of operation, using first-order logic and theorem provers to supply the programmer with tools to classify consistency in operations while maintaining system invariants. Notwithstanding, the use of RDT or the descriptions of application integrity constraints in languages using first-order logic is still difficult to be used by programmers. Aiming to simplify the definition of most common database constraints, this paper proposes a mechanism to extract usual integrity constraints in an intermediate model, taking into account the semantics of invariances, using a mix of RDTs and first-order logic. The aim of this paper is to demonstrate how the proposed mechanism simplifies and guarantees safer programming with consistency control being performed at the application level.},
  added-at = {2019-01-24T18:12:27.000+0100},
  author = {Flores, Paulo Arion and Siqueira, Frank},
  biburl = {https://www.bibsonomy.org/bibtex/28792896fd5c42daaeefd2145f8311728/vngudivada},
  booktitle = {WebDB},
  crossref = {conf/webdb/2017},
  editor = {Meliou, Alexandra and Senellart, Pierre},
  ee = {https://doi.org/10.1145/3068839.3068844},
  interhash = {ac8ebb2831533385b8c853f45eb069f6},
  intrahash = {8792896fd5c42daaeefd2145f8311728},
  isbn = {978-1-4503-4983-3},
  keywords = {NoSQL},
  pages = {13-18},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Invariant Control in Eventually Consistent Databases.},
  url = {http://dblp.uni-trier.de/db/conf/webdb/webdb2017.html#FloresS17},
  year = 2017
}

@inproceedings{huang2016yinmem,
  abstract = {Machine learning and graph analytics typically process data in an iterative way, reading the same data multiple times and sharing intermediate results across the worker nodes in cluster. Hadoop MapReduce and Spark are two popular open source cluster compute frameworks for large scale data analytics. Apache Spark is currently the state-of-the-art in-memory computation model extending MapReduce by transforming data into RDDs stored in memory. One limitation of Spark, however, lies in the fact that data transformation and distribution is implicitly managed by HDFS. Data locality is not guaranteed for iterative machine learning algorithms which read the same data multiple times. For example, data needed for operations to one worker node might reside in RDDs stored in other worker nodes. The resulting data shuffling becomes a bottleneck when iteratively reading such RDDs. We propose YinMem, a parallel distributed indexed in-memory computation system, bridging the gap between Hadoop ecosystem and HPC by replacing MapReduce with MPI while obtaining the advantage of the distributed data storage. YinMem achieves fair load balancing prior to computation for large sparse matrix by scheduling and distributing indexed data from NoSQL database to the RAM of working nodes. YinMem explores Alluxio as the in-memory storage system and enables efficient data sharing of intermediate results. Preliminary results show that YinMem has achieved 3× speedup to Spark, for computing eigenvalue and eigenvectors of a 16-million scale sparse matrix.},
  added-at = {2019-01-27T21:59:47.000+0100},
  author = {Huang, Yin and Yesha, Yelena and Halem, Milton and Yesha, Yaacov and Zhou, Shujia},
  biburl = {https://www.bibsonomy.org/bibtex/24c6f8f8f6efb9a71a1d33b07c3caaea2/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2016},
  editor = {Joshi, James and Karypis, George and Liu, Ling and Hu, Xiaohua and Ak, Ronay and Xia, Yinglong and Xu, Weijia and Sato, Aki-Hiro and Rachuri, Sudarsan and Ungar, Lyle H. and Yu, Philip S. and Govindaraju, Rama and Suzumura, Toyotaro},
  ee = {http://dx.doi.org/10.1109/BigData.2016.7840607},
  interhash = {05f4aac66392dade3628b9a4c882df87},
  intrahash = {4c6f8f8f6efb9a71a1d33b07c3caaea2},
  isbn = {978-1-4673-9005-7},
  keywords = {NoSQL},
  pages = {214-222},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {YinMem: A distributed parallel indexed in-memory computation system for large scale data analytics.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2016.html#HuangYHYZ16},
  year = 2016
}

@inproceedings{kim2017comparative,
  abstract = {The proliferation of GPS-enabled mobile devices has generated geo-tagged data at an unprecedented rate over the past decade. Data-processing systems that aim to ingest, store, index, and analyze Big Data must deal with such geo-tagged data efficiently. In this paper, among representative, disk-resident spatial indexing methods that have been adopted by major SQL and NoSQL systems, we implement five variants of these methods in the form of Log-Structured Merge-tree-based (LSM) spatial indexes in order to evaluate their pros and cons for dynamic geo-tagged Big Data. We have implemented the alternatives, including LSM-based B-tree, R-tree, and inverted index variants, in Apache AsterixDB, an open source Big Data management system. This implementation enabled comparison in terms of real end-to-end performance, including logging and locking overheads, in a full-function, query-based system setting. Our evaluation includes both static and dynamic workloads, ranging from a "load once, query many" case to a case where continuous concurrent incremental inserts are mixed with concurrent queries. Based on the results, we discuss the pros and cons of the five index variants.},
  added-at = {2019-01-24T20:14:02.000+0100},
  author = {Kim, Young-Seok and Kim, Taewoo and Carey, Michael J. and Li, Chen},
  biburl = {https://www.bibsonomy.org/bibtex/225dd9d0cb016b4cc3edf85a06dcd1128/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2017},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2017.61},
  interhash = {bdd46d0c59a67e32cde40253216f47ad},
  intrahash = {25dd9d0cb016b4cc3edf85a06dcd1128},
  isbn = {978-1-5090-6543-1},
  keywords = {NoSQL},
  pages = {147-150},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {A Comparative Study of Log-Structured Merge-Tree-Based Spatial Indexes for Big Data.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2017.html#KimKC017},
  year = 2017
}

@inproceedings{papailiou2014h2rdf,
  abstract = {The proliferation of data in RDF format has resulted in the emergence of a plethora of specialized management systems. While the ability to adapt to the complexity of a SPARQL query -- given their inherent diversity -- is crucial, current approaches do not scale well when faced with substantially complex, non-selective joins, resulting in exponential growth of execution times. In this demonstration we present H2 RDF+, an RDF store that efficiently performs distributed Merge and Sort-Merge joins using a multiple-index scheme over HBase indexes. Through a greedy planner that incorporates our cost-model, it adaptively commands for either single or multi-machine query execution based on join complexity. In this paper, we present its key scientific contributions and allow participants to interact with an H2RDF+ deployment over a Cloud infrastructure. Using a web-based GUI we allow users to load different datasets (both real and synthetic), apply any query (custom or predefined) and monitor its execution. By allowing real-time inspection of cluster status, response times and committed resources the audience will evaluate the validity of H2RDF+'s claims and perform direct comparisons to two other state-of-the-art RDF stores.},
  added-at = {2019-01-24T18:16:15.000+0100},
  author = {Papailiou, Nikolaos and Tsoumakos, Dimitrios and Konstantinou, Ioannis and Karras, Panagiotis and Koziris, Nectarios},
  biburl = {https://www.bibsonomy.org/bibtex/290e18e3cccac6c6cf4ccf6903694258d/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2014},
  editor = {Dyreson, Curtis E. and Li, Feifei and Özsu, M. Tamer},
  ee = {https://doi.org/10.1145/2588555.2594535},
  interhash = {9455c8782da1ee0aec283c7661d447a6},
  intrahash = {90e18e3cccac6c6cf4ccf6903694258d},
  isbn = {978-1-4503-2376-5},
  keywords = {NoSQL},
  pages = {909-912},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {H2RDF+: an efficient data management system for big RDF graphs.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2014.html#PapailiouTKKK14},
  year = 2014
}

@book{raynal2010faulttolerant,
  abstract = {Understanding distributed computing is not an easy task. This is due to the many facets of uncertainty one has to cope with and master in order to produce correct distributed software. A previous book Communication and Agreement Abstraction for Fault-tolerant Asynchronous Distributed Systems (published by Morgan & Claypool, 2010) was devoted to the problems created by crash failures in asynchronous message-passing systems.

The present book focuses on the way to cope with the uncertainty created by process failures (crash, omission failures and Byzantine behavior) in synchronous message-passing systems (i.e., systems whose progress is governed by the passage of time). To that end, the book considers fundamental problems that distributed synchronous processes have to solve. These fundamental problems concern agreement among processes (if processes are unable to agree in one way or another in presence of failures, no non-trivial problem can be solved). They are consensus, interactive consistency, k-set agreement and non-blocking atomic commit.

Being able to solve these basic problems efficiently with provable guarantees allows applications designers to give a precise meaning to the words "cooperate" and "agree" despite failures, and write distributed synchronous programs with properties that can be stated and proved.

Hence, the aim of the book is to present a comprehensive view of agreement problems, algorithms that solve them and associated computability bounds in synchronous message-passing distributed systems.},
  added-at = {2019-02-04T19:31:24.000+0100},
  author = {Raynal, Michel},
  biburl = {https://www.bibsonomy.org/bibtex/2e9b609112eceb3d311f5d0a3b5b1a8c7/vngudivada},
  ee = {http://dx.doi.org/10.2200/S00294ED1V01Y201009DCT003},
  interhash = {3f61119ef7bcf7427d3a5baee634029a},
  intrahash = {e9b609112eceb3d311f5d0a3b5b1a8c7},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Fault-tolerant Agreement in Synchronous Message-passing Systems},
  url = {http://dx.doi.org/10.2200/S00294ED1V01Y201009DCT003},
  year = 2010
}

@inproceedings{sedlar2014learned,
  abstract = {The modern platforms that we want to use to manage our data are far more complex to program efficiently than the machines we used in the past. Every computer we run on is a massively parallel machine with many architectural "surprises" for programmers who are unaware of the way the underlying hardware architecture works. A simple way to think about this is that optimal programs must specify how & where the data should move, not just what computations should be performed and in what order. Architecture-oblivious software that leaves the decisions about data movement to a low-level coherence protocol is becoming much less efficient, relatively speaking. After an extended flirtation with using imperative programming frameworks such as Map-Reduce and NoSQL, many people are returning back to declarative languages like SQL, where the language compiler & runtime are free to make most of the data movement decisions for the programmer. Another way to think about a SQL compiler is that it includes an "algorithm picker" and the runtime includes libraries of useful algorithm implementations (which contain the data movement specifications). This talk will discuss the needs and opportunities for expanding the domain of algorithm-picking languages like SQL. Doing so will require integration with managed-language runtime compilers (e.g. Java or Javascript compilers) that are integrated with the SQL compiler not just to provide efficiency gains during query execution, but also to use managed language runtime profiling to help in algorithm selection as well as assembly-level compilation decisions.},
  added-at = {2019-01-24T18:51:00.000+0100},
  author = {Sedlar, Eric},
  biburl = {https://www.bibsonomy.org/bibtex/2eed636f1163d7fcca83b9c8c4623a6f5/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2014},
  editor = {Dyreson, Curtis E. and Li, Feifei and Özsu, M. Tamer},
  ee = {https://doi.org/10.1145/2588555.2602133},
  interhash = {c9ca8eb7d2c7452e5bdad2b7be0365c9},
  intrahash = {eed636f1163d7fcca83b9c8c4623a6f5},
  isbn = {978-1-4503-2376-5},
  keywords = {NoSQL},
  pages = {1-2},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {How i learned to stop worrying and love compilers.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2014.html#Sedlar14},
  year = 2014
}

@inproceedings{zoun2018internet,
  abstract = {Metaproteomics is a field of biology to analyze a microbial community. However, the growth of available data and of the scientific community itself is not reflected in the currently used software tools in this field. The standard tools are file-based and use proprietary methods or implementations. Thus, calculation performance does not increase with the same speed that new data is generated. Furthermore, a reasonable data exchange mechanism is missing which makes collaboration across departments cumbersome. As a solution, we identified the internet of things workflow as promising, because it counters the above-mentioned challenges. To this end, we are going to implement a streaming-based cloud platform using a fast data architecture and integrate the algorithms into a new cloud-based system. In this paper, we elaborate arising challenges and how we envision solving them.},
  added-at = {2019-01-24T20:11:15.000+0100},
  author = {Zoun, Roman},
  biburl = {https://www.bibsonomy.org/bibtex/25cb7086483c66a7bdcf56c235c65d025/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2018},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2018.00221},
  interhash = {220146864d8e2f2452c34217514e93fb},
  intrahash = {5cb7086483c66a7bdcf56c235c65d025},
  isbn = {978-1-5386-5520-7},
  keywords = {NoSQL},
  pages = {1714-1718},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Internet of Metaproteomics.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2018.html#Zoun18},
  year = 2018
}

@article{brewer2012twelve,
  abstract = {The CAP theorem asserts that any networked shared-data system can have only two of three desirable properties. However, by explicitly handling partitions, designers can optimize consistency and availability, thereby achieving some trade-off of all three. The featured Web extra is a podcast from Software Engineering Radio, in which the host interviews Dwight Merriman about the emerging NoSQL movement, the three types of nonrelational data stores, Brewer's CAP theorem, and much more.},
  added-at = {2019-01-22T01:37:24.000+0100},
  author = {Brewer, Eric},
  biburl = {https://www.bibsonomy.org/bibtex/298853204148c62a33fe148702ba20223/vngudivada},
  doi = {10.1109/MC.2012.37},
  file = {IEEE Digital Library:2012/Brewer12computer.pdf:PDF},
  groups = {public},
  interhash = {42048ae3c9aad919da1d5d86dc2710ff},
  intrahash = {98853204148c62a33fe148702ba20223},
  issn = {0018-9162},
  journal = {Computer},
  keywords = {NoSQL},
  month = {#feb#},
  number = 2,
  pages = {23--29},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {CAP Twelve Years Later: How the ''Rules'' Have Changed},
  username = {flint63},
  volume = 45,
  year = 2012
}

@inproceedings{chirigati2016virtual,
  abstract = {Increasingly, applications that deal with big data need to run analytics concurrently with updates. But bridging the gap between big and fast data is challenging: most of these applications require analytics' results that are fresh and consistent, but without impacting system latency and throughput. We propose virtual lightweight snapshots (VLS), a mechanism that enables consistent analytics without blocking incoming updates in NoSQL stores. VLS requires neither native support for database versioning nor a transaction manager. Besides, it is storage-efficient, keeping additional versions of records only when needed to guarantee consistency, and sharing versions across multiple concurrent snapshots. We describe an implementation of VLS in MongoDB and present a detailed experimental evaluation which shows that it supports consistency for analytics with small impact on query evaluation time, update throughput, and latency.},
  added-at = {2019-01-24T20:56:32.000+0100},
  author = {Chirigati, Fernando and Siméon, Jérôme and Hirzel, Martin and Freire, Juliana},
  biburl = {https://www.bibsonomy.org/bibtex/2bd2b6fb0f0640b90acc5c5be04d4ac9b/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2016},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2016.7498334},
  interhash = {6f139310770768cd88d325a0cfa554c8},
  intrahash = {bd2b6fb0f0640b90acc5c5be04d4ac9b},
  isbn = {978-1-5090-2020-1},
  keywords = {NoSQL},
  pages = {1310-1321},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Virtual lightweight snapshots for consistent analytics in NoSQL stores.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2016.html#ChirigatiSHF16},
  year = 2016
}

@inproceedings{jananthan2017polystore,
  abstract = {Financial transactions, internet search, and data analysis are all placing increasing demands on databases. SQL, NoSQL, and NewSQL databases have been developed to meet these demands and each offers unique benefits. SQL, NoSQL, and NewSQL databases also rely on different underlying mathematical models. Polystores seek to provide a mechanism to allow applications to transparently achieve the benefits of diverse databases while insulating applications from the details of these databases. Integrating the underlying mathematics of these diverse databases can be an important enabler for polystores as it enables effective reasoning across different databases. Associative arrays provide a common approach for the mathematics of polystores by encompassing the mathematics found in different databases: sets (SQL), graphs (NoSQL), and matrices (NewSQL). Prior work presented the SQL relational model in terms of associative arrays and identified key mathematical properties that are preserved within SQL. This work provides the rigorous mathematical definitions, lemmas, and theorems underlying these properties. Specifically, SQL Relational Algebra deals primarily with relations - multisets of tuples - and operations on and between those relations. These relations can be modeled as associative arrays by treating tuples as non-zero rows in an array. Operations in relational algebra are built as compositions of standard operations on associative arrays which mirror their matrix counterparts. These constructions provide insight into how relational algebra can be handled via array operations. As an example application, the composition of two projection operations is shown to also be a projection, and the projection of a union is shown to be equal to the union of the projections.},
  added-at = {2019-01-25T20:55:08.000+0100},
  author = {Jananthan, Hayden and Zhou, Ziqi and Gadepally, Vijay and Hutchison, Dylan and Kim, Suna and Kepner, Jeremy},
  biburl = {https://www.bibsonomy.org/bibtex/252be1c9aef8e45a228eebaba6c78e619/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258298},
  interhash = {f9352122899dd9007e2f4af33b64c825},
  intrahash = {52be1c9aef8e45a228eebaba6c78e619},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {3180-3189},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Polystore mathematics of relational algebra.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#JananthanZGHKK17},
  year = 2017
}

@inproceedings{bordawekar2016accelerating,
  abstract = {The key objective of this tutorial is to provide a broad, yet an in-depth survey of the emerging field of co-designing software, hardware, and systems components for accelerating enterprise data management workloads. The overall goal of this tutorial is two-fold. First, we provide a concise system-level characterization of different types of data management technologies, namely, the relational and NoSQL databases and data stream management systems from the perspective of analytical workloads. Using the characterization, we discuss opportunities for accelerating key data management workloads using software and hardware approaches. Second, we dive deeper into the hardware acceleration opportunities using Graphics Processing Units (GPUs) and Field-Programmable Gate Arrays (FPGAs) for the query execution pipeline. Furthermore, we explore other hardware acceleration mechanisms such as single-instruction multiple-data (SIMD) that enables short-vector data parallelism.},
  added-at = {2019-01-24T20:59:12.000+0100},
  author = {Bordawekar, Rajesh R. and Sadoghi, Mohammad},
  biburl = {https://www.bibsonomy.org/bibtex/2b542d7fd77ab82df7e913c324eda7d89/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2016},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2016.7498362},
  interhash = {6f6f0cd7fe6fe6bd2a0d1bd152f89f49},
  intrahash = {b542d7fd77ab82df7e913c324eda7d89},
  isbn = {978-1-5090-2020-1},
  keywords = {NoSQL},
  pages = {1428-1431},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Accelerating database workloads by software-hardware-system co-design.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2016.html#BordawekarS16},
  year = 2016
}

@inproceedings{luckow2015automotive,
  abstract = {Data is increasingly affecting the automotive industry, from vehicle development, to manufacturing and service processes, to online services centered around the connected vehicle. Connected, mobile and Internet of Things devices and machines generate immense amounts of sensor data. The ability to process and analyze this data to extract insights and knowledge that enable intelligent services, new ways to understand business problems, improvements of processes and decisions, is a critical capability. Hadoop is a scalable platform for compute and storage and emerged as de-facto standard for Big Data processing at Internet companies and in the scientific community. However, there is a lack of understanding of how and for what use cases these new Hadoop capabilities can be efficiently used to augment automotive applications and systems. This paper surveys use cases and applications for deploying Hadoop in the automotive industry. Over the years a rich ecosystem emerged around Hadoop comprising tools for parallel, in-memory and stream processing (most notable MapReduce and Spark), SQL and NOSQL engines (Hive, HBase), and machine learning (Mahout, MLlib). It is critical to develop an understanding of automotive applications and their characteristics and requirements for data discovery, integration, exploration and analytics. We then map these requirements to a confined technical architecture consisting of core Hadoop services and libraries for data ingest, processing and analytics. The objective of this paper is to address questions, such as: What applications and datasets are suitable for Hadoop? How can a diverse set of frameworks and tools be managed on multi-tenant Hadoop cluster? How do these tools integrate with existing relational data management systems? How can enterprise security requirements be addressed? What are the performance characteristics of these tools for real-world automotive applications? To address the last question, we utilize a standard benchmark (TPCx-HS), and two application benchmarks (SQL and machine learning) that operate on a dataset of multiple Terabytes and billions of rows.},
  added-at = {2019-01-27T22:12:24.000+0100},
  author = {Luckow, André and Kennedy, Ken and Manhardt, Fabian and Djerekarov, Emil and Vorster, Bennie and Apon, Amy W.},
  biburl = {https://www.bibsonomy.org/bibtex/2bd154cfde8e02ac53aebd88a3ddcd1b9/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2015},
  ee = {https://doi.org/10.1109/BigData.2015.7363874},
  interhash = {68c1307969d81e6014adc83c86456cae},
  intrahash = {bd154cfde8e02ac53aebd88a3ddcd1b9},
  isbn = {978-1-4799-9926-2},
  keywords = {NoSQL},
  pages = {1201-1210},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Automotive big data: Applications, workloads and infrastructures.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2015.html#LuckowKMDVA15},
  year = 2015
}

@inproceedings{pirzadeh2017performance,
  abstract = {Big Data analytics has become an invaluable tool in a wide variety of businesses for exploiting the wealth of Big Data that they now have access to. As a result, various solutions within different categories of Big Data systems are emerging to meet their needs. In this paper we use the TPC-H benchmark to compare the performance of four Big Data systems picked from the major categories of Big Data platforms: a commercial parallel relational database (from the traditional DBMS world), Hive and Spark SQL (from the SQL-on-Hadoop world), and AsterixDB (from the world of NoSQL systems). All of these systems have sufficiently rich query APIs and runtime systems to run TPC-H in its full form. On the other hand, the systems also have major differences in terms of their architectures, preferred storage formats, support for complex schema definitions, and approaches to query processing. This makes them a very interesting set of representative Big Data systems to compare. We present the results that we obtained through running these systems at different TPC-H scales using various settings, and we analyze a selected set of interesting query results in more detail to explore the trade-offs between performance, storage formats, and schema definitions. A follow-up discussion is included as well to summarize the lessons learned from this effort.},
  added-at = {2019-01-25T20:18:55.000+0100},
  author = {Pirzadeh, Pouria and Carey, Michael J. and Westmann, Till},
  biburl = {https://www.bibsonomy.org/bibtex/2919011d72d8d529ef2cabc2241574c20/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258260},
  interhash = {0267aa18efbd51e2165369595e090573},
  intrahash = {919011d72d8d529ef2cabc2241574c20},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {2911-2920},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {A performance study of big data analytics platforms.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#Pirzadeh0W17},
  year = 2017
}

@inproceedings{chatziantoniou2014introducing,
  abstract = {Until recently, when relational systems was the main data management option and SQL the de facto language for querying/analyzing data, ODBC was an excellent API for applications to interact with the data provider. Standardization of data retrieval has helped innovation and productivity, allowing application developers to focus on the core of their ideas. However, the big data era added variety to all aspects of data facilitation: variety in data management options, variety in data formats, variety in querying/analyzing tasks. In this chaotic situation, standardizing data connectivity is more important than ever. What should be the replacement of ODBC? In this paper, we propose ODMC (Open Data Management Connectivity), a client-server protocol between data management entities (DMEs). A DME is anything that manages/manipulates data. In that respect, spreadsheets, java programs, Hadoop, RDBMs, stream engines, NoSQL, etc., all act as DMEs. In addition, there is no distinction between applications and data management servers, as in ODBC. A DME can be a data consumer in an ODMC instance and a data producer in another. This composability principle allows for the definition of analysis workflows. We present a preliminary implementation of ODMC for python-based DMEs. We argue that ODMC is simple, intuitive, scalable and suitable for both persistent and stream data.},
  added-at = {2019-01-24T19:39:02.000+0100},
  author = {Chatziantoniou, Damianos and Tselai, Florents},
  biburl = {https://www.bibsonomy.org/bibtex/29b739620d01c1baaf3821c79ec6aaa3c/vngudivada},
  booktitle = {DanaC@SIGMOD},
  crossref = {conf/sigmod/2014danac},
  editor = {Katsifodimos, Asterios and Tzoumas, Kostas and Babu, Shivnath},
  ee = {https://doi.org/10.1145/2627770.2627773},
  interhash = {c3db7624730d796281753e30e9315eaf},
  intrahash = {9b739620d01c1baaf3821c79ec6aaa3c},
  isbn = {978-1-4503-2997-2},
  keywords = {NoSQL},
  pages = {7:1-7:4},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Introducing Data Connectivity in a Big Data Web.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/danac2014.html#ChatziantoniouT14},
  year = 2014
}

@inproceedings{sezer2016extended,
  abstract = {Many experts claim that data will be the most valuable commodity in the 21st century. At the same time, two of the most influential components of this era, Big Data and IoT are moving very fast, on a collision course with the methodologies that are associated with conventional data processing and database systems. As a result, new approaches like NoSQL databases, distributed architectures, etc. started appearing on the stage. Meanwhile, another technology, ontology and semantic data processing can be a very convenient catalyzer that might assist in smoothly providing this transformation process. In this paper, we propose a combined framework that brings Big Data, IoT, and semantic web together to build an augmented framework for this new era. We not only list the components of such a system and define the necessary bindings that needs to be integrated together, but also provide a realistic use case that demonstrates how the model can implement the desired functionality and achieve the goals of such a model.},
  added-at = {2019-01-27T22:01:07.000+0100},
  author = {Sezer, Omer Berat and Dogdu, Erdogan and Ozbayoglu, A. Murat and Onal, Aras},
  biburl = {https://www.bibsonomy.org/bibtex/252adbdbe14119326709fa89580c506a8/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2016},
  editor = {Joshi, James and Karypis, George and Liu, Ling and Hu, Xiaohua and Ak, Ronay and Xia, Yinglong and Xu, Weijia and Sato, Aki-Hiro and Rachuri, Sudarsan and Ungar, Lyle H. and Yu, Philip S. and Govindaraju, Rama and Suzumura, Toyotaro},
  ee = {https://doi.org/10.1109/BigData.2016.7840803},
  interhash = {013a03bc638ad2355de19ed6166d7a01},
  intrahash = {52adbdbe14119326709fa89580c506a8},
  isbn = {978-1-4673-9005-7},
  keywords = {NoSQL},
  pages = {1849-1856},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {An extended IoT framework with semantics, big data, and analytics.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2016.html#SezerDOO16},
  year = 2016
}

@inproceedings{qader2018comparative,
  abstract = {NoSQL databases are increasingly used in big data applications, because they achieve fast write throughput and fast lookups on the primary key. Many of these applications also require queries on non-primary attributes. For that reason, several NoSQL databases have added support for secondary indexes. However, these works are fragmented, as each system generally supports one type of secondary index, and may be using different names or no name at all to refer to such indexes. As there is no single system that supports all types of secondary indexes, no experimental head-to-head comparison or performance analysis of the various secondary indexing techniques in terms of throughput and space exists. In this paper, we present a taxonomy of NoSQL secondary indexes, broadly split into two classes: Embedded Indexes (i.e. lightweight filters embedded inside the primary table) and Stand-Alone Indexes (i.e. separate data structures). To ensure the fairness of our comparative study, we built a system, LevelDB++, on top of Google's popular open-source LevelDB key-value store. There, we implemented two Embedded Indexes and three state-of-the-art Stand-Alone indexes, which cover most of the popular NoSQL databases. Our comprehensive experimental study and theoretical evaluation show that none of these indexing techniques dominate the others: the embedded indexes offer superior write throughput and are more space efficient, whereas the stand-alone secondary indexes achieve faster query response times. Thus, the optimal choice of secondary index depends on the application workload. This paper provides an empirical guideline for choosing secondary indexes},
  added-at = {2019-01-24T18:31:31.000+0100},
  author = {Qader, Mohiuddin Abdul and Cheng, Shiwen and Hristidis, Vagelis},
  biburl = {https://www.bibsonomy.org/bibtex/28aa0c5d2ad6e8594f8852602d934ee5b/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2018},
  editor = {Das, Gautam and Jermaine, Christopher M. and Bernstein, Philip A.},
  ee = {https://doi.org/10.1145/3183713.3196900},
  interhash = {0e8de0d78aba4b99b13c3bf77293fae0},
  intrahash = {8aa0c5d2ad6e8594f8852602d934ee5b},
  keywords = {NoSQL},
  pages = {551-566},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {A Comparative Study of Secondary Indexing Techniques in LSM-based NoSQL Databases.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2018.html#QaderCH18},
  year = 2018
}

@article{robinson2014elephant,
  added-at = {2019-01-22T02:25:44.000+0100},
  author = {Robinson, Henry},
  biburl = {https://www.bibsonomy.org/bibtex/2d93fdfac00efc4d17c8487082d7df0fc/vngudivada},
  interhash = {04e0fe2d45b1bded1def7f0093bcbfae},
  intrahash = {d93fdfac00efc4d17c8487082d7df0fc},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {The Elephant Was a Trojan Horse: On the Death of Map-Reduce at Google},
  year = 2014
}

@inproceedings{ameri2015approach,
  abstract = {Considering the wide usage of databases and their ever growing size, it is crucial to improve the query processing performance. Selection of an appropriate set of indexes for the workload processed by the database system is an important part of physical design and performance tuning. This selection is a non-trivial tasks, especially considering possible number of native indexes in modern databases. We introduce a new approach to the index selection problem using data mining. The method recommends the creation of indexes as well as the type of each index. This results in more precise index recommendations that allows not only to create ascending and descending indexes, but also special indexes supported by the database system. Mining of queries results in candidate indexes for which virtual indexes get created. As the approach does not require modifications of the database system, it is generically applicable. Evaluations of the scalability are given for different workloads for the document-based NoSQL database MongoDB.},
  added-at = {2019-01-27T22:23:44.000+0100},
  author = {Ameri, Parinaz and Meyer, Jörg and Streit, Achim},
  biburl = {https://www.bibsonomy.org/bibtex/2122583c8f8a795702edc70b03a7540b1/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2015},
  ee = {https://doi.org/10.1109/BigData.2015.7364084},
  interhash = {d938c1410c42a1e5f973e9894c4dd021},
  intrahash = {122583c8f8a795702edc70b03a7540b1},
  isbn = {978-1-4799-9926-2},
  keywords = {NoSQL},
  pages = {2801-2810},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {On a new approach to the index selection problem using mining algorithms.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2015.html#AmeriMS15},
  year = 2015
}

@article{cockcroft2014migrating,
  added-at = {2019-01-22T01:43:32.000+0100},
  author = {Cockcroft, Adrian},
  biburl = {https://www.bibsonomy.org/bibtex/2a2f1c92bc303cadca277237d3db4598d/vngudivada},
  interhash = {0e6305abc9efbbaf8e87af3c846eb2e4},
  intrahash = {a2f1c92bc303cadca277237d3db4598d},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Migrating to Microservices},
  year = 2014
}

@inproceedings{scherzinger2015controvol,
  abstract = {Building scalable web applications on top of NoSQL data stores is becoming common practice. Many of these data stores can easily be accessed programmatically, and do not enforce a schema. Software engineers can design the data model on the go, a flexibility that is crucial in agile software development. The typical tasks of database schema management are now handled within the application code, usually involving object mapper libraries. However, today's Integrated Development Environments (IDEs) lack the proper tool support when it comes to managing the combined evolution of the application code and of the schema. Yet simple refactorings such as renaming an attribute at the source code level can cause irretrievable data loss or runtime errors once the application is serving in production. In this demo, we present ControVol, a framework for controlled schema evolution in application development against NoSQL data stores. ControVol is integrated into the IDE and statically type checks object mapper class declarations against the schema evolution history, as recorded by the code repository. ControVol is capable of warning of common yet risky cases of mismatched data and schema. ControVol is further able to suggest quick fixes by which developers can have these issues automatically resolved.},
  added-at = {2019-01-24T21:00:55.000+0100},
  author = {Scherzinger, Stefanie and Cerqueus, Thomas and de Almeida, Eduardo Cunha},
  biburl = {https://www.bibsonomy.org/bibtex/2de168f4b4d6a967d9a833ba56e47ea09/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2015},
  editor = {Gehrke, Johannes and Lehner, Wolfgang and Shim, Kyuseok and Cha, Sang Kyun and Lohman, Guy M.},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2015.7113402},
  interhash = {7fdb56fa55a35a3382e4edb7da3a2b66},
  intrahash = {de168f4b4d6a967d9a833ba56e47ea09},
  isbn = {978-1-4799-7964-6},
  keywords = {NoSQL},
  pages = {1464-1467},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {ControVol: A framework for controlled schema evolution in NoSQL application development.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2015.html#ScherzingerCA15},
  year = 2015
}

@inproceedings{baralis2017versus,
  abstract = {In the last years, we are witnessing an increasing availability of geolocated data, ranging from satellite images to user generated content (e.g., tweets). This big amount of data is exploited by several cloud-based applications to deliver effective and customized services to end users. In order to provide a good user experience, a low-latency response time is needed, both when data are retrieved and provided. To achieve this goal, current geospatial applications need to exploit efficient and scalable geospatial databases, the choice of which has a high impact on the overall performance of the deployed applications. In this paper, we compare, from a qualitative point of view, four state-of-the-art SQL and NoSQL databases with geospatial features, and then we analyze the performances of two of them, selecting the ones based on the Database-as-a-service (DBaaS) model: Azure SQL Database and Azure DocumentDB (i.e., an SQL database versus a NoSQL one). The empirical evaluation shows pros and cons of both solutions and it is performed on a real use case related to an emergency management application.},
  added-at = {2019-01-27T20:23:47.000+0100},
  author = {Baralis, Elena and Valle, Andrea Dalla and Garza, Paolo and Rossi, Claudio and Scullino, Francesco},
  biburl = {https://www.bibsonomy.org/bibtex/24fa1172c9b8587456788d20c1790670e/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258324},
  interhash = {3960d175f091bc207587bb7ee249341b},
  intrahash = {4fa1172c9b8587456788d20c1790670e},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {3388-3397},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {SQL versus NoSQL databases for geospatial applications.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#BaralisVGRS17},
  year = 2017
}

@inproceedings{kim2014partial,
  abstract = {In-memory transactional data girds, often referred to as NoSQL data grids demand high concurrency for scalability and high performance in data-intensive applications. As an alternative concurrency control model, distributed transactional memory (DTM) promises to alleviate the difficulties of lock-based distributed synchronization. We consider the multi-versioning (MV) model of using multiple object versions in DTM to avoid unnecessary aborts. MV transactional memory inherently guarantees commits of read-only transactions, but limits concurrency of write transactions. We present a transactional scheduler, called partial rollback-based transactional scheduler (or PTS), for a multi-versioned DTM model. The model supports multiple object versions to exploit concurrency of read-only transactions, and detects conflicts of write transactions at an object level. Instead of aborting a transaction, PTS assigns backoff times for conflicting transactions, and the transaction is rolled-back partially. Our implementation, integrated with a popular open-source transactional in-memory data store (i.e., Red Hat's Infinispan) reveals that PTS improves transactional throughput over MV DTM without PTS by as much as 2.4×.},
  added-at = {2019-01-27T22:26:01.000+0100},
  author = {Kim, Junwhan},
  biburl = {https://www.bibsonomy.org/bibtex/284cd6463f81e53123afa8865f981d719/vngudivada},
  booktitle = {BigData Conference},
  crossref = {conf/bigdataconf/2014},
  editor = {Lin, Jimmy J. and Pei, Jian and Hu, Xiaohua and Chang, Wo and Nambiar, Raghunath and Aggarwal, Charu C. and Cercone, Nick and Honavar, Vasant and Huan, Jun and Mobasher, Bamshad and Pyne, Saumyadipta},
  ee = {https://doi.org/10.1109/BigData.2014.7004216},
  interhash = {8491380b990404b8e05588a4c1182dfc},
  intrahash = {84cd6463f81e53123afa8865f981d719},
  isbn = {978-1-4799-5665-4},
  keywords = {NoSQL},
  pages = {80-89},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Partial rollback-based scheduling on in-memory transactional data grids.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2014.html#Kim14},
  year = 2014
}

@book{attiya2014impossibility,
  abstract = {To understand the power of distributed systems, it is necessary to understand their inherent limitations: what problems cannot be solved in particular systems, or without sufficient resources (such as time or space). This book presents key techniques for proving such impossibility results and applies them to a variety of different problems in a variety of different system models. Insights gained from these results are highlighted, aspects of a problem that make it difficult are isolated, features of an architecture that make it inadequate for solving certain problems efficiently are identified, and different system models are compared.},
  added-at = {2019-02-04T19:10:52.000+0100},
  author = {Attiya, Hagit and Ellen, Faith},
  biburl = {https://www.bibsonomy.org/bibtex/2bb2c8dd5f86913e67b7a544a24f50fc8/vngudivada},
  booktitle = {Impossibility Results for Distributed Computing},
  ee = {http://dx.doi.org/10.2200/S00551ED1V01Y201311DCT012},
  interhash = {3dbf1b4914a25b1afb64558f0f55af44},
  intrahash = {bb2c8dd5f86913e67b7a544a24f50fc8},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Impossibility Results for Distributed Computing},
  year = 2014
}

@inproceedings{kaoudi2014cloudbased,
  abstract = {The W3C's Resource Description Framework (or RDF, in short) is set to deliver many of the original semi-structured data promises: flexible structure, optional schema, and rich, flexible URIs as a basis for information sharing. Moreover, RDF is uniquely positioned to benefit from the efforts of scientific communities studying databases, knowledge representation, and Web technologies. As a consequence, numerous collections of RDF data are published, going from scientific data to general-purpose ontologies to open government data, in particular published as part of the Linked Data movement. Managing such large volumes of RDF data is challenging, due to the sheer size, the heterogeneity, and the further complexity brought by RDF reasoning. To tackle the size challenge, distributed storage architectures are required. Cloud computing is an emerging distributed paradigm massively adopted in many applications for the scalability, fault-tolerance and elasticity features it provides. This tutorial presents the challenges faced in order to efficiently handle massive amounts of RDF data in a cloud environment. We provide the necessary background, analyze and classify existing solutions, and discuss open problems and perspectives.},
  added-at = {2019-01-24T18:13:37.000+0100},
  author = {Kaoudi, Zoi and Manolescu, Ioana},
  biburl = {https://www.bibsonomy.org/bibtex/24f0a011cfb41b8584d94c2d15780a8e6/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2014},
  editor = {Dyreson, Curtis E. and Li, Feifei and Özsu, M. Tamer},
  ee = {https://doi.org/10.1145/2588555.2588891},
  interhash = {debd80179079f932b75af2ab3feac898},
  intrahash = {4f0a011cfb41b8584d94c2d15780a8e6},
  isbn = {978-1-4503-2376-5},
  keywords = {NoSQL},
  pages = {725-729},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Cloud-based RDF data management.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2014.html#KaoudiM14},
  year = 2014
}

@inproceedings{seybold2016elasticity,
  abstract = {The age of cloud computing has introduced all the mechanisms needed to elastically scale distributed, cloud-enabled applications. At roughly the same time, NoSQL databases have been proclaimed as the scalable alternative to relational databases. Since then, NoSQL databases are a core component of many large-scale distributed applications. This paper evaluates the scalability and elasticity features of the three widely used NoSQL database systems Couchbase, Cassandra and MongoDB under various workloads and settings using throughput and latency as metrics. The numbers show that the three database systems have dramatically different baselines with respect to both metrics and also behave unexpected when scaling out. For instance, while Couchbase's throughput increases by 17% when scaled out from 1 to 4 nodes, MongoDB's throughput decreases by more than 50%. These surprising results show that not all tested NoSQL databases do scale as expected and even worse, in some cases scaling harms performances.},
  added-at = {2019-01-27T22:07:09.000+0100},
  author = {Seybold, Daniel and Wagner, Nicolas and Erb, Benjamin and Domaschka, Jörg},
  biburl = {https://www.bibsonomy.org/bibtex/2767ccf9cf234a8c35d18780344d8f34d/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2016},
  editor = {Joshi, James and Karypis, George and Liu, Ling and Hu, Xiaohua and Ak, Ronay and Xia, Yinglong and Xu, Weijia and Sato, Aki-Hiro and Rachuri, Sudarsan and Ungar, Lyle H. and Yu, Philip S. and Govindaraju, Rama and Suzumura, Toyotaro},
  ee = {http://dx.doi.org/10.1109/BigData.2016.7840931},
  interhash = {170291ddb80d8f83d53b15d4d8094877},
  intrahash = {767ccf9cf234a8c35d18780344d8f34d},
  isbn = {978-1-4673-9005-7},
  keywords = {NoSQL},
  pages = {2827-2836},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Is elasticity of scalable databases a Myth?},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2016.html#SeyboldWED16},
  year = 2016
}

@book{taubenfeld2018distributed,
  abstract = {Computers and computer networks are one of the most incredible inventions of the 20th century, having an ever-expanding role in our daily lives by enabling complex human activities in areas such as entertainment, education, and commerce. One of the most challenging problems in computer science for the 21st century is to improve the design of distributed systems where computing devices have to work together as a team to achieve common goals.

In this book, I have tried to gently introduce the general reader to some of the most fundamental issues and classical results of computer science underlying the design of algorithms for distributed systems, so that the reader can get a feel of the nature of this exciting and fascinating field called distributed computing. The book will appeal to the educated layperson and requires no computer-related background. I strongly suspect that also most computer-knowledgeable readers will be able to learn something new.},
  added-at = {2019-02-04T19:04:37.000+0100},
  author = {Taubenfeld, Gadi},
  biburl = {https://www.bibsonomy.org/bibtex/271d7d118a70460e8a691d5ac28b5f43d/vngudivada},
  booktitle = {Distributed Computing Pearls},
  ee = {https://doi.org/10.2200/S00845ED1V01Y201804DCT014},
  interhash = {dcf61b37d88f5740de70495d0eeb7b2a},
  intrahash = {71d7d118a70460e8a691d5ac28b5f43d},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Distributed Computing Pearls},
  year = 2018
}

@inproceedings{colombo2016towards,
  abstract = {Many modern applications use context related information to provide highly personalized services, and use NoSQL databases for data management, as these systems show outstanding performance and support high volumes of data. However, NoSQL databases integrate poor data protection features with basic coarse grained access control and no support for context aware policies. Therefore, we believe that a general approach is required to enhance NoSQL datastores with fine grained context aware access control. In this paper, we start to fill this void by targeting MongoDB, a very popular datastore. The contribution is twofold. We enhance MongoDB's access control model with advanced features and we define an enforcement monitor for the proposed enhanced model, which can be straightforwardly used in any MongoDB deployment. Technological limitations of MongoDB do not allow implementing the same efficient enforcement mechanism for all query types. As a consequence, experimental results show an enforcement overhead that is significant for aggregate queries, which contrasts with a low overhead measured for find and map-reduce queries.},
  added-at = {2019-01-24T20:55:37.000+0100},
  author = {Colombo, Pietro and Ferrari, Elena},
  biburl = {https://www.bibsonomy.org/bibtex/268bc6dd77321e8a8d75b7bb16c99f0ae/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2016},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2016.7498240},
  interhash = {cc26e01f18a62e4b816bf9cb737a975a},
  intrahash = {68bc6dd77321e8a8d75b7bb16c99f0ae},
  isbn = {978-1-5090-2020-1},
  keywords = {NoSQL},
  pages = {193-204},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Towards Virtual Private NoSQL datastores.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2016.html#ColomboF16},
  year = 2016
}

@inproceedings{storl2018surprise,
  abstract = {Schema-flexible NoSQL data stores lend themselves nicely for storing versioned data, a product of schema evolution. In this lightning talk, we apply pending schema changes to records that have been persisted several schema versions back. We present first experiments with MongoDB and Cassandra, where we explore the trade-off between applying chains of pending changes stepwise (one after the other), and as composite operations. Contrary to intuition, composite migration is not necessarily faster. The culprit is the computational overhead for deriving the compositions. However, caching composition formulae achieves a speed up: For Cassandra, we can cut the runtime by nearly 80%. Surprisingly, the relative speedup seems to be system-dependent. Our take away message is that in applying pending schema changes in NoSQL data stores, we need to base our design decisions on experimental evidence rather than on intuition alone.},
  added-at = {2019-01-24T20:08:29.000+0100},
  author = {Störl, Uta and Tekleab, Alexander and Klettke, Meike and Scherzinger, Stefanie},
  biburl = {https://www.bibsonomy.org/bibtex/2bc9e8e886a0827745b7ffcda8c336fd9/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2018},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2018.00202},
  interhash = {fee436134f01a9b78d391f5e2858e178},
  intrahash = {bc9e8e886a0827745b7ffcda8c336fd9},
  isbn = {978-1-5386-5520-7},
  keywords = {NoSQL},
  pages = 1662,
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {In for a Surprise When Migrating NoSQL Data.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2018.html#StorlTKS18},
  year = 2018
}

@inproceedings{li2017scheduling,
  abstract = {In the era of big data, many tools and algorithms are designed to deal with the increasing data. Because data management on the traditional relational database causes scalability and performance problem, data management across multiple data centers has been proposed. Cassandra is a NoSQL database, which is built to store huge volumes of data and manage data across multiple data centers. Generally, Cassandra assign data to different nodes based on consistent hashing algorithm. So the performance of Cassandra is excellent when most random read and write are requested. However, when the popular data is read or written frequently and the data is distributed to different data centers. Each operation brings the communication delay that could not be ignored. In this article, we propose a scheduling strategy based on multi-queues to reduce communication delay when data are accessed across different data centers. To validate the effectiveness of this strategy, we implemented our approach on Cassandra and evaluation results showed the average response time of data access is reduced across multiple data centers.},
  added-at = {2019-01-25T20:15:20.000+0100},
  author = {Li, Haopeng and Li, Hui},
  biburl = {https://www.bibsonomy.org/bibtex/2ba9f39eb3d5e0b99239d2321eac6891a/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258228},
  interhash = {826025de68d28339c290adafc87a30d4},
  intrahash = {ba9f39eb3d5e0b99239d2321eac6891a},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {2664-2669},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {A scheduling strategy based on multi-queues of Cassandra.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#LiL17},
  year = 2017
}

@book{georgiou2011cooperative,
  abstract = {Cooperative network supercomputing is becoming increasingly popular for harnessing the power of the global Internet computing platform. A typical Internet supercomputer consists of a master computer or server and a large number of computers called workers, performing computation on behalf of the master. Despite the simplicity and benefits of a single master approach, as the scale of such computing environments grows, it becomes unrealistic to assume the existence of the infallible master that is able to coordinate the activities of multitudes of workers. Large-scale distributed systems are inherently dynamic and are subject to perturbations, such as failures of computers and network links, thus it is also necessary to consider fully distributed peer-to-peer solutions.

We present a study of cooperative computing with the focus on modeling distributed computing settings, algorithmic techniques enabling one to combine efficiency and fault-tolerance in distributed systems, and the exposition of trade-offs between efficiency and fault-tolerance for robust cooperative computing. The focus of the exposition is on the abstract problem, called Do-All, and formulated in terms of a system of cooperating processors that together need to perform a collection of tasks in the presence of adversity. Our presentation deals with models, algorithmic techniques, and analysis. Our goal is to present the most interesting approaches to algorithm design and analysis leading to many fundamental results in cooperative distributed computing. The algorithms selected for inclusion are among the most efficient that additionally serve as good pedagogical examples. Each chapter concludes with exercises and bibliographic notes that include a wealth of references to related work and relevant advanced results.},
  added-at = {2019-02-04T19:23:14.000+0100},
  author = {Georgiou, Chryssis and Shvartsman, Alexander A.},
  biburl = {https://www.bibsonomy.org/bibtex/27a8d668285b425578c511eb77e537c39/vngudivada},
  ee = {http://dx.doi.org/10.2200/S00376ED1V01Y201108DCT007},
  interhash = {79f4701a4e274f0903fd73f615ffc688},
  intrahash = {7a8d668285b425578c511eb77e537c39},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Cooperative Task-Oriented Computing: Algorithms and Complexity},
  url = {http://dx.doi.org/10.2200/S00376ED1V01Y201108DCT007},
  year = 2011
}

@inproceedings{tahara2014sinew,
  abstract = {As applications are becoming increasingly dynamic, the notion that a schema can be created in advance for an application and remain relatively stable is becoming increasingly unrealistic. This has pushed application developers away from traditional relational database systems and away from the SQL interface, despite their many well-established benefits. Instead, developers often prefer self-describing data models such as JSON, and NoSQL systems designed specifically for their relaxed semantics.

In this paper, we discuss the design of a system that enables developers to continue to represent their data using self-describing formats without moving away from SQL and traditional relational database systems. Our system stores arbitrary documents of key-value pairs inside physical and virtual columns of a traditional relational database system, and adds a layer above the database system that automatically provides a dynamic relational view to the user against which fully standard SQL queries can be issued. We demonstrate that our design can achieve an order of magnitude improvement in performance over alternative solutions, including existing relational database JSON extensions, MongoDB, and shredding systems that store flattened key-value data inside a relational database.},
  added-at = {2019-01-24T18:18:06.000+0100},
  author = {Tahara, Daniel and Diamond, Thaddeus and Abadi, Daniel J.},
  biburl = {https://www.bibsonomy.org/bibtex/25976f1e25df894494d3a8c2d260d9300/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2014},
  editor = {Dyreson, Curtis E. and Li, Feifei and Özsu, M. Tamer},
  ee = {https://doi.org/10.1145/2588555.2612183},
  interhash = {ed31f54d7f3d398748e4aebaa18a37aa},
  intrahash = {5976f1e25df894494d3a8c2d260d9300},
  isbn = {978-1-4503-2376-5},
  keywords = {NoSQL},
  pages = {815-826},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Sinew: a SQL system for multi-structured data.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2014.html#TaharaDA14},
  year = 2014
}

@inproceedings{li2015flexible,
  abstract = {In the era of big data and cloud, distributed key-value stores are increasingly used as building blocks of large-scale applications. Comparing to traditional relational databases, key-value stores are particularly compelling due to their low latency and excellent scalability. Many big companies, such as Facebook and Amazon, run multiple different applications and services on top of a single key-value store deployment to reduce the deployment and maintenance complexity as well as economic cost. However, every application has its performance requirement but most current key-value store systems are designed to serve every application request equally. This design works well when a single application accesses the key-value store, but it is not as good for the emerging concurrent multi-application scenario. In this paper, we present ZHT/Q, a flexible QoS (Quality of Service) fortified distributed key-value storage system for clouds and data centers. It improves the overall throughput by an order of magnitude and still satisfies different applications' latency requirements with QoS using dynamic and adaptive request batching mechanisms. The experiment results show that our new system delivers up to 28 times higher throughput than the base solution while more than 99% of requests' latency requirements are satisfied.},
  added-at = {2019-01-27T22:10:37.000+0100},
  author = {Li, Tonglin and Wang, Ke and Zhao, Dongfang and Qiao, Kan and Sadooghi, Iman and Zhou, Xiaobing and Raicu, Ioan},
  biburl = {https://www.bibsonomy.org/bibtex/27f88749facc02be0ea334a21ab64d3f2/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2015},
  ee = {https://doi.org/10.1109/BigData.2015.7363794},
  interhash = {839abcea60bc37eb8d93db5a51a77437},
  intrahash = {7f88749facc02be0ea334a21ab64d3f2},
  isbn = {978-1-4799-9926-2},
  keywords = {NoSQL},
  pages = {515-522},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {A flexible QoS fortified distributed key-value storage system for the cloud.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2015.html#LiWZQSZR15},
  year = 2015
}

@article{davidson1985consistency,
  abstract = {Recently, several strategies have been proposed for transaction processing in partitioned distributed database systems with replicated data. These strategies are surveyed in light of the competing goals of maintaining correctness and achieving high availability. Extensions and combinations are then discussed, and guidelines are presented for selecting strategies for particular applications.},
  added-at = {2019-01-22T01:40:37.000+0100},
  author = {Davidson, Susan B. and Garcia-Molina, Hector and Skeen, Dale},
  biburl = {https://www.bibsonomy.org/bibtex/222bb294b806bd892c7c53253371e3f57/vngudivada},
  ee = {https://www.wikidata.org/entity/Q29308549},
  interhash = {68ded49a0191122a3a038a77d261f402},
  intrahash = {22bb294b806bd892c7c53253371e3f57},
  journal = {ACM Comput. Surv.},
  keywords = {NoSQL},
  number = 3,
  pages = {341-370},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Consistency in Partitioned Networks.},
  url = {http://dblp.uni-trier.de/db/journals/csur/csur17.html#DavidsonG85},
  volume = 17,
  year = 1985
}

@inproceedings{dahlgren2018towards,
  abstract = {NoSQL databases offer powerful abstractions for querying non-relational data. However, NoSQL products generally pursue superior flexibility, customizability, scalability, and performance goals while neglecting support for generally useful data management tools. In particular, products typically ship without integrated support for management features rendered conventional by the long history of RDBMSs, such as sophisticated query processing systems, join operations, aggregate functions, and integrity constraints. The design decision forces users of NoSQL technologies to find alternative methods for providing missing tools by engaging either directly or indirectly in a suboptimal k-implementation cycle as developers re-invent new instances of the same data management tools across NoSQL products. This paper articulates the problem associated with the lax regard for data management support currently defining the class of NoSQL databases and introduces the Piper package index and management system as an exploratory solution.},
  added-at = {2019-01-24T20:09:43.000+0100},
  author = {Dahlgren, Kathryn},
  biburl = {https://www.bibsonomy.org/bibtex/20badfea2ada3d1c368652421471a560d/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2018},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2018.00220},
  interhash = {e879e93817a20336e6b03cdbb7128005},
  intrahash = {0badfea2ada3d1c368652421471a560d},
  isbn = {978-1-5386-5520-7},
  keywords = {NoSQL},
  pages = {1709-1713},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Towards Not Re-Inventing the Wheel: Managing Data Management Tools.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2018.html#Dahlgren18},
  year = 2018
}

@article{schneider1990implementing,
  abstract = {The state machine approach is a general method for implementing fault-tolerant services in distributed systems. This paper reviews the approach and describes protocols for two different failure models—Byzantine and fail stop. Systems reconfiguration techniques for removing faulty components and integrating repaired components are also discussed.},
  added-at = {2019-01-22T02:16:55.000+0100},
  author = {Schneider, Fred B.},
  biburl = {https://www.bibsonomy.org/bibtex/2edbd57cdc40a01db00ffff5f8b01d85d/vngudivada},
  ee = {https://www.wikidata.org/entity/Q56523309},
  interhash = {454d122d5a397b1bea2cd13d76033e47},
  intrahash = {edbd57cdc40a01db00ffff5f8b01d85d},
  journal = {ACM Comput. Surv.},
  keywords = {NoSQL},
  number = 4,
  pages = {299-319},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial.},
  url = {http://dblp.uni-trier.de/db/journals/csur/csur22.html#Schneider90},
  volume = 22,
  year = 1990
}

@inproceedings{colombo2017towards,
  abstract = {NoSQL datastores allow the efficient management of high volumes of heterogeneous and unstructured data, meeting the requirements of a variety of today ICT applications. However, most of these systems poorly support data security, and recent surveys show that their simplistic support for data protection is considered as a reason not to use them.1 In recent years, Attribute Based Access Control (ABAC) is getting more and more popularity, for its ability to provide highly flexible and customized forms of data protection at different granularity levels. In the current work, with the aim to raise users' confidence in the protection of data managed by NoSQL systems, we define a general approach to enforce ABAC within NoSQL systems. Our approach relies on SQL++[20], a unifying query language for NoSQL platforms. In particular, we develop a novel SQL++ query rewriting mechanism able to enforce heterogeneous types of ABAC policies specified up to cell level. Experimental results show an overhead which is not negligible for policies covering high percentage of the fields characterizing the protected documents, but which is far more contained when field level policies are more sparsely specified.},
  added-at = {2019-01-24T20:15:34.000+0100},
  author = {Colombo, Pietro and Ferrari, Elena},
  biburl = {https://www.bibsonomy.org/bibtex/2458a938e654821a35cbb37513b032eea/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2017},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2017.123},
  interhash = {8dcda8484331779f7dc9ecd64d717e3b},
  intrahash = {458a938e654821a35cbb37513b032eea},
  isbn = {978-1-5090-6543-1},
  keywords = {NoSQL},
  pages = {709-720},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Towards a Unifying Attribute Based Access Control Approach for NoSQL Datastores.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2017.html#ColomboF17},
  year = 2017
}

@inproceedings{touloupas2017realtime,
  abstract = {In this paper we present RASP, a system that combines latest distributed stream processing and NoSQL engines to enable the real-time low latency storage and joining of incoming data streams with external datasets of arbitrary sizes through an extensible, SQL compliant manner. We achieve low latency, real time execution by employing the Kafka and Storm frameworks to join incoming tuples as they arrive, while the denormalized result is being stored in HBase, a distributed NoSQL engine with the use of Phoenix, a framework that fully supports SQL. We fine-tune the topology execution to achieve maximum performance and we also apply a set of optimizations both in the HBase storage and the Phoenix SQL execution framework. We use RASP to solve a network analytics problem using real data. RASP performs its computations utilizing an extensible pipeline of Storm bolts that incrementally augment incoming tuples with the execution of different algorithms. We deploy our system over an IaaS cloud and we evaluate its performance for various workloads, cluster sizes and configurations, where we show that in some cases RASP achieves a throughput increase of more than 140% and a latency drop of more than 65% compared to a vanilla setting.},
  added-at = {2019-01-24T22:38:05.000+0100},
  author = {Touloupas, Georgios and Konstantinou, Ioannis and Koziris, Nectarios},
  biburl = {https://www.bibsonomy.org/bibtex/212655632557fb698a8aefe48651831a1/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258198},
  interhash = {93db4172f09e5c32141440d7c4799425},
  intrahash = {12655632557fb698a8aefe48651831a1},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {2414-2419},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {RASP: Real-time network analytics with distributed NoSQL stream processing.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#TouloupasKK17},
  year = 2017
}

@inproceedings{doudali2017spaten,
  abstract = {Social networking users have the ability to check into Points of Interest (POIs) and associate location with their posts or tweets, leading to the creation of Geo-Social Networks (GeoSNs). There are many systems that aim to efficiently store and analyze plain and socially enhanced spatio-temporal data. A proper evaluation of these systems should be done using real data from popular GeoSNs, such as Foursquare, Facebook, etc. However, privacy restrictions prohibit the access to such real data in a large scale. Therefore, evaluations are done using real or synthetic data sets that include either only spatio-textual data (e.g. tweets) or plain spatial data (e.g. GPS traces) that are not socially enhanced. In this paper, we present Spaten, an open-source configurable spatio-temporal and textual data set generator, that extracts GPS traces from realistic routes utilizing Google Maps API, combines them with real POIs and relevant user comments crawled from TripAdvisor and makes the data available for further analysis. The injection of social properties extracted by existing Twitter graphs to the generated data along with further parameterization leads to realistic GeoSN data sets. We create and publicly offer GB-size datasets with millions of check-ins and GPS traces. As a proof of concept, we loaded the generated data into a Big Data enabled NoSQL system, and we evaluated its scalability by performing queries typically found in social networking sites. We hope that Spaten can provide the research community with the ability to generate realistic GeoSN data in a large scale, so as to properly evaluate their work.},
  added-at = {2019-01-27T20:24:56.000+0100},
  author = {Doudali, Thaleia Dimitra and Konstantinou, Ioannis and Koziris, Nectarios},
  biburl = {https://www.bibsonomy.org/bibtex/2b6fe96d1ef7c8e1938f0dc7f82df6f05/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258327},
  interhash = {eea5ddd71074c3293320cc5befb60c2c},
  intrahash = {b6fe96d1ef7c8e1938f0dc7f82df6f05},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {3416-3421},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Spaten: A spatio-temporal and textual big data generator.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#DoudaliKK17},
  year = 2017
}

@inproceedings{kang2014durable,
  abstract = {In order to meet the stringent requirements of low latency as well as high throughput, web service providers with large data centers have been replacing magnetic disk drives with flash memory solid-state drives (SSDs). They commonly use relational and NoSQL database engines to manage OLTP workloads in the warehouse-scale computing environments. These modern database engines rely heavily on redundant writes and frequent cache flushes to guarantee the atomicity and durability of transactional updates. This has become a serious bottleneck of performance in both relational and NoSQL database engines. This paper presents a new SSD prototype called DuraSSD equipped with tantalum capacitors. The tantalum capacitors make the device cache inside DuraSSD durable, and additional firmware features of DuraSSD take advantage of the durable cache to support the atomicity and durability of page writes. It is the first time that a flash memory SSD with durable cache has been used to achieve an order of magnitude improvement in transaction throughput without compromising the atomicity and durability. Considering that the simple capacitors increase the total cost of an SSD no more than one percent, DuraSSD clearly provides a cost-effective means for transactional support. DuraSSD is also expected to alleviate the problem of high tail latency by minimizing write stalls.},
  added-at = {2019-01-24T18:25:38.000+0100},
  author = {Kang, Woon-Hak and Lee, Sang-Won and Moon, Bongki and Kee, Yang-Suk and Oh, Moonwook},
  biburl = {https://www.bibsonomy.org/bibtex/2861ef122d8c99be0ae6887d9d96b90b0/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2014},
  editor = {Dyreson, Curtis E. and Li, Feifei and Özsu, M. Tamer},
  ee = {https://doi.org/10.1145/2588555.2595632},
  interhash = {3948476d36f02e68dd13cfff8b9cade3},
  intrahash = {861ef122d8c99be0ae6887d9d96b90b0},
  isbn = {978-1-4503-2376-5},
  keywords = {NoSQL},
  pages = {529-540},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Durable write cache in flash memory SSD for relational and NoSQL databases.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2014.html#KangLMKO14},
  year = 2014
}

@inproceedings{ajoux2015challenges,
  abstract = {There have been many recent advances in distributed systems that provide stronger semantics for geo-replicated
data stores like those underlying Facebook. These research systems provide a range of consistency models
and transactional abilities while demonstrating good performance and scalability on experimental workloads. At
Facebook we are excited by these lines of research, but
fundamental and operational challenges currently make
it infeasible to incorporate these advances into deployed
systems. This paper describes some of these challenges
with the hope that future advances will address them.},
  added-at = {2019-01-22T01:56:38.000+0100},
  author = {Ajoux, Phillipe and Bronson, Nathan and Kumar, Sanjeev and Lloyd, Wyatt and Veeraraghavan, Kaushik},
  biburl = {https://www.bibsonomy.org/bibtex/2b6236b2fd053d25a5fd7be7bcb8cd23a/vngudivada},
  booktitle = {HotOS},
  crossref = {conf/hotos/2015},
  editor = {Candea, George},
  ee = {https://www.usenix.org/conference/hotos15/workshop-program/presentation/ajoux},
  interhash = {9dc201f3c910f62f459ac8ce698265a3},
  intrahash = {b6236b2fd053d25a5fd7be7bcb8cd23a},
  keywords = {NoSQL},
  publisher = {USENIX Association},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Challenges to Adopting Stronger Consistency at Scale.},
  url = {http://dblp.uni-trier.de/db/conf/hotos/hotos2015.html#AjouxBKLV15},
  year = 2015
}

@inproceedings{kolev2016cloudmdsql,
  abstract = {The blooming of different cloud data management infrastructures has turned multistore systems to a major topic in the nowadays cloud landscape. In this demonstration, we present a Cloud Multidatastore Query Language (CloudMdsQL), and its query engine. CloudMdsQL is a functional SQL-like language, capable of querying multiple heterogeneous data stores (relational and NoSQL) within a single query that may contain embedded invocations to each data store's native query interface. The major innovation is that a CloudMdsQL query can exploit the full power of local data stores, by simply allowing some local data store native queries (e.g. a breadth-first search query against a graph database) to be called as functions, and at the same time be optimized. Within our demonstration, we focus on two use cases each involving four diverse data stores (graph, document, relational, and key-value) with its corresponding CloudMdsQL queries. The query execution flows are visualized by an embedded real-time monitoring subsystem. The users can also try out different ad-hoc queries, not necessarily in the context of the use cases.},
  added-at = {2019-01-24T18:26:59.000+0100},
  author = {Kolev, Boyan and Bondiombouy, Carlyna and Valduriez, Patrick and Jiménez-Peris, Ricardo and Pau, Raquel and Pereira, José},
  biburl = {https://www.bibsonomy.org/bibtex/25131942ba23137d6f939512aeee8e776/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2016},
  editor = {Özcan, Fatma and Koutrika, Georgia and Madden, Sam},
  ee = {https://www.wikidata.org/entity/Q58023632},
  interhash = {ddfe4fc2cb6225a8ffcf9de411adf38f},
  intrahash = {5131942ba23137d6f939512aeee8e776},
  isbn = {978-1-4503-3531-7},
  keywords = {NoSQL},
  pages = {2113-2116},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {The CloudMdsQL Multistore System.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2016.html#KolevBVJPP16},
  year = 2016
}

@inproceedings{storl2018curating,
  abstract = {Building applications for processing data lakes is a software engineering challenge. We present Darwin, a middleware for applications that operate on variational data. This concerns data with heterogeneous structure, usually stored within a schema-flexible NoSQL database. Darwin assists application developers in essential data and schema curation tasks: Upon request, Darwin extracts a schema description, discovers the history of schema versions, and proposes mappings between these versions. Users of Darwin may interactively choose which mappings are most realistic. Darwin is further capable of rewriting queries at runtime, to ensure that queries also comply with legacy data. Alternatively, Darwin can migrate legacy data to reduce the structural heterogeneity. Using Darwin, developers may thus evolve their data in sync with their code. In our hands-on demo, we curate synthetic as well as real-life datasets.},
  added-at = {2019-01-24T19:45:20.000+0100},
  author = {Störl, Uta and Müller, Daniel and Tekleab, Alexander and Tolale, Stephane and Stenzel, Julian and Klettke, Meike and Scherzinger, Stefanie},
  biburl = {https://www.bibsonomy.org/bibtex/2045f5d0c2e0eddbb7da973f785b12099/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2018},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2018.00187},
  interhash = {f0c45ba57c94387b6975f719f8b93838},
  intrahash = {045f5d0c2e0eddbb7da973f785b12099},
  isbn = {978-1-5386-5520-7},
  keywords = {NoSQL},
  pages = {1605-1608},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Curating Variational Data in Application Development.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2018.html#Storl0TTSKS18},
  year = 2018
}

@inproceedings{comynwattiau2017model,
  abstract = {Most NoSQL databases are schemaless. Although they offer some flexibility, they do not have any knowledge of the database schema, losing the benefits provided by these schemas. It is generally accepted that data modelling can have an impact on performance, consistency, usability, and maintainability. We argue that NoSQL databases need data models that ensure the proper storage and the relevant querying of the data. This paper seeks to present and illustrate an MDA-based approach, allowing us to achieve a reverse engineering of NoSQL property graph databases into an Extended Entity-Relationship schema. The approach is applied to the case of Neo4j graph database. We present an illustrative scenario and evaluate the reverse engineering approach.},
  added-at = {2019-01-24T22:32:32.000+0100},
  author = {Comyn-Wattiau, Isabelle and Akoka, Jacky},
  biburl = {https://www.bibsonomy.org/bibtex/208a9ad18aad2a0c851380ea15b784bee/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8257957},
  interhash = {054c72398fa796fb2e1ccd758b7bdbc4},
  intrahash = {08a9ad18aad2a0c851380ea15b784bee},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {453-458},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Model driven reverse engineering of NoSQL property graph databases: The case of Neo4j.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#Comyn-WattiauA17},
  year = 2017
}

@inproceedings{borkar2016query,
  abstract = {Couchbase Server is a rethinking of the database given the current set of realities. Memory today is much cheaper than disks were when traditional databases were designed back in the 1970's, and networks are much faster and much more reliable than ever before. Application agility is also an extremely important requirement. Today's Couchbase Server is a memory- and network-centric, shared-nothing, auto-partitioned, and distributed NoSQL database system that offers both key-based and secondary index-based data access paths as well as API- and query-based data access capabilities. This is a major change from Couchbase's roots; in its early days, its focus was entirely on high performance and highly available key-value (memcache) based caching. Customer needs and competitive pressures in the evolving non-relational database market also accelerated this change. This paper describes the architectural changes needed to address the requirements posed by next-generation database applications. In addition, it details the implementation of such an architecture using Couchbase Server and explains the evolution of Couchbase Server from its early roots to its present form. Particular attention is paid to how today's Couchbase Server cluster architecture is influenced by the memory-first, high-performance, and scalability demands of typical customer deployments. Key features include a layer-consolidated cache, a consistency-controllable interplay between updates, indexes, and queries, and a unique "multi-dimensional" approach to cluster scaling. The paper closes with a look at future plans for supporting semi-structured operational data analytics in addition to today's more OLTP-like, front-facing use cases.},
  added-at = {2019-01-24T18:11:06.000+0100},
  author = {Borkar, Dipti and Mayuram, Ravi and Sangudi, Gerald and Carey, Michael J.},
  biburl = {https://www.bibsonomy.org/bibtex/2264d5fa7a0872d88ae2421fdae0fe50c/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2016},
  editor = {Özcan, Fatma and Koutrika, Georgia and Madden, Sam},
  ee = {https://doi.org/10.1145/2882903.2904443},
  interhash = {929b869b4d9b34613cfb6badfd516d11},
  intrahash = {264d5fa7a0872d88ae2421fdae0fe50c},
  isbn = {978-1-4503-3531-7},
  keywords = {NoSQL},
  pages = {239-251},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Have Your Data and Query It Too: From Key-Value Caching to Big Data Management.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2016.html#BorkarMSC16},
  year = 2016
}

@inproceedings{mior2014automated,
  abstract = {Selecting appropriate indices and materialized views is critical for high performance in relational databases. By example, we show that the problem of schema optimization is also highly relevant for NoSQL databases. We explore the problem of schema design in NoSQL databases with a goal of optimizing query performance while minimizing storage overhead. Our suggested approach uses the cost of executing a given workload for a given schema to guide the mapping from the application data model to a physical schema. We propose a cost-driven approach for optimization and discuss its usefulness as part of an automated schema design tool.},
  added-at = {2019-01-24T18:07:26.000+0100},
  author = {Mior, Michael J.},
  biburl = {https://www.bibsonomy.org/bibtex/251810b193b9c7ab6a0ee624b04ba9bcc/vngudivada},
  booktitle = {SIGMOD PhD Symposium},
  crossref = {conf/sigmod/2014phds},
  editor = {Barceló, Pablo and Chen, Lei},
  ee = {https://doi.org/10.1145/2602622.2602624},
  interhash = {8690c3cf25a9ff67663a407d8e6c648f},
  intrahash = {51810b193b9c7ab6a0ee624b04ba9bcc},
  isbn = {978-1-4503-2924-8},
  keywords = {NoSQL},
  pages = {41-45},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Automated schema design for NoSQL databases.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2014phds.html#Mior14},
  year = 2014
}

@book{flocchini2012distributed,
  abstract = {The study of what can be computed by a team of autonomous mobile robots, originally started in robotics and AI, has become increasingly popular in theoretical computer science (especially in distributed computing), where it is now an integral part of the investigations on computability by mobile entities. The robots are identical computational entities located and able to move in a spatial universe; they operate without explicit communication and are usually unable to remember the past; they are extremely simple, with limited resources, and individually quite weak. However, collectively the robots are capable of performing complex tasks, and form a system with desirable fault-tolerant and self-stabilizing properties. The research has been concerned with the computational aspects of such systems. In particular, the focus has been on the minimal capabilities that the robots should have in order to solve a problem.

This book focuses on the recent algorithmic results in the field of distributed computing by oblivious mobile robots (unable to remember the past). After introducing the computational model with its nuances, we focus on basic coordination problems: pattern formation, gathering, scattering, leader election, as well as on dynamic tasks such as flocking. For each of these problems, we provide a snapshot of the state of the art, reviewing the existing algorithmic results. In doing so, we outline solution techniques, and we analyze the impact of the different assumptions on the robots' computability power.},
  added-at = {2019-02-04T19:14:35.000+0100},
  author = {Flocchini, Paola and Prencipe, Giuseppe and Santoro, Nicola},
  biburl = {https://www.bibsonomy.org/bibtex/2940bb22b096423467b0abc2383580d37/vngudivada},
  ee = {http://dx.doi.org/10.2200/S00440ED1V01Y201208DCT010},
  interhash = {580f36234f17cc54ff54ea30d0e8cbbf},
  intrahash = {940bb22b096423467b0abc2383580d37},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Distributed Computing by Oblivious Mobile Robots},
  url = {http://dx.doi.org/10.2200/S00440ED1V01Y201208DCT010},
  year = 2012
}

@book{bloem2015decidability,
  abstract = {While the classic model checking problem is to decide whether a finite system satisfies a specification, the goal of parameterized model checking is to decide, given finite systems M(n) parameterized by n ∈ ℕ, whether, for all n ∈ ℕ, the system M(n) satisfies a specification. In this book we consider the important case of M(n) being a concurrent system, where the number of replicated processes depends on the parameter n but each process is independent of n. Examples are cache coherence protocols, networks of finite-state agents, and systems that solve mutual exclusion or scheduling problems. Further examples are abstractions of systems, where the processes of the original systems actually depend on the parameter.

The literature in this area has studied a wealth of computational models based on a variety of synchronization and communication primitives, including token passing, broadcast, and guarded transitions. Often, different terminology is used in the literature, and results are based on implicit assumptions. In this book, we introduce a computational model that unites the central synchronization and communication primitives of many models, and unveils hidden assumptions from the literature. We survey existing decidability and undecidability results, and give a systematic view of the basic problems in this exciting research area.},
  added-at = {2019-02-04T19:09:03.000+0100},
  author = {Bloem, Roderick and Jacobs, Swen and Khalimov, Ayrat and Konnov, Igor and Rubin, Sasha and Veith, Helmut and Widder, Josef},
  biburl = {https://www.bibsonomy.org/bibtex/22a6d160a1e5bb8c0881f299a185c5688/vngudivada},
  booktitle = {Decidability of Parameterized Verification},
  ee = {https://doi.org/10.2200/S00658ED1V01Y201508DCT013},
  interhash = {0d0c20e0978bf01088a0dacbe6747157},
  intrahash = {2a6d160a1e5bb8c0881f299a185c5688},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Decidability of Parameterized Verification},
  year = 2015
}

@inproceedings{klettke2016nosql,
  abstract = {This paper explores scalable implementation strategies for carrying out lazy schema evolution in NoSQL data stores. For decades, schema evolution has been an evergreen in database research. Yet new challenges arise in the context of cloud-hosted data backends: With all database reads and writes charged by the provider, migrating the entire data instance eagerly into a new schema can be prohibitively expensive. Thus, lazy migration may be more cost-efficient, as legacy entities are only migrated in case they are actually accessed by the application. Related work has shown that the overhead of migrating data lazily is affordable when a single evolutionary change is carried out, such as adding a new property. In this paper, we focus on long-term schema evolution, where chains of pending schema evolution operations may have to be applied. Chains occur when legacy entities written several application releases back are finally accessed by the application. We discuss strategies for dealing with chains of evolution operations, in particular, the composition into a single, equivalent composite migration that performs the required version jump. Our experiments with MongoDB focus on scalable implementation strategies. Our lineup further compares the number of write operations, and thus, the operational costs of different data migration strategies.},
  added-at = {2019-01-27T22:06:11.000+0100},
  author = {Klettke, Meike and Störl, Uta and Shenavai, Manuel and Scherzinger, Stefanie},
  biburl = {https://www.bibsonomy.org/bibtex/2a2892b04f3dd019294efb960f42657c5/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2016},
  editor = {Joshi, James and Karypis, George and Liu, Ling and Hu, Xiaohua and Ak, Ronay and Xia, Yinglong and Xu, Weijia and Sato, Aki-Hiro and Rachuri, Sudarsan and Ungar, Lyle H. and Yu, Philip S. and Govindaraju, Rama and Suzumura, Toyotaro},
  ee = {http://dx.doi.org/10.1109/BigData.2016.7840924},
  interhash = {880957f4421487776eb89bc674cb03df},
  intrahash = {a2892b04f3dd019294efb960f42657c5},
  isbn = {978-1-4673-9005-7},
  keywords = {NoSQL},
  pages = {2764-2774},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {NoSQL schema evolution and big data migration at scale.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2016.html#KlettkeSSS16},
  year = 2016
}

@inproceedings{ozcan2017hybrid,
  abstract = {The popularity of large-scale real-time analytics applications (real-time inventory/pricing, recommendations from mobile apps, fraud detection, risk analysis, IoT, etc.) keeps rising. These applications require distributed data management systems that can handle fast concurrent transactions (OLTP) and analytics on the recent data. Some of them even need running analytical queries (OLAP) as part of transactions. Efficient processing of individual transactional and analytical requests, however, leads to different optimizations and architectural decisions while building a data management system.

For the kind of data processing that requires both analytics and transactions, Gartner recently coined the term Hybrid Transactional/Analytical Processing (HTAP). Many HTAP solutions are emerging both from the industry as well as academia that target these new applications. While some of these are single system solutions, others are a looser coupling of OLTP databases or NoSQL systems with analytical big data platforms, like Spark. The goal of this tutorial is to 1-) quickly review the historical progression of OLTP and OLAP systems, 2-) discuss the driving factors for HTAP, and finally 3-) provide a deep technical analysis of existing and emerging HTAP solutions, detailing their key architectural differences and trade-offs.},
  added-at = {2019-01-24T19:40:57.000+0100},
  author = {Özcan, Fatma and Tian, Yuanyuan and Tözün, Pinar},
  biburl = {https://www.bibsonomy.org/bibtex/2c10ffe22cfd019609c25c9ff9d074a88/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2017},
  editor = {Salihoglu, Semih and Zhou, Wenchao and Chirkova, Rada and Yang, Jun and Suciu, Dan},
  ee = {https://doi.org/10.1145/3035918.3054784},
  interhash = {9d0db03a7d7cc3448587f440cdd42b53},
  intrahash = {c10ffe22cfd019609c25c9ff9d074a88},
  isbn = {978-1-4503-4197-4},
  keywords = {NoSQL},
  pages = {1771-1775},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Hybrid Transactional/Analytical Processing: A Survey.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2017.html#OzcanTT17},
  year = 2017
}

@inproceedings{schildgen2016crosssystem,
  abstract = {The rising adoption of NoSQL technology in enterprises causes a heterogeneous landscape of different data stores. Different stores provide distinct advantages and disadvantages, making it necessary for enterprises to facilitate multiple systems for specific purposes. This resulting polyglot persistence is difficult to handle for developers since some data needs to be replicated and aggregated between different and within the same stores. Currently, there are no uniform tools to perform these data transformations since all stores feature different APIs and data models. In this paper, we present the transformation language NotaQL that allows cross-system data transformations. These transformations are output-oriented, meaning that the structure of a transformation script is similar to that of the output. Besides, we provide an aggregation-centric approach, which makes aggregation operations as easy as possible.},
  added-at = {2019-01-24T18:24:21.000+0100},
  author = {Schildgen, Johannes and Lottermann, Thomas and Deßloch, Stefan},
  biburl = {https://www.bibsonomy.org/bibtex/293b73cd31322c175e0b1e98acc8cc593/vngudivada},
  booktitle = {BeyondMR@SIGMOD},
  crossref = {conf/sigmod/2016beyondmr},
  editor = {Afrati, Foto N. and Sroka, Jacek and Hidders, Jan},
  ee = {https://doi.org/10.1145/2926534.2926535},
  interhash = {61b7254693da99d5cf44ee8c713df8b8},
  intrahash = {93b73cd31322c175e0b1e98acc8cc593},
  isbn = {978-1-4503-4311-4},
  keywords = {NoSQL},
  pages = 5,
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Cross-system NoSQL data transformations with NotaQL.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/beyondmr2016.html#SchildgenLD16},
  year = 2016
}

@inproceedings{maccioni2016quepa,
  abstract = {Polystore systems (or simply polystores) have been recently proposed to support a common scenario in which enterprise data are stored in a variety of database technologies relying on different data models and languages. Polystores provide a loosely coupled integration of data sources and support the direct access, with the local language, to each specific storage engine to exploit its distinctive features. Given the absence of a global schema, new challenges for accessing data arise in these environments. In fact, it is usually hard to know in advance if a query to a specific data store can be satisfied with data stored elsewhere in the polystore. QUEPA addresses these issues by introducing augmented search and augmented exploration in a polystore, two access methods based on the automatic enrichment of the result of a query over a storage system with related data in the rest of the polystore. These features do not impact on the applications running on top of the polystore and are compatible with the most common database systems. QUEPA implements in this way a lightweight mechanism for data integration in the polystore and operates in a plug-and-play mode, thus reducing the need for ad-hoc configurations and for middleware layers involving standard APIs, unified query languages or shared data models. In our demonstration audience can experience with the augmentation construct by using the native query languages of the database systems available in the polystore.},
  added-at = {2019-01-24T18:34:26.000+0100},
  author = {Maccioni, Antonio and Basili, Edoardo and Torlone, Riccardo},
  biburl = {https://www.bibsonomy.org/bibtex/28d453f13f2461bd972624de6ef1037c4/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2016},
  editor = {Özcan, Fatma and Koutrika, Georgia and Madden, Sam},
  ee = {https://doi.org/10.1145/2882903.2899393},
  interhash = {0cc4508571281c6e2a727d09322513d7},
  intrahash = {8d453f13f2461bd972624de6ef1037c4},
  isbn = {978-1-4503-3531-7},
  keywords = {NoSQL},
  pages = {2133-2136},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {QUEPA: QUerying and Exploring a Polystore by Augmentation.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2016.html#MaccioniBT16},
  year = 2016
}

@inproceedings{huang2015databasebased,
  abstract = {NoSQL distributed databases have been devised to tackle the challenges resulting from volume, velocity and variety of big data. Graph representation of datasets requires efficient distributed linear algebra operations for large sparse matrix constructed from big data. Storing the transformed matrix into the database not only speeds up the big data analysis process but also facilitates the computation because of indexing. The Hadoop based approach does not natively support iterative algorithms due to data shuffling during each iteration. This paper presents a novel database-based distributed computation architecture bridging the gap between Hadoop and HPC. The novelty results from exploring the indexing capability of D4M (Dynamic Distributed Dimensional Data Model) to support linear algebra operations in a distributed computation environment. The idea is to store input data and intermediate results in associative array format inside Accumulo table to facilitate the data sharing among working nodes. pMatlab is deployed as the parallel computation engine. Our proposed architecture is proved to be lighter, easier and faster than MapReduce based approach. One example application is calculating top k eigenvalues and eigenvectors for large sparse matrix. Experiments on Graph500 benchmark datasets demonstrate 2X speedup of our architecture as compared to HEIGEN (An eigensolver for billion-scale matrices using MapReduce).},
  added-at = {2019-01-27T22:21:53.000+0100},
  author = {Huang, Yin and Yesha, Yelena and Zhou, Shujia},
  biburl = {https://www.bibsonomy.org/bibtex/26c43b6f126af06b454b6d4f4ebbe0d42/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2015},
  ee = {https://doi.org/10.1109/BigData.2015.7364045},
  interhash = {0f7c33065c5672621bc81ab40de3fe37},
  intrahash = {6c43b6f126af06b454b6d4f4ebbe0d42},
  isbn = {978-1-4799-9926-2},
  keywords = {NoSQL},
  pages = {2493-2500},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {A database-based distributed computation architecture with Accumulo and D4M: An application of eigensolver for large sparse matrix.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2015.html#HuangYZ15},
  year = 2015
}

@inproceedings{athanassoulis2016design,
  abstract = {Database researchers and practitioners have been building methods to store, access, and update data for more than five decades. Designing access methods has been a constant effort to adapt to the ever changing underlying hardware and workload requirements. The recent explosion in data system designs - including, in addition to traditional SQL systems, NoSQL, NewSQL, and other relational and non-relational systems - makes understanding the tradeoffs of designing access methods more important than ever. Access methods are at the core of any new data system. In this tutorial we survey recent developments in access method design and we place them in the design space where each approach focuses primarily on one or a subset of read performance, update performance, and memory utilization. We discuss how to utilize designs and lessons-learned from past research. In addition, we discuss new ideas on how to build access methods that have tunable behavior, as well as, what is the scenery of open research problems.},
  added-at = {2019-01-24T18:37:35.000+0100},
  author = {Athanassoulis, Manos and Idreos, Stratos},
  biburl = {https://www.bibsonomy.org/bibtex/2416f3a15334936fda3aca1d54ed9820d/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2016},
  editor = {Özcan, Fatma and Koutrika, Georgia and Madden, Sam},
  ee = {https://doi.org/10.1145/2882903.2912569},
  interhash = {2482d3114ef34b3deccfeb5bfaf8bb9a},
  intrahash = {416f3a15334936fda3aca1d54ed9820d},
  isbn = {978-1-4503-3531-7},
  keywords = {NoSQL},
  pages = {2195-2200},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Design Tradeoffs of Data Access Methods.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2016.html#AthanassoulisI16},
  year = 2016
}

@inproceedings{oh2016share,
  abstract = {Database consistency and recoverability require guaranteeing write atomicity for one or more pages. However, contemporary database systems consider write operations non-atomic. Thus, many database storage engines have traditionally relied on either journaling or copy-on-write approaches for atomic propagation of updated pages to the storage. This reliance achieves write atomicity at the cost of various write amplifications such as redundant writes, tree-wandering, and compaction. This write amplification results in reduced performance and, for flash storage, accelerates device wear-out. In this paper, we propose a flash storage interface, SHARE. Being able to explicitly remap the address mapping inside flash storage using SHARE interface enables host-side database storage engines to achieve write atomicity without causing write amplification. We have implemented SHARE on a real SSD board, OpenSSD, and modified MySQL/InnoDB and Couchbase NoSQL storage engines to make them compatible with the extended SHARE interface. Our experimental results show that this SHARE-based MySQL/InnoDB and Couchbase configurations can significantly boost database performance. In particular, the inevitable and costly Couchbase compaction process can complete without copying any data pages.},
  added-at = {2019-01-24T18:21:02.000+0100},
  author = {Oh, Gihwan and Seo, Chiyoung and Mayuram, Ravi and Kee, Yang-Suk and Lee, Sang-Won},
  biburl = {https://www.bibsonomy.org/bibtex/2e8dfe5353506e07a5c0a7d3cd14788ef/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2016},
  editor = {Özcan, Fatma and Koutrika, Georgia and Madden, Sam},
  ee = {https://doi.org/10.1145/2882903.2882910},
  interhash = {9d6bc0b36961c35aa0eed40d1228ab53},
  intrahash = {e8dfe5353506e07a5c0a7d3cd14788ef},
  isbn = {978-1-4503-3531-7},
  keywords = {NoSQL},
  pages = {343-354},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {SHARE Interface in Flash Storage for Relational and NoSQL Databases.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2016.html#OhSMKL16},
  year = 2016
}

@article{kleppmann2015critique,
  abstract = {The CAP Theorem is a frequently cited impossibility result in distributed systems, especially among NoSQL distributed databases. In this paper we survey some of the confusion about the meaning of CAP, including inconsistencies and ambiguities in its definitions, and we highlight some problems in its formalization and proofs. CAP is often interpreted as proof that eventually consistent databases have better availability properties than strongly consistent databases; although there is some truth in this, we show that more careful reasoning is required. These problems cast doubt on the utility of CAP as a tool for reasoning about trade-offs in practical systems. As alternative to CAP, we propose a "delay-sensitivity" framework, which analyzes the sensitivity of operation latency to network delay, and which may help practitioners reason about the trade-offs between consistency guarantees and tolerance of network faults.},
  added-at = {2019-01-22T01:46:08.000+0100},
  author = {Kleppmann, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/229b5c7319314e8aaefd931ac35395e5e/vngudivada},
  ee = {http://arxiv.org/abs/1509.05393},
  interhash = {4134ed8477ffa3a6803f77ab320d1ccb},
  intrahash = {29b5c7319314e8aaefd931ac35395e5e},
  journal = {CoRR},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {A Critique of the CAP Theorem.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1509.html#Kleppmann15},
  volume = {abs/1509.05393},
  year = 2015
}

@inproceedings{mior2016schema,
  abstract = {Database design is critical for high performance in relational databases and many tools exist to aid application designers in selecting an appropriate schema. While the problem of schema optimization is also highly relevant for NoSQL databases, existing tools for relational databases are inadequate for this setting. Application designers wishing to use a NoSQL database instead rely on rules of thumb to select an appropriate schema. We present a system for recommending database schemas for NoSQL applications. Our cost-based approach uses a novel binary integer programming formulation to guide the mapping from the application's conceptual data model to a database schema. We implemented a prototype of this approach for the Cassandra extensible record store. Our prototype, the NoSQL Schema Evaluator (NoSE) is able to capture rules of thumb used by expert designers without explicitly encoding the rules. Automating the design process allows NoSE to produce efficient schemas and to examine more alternatives than would be possible with a manual rule-based approach.},
  added-at = {2019-01-24T20:54:32.000+0100},
  author = {Mior, Michael J. and Salem, Kenneth and Aboulnaga, Ashraf and Liu, Rui},
  biburl = {https://www.bibsonomy.org/bibtex/26ec9656728e43956d9fa310b2ea4911d/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2016},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2016.7498239},
  interhash = {11cd52345ff7b0b656bf5e0e58298cf0},
  intrahash = {6ec9656728e43956d9fa310b2ea4911d},
  isbn = {978-1-5090-2020-1},
  keywords = {NoSQL},
  pages = {181-192},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {NoSE: Schema design for NoSQL applications.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2016.html#MiorSAL16},
  year = 2016
}

@inproceedings{mckenzie2015finetuning,
  abstract = {NoSQL storage systems are used extensively by web applications and provide an attractive alternative to conventional databases when the need for scalability outweighs the need for transactions. Several of these systems, notably Amazon's Dynamo and its open-source derivatives, provide quorum-based replication and present the application developer with a choice of multiple client-side "consistency levels" that determine the number of replicas accessed by reads and writes. This setting, in turn, affects both the latency and the consistency observed by the client application. Since using a fixed combination of read and write consistency levels for a given application provides only a limited number of discrete options for tuning the consistency-latency trade-off, we investigate techniques that allow more fine-grained tuning as may be required to support consistency guarantees through service level agreements (SLAs). We consider two such techniques, a novel technique that assigns the consistency level on a peroperation basis by choosing randomly between two options (e.g., weak vs. strong consistency) with a tunable probability, and a known technique that uses weak consistency and injects delays into storage operations artificially. We compare and contrast these two techniques experimentally against each other and against combinations of fixed consistency levels using Apache Cassandra deployed in Amazon's EC2 environment.},
  added-at = {2019-01-27T22:15:08.000+0100},
  author = {McKenzie, Marlon and Fan, Hua and Golab, Wojciech M.},
  biburl = {https://www.bibsonomy.org/bibtex/2fcd6fc2a1b3925b0311a88cf9d9c6122/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2015},
  ee = {https://doi.org/10.1109/BigData.2015.7363942},
  interhash = {6c94bf8bda2b7479351e9abe753a833c},
  intrahash = {fcd6fc2a1b3925b0311a88cf9d9c6122},
  isbn = {978-1-4799-9926-2},
  keywords = {NoSQL},
  pages = {1708-1717},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Fine-tuning the consistency-latency trade-off in quorum-replicated distributed storage systems.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2015.html#McKenzieFG15},
  year = 2015
}

@inproceedings{klettke2017uncovering,
  abstract = {Data accumulating in data lakes can become inaccessible in the long run when its semantics are not available. The heterogeneity of data formats and the sheer volumes of data collections prohibit cleaning and unifying the data manually. Thus, tools for automated data lake analysis are of great interest. In this paper, we target the particular problem of reconstructing the schema evolution history from data lakes. Knowing how the data is structured, and how this structure has evolved over time, enables programmatic access to the lake. By deriving a sequence of schema versions, rather than a single schema, we take into account structural changes over time. Moreover, we address the challenge of detecting inclusion dependencies. This is a prerequisite for mapping between succeeding schema versions, and in particular, detecting nontrivial changes such as a property having been moved or copied. We evaluate our approach for detecting inclusion dependencies using the MovieLens dataset, as well an adaption of a dataset containing botanical descriptions, to cover specific edge cases.},
  added-at = {2019-01-25T20:14:23.000+0100},
  author = {Klettke, Meike and Awolin, Hannes and Störl, Uta and Müller, Daniel and Scherzinger, Stefanie},
  biburl = {https://www.bibsonomy.org/bibtex/21eb067b9d683001dd295ffc39f74e6b2/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258204},
  interhash = {40fbb0bdd0bca429d98d78283bebceb4},
  intrahash = {1eb067b9d683001dd295ffc39f74e6b2},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {2462-2471},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Uncovering the evolution history of data lakes.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#KlettkeAS0S17},
  year = 2017
}

@book{michail2011models,
  abstract = {Wireless sensor networks are about to be part of everyday life. Homes and workplaces capable of self-controlling and adapting air-conditioning for different temperature and humidity levels, sleepless forests ready to detect and react in case of a fire, vehicles able to avoid sudden obstacles or possibly able to self-organize routes to avoid congestion, and so on, will probably be commonplace in the very near future. Mobility plays a central role in such systems and so does passive mobility, that is, mobility of the network stemming from the environment itself. The population protocol model was an intellectual invention aiming to describe such systems in a minimalistic and analysis-friendly way. Having as a starting-point the inherent limitations but also the fundamental establishments of the population protocol model, we try in this monograph to present some realistic and practical enhancements that give birth to some new and surprisingly powerful (for these kind of systems) computational models.},
  added-at = {2019-02-04T19:26:01.000+0100},
  author = {Michail, Othon and Chatzigiannakis, Ioannis and Spirakis, Paul G.},
  biburl = {https://www.bibsonomy.org/bibtex/2d0ca56782ccbe75213e8d937ada55b15/vngudivada},
  ee = {http://dx.doi.org/10.2200/S00328ED1V01Y201101DCT006},
  interhash = {00c0c27e50a4fe7330bb632339a7e7c9},
  intrahash = {d0ca56782ccbe75213e8d937ada55b15},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {New Models for Population Protocols},
  url = {http://dx.doi.org/10.2200/S00328ED1V01Y201101DCT006},
  year = 2011
}

@inproceedings{cazzanti2015documentbased,
  abstract = {Computational Maritime Situational Awareness (MSA) supports the maritime industry, governments, and international organizations with machine learning and big data techniques for analyzing vessel traffic data available through the Automatic Identification System (AIS). A critical challenge of scaling computational MSA to big data regimes is integrating the core learning algorithms with big data storage modes and data models. To address this challenge, we report results from our experimentation with MongoDB, a NoSQL document-based database which we test as a supporting platform for computational MSA. We experiment with a document model that avoids database joins when linking position and voyage AIS vessel information and allows tuning the database index and document sizes in response to the AIS data rate. We report results for the AIS data ingested and analyzed daily at the NATO Centre for Maritime Research and Experimentation (CMRE).},
  added-at = {2019-01-27T22:14:01.000+0100},
  author = {Cazzanti, Luca and Millefiori, Leonardo M. and Arcieri, Gianfranco},
  biburl = {https://www.bibsonomy.org/bibtex/2741d52b2b3051c7b7f2a20f04a71f371/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2015},
  ee = {https://doi.org/10.1109/BigData.2015.7363894},
  interhash = {32c0c9b78841150a42193ab336c36945},
  intrahash = {741d52b2b3051c7b7f2a20f04a71f371},
  isbn = {978-1-4799-9926-2},
  keywords = {NoSQL},
  pages = {1350-1356},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {A document-based data model for large scale computational maritime situational awareness.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2015.html#CazzantiMA15},
  year = 2015
}

@inproceedings{attiya2015limitations,
  abstract = {Modern replicated data stores aim to provide high availability, by immediately responding to client requests, often by implementing objects that expose concurrency. Such objects, for example, multi-valued registers (MVRs), do not have sequential specifications. This paper explores a recent model for replicated data stores that can be used to precisely specify causal consistency for such objects, and liveness properties like eventual consistency, without revealing details of the underlying implementation. The model is used to prove the following results: An eventually consistent data store implementing MVRs cannot satisfy a consistency model strictly stronger than observable causal consistency (OCC). OCC is a model somewhat stronger than causal consistency, which captures executions in which client observations can use causality to infer concurrency of operations. This result holds under certain assumptions about the data store. Under the same assumptions, an eventually consistent and causally consistent replicated data store must send messages of unbounded size: If s objects are supported by n replicas, then, for every k > 1, there is an execution in which an Ω({n,s} k)-bit message is sent.},
  added-at = {2019-01-22T01:50:12.000+0100},
  author = {Attiya, Hagit and Ellen, Faith and Morrison, Adam},
  biburl = {https://www.bibsonomy.org/bibtex/229c4ad2b8f016a1d3c2cd71d6fa8c431/vngudivada},
  booktitle = {PODC},
  crossref = {conf/podc/2015},
  editor = {Georgiou, Chryssis and Spirakis, Paul G.},
  ee = {https://doi.org/10.1145/2767386.2767419},
  interhash = {c55132c3c5c9f2e6294f21d0c47fe977},
  intrahash = {29c4ad2b8f016a1d3c2cd71d6fa8c431},
  isbn = {978-1-4503-3617-8},
  keywords = {NoSQL},
  pages = {385-394},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Limitations of Highly-Available Eventually-Consistent Data Stores.},
  url = {http://dblp.uni-trier.de/db/conf/podc/podc2015.html#AttiyaEM15},
  year = 2015
}

@inproceedings{abadi2016holistic,
  abstract = {In recent years, NoSQL databases have emerged as an answer to Internet scale datasets. However, these solutions do not support out of the box the necessary backup, restore and disaster recovery capabilities required by many big data scenarios. In this paper we address this gap: we present a novel disaster recovery approach designed for big data NoSQL workloads. This approach can support low recovery point and time objectives at low costs. Our approach is highly scalable and elastic, based on open source components and suitable for any document-based NoSQL solution, which makes it highly attractive for diverse big data workloads.},
  added-at = {2019-01-27T22:01:59.000+0100},
  author = {Abadi, Aharon and Haib, Ashraf and Melamed, Roie and Nassar, Alaa and Shribman, Aidan and Yasin, Hisham},
  biburl = {https://www.bibsonomy.org/bibtex/2541c7e1e80d55ca2a0f4164121bc73c1/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2016},
  editor = {Joshi, James and Karypis, George and Liu, Ling and Hu, Xiaohua and Ak, Ronay and Xia, Yinglong and Xu, Weijia and Sato, Aki-Hiro and Rachuri, Sudarsan and Ungar, Lyle H. and Yu, Philip S. and Govindaraju, Rama and Suzumura, Toyotaro},
  ee = {http://dx.doi.org/10.1109/BigData.2016.7840833},
  interhash = {bc574b52649de113a4f3b34b643e5a5e},
  intrahash = {541c7e1e80d55ca2a0f4164121bc73c1},
  isbn = {978-1-4673-9005-7},
  keywords = {NoSQL},
  pages = {2075-2080},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Holistic disaster recovery approach for big data NoSQL workloads.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2016.html#AbadiHMNSY16},
  year = 2016
}

@inproceedings{jordao2014secure,
  abstract = {In order to prevent the attack and discovery of sensitive data, a model is presented to protect the confidentiality of data even when the attackers have access to all data from the server: using systems that perform queries and inserts on encrypted data without the decryption key. The model set is applied in a real scenario: a distributed system hosted by Google (Encrypted BigQuery), which includes the use of encryption in data stored in non-relational databases for massive data processing. As a result, it is shown that this system supports a variety of applications with low overhead.},
  added-at = {2019-01-27T22:37:46.000+0100},
  author = {Jordao, Renata and Martins, Valério Aymoré and Buiati, Fábio and de Sousa Júnior, Rafael Timóteo and de Deus, Flavio E.},
  biburl = {https://www.bibsonomy.org/bibtex/257fc0c1a8bb8a520ae00cfe69f1213c5/vngudivada},
  booktitle = {BigData Conference},
  crossref = {conf/bigdataconf/2014},
  editor = {Lin, Jimmy J. and Pei, Jian and Hu, Xiaohua and Chang, Wo and Nambiar, Raghunath and Aggarwal, Charu and Cercone, Nick and Honavar, Vasant and Huan, Jun and Mobasher, Bamshad and Pyne, Saumyadipta},
  ee = {http://dx.doi.org/10.1109/BigData.2014.7004383},
  interhash = {8b2486d02804ad4d30cab845433dabed},
  intrahash = {57fc0c1a8bb8a520ae00cfe69f1213c5},
  isbn = {978-1-4799-5665-4},
  keywords = {NoSQL},
  pages = {6-12},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Secure data storage in distributed cloud environments.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2014.html#JordaoMBSD14},
  year = 2014
}

@inproceedings{ozcan2014experiencing,
  added-at = {2019-01-24T18:09:51.000+0100},
  author = {Özcan, Fatma and Tatbul, Nesime and Abadi, Daniel J. and Kornacker, Marcel and Mohan, C. and Ramasamy, Karthik and Wiener, Janet L.},
  biburl = {https://www.bibsonomy.org/bibtex/2836f6bfcb9867a6c4521fe88ba16befd/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2014},
  editor = {Dyreson, Curtis E. and Li, Feifei and Özsu, M. Tamer},
  ee = {https://doi.org/10.1145/2588555.2618215},
  interhash = {2f7e05de1cba97c64c8047256ca63818},
  intrahash = {836f6bfcb9867a6c4521fe88ba16befd},
  isbn = {978-1-4503-2376-5},
  keywords = {NoSQL},
  pages = {1407-1408},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Are we experiencing a big data bubble?},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2014.html#OzcanTAKMRW14},
  year = 2014
}

@book{kaynar2010theory,
  abstract = {This monograph presents the Timed Input/Output Automaton (TIOA) modeling framework, a basic mathematical framework to support description and analysis of timed (computing) systems. Timed systems are systems in which desirable correctness or performance properties of the system depend on the timing of events, not just on the order of their occurrence. Timed systems are employed in a wide range of domains including communications, embedded systems, real-time operating systems, and automated control. Many applications involving timed systems have strong safety, reliability, and predictability requirements, which make it important to have methods for systematic design of systems and rigorous analysis of timing-dependent behavior. The TIOA framework also supports description and analysis of timed distributed algorithms -- distributed algorithms whose correctness and performance depend on the relative speeds of processors, accuracy of local clocks, or communication delay bounds. Such algorithms arise, for example, in traditional and wireless communications, networks of mobile devices, and shared-memory multiprocessors. The need to prove rigorous theoretical results about timed distributed algorithms makes it important to have a suitable mathematical foundation.

An important feature of the TIOA framework is its support for decomposing timed system descriptions. In particular, the framework includes a notion of external behavior for a timed I/O automaton, which captures its discrete interactions with its environment. The framework also defines what it means for one TIOA to implement another, based on an inclusion relationship between their external behavior sets, and defines notions of simulations, which provide sufficient conditions for demonstrating implementation relationships. The framework includes a composition operation for TIOAs, which respects external behavior, and a notion of receptiveness, which implies that a TIOA does not block the passage of time.

The TIOA framework also defines the notion of a property and what it means for a property to be a safety or a liveness property. It includes results that capture common proof methods for showing that automata satisfy properties.},
  added-at = {2019-02-04T19:28:03.000+0100},
  author = {Kaynar, Dilsun Kirli and Lynch, Nancy A. and Segala, Roberto and Vaandrager, Frits W.},
  biburl = {https://www.bibsonomy.org/bibtex/28b5979ba2c2529a6b9e0a4568e5c0803/vngudivada},
  ee = {http://dx.doi.org/10.2200/S00310ED1V01Y201011DCT005},
  interhash = {f45e0363e6dc80d729afacfe31a008f4},
  intrahash = {8b5979ba2c2529a6b9e0a4568e5c0803},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {The Theory of Timed I/O Automata, Second Edition},
  url = {http://dx.doi.org/10.2200/S00310ED1V01Y201011DCT005},
  year = 2010
}

@inproceedings{naskos2018elton,
  abstract = {We present the Elton tool, a publicly available cloud resource elasticity management system tailored to NoSQL databases. Elton is integrated in the Ganetimgr web platform, and offers an easy to use web interface, through which monitoring and horizontal scaling of NoSQL databases can be performed and what-if analysis queries are enabled. Elton uses Markov Decision Processes (MDPs) as the underlying modeling framework, and encapsulates state-of-the-art horizontal scaling policies that offer different trade-offs between performance and monetary deployment cost. Its main novelty is that it employs probabilistic model checking to allow for both efficient elasticity decisions and analysis of scaling actions and serves as a case study about the benefits of model checking in online decision making and analysis.},
  added-at = {2019-01-24T20:05:34.000+0100},
  author = {Naskos, Athanasios and Gounaris, Anastasios and Konstantinou, Ioannis},
  biburl = {https://www.bibsonomy.org/bibtex/26e793f75854f41d9d3bbe9b01d5670ff/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2018},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2018.00196},
  interhash = {3c2240bc1c8bc165abf907cd288305ce},
  intrahash = {6e793f75854f41d9d3bbe9b01d5670ff},
  isbn = {978-1-5386-5520-7},
  keywords = {NoSQL},
  pages = {1641-1644},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Elton: A Cloud Resource Scaling-Out Manager for NoSQL Databases.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2018.html#NaskosGK18},
  year = 2018
}

@inproceedings{chen2015spatiotemporal,
  abstract = {Geoscience gives insights into our surroundings and benefits many aspects of our life. Nowadays, with massive sensors deployed to sense all kinds of parameters for environments, tens of billions, even trillions of sensed data are collected and need to be analyzed for surveillance or other purposes. From many perspectives, users always issue queries according to specific spatial and temporal predicates. For these applications, relational databases are overwhelmed by the large scale and high rate insertions, and NoSQL database could be considered a feasible solution. HBase, a popular key-value store system, is capable to solve the storage problem, but fails to provide in-built spatio-temporal querying capability. Many previous works tackle the problem by designing schema, i.e., designing row key and column key formation for HBase, which we don't believe is an effective solution. In this paper, we address this problem from nature level of HBase, and propose an index structure as a built-in component for HBase. STEHIX (Spatio-TEmporal Hbase IndeX) is adapted to two-level architecture of HBase and suitable for HBase to process spatio-temporal queries. It is composed of index in the meta table (the first level) and region index (the second level) for indexing inner structure of HBase regions. Base on this structure, two common queries, range query and kNN query are solved by proposing algorithms, respectively. For achieving load balancing and scalable kNN query, two optimizations are also presented. We implement STEHIX and conduct experiments on real dataset, and the results show our design outperforms a previous work in many aspects.},
  added-at = {2019-01-27T22:18:08.000+0100},
  author = {Chen, Xiaoying and Zhang, Chong and Ge, Bin and Xiao, Weidong},
  biburl = {https://www.bibsonomy.org/bibtex/286d5b54fb7a4c707ebcbe18545230a87/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2015},
  ee = {https://doi.org/10.1109/BigData.2015.7363970},
  interhash = {453330dfabdca4ed96171a9353f27687},
  intrahash = {86d5b54fb7a4c707ebcbe18545230a87},
  isbn = {978-1-4799-9926-2},
  keywords = {NoSQL},
  pages = {1929-1937},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Spatio-temporal queries in HBase.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2015.html#ChenZGX15},
  year = 2015
}

@inproceedings{liu2016closing,
  abstract = {Oracle release 12cR1 supports JSON data management that enables users to store, index and query JSON data along with relational data. The integration of the JSON data model into the RDBMS allows a new paradigm of data management where data is storable, indexable and queryable without upfront schema definition. We call this new paradigm Flexible Schema Data Management (FSDM). In this paper, we present enhancements to Oracle's JSON data management in the upcoming 12cR2 release. We present JSON DataGuide, an auto-computed dynamic soft schema for JSON collections that closes the functional gap between the fixed-schema SQL world and the schema-less NoSQL world. We present a self-contained query friendly binary format for encoding JSON (OSON) to close the query performance gap between schema-encoded relational data and schema free JSON textual data. The addition of these new features makes the Oracle RDBMS well suited to both fixedschema SQL and flexible-schema NoSQL use cases, and allows users to freely mix the two paradigms in a single data management system.},
  added-at = {2019-01-24T18:19:13.000+0100},
  author = {Liu, Zhen Hua and Hammerschmidt, Beda Christoph and McMahon, Doug and Liu, Ying and Chang, Hui Joe},
  biburl = {https://www.bibsonomy.org/bibtex/22e7ec5334b607121529bc720682b0f3e/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2016},
  editor = {Özcan, Fatma and Koutrika, Georgia and Madden, Sam},
  ee = {https://doi.org/10.1145/2882903.2903731},
  interhash = {af3b05f33428f183621fd55951f9fa23},
  intrahash = {2e7ec5334b607121529bc720682b0f3e},
  isbn = {978-1-4503-3531-7},
  keywords = {NoSQL},
  pages = {227-238},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Closing the functional and Performance Gap between SQL and NoSQL.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2016.html#LiuHMLC16},
  year = 2016
}

@article{abadi2012consistency,
  abstract = {The CAP theorem's impact on modern distributed database system design is more limited than is often perceived. Another tradeoff—between consistency and latency —has had a more direct influence on several well-known DDBSs. A proposed new formulation, PACELC, unifies this tradeoff with CAP.},
  added-at = {2019-01-22T01:53:54.000+0100},
  author = {Abadi, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/27240c0d83084635bf172ac68f36e23e0/vngudivada},
  ee = {http://doi.ieeecomputersociety.org/10.1109/MC.2012.33},
  interhash = {a3fee8096a11ef0f68c38a479f861b15},
  intrahash = {7240c0d83084635bf172ac68f36e23e0},
  journal = {IEEE Computer},
  keywords = {NoSQL},
  number = 2,
  pages = {37-42},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Consistency Tradeoffs in Modern Distributed Database System Design: CAP is Only Part of the Story.},
  url = {http://dblp.uni-trier.de/db/journals/computer/computer45.html#Abadi12},
  volume = 45,
  year = 2012
}

@inproceedings{greguska2017analyzing,
  abstract = {NEXUS is a software project developed by the NASA Jet Propulsion Laboratory which aims to enable scientific analysis of large datasets collected by various NASA missions. Historically, data analysis required an analyst to move the data to the system computing the analysis. As the volume of data available increases, the storage, CPU, and memory requirements for this type of traditional analysis increases to a point that makes it impractical. The NEXUS approach is to remove this data movement, transform the data into a format convenient for analysis, and provide analytic functions that can be massively parallelized.},
  added-at = {2019-01-27T20:31:29.000+0100},
  author = {Greguska, Frank R. and Huang, Thomas and Wilson, Brian and Quach, Nga and Jacob, Joe},
  biburl = {https://www.bibsonomy.org/bibtex/2593bd7261500b43b7429e7601ddb6a5b/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258530},
  interhash = {017a2eba089fd2ae085e387861fc6939},
  intrahash = {593bd7261500b43b7429e7601ddb6a5b},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = 4750,
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Analyzing big ocean science data with NEXUS.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#GreguskaHWQJ17},
  year = 2017
}

@inproceedings{maccioni2018augmented,
  abstract = {The huge diversity of database technologies in use inside organizations pose today new challenges of data management and integration. Polystores provide a solution to this scenario based on a loosely coupled integration of data sources and the direct access, with the local language, to each storage engine for exploiting its distinctive features. However, given the absence of a global schema, it is hard to know if a query to one system can be satisfied with data stored elsewhere in the polystore. We address this issue by introducing query augmentation, a data manipulation operator for polystores based on the automatic enrichment of the answer to a local query with related data in the rest of the polystore. Augmentation can be used to implement two effective methods for data access in polystores: augmented search and augmented exploration. We show that they provide effective tools for information discovery in polystores that avoid middleware layers, abstract query languages, and shared data models. We also illustrate the design of QUEPA, a system that fully implements our approach in an efficient way. A comprehensive campaign of experiments done with QUEPA shows that our approach is feasible and, unlike other approaches, scales nicely as the polystore grows in the number of stores and size of databases.},
  added-at = {2019-01-24T19:42:44.000+0100},
  author = {Maccioni, Antonio and Torlone, Riccardo},
  biburl = {https://www.bibsonomy.org/bibtex/2c5b62584d77a3feef133ea5768ec603b/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2018},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2018.00017},
  interhash = {db31303cb875e3a2cdcd3b9d79eee597},
  intrahash = {c5b62584d77a3feef133ea5768ec603b},
  isbn = {978-1-5386-5520-7},
  keywords = {NoSQL},
  pages = {77-88},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Augmented Access for Querying and Exploring a Polystore.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2018.html#MaccioniT18},
  year = 2018
}

@inproceedings{sun2015sqlgraph,
  abstract = {We show that existing mature, relational optimizers can be exploited with a novel schema to give better performance for property graph storage and retrieval than popular noSQL graph stores. The schema combines relational storage for adjacency information with JSON storage for vertex and edge attributes. We demonstrate that this particular schema design has benefits compared to a purely relational or purely JSON solution. The query translation mechanism translates Gremlin queries with no side effects into SQL queries so that one can leverage relational query optimizers. We also conduct an empirical evaluation of our schema design and query translation mechanism with two existing popular property graph stores. We show that our system is 2-8 times better on query performance, and 10-30 times better in throughput on 4.3 billion edge graphs compared to existing stores.},
  added-at = {2019-01-24T18:36:26.000+0100},
  author = {Sun, Wen and Fokoue, Achille and Srinivas, Kavitha and Kementsietsidis, Anastasios and Hu, Gang and Xie, Guo Tong},
  biburl = {https://www.bibsonomy.org/bibtex/2092035642d3a5acf0210e64565c4d761/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2015},
  editor = {Sellis, Timos K. and Davidson, Susan B. and Ives, Zachary G.},
  ee = {https://doi.org/10.1145/2723372.2723732},
  interhash = {b93a5eda2d4c1bf93ed79fdbf28706c5},
  intrahash = {092035642d3a5acf0210e64565c4d761},
  isbn = {978-1-4503-2758-9},
  keywords = {NoSQL},
  pages = {1887-1901},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {SQLGraph: An Efficient Relational-Based Property Graph Store.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2015.html#SunFSKHX15},
  year = 2015
}

@article{drake2014commandline,
  added-at = {2019-01-22T02:28:51.000+0100},
  author = {Drake, Adam},
  biburl = {https://www.bibsonomy.org/bibtex/25831f272eab7287b3331cceafe0007ca/vngudivada},
  interhash = {228d7c874458972551d826e1a077a2e1},
  intrahash = {5831f272eab7287b3331cceafe0007ca},
  keywords = {NoSQL},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Command-Line Tools Can Be 235x Faster than Your Hadoop Cluster},
  year = 2014
}

@inproceedings{gessert2016scalable,
  abstract = {The unprecedented scale at which data is consumed and generated today has shown a large demand for scalable data management and given rise to non-relational, distributed “NoSQL” database systems. Two central problems triggered this process: 1) vast amounts of user-generated content in modern applications and the resulting requests loads and data volumes 2) the desire of the developer community to employ problem-specific data models for storage and querying. To address these needs, various data stores have been developed by both industry and research, arguing that the era of one-size-fits-all database systems is over. The heterogeneity and sheer amount of these systems - now commonly referred to as NoSQL data stores - make it increasingly difficult to select the most appropriate system for a given application. Therefore, these systems are frequently combined in polyglot persistence architectures to leverage each system in its respective sweet spot. This tutorial gives an in-depth survey of the most relevant NoSQL databases to provide comparative classification and highlight open challenges. To this end, we analyze the approach of each system to derive its scalability, availability, consistency, data modeling and querying characteristics. We present how each system's design is governed by a central set of trade-offs over irreconcilable system properties. We then cover recent research results in distributed data management to illustrate that some shortcomings of NoSQL systems could already be solved in practice, whereas other NoSQL data management problems pose interesting and unsolved research challenges.},
  added-at = {2019-01-24T20:58:11.000+0100},
  author = {Gessert, Felix and Ritter, Norbert},
  biburl = {https://www.bibsonomy.org/bibtex/2139f6dfe9889cd3306629d0720099a59/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2016},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2016.7498360},
  interhash = {2b36eb148b55022f832d0d5b7c83a463},
  intrahash = {139f6dfe9889cd3306629d0720099a59},
  isbn = {978-1-5090-2020-1},
  keywords = {NoSQL},
  pages = {1420-1423},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Scalable data management: NoSQL data stores in research and practice.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2016.html#GessertR16},
  year = 2016
}

@inproceedings{ceesay2017bench,
  abstract = {The recent boom of big data, coupled with the challenges of its processing and storage gave rise to the development of distributed data processing and storage paradigms like MapReduce, Spark, and NoSQL databases. With the advent of cloud computing, processing and storing such massive datasets on clusters of machines is now feasible with ease. However, there are limited tools and approaches, which users can rely on to gauge and comprehend the performance of their big data applications deployed locally on clusters, or in the cloud. Researchers have started exploring this area by providing benchmarking suites suitable for big data applications. However, many of these tools are fragmented, complex to deploy and manage, and do not provide transparency with respect to the monetary cost of benchmarking an application. In this paper, we present Plug And Play Bench (PAPB1): an infrastructure aware abstraction built to integrate and simplify the deployment of big data benchmarking tools on clusters of machines. PAPB automates the tedious process of installing, configuring and executing common big data benchmark workloads by containerising the tools and settings based on the underlying cluster deployment framework. Our proof of concept implementation utilises HiBench as the benchmark suite, HDP as the cluster deployment framework and Azure as the cloud platform. The paper further illustrates the inclusion of cost metrics based on the underlying Microsoft Azure cloud platform.},
  added-at = {2019-01-25T20:16:20.000+0100},
  author = {Ceesay, Sheriffo and Barker, Adam and Varghese, Blesson},
  biburl = {https://www.bibsonomy.org/bibtex/2e07696691904c18dc0c58bece94aa97f/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258249},
  interhash = {3ec9d882a69fe0f0e066314285769cf9},
  intrahash = {e07696691904c18dc0c58bece94aa97f},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {2821-2828},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Plug and play bench: Simplifying big data benchmarking using containers.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#CeesayBV17},
  year = 2017
}

@book{welch2011reversal,
  abstract = {Link reversal is a versatile algorithm design technique that has been used in numerous distributed algorithms for a variety of problems. The common thread in these algorithms is that the distributed system is viewed as a graph, with vertices representing the computing nodes and edges representing some other feature of the system (for instance, point-to-point communication channels or a conflict relationship). Each algorithm assigns a virtual direction to the edges of the graph, producing a directed version of the original graph. As the algorithm proceeds, the virtual directions of some of the links in the graph change in order to accomplish some algorithm-specific goal. The criterion for changing link directions is based on information that is local to a node (such as the node having no outgoing links) and thus this approach scales well, a feature that is desirable for distributed algorithms.

This monograph presents, in a tutorial way, a representative sampling of the work on link-reversal-based distributed algorithms. The algorithms considered solve routing, leader election, mutual exclusion, distributed queueing, scheduling, and resource allocation. The algorithms can be roughly divided into two types, those that assume a more abstract graph model of the networks, and those that take into account more realistic details of the system. In particular, these more realistic details include the communication between nodes, which may be through asynchronous message passing, and possible changes in the graph, for instance, due to movement of the nodes.

We have not attempted to provide a comprehensive survey of all the literature on these topics. Instead, we have focused in depth on a smaller number of fundamental papers, whose common thread is that link reversal provides a way for nodes in the system to observe their local neighborhoods, take only local actions, and yet cause global problems to be solved. We conjecture that future interesting uses of link reversal are yet to be discovered.},
  added-at = {2019-02-04T19:21:06.000+0100},
  author = {Welch, Jennifer L. and Walter, Jennifer E.},
  biburl = {https://www.bibsonomy.org/bibtex/2f7255f9991e630ba28b51039f6f31371/vngudivada},
  ee = {http://dx.doi.org/10.2200/S00389ED1V01Y201111DCT008},
  interhash = {1d8e25f9791b79c7b72cb9d0c1b38d4f},
  intrahash = {f7255f9991e630ba28b51039f6f31371},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Link Reversal Algorithms},
  url = {http://dx.doi.org/10.2200/S00389ED1V01Y201111DCT008},
  year = 2011
}

@inproceedings{xu2014aquas,
  abstract = {NoSQL key-value data stores provide an attractive solution for big data management. With the help of data partitioning and replication, those data stores achieve higher levels of availability, scalability and reliability. Such design choices typically exhibit a tradeoff in which data freshness is sacrificed in favor of reduced access latency. At the replica-level, this tradeoff is primarily shaped by the resource allocation strategies deployed for managing the processing of user queries and replica updates. In this demonstration, we showcase AQUAS: a quality-aware scheduler for Cassandra, which allows application developers to specify requirements on quality of service (QoS) and quality of data (QoD). AQUAS efficiently allocates the available replica resources to execute the incoming read/write tasks so that to minimize the penalties incurred by violating those requirements. We demonstrate AQUAS based on our implementation of a microblogging system.},
  added-at = {2019-01-24T21:03:30.000+0100},
  author = {Xu, Chen and Xia, Fan and Sharaf, Mohamed A. and Zhou, Minqi and Zhou, Aoying},
  biburl = {https://www.bibsonomy.org/bibtex/29db8757430cbaefc6e4e4af7c6199049/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2014},
  editor = {Cruz, Isabel F. and Ferrari, Elena and Tao, Yufei and Bertino, Elisa and Trajcevski, Goce},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2014.6816743},
  interhash = {5a121c0d59b1198199228584d302dedd},
  intrahash = {9db8757430cbaefc6e4e4af7c6199049},
  isbn = {978-1-4799-3480-5},
  keywords = {NoSQL},
  pages = {1210-1213},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {AQUAS: A quality-aware scheduler for NoSQL data stores.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2014.html#XuXSZZ14},
  year = 2014
}

@book{barenboim2013distributed,
  abstract = {The focus of this monograph is on symmetry breaking problems in the message-passing model of distributed computing. In this model a communication network is represented by a n-vertex graph G = (V,E), whose vertices host autonomous processors. The processors communicate over the edges of G in discrete rounds. The goal is to devise algorithms that use as few rounds as possible.

A typical symmetry-breaking problem is the problem of graph coloring. Denote by Δ the maximum degree of G. While coloring G with Δ + 1 colors is trivial in the centralized setting, the problem becomes much more challenging in the distributed one. One can also compromise on the number of colors, if this allows for more efficient algorithms. Other typical symmetry-breaking problems are the problems of computing a maximal independent set (MIS) and a maximal matching (MM). The study of these problems dates back to the very early days of distributed computing. The founding fathers of distributed computing laid firm foundations for the area of distributed symmetry breaking already in the eighties. In particular, they showed that all these problems can be solved in randomized logarithmic time. Also, Linial showed that an O(Δ2)-coloring can be solved very efficiently deterministically.

However, fundamental questions were left open for decades. In particular, it is not known if the MIS or the (Δ + 1)-coloring can be solved in deterministic polylogarithmic time. Moreover, until recently it was not known if in deterministic polylogarithmic time one can color a graph with significantly fewer than Δ2 colors. Additionally, it was open (and still open to some extent) if one can have sublogarithmic randomized algorithms for the symmetry breaking problems.

Recently, significant progress was achieved in the study of these questions. More efficient deterministic and randomized (Δ + 1)-coloring algorithms were achieved. Deterministic Δ1 + o(1)-coloring algorithms with polylogarithmic running time were devised. Improved (and often sublogarithmic-time) randomized algorithms were devised. Drastically improved lower bounds were given. Wide families of graphs in which these problems are solvable much faster than on general graphs were identified.

The objective of our monograph is to cover most of these developments, and as a result to provide a treatise on theoretical foundations of distributed symmetry breaking in the message-passing model. We hope that our monograph will stimulate further progress in this exciting area.},
  added-at = {2019-02-04T19:12:52.000+0100},
  author = {Barenboim, Leonid and Elkin, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/287d7083a2f7d9c7f8e978a5ddcd29339/vngudivada},
  booktitle = {Distributed Graph Coloring: Fundamentals and Recent Developments},
  ee = {http://dx.doi.org/10.2200/S00520ED1V01Y201307DCT011},
  interhash = {3da2eb1bd79abf552f0d1fb6f6f95b4e},
  intrahash = {87d7083a2f7d9c7f8e978a5ddcd29339},
  isbn = {9781627050197},
  keywords = {Book DistributedComputing SynthesisSeries},
  pages = {1-171},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Distributed Graph Coloring: Fundamentals and Recent Developments},
  url = {http://dx.doi.org/10.2200/S00520ED1V01Y201307DCT011},
  year = 2013
}

@book{vukolic2012quorum,
  abstract = {A quorum system is a collection of subsets of nodes, called quorums, with the property that each pair of quorums have a non-empty intersection. Quorum systems are the key mathematical abstraction for ensuring consistency in fault-tolerant and highly available distributed computing. Critical for many applications since the early days of distributed computing, quorum systems have evolved from simple majorities of a set of processes to complex hierarchical collections of sets, tailored for general adversarial structures. The initial non-empty intersection property has been refined many times to account for, e.g., stronger (Byzantine) adversarial model, latency considerations or better availability. This monograph is an overview of the evolution and refinement of quorum systems, with emphasis on their role in two fundamental applications: distributed read/write storage and consensus.},
  added-at = {2019-02-04T19:18:19.000+0100},
  author = {Vukolic, Marko},
  biburl = {https://www.bibsonomy.org/bibtex/2fa078b08144be173b7bfa76089b0953f/vngudivada},
  ee = {http://dx.doi.org/10.2200/S00402ED1V01Y201202DCT009},
  interhash = {70cf11760b1111fe92161c08add67b63},
  intrahash = {fa078b08144be173b7bfa76089b0953f},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Quorum Systems: With Applications to Storage and Consensus},
  url = {http://dx.doi.org/10.2200/S00402ED1V01Y201202DCT009},
  year = 2012
}

@book{guerraoui2010principles,
  abstract = {Transactional memory (TM) is an appealing paradigm for concurrent programming on shared memory architectures. With a TM, threads of an application communicate, and synchronize their actions, via in-memory transactions. Each transaction can perform any number of operations on shared data, and then either commit or abort. When the transaction commits, the effects of all its operations become immediately visible to other transactions; when it aborts, however, those effects are entirely discarded. Transactions are atomic: programmers get the illusion that every transaction executes all its operations instantaneously, at some single and unique point in time. Yet, a TM runs transactions concurrently to leverage the parallelism offered by modern processors. The aim of this book is to provide theoretical foundations for transactional memory. This includes defining a model of a TM, as well as answering precisely when a TM implementation is correct, what kind of properties it can ensure, what are the power and limitations of a TM, and what inherent trade-offs are involved in designing a TM algorithm. While the focus of this book is on the fundamental principles, its goal is to capture the common intuition behind the semantics of TMs and the properties of existing TM implementations.},
  added-at = {2019-02-04T19:29:19.000+0100},
  author = {Guerraoui, Rachid and Kapalka, Michal},
  biburl = {https://www.bibsonomy.org/bibtex/2db5f36254cb69274b45a5bfe06afdd31/vngudivada},
  ee = {http://dx.doi.org/10.2200/S00253ED1V01Y201009DCT004},
  interhash = {dd68fa0a0291e51a253e85c93a5ea206},
  intrahash = {db5f36254cb69274b45a5bfe06afdd31},
  keywords = {Book DistributedComputing SynthesisSeries},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Distributed Computing Theory},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Principles of Transactional Memory},
  url = {http://dx.doi.org/10.2200/S00253ED1V01Y201009DCT004},
  year = 2010
}

@inproceedings{shah2017towards,
  abstract = {Voluminous and variety of disparate information is generated and consumed in agriculture domain at a higher velocity. In agriculture, information is available in the form of weather and soil conditions reports, GPS mapping, water resources, fertilizer/ pesticide use, field characteristics, and commodity market conditions. Big data technology has a huge potential to refer these information and produce comprehensive insight via Geo-spatial processing, remote sensing, advance analytics algorithms, cloud resources, and advance storage systems. The paper, proposes a spark based information management system for agriculture and intend to reduce the technological gap between agro users and information. The system is proposed to collect, query, analyze, and visualize heterogeneous and distributed data including Geo-spatial data at scale using open source. The implementation is done on big data open source architectures by developing various web based analytical and visualization services for cotton crop in Gujarat state, India. The analytical results are explored through interactive maps and Restful ad-hoc APIs.},
  added-at = {2019-01-27T20:26:53.000+0100},
  author = {Shah, Purnima and Hiremath, Deepak B. and Chaudhary, Sanjay},
  biburl = {https://www.bibsonomy.org/bibtex/2f8a0bfb1a0c16f85f788498219570230/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258336},
  interhash = {be07c52c965345278339a6ec9d0fefa2},
  intrahash = {f8a0bfb1a0c16f85f788498219570230},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {3476-3481},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Towards development of spark based agricultural information system including geo-spatial data.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#ShahHC17},
  year = 2017
}

@inproceedings{sheth2014transforming,
  abstract = {Big Data has captured a lot of interest in industry, with anticipation of better decisions, efficient organizations, and many new jobs. Much of the emphasis is on the challenges of the four V's of Big Data: Volume, Variety, Velocity, and Veracity, and technologies that handle volume, including storage and computational techniques to support analysis (Hadoop, NoSQL, MapReduce, etc). However, the most important feature of Big Data, the raison d'etre, is none of these 4 V's — but value. In this talk, I will forward the concept of Smart Data that is realized by extracting value from a variety of data, and how Smart Data for growing variety (e.g., social, sensor/IoT, health care) of Big Data enable a much larger class of applications that can benefit not just large companies but each individual. This requires organized ways to harness and overcome the four V-challenges. In particular, we will need to utilize metadata, employ semantics and intelligent processing, and go beyond traditional reliance on ML and NLP.},
  added-at = {2019-01-24T21:02:27.000+0100},
  author = {Sheth, Amit P.},
  biburl = {https://www.bibsonomy.org/bibtex/2bf46450d38d3a91f9770a5643d94aa1c/vngudivada},
  booktitle = {ICDE},
  crossref = {conf/icde/2014},
  editor = {Cruz, Isabel F. and Ferrari, Elena and Tao, Yufei and Bertino, Elisa and Trajcevski, Goce},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2014.6816634},
  interhash = {e776400df52318ae78d7e4fb54eed251},
  intrahash = {bf46450d38d3a91f9770a5643d94aa1c},
  isbn = {978-1-4799-3480-5},
  keywords = {NoSQL},
  pages = 2,
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Transforming Big Data into Smart Data: Deriving value via harnessing Volume, Variety, and Velocity using semantic techniques and technologies.},
  url = {http://dblp.uni-trier.de/db/conf/icde/icde2014.html#Sheth14},
  year = 2014
}

@inproceedings{kolev2016benchmarking,
  abstract = {The CloudMdsQL polystore provides integrated access to multiple heterogeneous data stores, such as RDBMS, NoSQL or even HDFS through a big data analytics framework such as MapReduce or Spark. The CloudMdsQL language is a functional SQL-like query language with a flexible nested data model. A major capability is to exploit the full power of each of the underlying data stores by allowing native queries to be expressed as functions and involved in SQL statements. The CloudMdsQL polystore has been validated with a good number of different data stores: HDFS, key-value, document, graph, RDBMS and OLAP engine. In this paper, we introduce the benchmarking of the CloudMdsQL polystore and evaluate the performance benefits of important features enabled by the query language and engine.},
  added-at = {2019-01-27T22:03:52.000+0100},
  author = {Kolev, Boyan and Pau, Raquel and Levchenko, Oleksandra and Valduriez, Patrick and Jiménez-Peris, Ricardo and Pereira, José Orlando},
  biburl = {https://www.bibsonomy.org/bibtex/2a4454b0ce1d1ff00aac5407441712813/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2016},
  editor = {Joshi, James and Karypis, George and Liu, Ling and Hu, Xiaohua and Ak, Ronay and Xia, Yinglong and Xu, Weijia and Sato, Aki-Hiro and Rachuri, Sudarsan and Ungar, Lyle H. and Yu, Philip S. and Govindaraju, Rama and Suzumura, Toyotaro},
  ee = {https://www.wikidata.org/entity/Q58023614},
  interhash = {ea1f103925a08dcb30848269775120aa},
  intrahash = {a4454b0ce1d1ff00aac5407441712813},
  isbn = {978-1-4673-9005-7},
  keywords = {NoSQL},
  pages = {2574-2579},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Benchmarking polystores: The CloudMdsQL experience.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2016.html#KolevPLVJP16},
  year = 2016
}

@inproceedings{prabhune2016metastore,
  abstract = {In this paper, we present MetaStore, a metadata management framework for scientific data repositories. Scientific experiments are generating a deluge of data and metadata. Metadata is critical for scientific research, as it enables discovering, analysing, reusing, and sharing of scientific data. Moreover, metadata produced by scientific experiments is heterogeneous and subject to frequent changes, demanding a flexible data model. Currently, there does not exist an adaptive and a generic solution that is capable of handling heterogeneous metadata models. To address this challenge, we present MetaStore, an adaptive metadata management framework based on a NoSQL database. To handle heterogeneous metadata models and standards, the MetaStore automatically generates the necessary software code (services) and extends the functionality of the framework. To leverage the functionality of NoSQL databases, the MetaStore framework allows full-text search over metadata through automated creation of indexes. Finally, a dedicated REST service is provided for efficient harvesting (sharing) of metadata using the METS metadata standard over the OAI-PMH protocol.},
  added-at = {2019-01-27T22:08:14.000+0100},
  author = {Prabhune, Ajinkya and Ansari, Hasebullah and Keshav, Anil and Stotzka, Rainer and Gertz, Michael and Hesser, Jürgen},
  biburl = {https://www.bibsonomy.org/bibtex/2419d7eaa2fccb97e955ea9206faa8e7e/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2016},
  editor = {Joshi, James and Karypis, George and Liu, Ling and Hu, Xiaohua and Ak, Ronay and Xia, Yinglong and Xu, Weijia and Sato, Aki-Hiro and Rachuri, Sudarsan and Ungar, Lyle H. and Yu, Philip S. and Govindaraju, Rama and Suzumura, Toyotaro},
  ee = {http://dx.doi.org/10.1109/BigData.2016.7840956},
  interhash = {3f3af05f272b6a06cb8c9e18e14315cc},
  intrahash = {419d7eaa2fccb97e955ea9206faa8e7e},
  isbn = {978-1-4673-9005-7},
  keywords = {NoSQL},
  pages = {3026-3035},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {MetaStore: A metadata framework for scientific data repositories.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2016.html#PrabhuneAKSGH16},
  year = 2016
}

@inproceedings{xu2017online,
  abstract = {dbDedup is a similarity-based deduplication scheme for on-line database management systems (DBMSs). Beyond block-level compression of individual database pages or operation log (oplog) messages, as used in today's DBMSs, dbDedup uses byte-level delta encoding of individual records within the database to achieve greater savings. dbDedup's single-pass encoding method can be integrated into the storage and logging components of a DBMS to provide two benefits: (1) reduced size of data stored on disk beyond what traditional compression schemes provide, and (2) reduced amount of data transmitted over the network for replication services. To evaluate our work, we implemented dbDedup in a distributed NoSQL DBMS and analyzed its properties using four real datasets. Our results show that dbDedup achieves up to 37x reduction in the storage size and replication traffic of the database on its own and up to 61x reduction when paired with the DBMS's block-level compression. dbDedup provides both benefits with negligible effect on DBMS throughput or client latency (average and tail).},
  added-at = {2019-01-24T18:38:40.000+0100},
  author = {Xu, Lianghong and Pavlo, Andrew and Sengupta, Sudipta and Ganger, Gregory R.},
  biburl = {https://www.bibsonomy.org/bibtex/27d29aeef2dc7f0afb05500bacb8e80bb/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2017},
  editor = {Salihoglu, Semih and Zhou, Wenchao and Chirkova, Rada and Yang, Jun and Suciu, Dan},
  ee = {https://doi.org/10.1145/3035918.3035938},
  interhash = {d9cedb0b60b1ffd569c9084a3496cb18},
  intrahash = {7d29aeef2dc7f0afb05500bacb8e80bb},
  isbn = {978-1-4503-4197-4},
  keywords = {NoSQL},
  pages = {1355-1368},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Online Deduplication for Databases.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2017.html#XuPSG17},
  year = 2017
}

@inproceedings{kassela2014automated,
  abstract = {The use of cloud computing has gained extreme popularity. Through cloud platforms that provide infrastructure as a service (IaaS), users can elastically provision resources enabling automated application throttling. Usually, scaling is either manually performed or through a service that dynamically consolidates cloud resources based on a predefined policy. However, these policies are simplistic, threshold based and may not be able to capture specific application behaviors according to configuration parameters and applied workload type. In this work, we extend TIRAMOLA, a cloud-enabled framework that allows automated resizing of NoSQL clusters, in order to identify different workload types and apply the most beneficial scaling action according to user defined policies. We perform a thorough analysis of how different query types are handled by modern NoSQL systems and evaluate the performance of a NoSQL cluster of varying size, over mixed workload types and magnitudes. We utilize this knowledge to fine tune the extended TIRAMOLA's policies in order to take accurate scaling decisions. We perform an extensive experimental evaluation of workload aware and unaware versions on an HBase cluster and our analysis confirms that the former can operate successfully in any environment, behaving accordingly to any input load.},
  added-at = {2019-01-27T22:36:24.000+0100},
  author = {Kassela, Evie and Boumpouka, Christina and Konstantinou, Ioannis and Koziris, Nectarios},
  biburl = {https://www.bibsonomy.org/bibtex/25affd2237823cc94c63decf4ade7fd11/vngudivada},
  booktitle = {BigData Conference},
  crossref = {conf/bigdataconf/2014},
  editor = {Lin, Jimmy J. and Pei, Jian and Hu, Xiaohua and Chang, Wo and Nambiar, Raghunath and Aggarwal, Charu C. and Cercone, Nick and Honavar, Vasant and Huan, Jun and Mobasher, Bamshad and Pyne, Saumyadipta},
  ee = {https://doi.org/10.1109/BigData.2014.7004232},
  interhash = {64100bcc45745ccd9fc4d430036c6cf7},
  intrahash = {5affd2237823cc94c63decf4ade7fd11},
  isbn = {978-1-4799-5665-4},
  keywords = {NoSQL},
  pages = {195-200},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Automated workload-aware elasticity of NoSQL clusters in the cloud.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2014.html#KasselaBKK14},
  year = 2014
}

@inproceedings{kalakanti2015comprehensive,
  abstract = {Data historians[1] are today transitioning from their traditional role as record-keepers and planners, to tools that provide the required flexibility and responsiveness to customers' requirements in terms of the type and volume of data stored, archived and queried. Added dimensions to these requirements are the need for high performance and scalability. Businesses are realizing that traditional database management systems i.e. Relational Database Management Systems (RDBMS) might not be able to handle the deluge of industrial data they are experiencing. With the emerging NoSQL paradigm, there are different kinds of datastores which addresses specific requirements such as improved performance, reliability or user experience. Our study of two NoSQL datastores, HBase and Cassandra, provide the required insights for business units to choose the right technology for their next generation historian systems. To facilitate this study, we propose and use a benchmarking studio that has the ability to generate data for a configurable schema and workload patterns, thus enabling us to perform business use-case specific evaluation of datastores while measuring the key performance indicators. Two industrial cases in the plant automation and energy management are considered for this evaluation. Efficient data modeling techniques and batching mechanisms are defined to store streaming time-series data from sensors and devices with high throughput of approximately one million inserts/second. Mixed workload scenarios are considered to align with the requirements of next generation historians. Detailed experiments for the evaluation of concurrency and load management, scalability, consistency, BI query performance and fault-tolerance are performed on Amazon EC2 dedicated infrastructure for reproducibility and verifiability in the future.},
  added-at = {2019-01-27T22:17:03.000+0100},
  author = {Kalakanti, Arun Kumar and Sudhakaran, Vinay and Raveendran, Varsha and Menon, Nisha},
  biburl = {https://www.bibsonomy.org/bibtex/298dee17260715943c2a8a8b4552954fb/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2015},
  ee = {https://doi.org/10.1109/BigData.2015.7363952},
  interhash = {f2ff2f01770f719c6d5817eed90482e0},
  intrahash = {98dee17260715943c2a8a8b4552954fb},
  isbn = {978-1-4799-9926-2},
  keywords = {NoSQL},
  pages = {1797-1806},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {A comprehensive evaluation of NoSQL datastores in the context of historians and sensor data analysis.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2015.html#KalakantiSRM15},
  year = 2015
}

@inproceedings{kaplunovich2017cloud,
  abstract = {Machine Learning algorithms on large datasets can be executed in the Cloud. Amazon Web Services (AWS) provides over 60 different On-Demand EC2 instances [1]. The instance prices range from 0.0059(t2.nano)to14.4 (p2.16xlarge) per hour. We decided to build an automatic recommendation system to choose the best instance for a dataset and a machine learning algorithm to optimize time and money spent. After running multiple algorithms for different Big Data sets on assorted AWS instances and collecting the results in the NoSQL DynamoDB database, we have trained machine learning models to predict time and cost using assorted regression ML methods.},
  added-at = {2019-01-27T20:28:14.000+0100},
  author = {Kaplunovich, Alex and Yesha, Yelena},
  biburl = {https://www.bibsonomy.org/bibtex/21e435a7a06429275d71b61c8f4b294ec/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258340},
  interhash = {b9575ec88b9f399a61272ef7dc0fcb17},
  intrahash = {1e435a7a06429275d71b61c8f4b294ec},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {3508-3516},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Cloud big data decision support system for machine learning on AWS: Analytics of analytics.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#KaplunovichY17},
  year = 2017
}

@inproceedings{cuzzocrea2017masking,
  abstract = {This paper first presents an in-depth study of potential security vulnerabilities in MongoDB and Cassandra, two popular NoSQL databases. We provide examples of attacks. We then explore some popular data masking techniques as ways of mitigating security threats in these databases.},
  added-at = {2019-01-27T20:29:11.000+0100},
  author = {Cuzzocrea, Alfredo and Shahriar, Hossain},
  biburl = {https://www.bibsonomy.org/bibtex/2afe1b329e852899489d85a96f7431a86/vngudivada},
  booktitle = {BigData},
  crossref = {conf/bigdataconf/2017},
  editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo A. and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
  ee = {https://doi.org/10.1109/BigData.2017.8258486},
  interhash = {a8ce885916ab49ef050a8bc401cb0004},
  intrahash = {afe1b329e852899489d85a96f7431a86},
  isbn = {978-1-5386-2715-0},
  keywords = {NoSQL},
  pages = {4467-4473},
  publisher = {IEEE},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Data masking techniques for NoSQL database security: A systematic review.},
  url = {http://dblp.uni-trier.de/db/conf/bigdataconf/bigdataconf2017.html#CuzzocreaS17},
  year = 2017
}

@inproceedings{papakonstantinou2016semistructured,
  abstract = {Numerous databases promoted as SQL-on-Hadoop, NewSQL and NoSQL support semi-structured, schemaless and heterogeneous data, typically in the form of enriched JSON. They also provide corresponding query languages. In addition to these genuine JSON databases, relational databases also provide special functions and language features for the support of JSON columns, typically piggybacking on non-1NF (non first normal form) features that SQL acquired over the years. We refer to SQL databases with JSON support as SQL/JSON databases.

The evolving query languages present multiple variations: Some are superficial syntactic ones, while other ones are genuine differences in modeling, language capabilities and semantics. Incompatibility with SQL presents a learning challenge for genuine JSON databases, while the table orientation of SQL/JSON databases often leads to cumbersome syntactic/semantic structures that are contrary to the semistructured nature of JSON. Furthermore, the query languages often fall short of full-fledged semistructured query language capabilities, when compared to the yardstick set by XQuery and prior works on semistructured data (even after superficial model differences are abstracted out).

We survey features, the designers' options and differences in the approaches taken by actual systems. In particular, we first present a SQL backwards-compatible language, named SQL++, which can access both SQL and JSON data. SQL++ is expected to be supported by Couchbase's CouchDB and UCI's AsterixDB semistructured databases. Then we expand SQL++ into the Configurable SQL++, whereas multiple possible (and different) semantics are formally captured by the multiple options that the language's semantic configuration options can take. We show how appropriate setting of the configuration options morphs the Configurable SQL++ semantics into the semantics of 10 surveyed languages, hence providing a compact and formal tool to understand the essential semantic differences between different systems. We briefly comment on the utility of formally capturing semantic variations in polystore systems.

Finally we discuss the comparison with prior nested and semistructured query languages (notably OQL and XQuery) and describe a key aspect of query processor implementation: set-oriented semistructured query algebras. In particular, we transfer into the JSON era lessons from the semistructured query processing research of the 90s and 00s and combine them with insights on current JSON databases. Again, the tutorial presents the algebras' fundamentals while it abstracts away modeling differences that are not applicable.},
  added-at = {2019-01-24T18:32:55.000+0100},
  author = {Papakonstantinou, Yannis},
  biburl = {https://www.bibsonomy.org/bibtex/2055b0a9d3206f9d5e58142757a864f6b/vngudivada},
  booktitle = {SIGMOD Conference},
  crossref = {conf/sigmod/2016},
  editor = {Özcan, Fatma and Koutrika, Georgia and Madden, Sam},
  ee = {https://doi.org/10.1145/2882903.2912573},
  interhash = {0db1bad94fde8bad3568d3a68a38c5bd},
  intrahash = {055b0a9d3206f9d5e58142757a864f6b},
  isbn = {978-1-4503-3531-7},
  keywords = {NoSQL},
  pages = {2229-2233},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Semistructured Models, Queries and Algebras in the Big Data Era: Tutorial Summary.},
  url = {http://dblp.uni-trier.de/db/conf/sigmod/sigmod2016.html#Papakonstantinou16},
  year = 2016
}

@inproceedings{lamport2002paxos,
  added-at = {2019-03-11T17:50:13.000+0100},
  author = {Lamport, Leslie},
  biburl = {https://www.bibsonomy.org/bibtex/2d37ab63202a4d30cd4046a092830ed5c/vngudivada},
  booktitle = {OPODIS},
  crossref = {conf/opodis/2002},
  description = {dblp},
  editor = {Bui, Alain and Fouchal, Hacène},
  interhash = {664f3e7186cbbcad7c5afdb66540857a},
  intrahash = {d37ab63202a4d30cd4046a092830ed5c},
  isbn = {2-912590-26-4},
  keywords = {DistributedComputing Paxos},
  pages = {7-9},
  publisher = {Suger, Saint-Denis, rue Catulienne, France},
  series = {Studia Informatica Universalis},
  timestamp = {2019-03-19T15:44:14.000+0100},
  title = {Paxos Made Simple, Fast, and Byzantine},
  url = {http://dblp.uni-trier.de/db/conf/opodis/opodis02.html#Lamport02},
  volume = 3,
  year = 2002
}

@inproceedings{oki1988viewstamped,
  abstract = {One of the potential benefits of distributed systems is their use in providing highly-available services that are likely to be usable when needed. Availabilay is achieved through replication. By having inore than one copy of information, a service continues to be usable even when some copies are inaccessible, for example, because of a crash of the computer where a copy was stored. This paper presents a new replication algorithm that has desirable performance properties. Our approach is based on the primary copy technique. Computations run at a primary. which notifies its backups of what it has done. If the primary crashes, the backups are reorganized, and one of the backups becomes the new primary. Our method works in a general network with both node crashes and partitions. Replication causes little delay in user computations and little information is lost in a reorganization; we use a special kind of timestamp called a viewstamp to detect lost information.},
  added-at = {2019-03-11T17:58:36.000+0100},
  author = {Oki, Brian M. and Liskov, Barbara},
  biburl = {https://www.bibsonomy.org/bibtex/2a112d03ab9dcabc76160e22cea412713/vngudivada},
  booktitle = {PODC},
  crossref = {conf/podc/1988},
  editor = {Dolev, Danny},
  ee = {https://www.wikidata.org/entity/Q56523310},
  interhash = {df4a16f9de6d150fbb8ad8f2623f564c},
  intrahash = {a112d03ab9dcabc76160e22cea412713},
  isbn = {0-89791-277-2},
  keywords = {DistributedSystems},
  pages = {8-17},
  publisher = {ACM},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {Viewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems},
  url = {http://dblp.uni-trier.de/db/conf/podc/podc88.html#OkiL88},
  year = 1988
}

@inproceedings{hunt2010zookeeper,
  abstract = {In this paper, we describe ZooKeeper, a service for coordinating processes of distributed applications. Since ZooKeeper is part of critical infrastructure, ZooKeeper aims to provide a simple and high performance kernel for building more complex coordination primitives at the client. It incorporates elements from group messaging, shared registers, and distributed lock services in a replicated, centralized service. The interface exposed by ZooKeeper has the wait-free aspects of shared registers with an event-driven mechanism similar to cache invalidations of distributed file systems to provide a simple, yet powerful coordination service. The ZooKeeper interface enables a high-performance service implementation. In addition to the wait-free property, ZooKeeper provides a per client guarantee of FIFO execution of requests and linearizability for all requests that change the ZooKeeper state. These design decisions enable the implementation of a high performance processing pipeline with read requests being satisfied by local servers. We show for the target workloads, 2:1 to 100:1 read to write ratio, that ZooKeeper can handle tens to hundreds of thousands of transactions per second. This performance allows ZooKeeper to be used extensively by client applications.},
  added-at = {2019-03-11T18:01:54.000+0100},
  author = {Hunt, Patrick and Konar, Mahadev and Junqueira, Flavio Paiva and Reed, Benjamin},
  biburl = {https://www.bibsonomy.org/bibtex/21cb02bd47b38d0dfa1989de23b2cfa02/vngudivada},
  booktitle = {USENIX Annual Technical Conference},
  crossref = {conf/usenix/2010},
  editor = {Barham, Paul and Roscoe, Timothy},
  ee = {https://www.usenix.org/conference/usenix-atc-10/zookeeper-wait-free-coordination-internet-scale-systems},
  interhash = {d3cabe1c411f908c30fb8715abbcf60c},
  intrahash = {1cb02bd47b38d0dfa1989de23b2cfa02},
  keywords = {DistributedComputing ZooKeeper},
  publisher = {USENIX Association},
  timestamp = {2019-03-15T17:08:25.000+0100},
  title = {ZooKeeper: Wait-free Coordination for Internet-scale Systems},
  url = {http://dblp.uni-trier.de/db/conf/usenix/usenix2010.html#HuntKJR10},
  year = 2010
}

@article{boroditsky2011language,
  abstract = {The languages we speak affect our perceptions of the world},
  added-at = {2016-07-02T20:50:59.000+0200},
  author = {Boroditsky, Lera},
  biburl = {https://www.bibsonomy.org/bibtex/2b6dc75e434867fbeb43a343085c5f9a7/vngudivada},
  interhash = {90d9fdd677112f56cb08227d6fc5f617},
  intrahash = {b6dc75e434867fbeb43a343085c5f9a7},
  journal = {Scientific American},
  keywords = {Language Thought},
  number = 2,
  pages = {62 - 65},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {How language shapes thought},
  volume = 304,
  year = 2011
}

@inproceedings{du2015neuromorphic,
  abstract = {A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, ...). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality.

Within the limit of our study (current SNN and machine-learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.},
  added-at = {2016-07-02T21:02:13.000+0200},
  address = {New York, NY, USA},
  author = {Du, Zidong and Ben-Dayan Rubin, Daniel D. and Chen, Yunji and He, Liqiang and Chen, Tianshi and Zhang, Lei and Wu, Chengyong and Temam, Olivier},
  biburl = {https://www.bibsonomy.org/bibtex/2926f9be643beb4776c97ac267a333fbf/vngudivada},
  booktitle = {Proceedings of the 48th International Symposium on Microarchitecture},
  doi = {10.1145/2830772.2830789},
  interhash = {6ea2e98d288fa7811ab5a9e39979a0f6},
  intrahash = {926f9be643beb4776c97ac267a333fbf},
  keywords = {NeuromorphicComputing},
  pages = {494--507},
  publisher = {ACM},
  series = {MICRO-48},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Neuromorphic Accelerators: A Comparison Between Neuroscience and Machine-learning Approaches},
  year = 2015
}

@article{biederman2006perceptual,
  abstract = {From hand-held DVD players to hundred-inch plasma screens, much of today's technology is driven by the human appetite for pleasure through visual and auditory stimulation. What creates this appetite? Neuropsychologists have found that visual input activates receptors in the parts of the brain associated with pleasure and reward, and that the brain associates new images with old while also responding strongly to new ones. Using functional MRI imaging and other findings, they are exploring how human beings are "infovores" whose brains love to learn. Children may enjoy Sesame Street's fast pace because they get a "click of comprehension" from each brief scene.},
  added-at = {2016-07-02T20:48:37.000+0200},
  author = {Biederman, Irving and Vessel, Edward A.},
  biburl = {https://www.bibsonomy.org/bibtex/276e33998d5fa835ba0745ee7a3610a44/vngudivada},
  interhash = {110cb02539875e533847b09466c76289},
  intrahash = {76e33998d5fa835ba0745ee7a3610a44},
  journal = {American Scientist},
  keywords = {fMRI},
  number = 3,
  pages = {247-253},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Perceptual Pleasure and the Brain},
  url = {http://www.americanscientist.org/issues/feature/2006/3/perceptual-pleasure-and-the-brain},
  volume = 94,
  year = 2006
}

@book{berman2013principles,
  abstract = {Principles of Big Data helps readers avoid the common mistakes that endanger all Big Data projects. By stressing simple, fundamental concepts, this book teaches readers how to organize large volumes of complex data, and how to achieve data permanence when the content of the data is constantly changing. General methods for data verification and validation, as specifically applied to Big Data resources, are stressed throughout the book. The book demonstrates how adept analysts can find relationships among data objects held in disparate Big Data resources, when the data objects are endowed with semantic support (i.e., organized in classes of uniquely identified data objects). Readers will learn how their data can be integrated with data from other resources, and how the data extracted from Big Data resources can be used for purposes beyond those imagined by the data creators.

Learn general methods for specifying Big Data in a way that is understandable to humans and to computers
Avoid the pitfalls in Big Data design and analysis
Understand how to create and use Big Data safely and responsibly with a set of laws, regulations and ethical standards that apply to the acquisition, distribution and integration of Big Data resources},
  added-at = {2016-07-02T20:48:37.000+0200},
  address = {New York, NY},
  author = {Berman, Jules J.},
  biburl = {https://www.bibsonomy.org/bibtex/23aef1304d49634a52cd466a2e5e7c07c/vngudivada},
  interhash = {6101885cebb4b86c8eb41bc9efe4fdb5},
  intrahash = {3aef1304d49634a52cd466a2e5e7c07c},
  isbn = {978-0124045767},
  keywords = {BigData Book},
  publisher = {Morgan Kaufmann},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Principles of Big Data: Preparing, Sharing, and Analyzing Complex Information},
  year = 2013
}

@article{eliasmith2014abuse,
  abstract = {We provide an overview and comparison of several recent large-scale brain models. In addition to discussing challenges involved with building large neural models, we identify several expected benefits of pursuing such a research program. We argue that these benefits are only likely to be realized if two basic guidelines are made central to the pursuit. The first is that such models need to be intimately tied to behavior. The second is that models, and more importantly their underlying methods, should provide mechanisms for varying the level of simulated detail. Consequently, we express concerns with models that insist on a `correct' amount of detail while expecting interesting behavior to simply emerge. },
  added-at = {2016-07-02T21:12:37.000+0200},
  author = {Eliasmith, Chris and Trujillo, Oliver},
  biburl = {https://www.bibsonomy.org/bibtex/2275e6d985a09e078adc0f20145786238/vngudivada},
  doi = {http://dx.doi.org/10.1016/j.conb.2013.09.009},
  interhash = {b07f789c6021ecec474ba539766aa1e0},
  intrahash = {275e6d985a09e078adc0f20145786238},
  issn = {0959-4388},
  journal = {Current Opinion in Neurobiology},
  keywords = {BrainModel},
  note = {Theoretical and computational neuroscience},
  pages = {1 - 6},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {The use and abuse of large-scale brain models},
  volume = 25,
  year = 2014
}

@inproceedings{chang2015scene,
  abstract = {The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.},
  added-at = {2016-07-02T21:02:13.000+0200},
  address = {Stroudsburg, PA},
  author = {Chang, Angel and Monroe, Will and Savva, Manolis and Potts, Christopher and Manning, Christopher D.},
  biburl = {https://www.bibsonomy.org/bibtex/21154bd14b5dbf25f15eb818f365e9b07/vngudivada},
  booktitle = {Proceedings of the 53rd Annual Meeting of the {A}ssociation for {C}omputational {L}inguistics and the 7th International Joint Conference on Natural Language Processing},
  interhash = {a79aca4da0bad848d5e36feafece0666},
  intrahash = {1154bd14b5dbf25f15eb818f365e9b07},
  keywords = {3DSceneGeneration TextToImage},
  location = {Beijing},
  month = {July},
  pages = {53--62},
  publisher = {Association for Computational Linguistics},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Text to 3D Scene Generation with Rich Lexical Grounding},
  year = 2015
}

@article{eliasmith2012largescale,
  abstract = {A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called “Spaun”) that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.},
  added-at = {2016-07-02T21:12:37.000+0200},
  author = {Eliasmith, Chris and Stewart, Terrence C. and Choo, Xuan and Bekolay, Trevor and DeWolf, Travis and Tang, Yichuan and Rasmussen, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/2250ec09eb7a73944e576b6ae76a9709c/vngudivada},
  doi = {10.1126/science.1225266},
  interhash = {ef0127ea270cf744497e874460de3789},
  intrahash = {250ec09eb7a73944e576b6ae76a9709c},
  journal = {Science},
  keywords = {FunctionalBrainModel},
  number = 6111,
  pages = {1202--1205},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {A Large-Scale Model of the Functioning Brain},
  volume = 338,
  year = 2012
}

@inproceedings{bowman2015learning,
  abstract = {Natural logic offers a powerful relational conception of meaning that is a natural counterpart to distributed semantic representations, which have proven valuable in a wide range of sophisticated language tasks. However, it remains an open question whether it is possible to train distributed representations to support the rich, diverse logical reasoning captured by natural logic. We address this question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks. Our experiments evaluate the models’ ability to learn the basic algebra of natural logic relations from simulated data and from the WordNet noun graph. The overall positive results are promising for the future of learned distributed representations in the applied modeling of logical semantics.},
  added-at = {2016-07-02T20:50:59.000+0200},
  author = {Bowman, Samuel R. and Potts, Christopher and Manning, Christopher D.},
  biburl = {https://www.bibsonomy.org/bibtex/28d069ebf0faa166ddb5957fba31fc074/vngudivada},
  booktitle = {Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches: Papers from the 2015 {AAAI} Spring Symposium},
  interhash = {0844ba0eafd2e6667f4d8fe59cf630f9},
  intrahash = {8d069ebf0faa166ddb5957fba31fc074},
  keywords = {DistributedWordRepresentation NaturalLogicReasoning},
  location = {Stanford, CA},
  month = {March},
  pages = {10 - 13},
  publisher = {AAAI Publications},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Learning Distributed Word Representations for Natural Logic Reasoning},
  year = 2015
}

@unpublished{bowman2016unified,
  abstract = {Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they can only operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25x over other tree-structured models, and its integrated parser allows it to operate on unparsed data with little loss of accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.},
  added-at = {2016-07-02T20:48:37.000+0200},
  author = {Bowman, Samuel R. and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christoper D. and Potts, Christopher},
  biburl = {https://www.bibsonomy.org/bibtex/2c838193aad68b9ae28374ec5c1366213/vngudivada},
  interhash = {08d128056fb0765b8ae03bc4bcc00551},
  intrahash = {c838193aad68b9ae28374ec5c1366213},
  keywords = {NLP Parsing},
  note = {arXiv:1603.06021},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {A Fast Unified Model for Parsing and Sentence Understanding},
  year = 2016
}

@article{davis1993knowledge,
  abstract = {Although knowledge representation is one of the central and in some ways most familiar concepts in AI, the most fundamental question about it---What is it?---has rarely been answered directly. Numerous papers have lobbied for one or another variety of representation, other papers have argued for various properties a representation should have, while still others have focused on properties that are important to the notion of representation in general. In this paper we go back to basics to address the question directly. We believe that the answer can best be understood in terms of five important and distinctly different roles that a representation plays, each of which places different and at times conflicting demands on the properties a representation should have. We argue that keeping in mind all five of these roles provides a usefully broad perspective that sheds light on some longstanding disputes and can invigorate both research and practice in the field.},
  added-at = {2016-07-02T21:02:13.000+0200},
  author = {Davis, All and Shrobe, Howard and Szolovits, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/2429057ecbaf0db657f62b7fad1bc51e3/vngudivada},
  interhash = {015a1557e56d2a61cf89dd7972d1f494},
  intrahash = {429057ecbaf0db657f62b7fad1bc51e3},
  journal = {AI Magazine},
  keywords = {KnowledgeRepresentation},
  pages = {17--33},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {What is a knowledge representation},
  volume = 14,
  year = 1993
}

@article{balduzzi2015semantics,
  abstract = {Deep learning is currently the subject of intensive study. However, fundamental concepts such as representations are not formally defined -- researchers "know them when they see them" -- and there is no common language for describing and analyzing algorithms. This essay proposes an abstract framework that identifies the essential features of current practice and may provide a foundation for future developments.

The backbone of almost all deep learning algorithms is backpropagation, which is simply a gradient computation distributed over a neural network. The main ingredients of the framework are thus, unsurprisingly: (i) game theory, to formalize distributed optimization; and (ii) communication protocols, to track the flow of zeroth and first-order information. The framework allows natural definitions of semantics (as the meaning encoded in functions), representations (as functions whose semantics is chosen to optimized a criterion) and grammars (as communication protocols equipped with first-order convergence guarantees).

Much of the essay is spent discussing examples taken from the literature. The ultimate aim is to develop a graphical language for describing the structure of deep learning algorithms that backgrounds the details of the optimization procedure and foregrounds how the components interact. Inspiration is taken from probabilistic graphical models and factor graphs, which capture the essential structural features of multivariate distributions.},
  added-at = {2016-07-02T20:48:37.000+0200},
  author = {Balduzzi, David},
  biburl = {https://www.bibsonomy.org/bibtex/2ce08196b1b7f661ddf62191cafc88344/vngudivada},
  interhash = {c78d9addad9a62afd5d01c6f0285869b},
  intrahash = {ce08196b1b7f661ddf62191cafc88344},
  journal = {CoRR},
  keywords = {DeepLearning Representations},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Semantics, Representations and Grammars for Deep Learning},
  volume = {abs/1509.08627},
  year = 2015
}

@book{evans2006cognitive,
  abstract = {An authoritative general introduction to cognitive linguistics, this book provides up-to-date coverage of all areas of the field and sets in context recent developments within cognitive semantics and cognitive approaches to grammar.},
  added-at = {2016-07-02T21:09:35.000+0200},
  author = {Evans, Vyvyan and Green, Melanie},
  biburl = {https://www.bibsonomy.org/bibtex/274907f29d22445d3ff00ef8ef62be24d/vngudivada},
  interhash = {2b6048bead8ecbbdec369994f50f7621},
  intrahash = {74907f29d22445d3ff00ef8ef62be24d},
  keywords = {Book CognitiveLinguistics},
  publisher = {Routledge},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Cognitive Linguistics: An Introduction},
  year = 2006
}

@article{dong2016word2visualvec,
  abstract = {This paper attacks the challenging problem of cross-media retrieval. That is, given an image find the text best describing its content, or the other way around. Different from existing works, which either rely on a joint space, or a text space, we propose to perform cross-media retrieval in a visual space only. We contribute \textit{Word2VisualVec}, a deep neural network architecture that learns to predict a deep visual encoding of textual input. We discuss its architecture for prediction of CaffeNet and GoogleNet features, as well as its loss functions for learning from text/image pairs in large-scale click-through logs and image sentences. Experiments on the Clickture-Lite and Flickr8K corpora demonstrate the robustness for both Text-to-Image and Image-to-Text retrieval, outperforming the state-of-the-art on both accounts. Interestingly, an embedding in predicted visual feature space is also highly effective when searching in text only.},
  added-at = {2016-07-02T21:02:13.000+0200},
  author = {Dong, Jianfeng and Li, Xirong and Snoek, Cees G. M.},
  biburl = {https://www.bibsonomy.org/bibtex/2c952ca0d1c7d64f490c1566a197cd038/vngudivada},
  interhash = {22c1ba273d1ec42dbf69928fd7becf32},
  intrahash = {c952ca0d1c7d64f490c1566a197cd038},
  journal = {CoRR},
  keywords = {CrossMediaRetrieval VisualFeaturePrediction},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Word2VisualVec: Cross-Media Retrieval by Visual Feature Prediction},
  url = {http://arxiv.org/abs/1604.06838},
  volume = {abs/1604.06838},
  year = 2016
}

@inproceedings{bai2014bagofwords,
  abstract = {This work targets image retrieval task hold by MSR-Bing Grand Challenge. Image retrieval is considered as a challenge task because of the gap between low-level image representation and high-level textual query representation. Recently further developed deep neural network sheds light on narrowing the gap by learning high-level image representation from raw pixels. In this paper, we proposed a bag-of-words based deep neural network for image retrieval task, which learns high-level image representation and maps images into bag-of-words space. The DNN model is trained on the large scale clickthrough data, and the relevance between query and image is measured by the cosine similarity of query's bag-of-words representation and image's bag-of-words representation predicted by DNN, the visual similarity of images is computed by high-level image representation extracted via the DNN model too. Finally, PageRank algorithm is used to further improve the ranking list by considering visual similarity of images for each query. The experimental results achieved state-of-the-art performance and verified the effectiveness of our proposed method.},
  added-at = {2016-07-02T20:48:37.000+0200},
  address = {New York, NY, USA},
  author = {Bai, Yalong and Yu, Wei and Xiao, Tianjun and Xu, Chang and Yang, Kuiyuan and Ma, Wei-Ying and Zhao, Tiejun},
  biburl = {https://www.bibsonomy.org/bibtex/2318f03c4e98febb1b7f5a53d565933f6/vngudivada},
  booktitle = {Proceedings of the 22Nd ACM International Conference on Multimedia},
  interhash = {78a8df0c3228c49ebebb61d283e20076},
  intrahash = {318f03c4e98febb1b7f5a53d565933f6},
  keywords = {DeepNeuralNetworks ImageRetrieval},
  location = {Orlando, Florida, USA},
  pages = {229--232},
  publisher = {ACM},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Bag-of-Words Based Deep Neural Network for Image Retrieval},
  year = 2014
}

@inproceedings{bowman2015recursive,
  abstract = {Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, but it remains an open question whether they can learn compositional semantic grammars that support logical deduction. We address this question directly by for the first time evaluating whether each of two classes of neural model --- plain RNNs and recursive neural tensor networks (RNTNs) --- can correctly learn relationships such as entailment and contradiction between pairs of sentences, where we have generated controlled data sets of sentences from a logical grammar. Our first experiment evaluates whether these models can learn the basic algebra of logical relations involved. Our second and third experiments extend this evaluation to complex recursive structures and sentences involving quantification. We find that the plain RNN achieves only mixed results on all three experiments, whereas the stronger RNTN model generalizes well in every setting and appears capable of learning suitable representations for natural language logical inference.},
  added-at = {2016-07-02T20:50:59.000+0200},
  address = {Stroudsburg, PA},
  author = {Bowman, Samuel R. and Potts, Christopher and Manning, Christopher D.},
  biburl = {https://www.bibsonomy.org/bibtex/268253cb4d30b268a71b4f43f1f5636c0/vngudivada},
  booktitle = {Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality},
  interhash = {ddcc51938aeda7d91fb019f919963d59},
  intrahash = {68253cb4d30b268a71b4f43f1f5636c0},
  keywords = {NeuralModel RNN},
  location = {Beijing},
  publisher = {Association for Computational Linguistics},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Recursive Neural Networks Can Learn Logical Semantics},
  year = 2015
}

@article{brill1995transformationbased,
  abstract = {Recently, there has been a rebirth of empiricism in the field of natural language processing. Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge. Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modeling indirectly in large opaque tables of statistics. This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior. In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging.},
  added-at = {2016-07-02T21:02:13.000+0200},
  address = {Cambridge, MA, USA},
  author = {Brill, Eric},
  biburl = {https://www.bibsonomy.org/bibtex/21fce63ec5d7da8e45e718ff95afc92ca/vngudivada},
  interhash = {fb0a1620a006e2eaa3ad3285210ce0b2},
  intrahash = {1fce63ec5d7da8e45e718ff95afc92ca},
  journal = {Comput. Linguist.},
  keywords = {NLP POSTagging},
  month = dec,
  number = 4,
  pages = {543--565},
  publisher = {MIT Press},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Transformation-based Error-driven Learning and Natural Language Processing: A Case Study in Part-of-speech Tagging},
  volume = 21,
  year = 1995
}

@unpublished{goldberg2015fitting,
  abstract = {A recurring theme in sociological research is the tradeoff between fitting in and standing out. Prior work examining this tension tends to take either a structural or a cultural perspective. We fuse these two traditions to develop a theory of how structural and cultural embeddedness jointly relate to individual attainment within organizations. Given that organizational culture is hard to observe, we develop a novel approach to assessing individuals’ cultural fit with their colleagues based on the language expressed in internal e-mail communications. Drawing on a unique dataset that includes a corpus of 10.24 million e-mail messages exchanged over five years among 601 employees in a high-technology firm, we find that network constraint impedes, whereas cultural fit promotes, individual attainment. More importantly, we find evidence of a tradeoff between the two forms of embeddedness: cultural fit benefits individuals with low network constraint (i.e., brokers), whereas network constraint promotes attainment for people with low cultural fit.},
  added-at = {2016-07-02T21:14:36.000+0200},
  author = {Goldberg, Amir and Srivastava, Sameer B. and Manian, V. Govind and Monroe, Will and Potts, Christopher},
  biburl = {https://www.bibsonomy.org/bibtex/29a01082704e8bd3c0120724d482e1f85/vngudivada},
  interhash = {807fb293e9c83cb3116e05cd35ae17a1},
  intrahash = {9a01082704e8bd3c0120724d482e1f85},
  keywords = {EmbeddedCognition},
  note = {MS., Stanford and UC Berkeley},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Fitting In or Standing Out? {T}he Tradeoffs of Structural and Cultural Embeddedness},
  year = 2015
}

@article{aleven2002effective,
  abstract = {Recent studies have shown that self-explanation is an effective metacognitive strategy, but how can it be leveraged to improve students’ learning in actual classrooms? How do instructional treatments that emphasizes self-explanation affect students’ learning, as compared to other instructional treatments? We investigated whether self-explanation can be scaffolded effectively in a classroom environment using a Cognitive Tutor, which is intelligent instructional software that supports guided learning by doing. In two classroom experiments, we found that students who explained their steps during problem-solving practice with a Cognitive Tutor learned with greater understanding compared to students who did not explain steps. The explainers better explained their solutions steps and were more successful on transfer problems. We interpret these results as follows: By engaging in explanation, students acquired better-integrated visual and verbal declarative knowledge and acquired less shallow procedural knowledge. The research demonstrates that the benefits of self-explanation can be achieved in a relatively simple computer-based approach that scales well for classroom use.},
  added-at = {2016-07-02T20:45:48.000+0200},
  author = {Aleven, Vincent and Koedinger, Kenneth R.},
  biburl = {https://www.bibsonomy.org/bibtex/25da325024f8fb415edc159fd01e64883/vngudivada},
  interhash = {cbe030c843aadef0e2edc98355ad3d04},
  intrahash = {5da325024f8fb415edc159fd01e64883},
  journal = {Cognitive Science},
  keywords = {CognitiveTutor Metacognition},
  pages = {147 - 179},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {An effective metacognitive strategy: learning by doing and explaining with a computer-based Cognitive Tutor},
  volume = 26,
  year = 2002
}

@book{grus2015science,
  abstract = {Data science libraries, frameworks, modules, and toolkits are great for doing data science, but they’re also a good way to dive into the discipline without actually understanding data science. In this book, you’ll learn how many of the most fundamental data science tools and algorithms work by implementing them from scratch.

If you have an aptitude for mathematics and some programming skills, author Joel Grus will help you get comfortable with the math and statistics at the core of data science, and with hacking skills you need to get started as a data scientist. Today’s messy glut of data holds answers to questions no one’s even thought to ask. This book provides you with the know-how to dig those answers out.},
  added-at = {2016-07-02T21:14:36.000+0200},
  author = {Grus, Joel},
  biburl = {https://www.bibsonomy.org/bibtex/2586c25e1ad4630584da8af63b895871e/vngudivada},
  interhash = {021ad3a8e8d33b5b649751392f935ebc},
  intrahash = {586c25e1ad4630584da8af63b895871e},
  keywords = {DataScience Python},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Data Science from Scratch: First Principles with Python},
  year = 2015
}

@article{anderson1996simple,
  abstract = {In the Adaptive Character of Thought (ACT-R) theory, complex cognition arises from an interaction of procedural and declarative knowledge. Procedural knowledge is represented in units called production rules, and declarative knowledge is represented in units called chunks. The individual units are created by simple encodings of objects in the environment (chunks) or simple encodings of transformations in the environment (production rules). A great many such knowledge units underlie human cognition. From this large database, the appropriate units are selected for a particular context by activation processes that are tuned to the statistical structure of the environment. According to the ACT-R theory, the power of human cognition depends on the amount of knowledge encoded and the effective deployment of the encoded knowledge},
  added-at = {2016-07-02T20:46:54.000+0200},
  author = {Anderson, John R.},
  biburl = {https://www.bibsonomy.org/bibtex/2a313c5a0e6052eb8ba3f06eaf4299d5e/vngudivada},
  interhash = {a0ec528fb0041fde959b9cfce8712e41},
  intrahash = {a313c5a0e6052eb8ba3f06eaf4299d5e},
  journal = {American Psychologist},
  keywords = {ACT-R CognitiveComputing},
  number = 4,
  pages = {355--365},
  publisher = {The American Psychological Association},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {ACT: A Simple Theory of Complex Cognition},
  volume = 51,
  year = 1996
}

@inproceedings{duch2008cognitive,
  abstract = {Cognitive architectures play a vital role in providing blueprints for building future intelligent systems supporting a broad range of capabilities similar to those of humans. How useful are existing architectures for creating artificial general intelligence? A critical survey of the state of the art in cognitive architectures is presented providing a useful insight into the possible frameworks for general intelligence. Grand challenges and an outline of the most promising future directions are described.},
  added-at = {2016-07-02T21:09:35.000+0200},
  address = {Amsterdam, The Netherlands, The Netherlands},
  author = {Duch, W\lodzis\law and Oentaryo, Richard J. and Pasquier, Michel},
  biburl = {https://www.bibsonomy.org/bibtex/23c0d7bed09183f24d4577aa3bcae1020/vngudivada},
  booktitle = {Proceedings of the 2008 Conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference},
  interhash = {01c467881642f45842a49ef4c81135c3},
  intrahash = {3c0d7bed09183f24d4577aa3bcae1020},
  keywords = {ArtificialGeneralIntelligence CognitiveArchitecture IntelligentAgent NeurocognitiveModel},
  pages = {122--136},
  publisher = {IOS Press},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Cognitive Architectures: Where Do We Go from Here?},
  year = 2008
}

@article{gruber1993translation,
  abstract = {To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse — definitions of classes, relations, functions, and other objects — is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations.

We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms.},
  added-at = {2016-07-02T21:14:36.000+0200},
  address = {London, UK, UK},
  author = {Gruber, Thomas R.},
  biburl = {https://www.bibsonomy.org/bibtex/216c1b1110604d6e3f7207d957518478b/vngudivada},
  doi = {10.1006/knac.1993.1008},
  interhash = {232576339f9eecc6915dec6a2ee77150},
  intrahash = {16c1b1110604d6e3f7207d957518478b},
  journal = {Knowl. Acquis.},
  keywords = {OntologySpecification},
  month = jun,
  number = 2,
  pages = {199--220},
  publisher = {Academic Press Ltd.},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {A Translation Approach to Portable Ontology Specifications},
  volume = 5,
  year = 1993
}

@article{garrett2014analytics,
  abstract = {The days of the lone astronomer with his optical telescope and photographic plates are long gone: Astronomy in 2025 will not only be multi-wavelength, but multi-messenger, and dominated by huge data sets and matching data rates. Catalogues listing detailed properties of billions of objects will in themselves require a new industrial-scale approach to scientific discovery, requiring the latest techniques of advanced data analytics and an early engagement with the first generation of cognitive computing systems. Astronomers have the opportunity to be early adopters of these new technologies and methodologies - the impact can be profound and highly beneficial to effecting rapid progress in the field. Areas such as SETI research might favourably benefit from cognitive intelligence that does not rely on human bias and preconceptions.},
  added-at = {2016-07-02T21:14:36.000+0200},
  author = {{Garrett}, M. A.},
  biburl = {https://www.bibsonomy.org/bibtex/20c3b0670fc54dae03caf632a1b10fd59/vngudivada},
  doi = {10.1088/1757-899X/67/1/012017},
  interhash = {62752f83833be7c175a15d05cf757636},
  intrahash = {0c3b0670fc54dae03caf632a1b10fd59},
  journal = {Materials Science and Engineering Conference Series},
  keywords = {BigDataAnalytics CognitiveComputing},
  month = oct,
  number = 1,
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Big Data analytics and cognitive computing - future opportunities for astronomical research},
  volume = 67,
  year = 2014
}

@article{anderson2004integrated,
  abstract = {Adaptive control of thought–rational (ACT–R; J. R. Anderson & C. Lebiere, 1998) has evolved into a theory that consists of multiple modules but also explains how these modules are integrated to produce coherent cognition. The perceptual-motor modules, the goal module, and the declarative memory module are presented as examples of specialized systems in ACT–R. These modules are associated with distinct cortical regions. These modules place chunks in buffers where they can be detected by a production system that responds to patterns of information in the buffers. At any point in time, a single production rule is selected to respond to the current pattern. Subsymbolic processes serve to guide the selection of rules to fire as well as the internal operations of some modules. Much of learning involves tuning of these subsymbolic processes. A number of simple and complex empirical examples are described to illustrate how these modules function singly and in concert.},
  added-at = {2016-07-02T20:46:54.000+0200},
  author = {Anderson, John R and Bothell, Daniel and Byrne, Michael D and Douglass, Scott and Lebiere, Christian and Qin, Yulin},
  biburl = {https://www.bibsonomy.org/bibtex/2bcd2d0408ba293bc85ef3cff747206dd/vngudivada},
  interhash = {60ce08db601a90230688dd0349a0825d},
  intrahash = {bcd2d0408ba293bc85ef3cff747206dd},
  journal = {Psychological Review},
  keywords = {CognitiveScience},
  number = 4,
  pages = 1036,
  publisher = {American Psychological Association},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {An integrated theory of the mind},
  volume = 111,
  year = 2004
}

@book{card1983psychology,
  abstract = {Defines the psychology of human-computer interaction, showing how to span the gap between science & application. Studies the behavior of users in interacting with computer systems.},
  added-at = {2016-07-02T21:02:13.000+0200},
  address = {Hillsdale, NJ, USA},
  author = {Card, Stuart K. and Newell, Allen and Moran, Thomas P.},
  biburl = {https://www.bibsonomy.org/bibtex/202cc5b058b05c97bf3619193fe9e02ff/vngudivada},
  interhash = {7a758f998778891d16c8e3343590d4f6},
  intrahash = {02cc5b058b05c97bf3619193fe9e02ff},
  keywords = {HCI},
  publisher = {L. Erlbaum Associates Inc.},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {The Psychology of Human-Computer Interaction},
  year = 1983
}

@article{andor2016globally,
  abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.},
  added-at = {2016-07-02T20:48:37.000+0200},
  author = {Andor, Daniel and Alberti, Chris and Weiss, David and Severyn, Aliaksei and Presta, Alessandro and Ganchev, Kuzman and Petrov, Slav and Collins, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/21fc5b4ae55941cbc59e5827aaaab8a91/vngudivada},
  interhash = {db3b21284fce9df6e23b83c53c0fcb4b},
  intrahash = {1fc5b4ae55941cbc59e5827aaaab8a91},
  journal = {CoRR},
  keywords = {NeuralNetwork POS},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Globally Normalized Transition-Based Neural Networks},
  url = {http://arxiv.org/abs/1603.06042},
  volume = {abs/1603.06042},
  year = 2016
}

@article{armbrust2010cloud,
  abstract = {CLOUD COMPUTING, the long-held dream of computing as a utility, has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased. Developers with innovative ideas for new Internet services no longer require the large capital outlays in hardware to deploy their service or the human expense to operate it. They need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1,000 servers for one hour costs no more than using one server for 1,000.},
  added-at = {2016-07-02T20:48:37.000+0200},
  address = {New York, NY, USA},
  author = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
  biburl = {https://www.bibsonomy.org/bibtex/2b98eaabe0628d9fa06f6ad0dd9bd48a4/vngudivada},
  doi = {10.1145/1721654.1721672},
  interhash = {61cd2c513f54f1e083f1e2e082baabb0},
  intrahash = {b98eaabe0628d9fa06f6ad0dd9bd48a4},
  journal = {Commun. ACM},
  keywords = {BigData CloudComputing},
  month = apr,
  number = 4,
  numpages = {9},
  pages = {50--58},
  publisher = {ACM},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {A View of Cloud Computing},
  volume = 53,
  year = 2010
}

@book{frankish2012cambridge,
  abstract = {Cognitive science is a cross-disciplinary enterprise devoted to understanding the nature of the mind. In recent years, investigators in philosophy, psychology, the neurosciences, artificial intelligence, and a host of other disciplines have come to appreciate how much they can learn from one another about the various dimensions of cognition. The result has been the emergence of one of the most exciting and fruitful areas of inter-disciplinary research in the history of science.

This volume of original essays surveys foundational, theoretical, and philosophical issues across the discipline, and introduces the foundations of cognitive science, the principal areas of research, and the major research programs. With a focus on broad philosophical themes rather than detailed technical issues, the volume will be valuable not only to cognitive scientists and philosophers of cognitive science, but also to those in other disciplines looking for an authoritative and up-to-date introduction to the field.

Each chapter is a specially commissioned survey article from a leading writer in the area – either a philosopher of cognitive science or a scientist with strong theoretical interests. The approach is thematic rather than historical, and the chapters are primarily survey pieces, though critical assessment is also included, where appropriate. The volume will be suitable for upper-level undergraduates, graduate students, and scholars, with a particular emphasis on the second group. No extensive background knowledge is assumed, either in philosophy or the primary subject areas themselves.

A companion Handbook of Artificial Intelligence has been prepared by the same editors, and the two volumes have been conceived as a pair.
Features
• Technical jargon is avoided as far as possible and no significant background knowledge of the field is assumed
• Includes supporting material, such as annotated chapter-specific further reading sections and an extensive glossary
• Concise, authoritative and up-to-date coverage of a rapidly developing and expanding field

Introduction, Keith Frankish and William M. Ramsey
Part I: Foundations
1. History and core themes, Adele Abrahamsen and William Bechtel
2. The representational theory of mind, Barbara Von Eckardt
3. Cognitive architectures, Paul Thagard
Part II: Aspects of cognition
4. Perception, Casey O’Callaghan
5. Action, Elisabeth Pacherie
6. Human learning and memory, Charan Ranganath, Laura A. Libby, and Ling Wong
7. Reasoning and decision making, Mike Oaksford, Nick Chater, and Neil Stewart
8. Concepts, Gregory L. Murphy and Aaron B. Hoffman
9. Language, Ray Jackendoff
10. Emotion, Jesse Prinz
11. Consciousness, William G. Lycan
Part III: Research programs
12. Cognitive neuroscience, Dominic Standage and Thomas Trappenberg
13. Evolutionary psychology, H. Clark Barrett
14. Embodied, embedded, and extended cognition, Andy Clark
15. Animal cognition, Sara J. Shettleworth
Glossary
Index
},
  added-at = {2016-07-02T21:12:37.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2c83795509b9ca8b938ab03112fca6388/vngudivada},
  editor = {Frankish, Keith and Ramsey, William M.},
  interhash = {c6d192b45ee14bd42cb55ce8d942fd7c},
  intrahash = {c83795509b9ca8b938ab03112fca6388},
  keywords = {CognitiveScience},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {The Cambridge Handbook of Cognitive Science},
  year = 2012
}

@inproceedings{george2005hierarchical,
  abstract = {We describe a hierarchical model of invariant visual pattern recognition in the visual cortex. In this model, the knowledge of how patterns change when objects move is learned and encapsulated in terms of high probability sequences at each level of the hierarchy. Configuration of object parts is captured by the patterns of coincident high probability sequences. This knowledge is then encoded in a highly efficient Bayesian network structure. The learning algorithm uses a temporal stability criterion to discover object concepts and movement patterns. We show that the architecture and algorithms are biologically plausible. The large scale architecture of the system matches the large scale organization of the cortex and the micro-circuits derived from the local computations match the anatomical data on cortical circuits. The system exhibits invariance across a wide variety of transformations and is robust in the presence of noise. Moreover, the model also offers alternative explanations for various known cortical phenomena.},
  added-at = {2016-07-02T21:14:36.000+0200},
  author = {George, D. and Hawkins, J.},
  biburl = {https://www.bibsonomy.org/bibtex/26ed97789949dc281f9b80493b2a8e995/vngudivada},
  booktitle = {Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.},
  interhash = {c4a4c93aa59a047be5c4cc4591285a6b},
  intrahash = {6ed97789949dc281f9b80493b2a8e995},
  keywords = {BayesianModel VisualCortex},
  month = {July},
  pages = {1812-1817 vol. 3},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {A hierarchical Bayesian model of invariant pattern recognition in the visual cortex},
  volume = 3,
  year = 2005
}

@inproceedings{bowman2015learning,
  abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.

Natural Language Inference (SNLI) corpus, a collection of sentence pairs labeled for entailment, contradiction, and semantic independence.

We use this corpus to evaluate a variety of models for natural language inference, including rule-based systems, simple linear classifiers, and neural network-based models. We find that two models achieve comparable performance: a feature-rich classifier model and a neural network model centered around a Long Short-Term Memory network (LSTM).

Three models: neural network classifier based on one-layer model in Bowman
et al. 2015, and two simple sequence embedding models: a plain RNN and an LSTM RNN.},
  added-at = {2016-07-02T20:50:59.000+0200},
  address = {Stroudsburg, PA},
  author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  biburl = {https://www.bibsonomy.org/bibtex/2061e91568ca3dfb2b00d78d89e8fc193/vngudivada},
  booktitle = {Proceedings of the 2015 Conference on {E}mpirical {M}ethods in {N}atural {L}anguage {P}rocessing},
  interhash = {b224bdea311c6b412aa52d86f23053ba},
  intrahash = {061e91568ca3dfb2b00d78d89e8fc193},
  keywords = {NaturalLanguageInference},
  location = {Lisboa, Portugal},
  pages = {632--642},
  publisher = {Association for Computational Linguistics},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Learning Natural Language Inference from a Large Annotated Corpus},
  year = 2015
}

@inproceedings{bowman2015treestructured,
  abstract = {Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find an LSTM-based sequence model can indeed learn to exploit the underlying tree structure. However, its performance consistently lags behind that of tree models, even on large training sets, suggesting that tree-structured models are more effective at exploiting recursive structure.},
  added-at = {2016-07-02T20:50:59.000+0200},
  author = {Bowman, Samuel R. and Manning, Christopher D. and Potts, Christopher},
  biburl = {https://www.bibsonomy.org/bibtex/2540a913c32828cf62789070c26585c01/vngudivada},
  booktitle = {Proceedings of Proceedings of the {NIPS} 2015 Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches},
  interhash = {66bff5f6fc4fd15ab4452b608a2660ca},
  intrahash = {540a913c32828cf62789070c26585c01},
  keywords = {NeuralModel},
  location = {Montreal},
  month = {December},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Tree-Structured Composition in Neural Networks without Tree-Structured Architectures},
  year = 2015
}

@article{franklin2014systemslevel,
  abstract = {We describe a cognitive architecture learning intelligent distribution agent (LIDA) that affords attention, action selection and human-like learning intended for use in controlling cognitive agents that replicate human experiments as well as performing real-world tasks. LIDA combines sophisticated action selection, motivation via emotions, a centrally important attention mechanism, and multimodal instructionalist and selectionist learning. Empirically grounded in cognitive science and cognitive neuroscience, the LIDA architecture employs a variety of modules and processes, each with its own effective representations and algorithms. LIDA has much to say about motivation, emotion, attention, and autonomous learning in cognitive agents. In this paper, we summarize the LIDA model together with its resulting agent architecture, describe its computational implementation, and discuss results of simulations that replicate known experimental data. We also discuss some of LIDA's conceptual modules, propose nonlinear dynamics as a bridge between LIDA's modules and processes and the underlying neuroscience, and point out some of the differences between LIDA and other cognitive architectures. Finally, we discuss how LIDA addresses some of the open issues in cognitive architecture research.},
  added-at = {2016-07-02T21:14:36.000+0200},
  author = {Franklin, S. and Madl, T. and D'Mello, S. and Snaider, J.},
  biburl = {https://www.bibsonomy.org/bibtex/29a012d5239841a3dc9ddee1d0a1d4cbb/vngudivada},
  interhash = {ca8e1e5a59a4f6fa3ca8f9a3ac0bbef3},
  intrahash = {9a012d5239841a3dc9ddee1d0a1d4cbb},
  issn = {1943-0604},
  journal = {IEEE Transactions on Autonomous Mental Development},
  keywords = {CognitiveArchitecture LIDA},
  month = {March},
  number = 1,
  pages = {19-41},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {LIDA: A Systems-level Architecture for Cognition, Emotion, and Learning},
  volume = 6,
  year = 2014
}

@article{evans2012cognitive,
  abstract = {Cognitive linguistics is one of the fastest growing and influential perspectives on the nature of language, the mind, and their relationship with sociophysical (embodied) experience. It is a broad theoretical and methodological enterprise, rather than a single, closely articulated theory. Its primary commitments are outlined. These are the Cognitive Commitment—a commitment to providing a characterization of language that accords with what is known about the mind and brain from other disciplines—and the Generalization Commitment—which represents a dedication to characterizing general principles that apply to all aspects of human language. The article also outlines the assumptions and worldview which arises from these commitments, as represented in the work of leading cognitive linguists.},
  added-at = {2016-07-02T21:09:35.000+0200},
  author = {Evans, Vyvyan},
  biburl = {https://www.bibsonomy.org/bibtex/2913abc850b489657b5f51c64bd8a0f5a/vngudivada},
  doi = {10.1002/wcs.1163},
  interhash = {0e470458501a966ba9ebd48d23105bf0},
  intrahash = {913abc850b489657b5f51c64bd8a0f5a},
  journal = {Wiley Interdisciplinary Reviews:Cognitive Science},
  keywords = {CognitiveLinguistics},
  number = 2,
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Cognitive Linguistics},
  volume = 3,
  year = 2012
}

@article{anderson1995cognitive,
  abstract = {This article reviews the 10-year history of tutor development based on the advanced computer tutoring theory (J. R. Anderson, 1983, 1993). We developed production system models in ACT of how students solved problems in LISP, geometry, and algebra. Computer tutors were developed around these cognitive models. Construction of these tutors was guided by a set of eight principles loosely based on the ACT theory. Early evaluations of these tutors usually, but not always, showed significant achievement gains. Best case evaluations showed that students could achieve at least the same level of proficiency as conventional instruction in one third of the time. Empirical studies showed that students were learning skills in production-rule units and that the best tutorial interaction style was one in which the tutor provides immediate feedback, consisting of short and directed error messages. The tutors appear to work better if they present themselves to students as nonhuman tools to assist learning rather than as emulations of human tutors. Students working with these tutors display transfer to other environments to the degree that they can map the tutor environment into the test environment. These experiences have coalesced into a new system for developing and deploying tutors. This system involves selecting a problem-solving interface, constructing a curriculum under the guidance of a domain expert, designing a cognitive model for solving problems in that environment, building instruction around the productions in that model, and deploying the tutor in the classroom. New tutors are being built in this system to achieve the National Council of Teachers of Mathematics (NCTM) standards for high-school mathematics in an urban setting.},
  added-at = {2016-07-02T20:46:54.000+0200},
  author = {Anderson, J. R. and Corbett, A. T. and Koedinger, K. R. and Pelletier, R.},
  biburl = {https://www.bibsonomy.org/bibtex/2b64f9683300f3886f81bd746aa7cd874/vngudivada},
  interhash = {4d16f8cd577b01e025ff416cb93ab1d1},
  intrahash = {b64f9683300f3886f81bd746aa7cd874},
  journal = {The Journal of the Learning Sciences},
  keywords = {CognitiveScience CognitiveTutor},
  number = 2,
  pages = {167 - 207},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Cognitive Tutors: Lessons Learned},
  volume = 4,
  year = 1995
}

@inproceedings{friedlander2008theory,
  abstract = {Every agent aspiring to human level intelligence, every AGI agent, must be capable of a theory of mind. That is, it must be able to attribute mental states, including intentions, to other agents, and must use such attributions in its action selection process. The LIDA conceptual and computational model of cognition offers an explanation of how theory of mind is accomplished in humans and some other animals, and suggests how this explanation could be implemented computationally. Here we describe how the LIDA version of theory of mind is accomplished, and illustrate it with an example taken from an experiment with monkeys, chosen for simplicity.},
  added-at = {2016-07-02T21:14:36.000+0200},
  address = {Amsterdam, The Netherlands},
  author = {Friedlander, David and Franklin, Stan},
  biburl = {https://www.bibsonomy.org/bibtex/2ddb28c92ce720064bbb94d587c98ef4b/vngudivada},
  booktitle = {Proceedings of the 2008 Conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference},
  interhash = {339b8214ff413aed19d98c881c6795a3},
  intrahash = {ddb28c92ce720064bbb94d587c98ef4b},
  keywords = {ArtificialIntelligence CognitiveArchitecture CognitiveScience GlobalWorkspaceTheory LIDA},
  pages = {137--148},
  publisher = {IOS Press},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {LIDA and a Theory of Mind},
  year = 2008
}

@article{ferrucci2010building,
  abstract = {IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV Quiz show, Jeopardy! The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy! Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After 3 years of intense research and development by a core team of about 20 researches, Watson is performing at human expert-levels in terms of precision, confidence and speed at the Jeopardy! Quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that may be used as a foundation for combining, deploying, evaluating and advancing a wide range of algorithmic techniques to rapidly advance the field of QA.},
  added-at = {2016-07-02T21:12:37.000+0200},
  author = {Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya A. and Lally, Adam and Murdock, J. William and Nyberg, Eric and Prager, John and Schlaefer, Nico and Welty, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/2460a61110ebf98c22be4669f6b8fe7fe/vngudivada},
  doi = {http://dx.doi.org/10.1609/aimag.v31i3.2303},
  interhash = {c3fa1d7b2cfb8fc1742b8a4ec0151392},
  intrahash = {460a61110ebf98c22be4669f6b8fe7fe},
  journal = {AI Magazine},
  keywords = {DeepQA IBMWatson},
  number = 3,
  pages = {59 - 79},
  publisher = {Association for the Advancement of Artificial Intelligence},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Building Watson: An Overview of the DeepQA Project},
  volume = 31,
  year = 2010
}

@book{anderson1983architecture,
  abstract = {The Architecture of Cognition is a classic work that remains relevant to theory and research in cognitive science. The new version of Anderson's theory of cognitive architecture -- Adaptive Control of Thought (ACT*) -- is a theory of the basic principles of operation built into the cognitive system and is the main focus of the book.},
  added-at = {2016-07-02T20:46:54.000+0200},
  author = {Anderson, John R.},
  biburl = {https://www.bibsonomy.org/bibtex/2b63563a7d492a730d0920b0448f200ac/vngudivada},
  interhash = {de9d88bd42508622db1e137b99693b26},
  intrahash = {b63563a7d492a730d0920b0448f200ac},
  keywords = {Book CognitiveScience},
  publisher = {Psychology Press},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {The Architecture of Cognition},
  year = 1983
}

@incollection{abrahamsen2012history,
  added-at = {2016-07-02T18:44:54.000+0200},
  author = {Abrahamsen, Adele and Bechtel, William},
  biburl = {https://www.bibsonomy.org/bibtex/24d53ed0dd73b0182889e5b85abb516d9/vngudivada},
  booktitle = {The Cambridge Handbook of Cognitive Science},
  editor = {Frankish, Keith and Ramsey, William M.},
  interhash = {94e3b9d20395cede78712156d7674f74},
  intrahash = {4d53ed0dd73b0182889e5b85abb516d9},
  keywords = {CognitiveScience},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {History and core themes},
  year = 2012
}

@article{demarneffe2015modeling,
  abstract = {A discourse typically involves numerous entities, but few are mentioned more than once. Distinguishing those that die out after just one mention (singleton) from those that lead longer lives (coreferent) would dramatically simplify the hypothesis space for coreference resolution models, leading to increased performance. To realize these gains, we build a classifier for predicting the singleton/coreferent distinction. The model's feature representations synthesize linguistic insights about the factors affecting discourse entity lifespans (especially negation, modality, and attitude predication) with existing results about the benefits of "surface" (part-of-speech and n-gram-based) features for coreference resolution. The model is effective in its own right, and the feature representations help to identify the anchor phrases in bridging anaphora as well. Furthermore, incorporating the model into two very different state-of-the-art coreference resolution systems, one rule-based and the other learning-based, yields significant performance improvements.},
  added-at = {2016-07-02T21:02:13.000+0200},
  author = {de Marneffe, Marie-Catherine and Recasens, Marta and Potts, Christopher},
  biburl = {https://www.bibsonomy.org/bibtex/262f3cc2a34a4628b5195406a29458a44/vngudivada},
  interhash = {eef30ad8847bd67e8160de36d729650e},
  intrahash = {62f3cc2a34a4628b5195406a29458a44},
  journal = {Journal of Artificial Intelligence Research},
  keywords = {CoreferenceResolution},
  pages = {445--475},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Modeling the Lifespan of Discourse Entities with Application to Coreference Resolution},
  volume = 52,
  year = 2015
}

@article{anderson1989skill,
  abstract = {An analysis of student learning with the LISP tutor indicates that while LISP is complex, learning it is simple. The key to factoring out the complexity of LISP is to monitor the learning of the 500 productions in the LISP tutor which describe the programming skill. The learning of these productions follows the power-law learning curve typical of skill acquisition. There is transfer from other programming experience to the extent that this programming experience involves the same productions. Subjects appear to differ only on the general dimensions of how well they acquire the productions and how well they retain the productions. Instructional manipulations such as remediation, content of feedback, and timing of feedback are effective to the extent they give students more practice programming, and explain to students why correct solutions work.},
  added-at = {2016-07-02T20:46:54.000+0200},
  author = {Anderson, John R. and Conrad, Frederick G. and Corbett, Albert T.},
  biburl = {https://www.bibsonomy.org/bibtex/23c09bd8664791920ff7000e2bb6e9309/vngudivada},
  interhash = {5420f081a34fcb1db876fa0ace934756},
  intrahash = {3c09bd8664791920ff7000e2bb6e9309},
  journal = {Cognitive Science},
  keywords = {CognitiveModel LispTutor},
  number = 4,
  pages = {467--505},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Skill Acquisition and the {LISP} Tutor},
  volume = 13,
  year = 1989
}

@article{bekolay2014nengo,
  abstract = {Neuroscience currently lacks a comprehensive theory of how cognitive processes can be implemented in a biological substrate. The Neural Engineering Framework (NEF) proposes one such theory, but has not yet gathered significant empirical support, partly due to the technical challenge of building and simulating large-scale models with the NEF. Nengo is a software tool that can be used to build and simulate large-scale models based on the NEF; currently, it is the primary resource for both teaching how the NEF is used, and for doing research that generates specific NEF models to explain experimental data. Nengo 1.4, which was implemented in Java, was used to create Spaun, the world's largest functional brain model (Eliasmith et al., 2012). Simulating Spaun highlighted limitations in Nengo 1.4's ability to support model construction with simple syntax, to simulate large models quickly, and to collect large amounts of data for subsequent analysis. This paper describes Nengo 2.0, which is implemented in Python and overcomes these limitations. It uses simple and extendable syntax, simulates a benchmark model on the scale of Spaun 50 times faster than Nengo 1.4, and has a flexible mechanism for collecting simulation results.},
  added-at = {2016-07-02T20:48:37.000+0200},
  author = {Bekolay, Trevor and Bergstra, James and Hunsberger, Eric and DeWolf, Travis and Stewart, Terrence C and Rasmussen, Daniel and Choo, Xuan and Voelker, Aaron and Eliasmith, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/2b318ef14e563bf7ff692984ff747c8b8/vngudivada},
  interhash = {d53bf2370c29618f24d037fc0714004a},
  intrahash = {b318ef14e563bf7ff692984ff747c8b8},
  journal = {Frontiers in Neuroinformatics},
  keywords = {FunctionalBrainModel},
  number = 48,
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Nengo: A Python tool for building large-scale functional brain models},
  volume = 7,
  year = 2014
}

@article{goldberg2015primer,
  abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
  added-at = {2016-07-02T21:14:36.000+0200},
  author = {Goldberg, Yoav},
  biburl = {https://www.bibsonomy.org/bibtex/2f4a34e71994dbd5db16a33cd4bdcfa5c/vngudivada},
  interhash = {0d4411af22df74aa8795081804889c29},
  intrahash = {f4a34e71994dbd5db16a33cd4bdcfa5c},
  journal = {CoRR},
  keywords = {NLP NeuralModel},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {A Primer on Neural Network Models for Natural Language Processing},
  volume = {abs/1510.00726},
  year = 2015
}

@book{friedenberg2015cognitive,
  abstract = {Throughout history, different fields of inquiry have attempted to understand the great mystery of mind and answer questions like: What is the mind? How do we see, think, and remember? Can we create machines that are conscious and capable of self-awareness? This books examines these questions and many more. Focusing on the approach of a particular cognitive science field in each chapter, the authors describe its methodology, theoretical perspective, and findings and then offer a critical evaluation of the field.},
  added-at = {2016-07-02T21:14:36.000+0200},
  author = {Friedenberg, Jay D. and Silverman, Gordon W.},
  biburl = {https://www.bibsonomy.org/bibtex/2edcbcb20af1523a7cc126ffd392cb43b/vngudivada},
  edition = {Third},
  interhash = {083b88c809b9eb7d03e80c0bc637cb46},
  intrahash = {edcbcb20af1523a7cc126ffd392cb43b},
  keywords = {CognitiveScience},
  note = {https://study.sagepub.com/friedenberg3e, https://us.sagepub.com/en-us/nam/cognitive-science/book242501},
  publisher = {SAGE Publications},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Cognitive Science: An Introduction to the Study of Mind},
  year = 2015
}

@article{brewster2004knowledge,
  abstract = {Recently, there?s been an explosion of interest in ontologies as artifacts to represent human knowledge and as critical components in knowledge management, the Semantic Web, business-to-business applications, and several other application areas. Various research communities commonly assume that ontologies are the appropriate modeling structure for representing knowledge. However, little discussion has occurred regarding the actual range of knowledge an ontology can successfully represent. This installment of Trends and Controversies brings together several practitioners to debate this issue.},
  added-at = {2016-07-02T21:02:13.000+0200},
  author = {Brewster, C. and O'Hara, K.},
  biburl = {https://www.bibsonomy.org/bibtex/2dd99b1e6e47a320db90ed7ca9f9ef299/vngudivada},
  doi = {10.1109/MIS.2004.1265889},
  interhash = {d0421a4c4e4a97b986ff798bcccf0b7b},
  intrahash = {dd99b1e6e47a320db90ed7ca9f9ef299},
  journal = {IEEE Intelligent Systems},
  keywords = {KnowledgeRepresentation Ontology},
  month = jan,
  number = 1,
  pages = {72-81},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Knowledge representation with ontologies: the present and future},
  volume = 19,
  year = 2004
}

@incollection{flusberg2014connectionism,
  abstract = {Connectionism is a computational modeling framework inspired by the principles of information processing that characterize biological neural systems, which rely on collections of simple processing units linked together into networks. These units communicate in parallel via connections of varying strength that can be modified by experience. Connectionist networks have a wide range of theoretical and practical applications because they exhibit sophisticated, flexible, and context-sensitive behavior that mirrors human cognitive performance in many domains, from perception to language processing. By emphasizing the commonalities underlying various cognitive abilities, connectionism considers how a basic set of computational principles might give rise to many different forms of complex behavior. Thus connectionism supports a novel way of thinking about the nature and origins of mental life, as the emergent consequence of a system based around principles of parallel processing, distributed representation, and statistical learning that interacts with its environment over the course of development.},
  added-at = {2016-07-02T21:12:37.000+0200},
  author = {Flusberg, Stephen J. and McClelland, James L.},
  biburl = {https://www.bibsonomy.org/bibtex/2d13f88689e3c92c57944c887c8f72e58/vngudivada},
  booktitle = {The Oxford Handbook of Cognitive Science, Volume 1},
  doi = {DOI: 10.1093/oxfordhb/9780199842193.013.5},
  editor = {Chipman, Susan},
  interhash = {701df06fd83a4dd13877045ba3fa6190},
  intrahash = {d13f88689e3c92c57944c887c8f72e58},
  keywords = {CognitiveScience Connectionism},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Connectionism and the Emergence of Mind},
  year = 2014
}

@book{chipman2015oxford,
  abstract = {The Oxford Handbook of Cognitive Science emphasizes the research and theory most central to modern cognitive science. Part I of the volume covers computational theories of human cognitive architecture aiming for broad coverage; topics include ACT-R, the EPIC cognitive architecture, the CAPS family of cognitive architectures, connectionism and emergence of mind, the Lebra cognitive architecture, and the CLARION cognitive architecture. The chapters of Part II address complex cognition such as problem-solving and decision-making as they have been studied with both experimental methods and formal modeling approaches. Conceptual relationships, the concept of psychological time, spatial cognition, causal relations, cognitive science approaches to memory and learning, and the nature of multitasking as revealed through brain imaging are additional topics covered. Part III on the cognitive science of language complements earlier Oxford handbooks of psycholinguistics and linguistics with chapters on recent developments; among topics covered are cognitive linguistics, WordNet, VerbNet, and natural language processing. Additional facets of cognitive science are briefly discussed in the handbook's introductory chapter with references to key readings.},
  added-at = {2016-07-02T21:02:13.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2440add936b8c30de25d77545fcee1bda/vngudivada},
  editor = {Chipman, Susan},
  interhash = {68ba63547b0e6cfb60f3dd4c7ad9b432},
  intrahash = {440add936b8c30de25d77545fcee1bda},
  keywords = {Book CognitiveScience},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {The Oxford Handbook of Cognitive Science, Volume 1},
  year = 2015
}

@article{dhar2013science,
  abstract = {Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.},
  added-at = {2016-07-02T21:02:13.000+0200},
  address = {New York, USA},
  author = {Dhar, Vasant},
  biburl = {https://www.bibsonomy.org/bibtex/292d04cdb0b74e74d17cecaec51871275/vngudivada},
  interhash = {529f5cbf155bbb189778365b3f783491},
  intrahash = {92d04cdb0b74e74d17cecaec51871275},
  journal = {Commun. ACM},
  keywords = {DataScience},
  month = dec,
  number = 12,
  pages = {64--73},
  publisher = {ACM},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Data Science and Prediction},
  volume = 56,
  year = 2013
}

@inproceedings{chen2015technologydesign,
  abstract = {Technology-design co-optimization methodologies of the resistive cross-point array are proposed for implementing the machine learning algorithms on a chip. A novel read and write scheme is designed to accelerate the training process, which realizes fully parallel operations of the weighted sum and the weight update. Furthermore, technology and design parameters of the resistive cross-point array are co-optimized to enhance the learning accuracy, latency and energy consumption, etc. In contrast to the conventional memory design, a set of reverse scaling rules is proposed on the resistive cross-point array to achieve high learning accuracy. These include 1) larger wire width to reduce the IR drop on interconnects thereby increasing the learning accuracy; 2) use of multiple cells for each weight element to alleviate the impact of the device variations, at an affordable expense of area, energy and latency. The optimized resistive cross-point array with peripheral circuitry is implemented at the 65 nm node. Its performance is benchmarked for handwritten digit recognition on the MNIST database using gradient-based sparse coding. Compared to state-of-the-art software approach running on CPU, it achieves >10^3 speed-up and >10^6 energy efficiency improvement, enabling real-time image feature extraction and learning.},
  added-at = {2016-07-02T21:02:13.000+0200},
  address = {San Jose, CA, USA},
  author = {Chen, Pai-Yu and Kadetotad, Deepak and Xu, Zihan and Mohanty, Abinash and Lin, Binbin and Ye, Jieping and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu and Yu, Shimeng},
  biburl = {https://www.bibsonomy.org/bibtex/2b50ec1abdac3f8268261dabf87dbebc9/vngudivada},
  booktitle = {Proceedings of the 2015 Design, Automation \& Test in Europe Conference \& Exhibition},
  interhash = {3aa954e74f6ac1bda49938e2f3058369},
  intrahash = {b50ec1abdac3f8268261dabf87dbebc9},
  keywords = {MachineLearning NeuromorphicComputing SynapticDevice},
  pages = {854--859},
  publisher = {EDA Consortium},
  series = {DATE '15},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Technology-design Co-optimization of Resistive Cross-point Array for Accelerating Learning Algorithms on Chip},
  year = 2015
}

@inproceedings{hua2014mining,
  abstract = {This paper introduces the MSR-Bing grand challenge on image retrieval. The challenge is based on a dataset generated from click logs of a real image search engine. The challenge is to mine semantic knowledge from the dataset and predict the relevance score of any image-query pair. A brief introduction to the dataset, the challenge task, and the evaluation method will be presented. And then the methods proposed by the challenge participants are introduced, followed by evaluation results and some discussions about the goal and future of the challenge.},
  added-at = {2016-07-02T21:14:36.000+0200},
  author = {Hua, X. S. and Ye, Ming and Li, Jin},
  biburl = {https://www.bibsonomy.org/bibtex/21629482bfd11bdbdd47fb6c13c752f13/vngudivada},
  booktitle = {IEEE International Conference on Multimedia and Expo Workshops},
  doi = {10.1109/ICMEW.2014.6890598},
  interhash = {b05da3e3b850c375606601f6dcb5dda1},
  intrahash = {1629482bfd11bdbdd47fb6c13c752f13},
  keywords = {CBIR MSR-BingImageRetrievalChallenge,},
  pages = {1-4},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Mining knowledge from clicks: MSR-Bing image retrieval challenge},
  year = 2014
}

@book{abumostafa2012learning,
  added-at = {2016-07-02T18:44:54.000+0200},
  author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
  biburl = {https://www.bibsonomy.org/bibtex/2079c807902a5b01cf801a8c7cec519ed/vngudivada},
  interhash = {5665353d0134ff1dea4ae32e99335a76},
  intrahash = {079c807902a5b01cf801a8c7cec519ed},
  keywords = {Book Learning, Machine},
  publisher = {AMLBook},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Learning From Data},
  year = 2012
}

@article{dbpedia2016towards,
  added-at = {2016-07-02T21:08:49.000+0200},
  author = {DBpedia},
  biburl = {https://www.bibsonomy.org/bibtex/2908a91b030d2814b0a35c816118f13d9/vngudivada},
  interhash = {fb3d1914abd156514e1b8deed9bc5dff},
  intrahash = {908a91b030d2814b0a35c816118f13d9},
  keywords = {DBpedia SemanticKnowledgeGraph},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Towards a Public Data Infrastructure for a Large, Multilingual, Semantic Knowledge Graph},
  url = {http://wiki.dbpedia.org/},
  urldate = {2016-07-01},
  year = 2016
}

@book{eliasmith2015build,
  abstract = {One goal of researchers in neuroscience, psychology, and artificial intelligence is to build theoretical models that can explain the flexibility and adaptiveness of biological systems. How to Build a Brain provides a guided exploration of a new cognitive architecture that takes biological detail seriously while addressing cognitive phenomena. The Semantic Pointer Architecture (SPA) introduced in this book provides a set of tools for constructing a wide range of biologically constrained perceptual, cognitive, and motor models.

Examples of such models are provided to explain a wide range of data including single-cell recordings, neural population activity, reaction times, error rates, choice behavior, and fMRI signals. Each of the models addressed in the book introduces a major feature of biological cognition, including semantics, syntax, control, learning, and memory. These models are presented as integrated considerations of brain function, giving rise to what is currently the world's largest functional brain model.

The book also compares the Semantic Pointer Architecture with the current state of the art, addressing issues of theory construction in the behavioral sciences, semantic compositionality, and scalability, among other considerations. The book concludes with a discussion of conceptual challenges raised by this architecture, and identifies several outstanding challenges for SPA and other cognitive architectures.

Along the way, the book considers neural coding, concept representation, neural dynamics, working memory, neuroanatomy, reinforcement learning, and spike-timing dependent plasticity. Eight detailed, hands-on tutorials exploiting the free Nengo neural simulation environment are also included, providing practical experience with the concepts and models presented throughout.

How to Build a Brain takes on a daunting task, focusing on those parts that we think are important for memory, attention, and planning. Previous attempts at building a cognitive architecture have used symbols or connectionist networks, but Eliasmith uses spiking neurons and models specific brain regions. Categories and semantics emerge from the architecture. The way that all these moving parts work together provides insights into both the nature of cognition and brain function.

Eliasmith offers a unified theory of cognition that rests on the mechanism of a semantic pointer, namely, a compressed neural representation that can stand as a symbol for a more detailed semantic state or be decompressed to reproduce it, in compositional cognitive processes. Ambitious state-of-the-art modeling grounds the semantic pointer architecture in populations of spiking neurons, providing concrete neural accounts of high-level processes, including attention, learning, memory, syntax, semantics, and reasoning. Along with offering a powerful new approach for integrating cognition and neuroscience, Eliasmith provides detailed technical accounts of his system, with accompanying software that will serve both students and fellow modelers well.},
  added-at = {2016-07-02T21:09:35.000+0200},
  author = {Eliasmith, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/2587ff3688ed82de6ffc279ec7780744d/vngudivada},
  interhash = {eb743d8109499af3ba03a99db84a5ec5},
  intrahash = {587ff3688ed82de6ffc279ec7780744d},
  keywords = {Book, BrainBuilding NeuralArchitecture,},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {How to Build a Brain: A Neural Architecture for Biological Cognition},
  year = 2015
}

@article{clark2016chips,
  added-at = {2016-07-02T21:08:05.000+0200},
  author = {Clark, Don},
  biburl = {https://www.bibsonomy.org/bibtex/29838a8cb078bfab9e1202c3bf76e8cfc/vngudivada},
  interhash = {97620d5fad3a1ce0efcbaa0baa520d9d},
  intrahash = {9838a8cb078bfab9e1202c3bf76e8cfc},
  keywords = {MachineLearning Nvidia TeslaP100GPU},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {New Chips Propel Machine Learning: Nvidia microchips are helping in detection of anomalies on CT scans},
  url = {http://www.wsj.com/articles/new-chips-propel-machine-learning-1463957238},
  urldate = {2016-07-01},
  year = 2016
}

@phdthesis{fielding2000architectural,
  abstract = {The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia system. The Web has been iteratively developed over the past ten years through a series of modifications to the standards that define its architecture. In order to identify those aspects of the Web that needed improvement and avoid undesirable modifications, a model for the modern Web architecture was needed to guide its design, definition, and deployment. Software architecture research investigates methods for determining how best to partition a system, how components identify and communicate with each other, how information is communicated, how elements of a system can evolve independently, and how all of the above can be described using formal and informal notations. My work is motivated by the desire to understand and evaluate the architectural design of network-based application software through principled use of architectural constraints, thereby obtaining the functional, performance, and social properties desired of an architecture. An architectural style is a named, coordinated set of architectural constraints. This dissertation defines a framework for understanding software architecture via architectural styles and demonstrates how styles can be used to guide the architectural design of network-based application software. A survey of architectural styles for network-based applications is used to classify styles according to the architectural properties they induce on an architecture for distributed hypermedia. I then introduce the Representational State Transfer (REST) architectural style and describe how REST has been used to guide the design and development of the architecture for the modern Web. REST emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. I describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. Finally, I describe the lessons learned from applying REST to the design of the Hypertext Transfer Protocol and Uniform Resource Identifier standards, and from their subsequent deployment in Web client and server software.},
  added-at = {2016-07-02T21:12:37.000+0200},
  author = {Fielding, Roy Thomas},
  biburl = {https://www.bibsonomy.org/bibtex/2fcc784824a3164633d617670bde32d5d/vngudivada},
  interhash = {fb841f4f2032a2b010b3c8342cc818c0},
  intrahash = {fcc784824a3164633d617670bde32d5d},
  keywords = {REST},
  school = {University of California, Irvine},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Architectural Styles and the Design of Network-based Software Architectures, Chapter 5: Representational State Transfer (REST)},
  year = 2000
}

@article{hashmi2011neuromorphic,
  abstract = {The desire to create novel computing systems, paired with recent advances in neuroscientific understanding of the brain, has led researchers to develop neuromorphic architectures that emulate the brain. To date, such models are developed, trained, and deployed on the same substrate. However, excessive co-dependence between the substrate and the algorithm prevents portability, or at the very least requires reconstructing and retraining the model whenever the substrate changes. This paper proposes a well-defined abstraction layer -- the Neuromorphic instruction set architecture, or NISA -- that separates a neural application's algorithmic specification from the underlying execution substrate, and describes the Aivo framework, which demonstrates the concrete advantages of such an abstraction layer. Aivo consists of a NISA implementation for a rate-encoded neuromorphic system based on the cortical column abstraction, a state-of-the-art integrated development and runtime environment (IDE), and various profile-based optimization tools. Aivo's IDE generates code for emulating cortical networks on the host CPU, multiple GPGPUs, or as boolean functions. Its runtime system can deploy and adaptively optimize cortical networks in a manner similar to conventional just-in-time compilers in managed runtime systems (e.g. Java, C\#).

We demonstrate the abilities of the NISA abstraction by constructing a cortical network model of the mammalian visual cortex, deploying on multiple execution substrates, and utilizing the various optimization tools we have created. For this hierarchical configuration, Aivo's profiling based network optimization tools reduce the memory footprint by 50% and improve the execution time by a factor of 3x on the host CPU. Deploying the same network on a single GPGPU results in a 30x speedup. We further demonstrate that a speedup of 480x can be achieved by deploying a massively scaled cortical network across three GPGPUs. Finally, converting a trained hierarchical network to C/C++ boolean constructs on the host CPU results in 44x speedup.},
  acmid = {1950385},
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Hashmi, Atif and Nere, Andrew and Thomas, James Jamal and Lipasti, Mikko},
  biburl = {https://www.bibsonomy.org/bibtex/2bd9137f8f9e51e19db62485087410e14/vngudivada},
  doi = {10.1145/1961295.1950385},
  interhash = {9bb33dd855eee32c1bc2b4ea974027e9},
  intrahash = {bd9137f8f9e51e19db62485087410e14},
  journal = {SIGARCH Comput. Archit. News},
  keywords = {CorticalLearningAlgorithm GPGPU NeuromorphicArchitecture},
  month = mar,
  number = 1,
  pages = {145--158},
  publisher = {ACM},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {A Case for Neuromorphic ISAs},
  volume = 39,
  year = 2011
}

@article{gudivada1995contentbased,
  abstract = {Images are being generated at an ever-increasing rate by sources such as defence and civilian satellites, military reconnaissance and surveillance flights, fingerprinting and mug-shot-capturing devices, scientific experiments, biomedical imaging, and home entertainment systems. For example, NASA's Earth Observing System will generate about 1 terabyte of image data per day when fully operational. A content-based image retrieval (CBIR) system is required to effectively and efficiently use information from these image repositories. Such a system helps users (even those unfamiliar with the database) retrieve relevant images based on their contents. Application areas in which CBIR is a principal activity are numerous and diverse. With the recent interest in multimedia systems, CBIR has attracted the attention of researchers across several disciplines},
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Gudivada, V. and Raghavan, V.},
  biburl = {https://www.bibsonomy.org/bibtex/2d16da9b83865e34a7bdf3a527d5dbd36/vngudivada},
  interhash = {e99c45626c4d00b9c26f560130af1af3},
  intrahash = {d16da9b83865e34a7bdf3a527d5dbd36},
  journal = {IEEE Computer},
  keywords = {CBIR},
  month = {September},
  number = 9,
  pages = {18 -- 22},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Content-based image retrieval systems},
  volume = 28,
  year = 1995
}

@article{hermann2014distributed,
  abstract = {The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level. Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP.

This dissertation explores methods for learning distributed semantic representations and models for composing these into representations for larger linguistic units. Our underlying hypothesis is that neural models are a suitable vehicle for learning semantically rich representations and that such representations in turn are suitable vehicles for solving important tasks in natural language processing. The contribution of this thesis is a thorough evaluation of our hypothesis, as part of which we introduce several new approaches to representation learning and compositional semantics, as well as multiple state-of-the-art models which apply distributed semantic representations to various tasks in NLP.},
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Hermann, Karl Moritz},
  biburl = {https://www.bibsonomy.org/bibtex/2e528cd8121531144018ecec02bcabe2c/vngudivada},
  interhash = {6bf7ade3904bd7506e69790113a4b9e2},
  intrahash = {e528cd8121531144018ecec02bcabe2c},
  journal = {CoRR},
  keywords = {CompositionalSemantics DistributedRepresentation},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Distributed Representations for Compositional Semantics},
  url = {http://arxiv.org/abs/1411.3146},
  volume = {abs/1411.3146},
  year = 2014
}

@article{gudivada2016renaissance,
  abstract = {Big data requirements are motivating new database management models that can process billions of data requests per second, and established relational models are changing to keep pace. The authors provide practical tools for navigating this shifting product landscape and finding candidate systems that best fit a data manager's application needs.},
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Gudivada, V. and Rao, D. and Raghavan, V.},
  biburl = {https://www.bibsonomy.org/bibtex/241fea44e8d9354cf053a69d3a2e1d60d/vngudivada},
  interhash = {552ccc82f54312fa9584d517cde3eb5a},
  intrahash = {41fea44e8d9354cf053a69d3a2e1d60d},
  journal = {IEEE Computer},
  keywords = {BigData NoSQL},
  number = 4,
  pages = {31 - 42},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Renaissance in Database Management: Navigating the Landscape of Candidate Systems},
  volume = 49,
  year = 2016
}

@incollection{gudivada2015driven,
  abstract = {Due to the inherent complexity of natural languages, many natural language tasks are ill-posed for mathematically precise algorithmic solutions. To circumvent this problem, statistical machine learning approaches are used for NLP tasks. The emergence of Big Data enables a new paradigm for solving NLP problems — managing the complexity of the problem domain by harnessing the power of data for building high quality models. This chapter provides an introduction to various core NLP tasks and highlights their data-driven solutions. Second, a few representative NLP applications that use the underlying infrastructure consisting of the core NLP tasks are described. Third, various sources of Big Data for NLP research are discussed. Fourth, Big Data driven NLP research and applications are outlined. Finally, the chapter concludes by indicating trends and future research directions.},
  added-at = {2016-07-02T21:16:48.000+0200},
  address = {New York, NY},
  author = {Gudivada, V. and Rao, D. and Raghavan, V.},
  biburl = {https://www.bibsonomy.org/bibtex/2947d455f9e051ffba624b6f8869d95f8/vngudivada},
  booktitle = {Big Data Analytics},
  editor = {Govindaraju, V. and Raghavan, V. and Rao, C. R.},
  interhash = {9bdf77f23f353f8cf3134fb3fc8ab83c},
  intrahash = {947d455f9e051ffba624b6f8869d95f8},
  keywords = {BigData NLP},
  pages = {203 - 238},
  publisher = {Elsevier},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {Big Data Driven Natural Language Processing Research and Applications},
  year = 2015
}

@incollection{guarino2009ontology,
  abstract = {The word “ontology” is used with different senses in different communities. The most radical difference is perhaps between the philosophical sense, which has of course a well-established tradition, and the computational sense, which emerged in the recent years in the knowledge engineering community, starting from an early informal definition of (computational) ontologies as “explicit specifications of conceptualizations”. In this paper we shall revisit the previous attempts to clarify and formalize such original definition, providing a detailed account of the notions of conceptualization and explicit specification, while discussing at the same time the importance of shared explicit specifications.},
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Guarino, Nicola and Oberle, Daniel and Staab, Steffen},
  biburl = {https://www.bibsonomy.org/bibtex/2d3acadc812f1f599304049f7c16ce362/vngudivada},
  booktitle = {Handbook on Ontologies},
  edition = {Second},
  editor = {Staab, Steffen and Studer, Ruder},
  interhash = {c0f4c3b6821595bdc50104f5a9af29a0},
  intrahash = {d3acadc812f1f599304049f7c16ce362},
  keywords = {Ontology},
  publisher = {Springer},
  timestamp = {2019-03-25T17:14:33.000+0100},
  title = {What is an Ontology?},
  year = 2009
}

@inproceedings{harman1993overview,
  abstract = {The first Text REtrieval Conference (TREC-1) was held in early November 1992 and was attended by about 100 people working in the 25 participating groups. The goal of the conference was to bring research groups together to discuss their work on a new large test collection. There was a large variety of retrieval techniques reported on, including methods using automatic thesaurii, sophisticated term weighting, natural language techniques, relevance feedback, and advanced pattern matching. As results had been run through a common evaluation package, groups were able to compare the effectiveness of different techniques, and discuss how differences among the sytems affected performance.},
  added-at = {2016-07-02T21:16:48.000+0200},
  address = {New York, NY, USA},
  author = {Harman, Donna},
  biburl = {https://www.bibsonomy.org/bibtex/29813167adf7c649d469252d9aa8d24ed/vngudivada},
  booktitle = {Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/160688.160692},
  interhash = {e14dff4d5e57b49bcb8fa7096f9dd103},
  intrahash = {9813167adf7c649d469252d9aa8d24ed},
  keywords = {IR TREC},
  pages = {36--47},
  publisher = {ACM},
  series = {SIGIR '93},
  timestamp = {2019-03-25T17:14:28.000+0100},
  title = {Overview of the First TREC Conference},
  year = 1993
}

@unpublished{hawkins2016biological,
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Hawkins, J. and Ahmad, S. and Purdy, S. and Lavin, A.},
  biburl = {https://www.bibsonomy.org/bibtex/2ced0a20abb3b48bb8170379aa3402303/vngudivada},
  interhash = {223738f3a8906894b543e2fe8abc77ac},
  intrahash = {ced0a20abb3b48bb8170379aa3402303},
  keywords = {BAMI},
  note = {Initial online release 0.4},
  timestamp = {2019-03-25T17:14:28.000+0100},
  title = {Biological and Machine Intelligence (BAMI)},
  url = {http://numenta.com/biological-and-machine-intelligence/},
  year = 2016
}

@article{huang2014learning,
  abstract = {Finding the right representations for words is critical for building accurate NLP systems when domain-specific labeled data for the task is scarce. This article investigates novel techniques for extracting features from n-gram models, Hidden Markov Models, and other statistical language models, including a novel Partial Lattice Markov Random Field model. Experiments on part-of-speech tagging and information extraction, among other tasks, indicate that features taken from statistical language models, in combination with more traditional features, outperform traditional representations alone, and that graphical model representations outperform n-gram models, especially on sparse and polysemous words.},
  added-at = {2016-07-02T21:16:48.000+0200},
  address = {Cambridge, MA, USA},
  author = {Huang, Fei and Ahuja, Arun and Downey, Doug and Yang, Yi and Guo, Yuhong and Yates, Alexander},
  biburl = {https://www.bibsonomy.org/bibtex/2ec86f408cfc2d2f9105633aaeb797074/vngudivada},
  interhash = {44fee8f61552f0bfbec735e274f3bbe5},
  intrahash = {ec86f408cfc2d2f9105633aaeb797074},
  journal = {Comput. Linguist.},
  keywords = {LearningRepresentations NLP TrainingData},
  month = mar,
  number = 1,
  pages = {85--120},
  publisher = {MIT Press},
  timestamp = {2019-03-25T17:14:28.000+0100},
  title = {Learning Representations for Weakly Supervised Natural Language Processing Tasks},
  volume = 40,
  year = 2014
}

@incollection{lehman1996gentle,
  abstract = {Many intellectual disciplines contribute to the field of cognitive science: psychology, linguistics, anthropology, and artificial intelligence, to name just a few. Cognitive science itself originated in the desire to integrate expertise in these traditionally separate disciplines in order to advance our insight into cognitive phenomena, phenom ena like problem solving, decision making, language, memory, and learning. Each discipline has a history of asking certain types of questions and accepting certain types of answers. And that, according to Allen Newell, a founder of the field of artificial intelligence, is both an advantage and a problem.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Lehman, Jill Fain and Laird, John and Rosenbloom, Paul},
  biburl = {https://www.bibsonomy.org/bibtex/22fd4fc349a30f5db33972a4d7f20f731/vngudivada},
  booktitle = {Invitation to Cognitive Science, Volume 4},
  editor = {Sternberg, S. and Scarborough, D.},
  interhash = {9eb6345e6b7acd8197e61c4f6c356ff6},
  intrahash = {2fd4fc349a30f5db33972a4d7f20f731},
  keywords = {SOAR},
  publisher = {MIT Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {A gentle introduction to Soar, an architecture for human cognition},
  year = 1996
}

@article{gudivada2015promises,
  abstract = {Despite some key problems, big data could fundamentally change scientific research methodology and how businesses develop products and provide services.},
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Gudivada, V. and Baeza-Yates, R. and Raghavan, V.},
  biburl = {https://www.bibsonomy.org/bibtex/298197dc801ca4936582bd0230bfc048c/vngudivada},
  interhash = {3968b54158bff3ec5e37bc8f59a2e425},
  intrahash = {98197dc801ca4936582bd0230bfc048c},
  journal = {IEEE Computer},
  keywords = {BigData},
  month = mar,
  number = 3,
  pages = {20-23},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Big Data: Promises and Problems},
  volume = 48,
  year = 2015
}

@article{mountcastle1997columnar,
  abstract = {The modular organization of nervous systems is a widely documented principle of design for both vertebrate and invertebrate brains of which the columnar organization of the neocortex is an example. The classical cytoarchitectural areas of the neocortex are composed of smaller units, local neural circuits repeated iteratively within each area. Modules may vary in cell type and number, in internal and external connectivity, and in mode of neuronal processing between different large entities; within any single large entity they have a basic similarity of internal design and operation. Modules are most commonly grouped into entities by sets of dominating external connections. This unifying factor is most obvious for the heterotypical sensory and motor areas of the neocortex. Columnar defining factors in homotypical areas are generated, in part, within the cortex itself. The set of all modules composing such an entity may be fractionated into different modular subsets by different extrinsic connections. Linkages between them and subsets in other large entities form distributed systems. The neighborhood relations between connected subsets of modules in different entities result in nested distributed systems that serve distributed functions. A cortical area defined in classical cytoarchitectural terms may belong to more than one and sometimes to several distributed systems. Columns in cytoarchitectural areas located at some distance from one another, but with some common properties, may be linked by long-range, intracortical connections.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Mountcastle, V. B.},
  biburl = {https://www.bibsonomy.org/bibtex/2c52f521bcd9bab52a0a84ee2372ccb95/vngudivada},
  interhash = {fa0ff9ca4d4cdac417be2d41144b8a6c},
  intrahash = {c52f521bcd9bab52a0a84ee2372ccb95},
  journal = {Brain},
  keywords = {ColumnarOrganization Neocortex},
  month = Apr,
  pages = {701--722},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {{{T}he columnar organization of the neocortex}},
  volume = {120 (Pt 4)},
  year = 1997
}

@book{newell1994unified,
  abstract = {Psychology is now ready for unified theories of cognition--so says Allen Newell, a leading investigator in computer science and cognitive psychology. Not everyone will agree on a single set of mechanisms that will explain the full range of human cognition, but such theories are within reach and we should strive to articulate them.

In this book, Newell makes the case for unified theories by setting forth a candidate. After reviewing the foundational concepts of cognitive science--knowledge, representation, computation, symbols, architecture, intelligence, and search--Newell introduces Soar, an architecture for general cognition. A pioneer system in artificial intelligence, Soar is the first problem solver to create its own subgoals and learn continuously from its own experience.

Newell shows how Soar's ability to operate within the real-time constraints of intelligent behavior, such as immediate-response and item-recognition tasks, illustrates important characteristics of the human cognitive structure. Throughout, Soar remains an exemplar: we know only enough to work toward a fully developed theory of cognition, but Soar's success so far establishes the viability of the enterprise.

Given its integrative approach, Unified Theories of Cognition will be of tremendous interest to researchers in a variety of fields, including cognitive science, artificial intelligence, psychology, and computer science. This exploration of the nature of mind, one of the great problems of philosophy, should also transcend disciplines and attract a large scientific audience.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Newell, Allen},
  biburl = {https://www.bibsonomy.org/bibtex/22d653df8bd6450988bdd3cca32399c93/vngudivada},
  interhash = {f86898399916ea156417903fe8226cb2},
  intrahash = {2d653df8bd6450988bdd3cca32399c93},
  keywords = {Cognition},
  publisher = {Harvard University Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Unified Theories of Cognition},
  year = 1994
}

@book{langacker2008cognitive,
  abstract = {Cognitive Grammar considers the basic units of language to be symbols or conventional pairings of a semantic structure with a phonological label. Grammar consists of constraints on how these units can be combined to generate larger phrases which are also a pairing of semantics and phonology. The semantic aspects are modeled as image schemas rather than propositions, and because of the tight binding with the label, each can invoke the other.

  Like construction grammar (conceived by Charles Fillmore and further developed by Langacker's student Adele Goldberg), and unlike many mainstream linguistic theories, cognitive grammar extends the notion of symbolic units to the grammar of languages. Langacker further assumes that linguistic structures are motivated by general cognitive processes. In formulating his theory, he makes extensive use of principles of gestalt psychology and draws analogies between linguistic structure and aspects of visual perception.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Langacker, Ronald W.},
  biburl = {https://www.bibsonomy.org/bibtex/2966c58041cb70132b8cc53e40e855987/vngudivada},
  interhash = {c511dd8e24f6d7c96e8c7b496e9fa320},
  intrahash = {966c58041cb70132b8cc53e40e855987},
  keywords = {CognitiveGrammar},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Cognitive Grammar: A Basic Introduction},
  year = 2008
}

@inproceedings{nere2010cortical,
  abstract = {As the number of devices available per chip continues to increase, the computational potential of future computer architectures grows likewise. While this is a clear benefit for future computing devices, future chips will also likely suffer from more faulty devices and increased power consumption. It is also likely that these chips will be difficult to program if the current trend of adding more parallel cores continues to follow in the future. However, recent advances in neuroscientific understanding make parallel computing devices modeled after the human neocortex a plausible, attractive, fault-tolerant, and energy-efficient possibility.

In this paper we describe a GPGPU extension to an intelligent model based on the mammalian neocortex. The GPGPU is a readily-available architecture that fits well with the parallel cortical architecture inspired by the basic building blocks of the human brain. Using NVIDIA's CUDA framework, we have achieved up to 273x speedup over our unoptimized C++ serial implementation. We also consider two inefficiencies inherent to our initial design: multiple kernel-launch overhead and poor utilization of GPGPU resources. We propose using a software work-queue structure to solve the former, and pipelining the cortical architecture during training phase for the latter. Additionally, from our success in extending our model to the GPU, we speculate the necessary hardware requirements for simulating the computational abilities of mammalian brains.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Nere, Andrew and Lipasti, Mikko},
  biburl = {https://www.bibsonomy.org/bibtex/2ce35816f6fc85c3a5c3d1fc52f3d1d03/vngudivada},
  booktitle = {Proceedings of the 3rd Workshop on General-Purpose Computation on Graphics Processing Units},
  doi = {10.1145/1735688.1735693},
  interhash = {4bd7597c80b634b672c23ce8ffd878bc},
  intrahash = {ce35816f6fc85c3a5c3d1fc52f3d1d03},
  isbn = {978-1-60558-935-0},
  keywords = {CUDA CorticalArchitecture GPGPU},
  location = {Pittsburgh, Pennsylvania, USA},
  pages = {12--18},
  publisher = {ACM},
  series = {GPGPU-3},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Cortical Architectures on a GPGPU},
  year = 2010
}

@book{mountcastle1998perceptual,
  abstract = {The cerebral cortex, occupying over 70 percent of our brain mass, is key to any understanding of the workings—and disorders—of the human brain. Offering a comprehensive account of the role of the cerebral cortex in perception, this monumental work by one of the world's greatest living neuroscientists does nothing short of creating a new subdiscipline in the field: perceptual neuroscience.

For this undertaking, Vernon Mountcastle has gathered information from a vast number of sources reaching back through two centuries of investigation into the intrinsic operations of the cortex. His survey includes phylogenetic, comparative, and neuroanatomical studies of the neocortex; studies of the large-scale organization of the neocortex, of neuronal histogenesis and the specification of cortical areas, of synaptic transmission between neurons in cortical microcircuits, and of rhythmicity and synchronization in neocortical networks; and inquiries into the binding problem—how activities among the separate processing nodes of distributed systems coalesce in a coherent activity that we call perception.

The first book to summarize what is known about the physiology of the cortex in perception, Perceptual Neuroscience will be a landmark in the literature of neuroscience.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {Cambridge, MA},
  author = {Mountcastle, Vernon B.},
  biburl = {https://www.bibsonomy.org/bibtex/2d115ec8327e02c54c5f3c934b0da8b8a/vngudivada},
  interhash = {e93adeed01743d20cc31400c51083914},
  intrahash = {d115ec8327e02c54c5f3c934b0da8b8a},
  keywords = {Book CerebralCortex CognitiveNeuroscience PerceptualNeuroscience},
  publisher = {Harvard University Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Perceptual Neuroscience: The Cerebral Cortex},
  year = 1998
}

@article{haber2013cognitive,
  abstract = {We introduce Mala, a cognitive architecture intended to bridge the gap between a robot’s sensorimotor and cognitive components. Mala is a multi-entity architecture inspired by Minsky’s Society of Mind and by experience in developing robots that must work in dynamic, unstructured environments. We identify several essential capabilities and characteristics of architectures needed for such robots to develop high levels of autonomy, in particular, modular and asynchronous processing, specialised representations, relational reasoning, mechanisms to translate between representations, and multiple types of integrated learning. We demonstrate an implemented Mala system and evaluate its performance on a challenging autonomous robotic urban search and rescue task. We then discuss the various types of learning required for rich and extended autonomy, and how the structure of the proposed architecture enables each type. We conclude by discussing the relationship of the system to previous work and its potential for developing robots capable of long-term autonomy.},
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Haber, Adam and Sammut, Claude},
  biburl = {https://www.bibsonomy.org/bibtex/206097d7f5217503c6b77864ed3c9cf60/vngudivada},
  interhash = {15d9a6969503a6a7961d565a3655f819},
  intrahash = {06097d7f5217503c6b77864ed3c9cf60},
  journal = {Advances in Cognitive Systems},
  keywords = {AutonomousRobot CognitiveArchitecture},
  pages = {257 - 275},
  publisher = {Cognitive Systems Foundation},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {A Cognitive Architecture for Autonomous Robots},
  year = 2013
}

@online{ng2015learning,
  abstract = {Machine learning is a very successful technology but applying it today often requires spending substantial effort hand-designing features. This is true for applications in vision, audio and text. To address this, Ng's group and others are working on "deep learning" algorithms, which can automatically learn feature representations (often from unlabeled data) thus avoiding a lot of time-consuming engineering. These algorithms are based on building massive artificial neural networks that were loosely inspired by cortical (brain) computations. As part of this work, Ng also founded and formerly led a project at Google to build massive deep learning algorithms. This work resulted in a highly distributed neural network with over 1 billion parameters trained on 16,000 CPU cores that learned by itself to discover high level concepts -- such as "cats" -- from watching unlabeled YouTube videos.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Ng, Andrew and Coates, Adam and Huval, Brody and Le, Quoc and Maas, Andrew and Saxe, Andrew and Socher, Richard and Tandon, Sameep and Wang, Tao},
  biburl = {https://www.bibsonomy.org/bibtex/2615002f5c7ab2ee61aeab0b86ee95be2/vngudivada},
  interhash = {b2e12947bab2697089462d47febc224d},
  intrahash = {615002f5c7ab2ee61aeab0b86ee95be2},
  keywords = {DeepLearning UnsupervisedFeatureLearning},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Deep Learning and Unsupervised Feature Learning},
  url = {http://www.andrewng.org/portfolio/deep-learning-and-unsupervised-feature-learning/},
  year = 2015
}

@book{mccreary2013making,
  abstract = {Making Sense of NoSQL clearly and concisely explains the concepts, features, benefits, potential, and limitations of NoSQL technologies. Using examples and use cases, illustrations, and plain, jargon-free writing, this guide shows how you can effectively assemble a NoSQL solution to replace or augment the traditional RDBMS you have now.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {McCreary, Dan and Kelly, Ann},
  biburl = {https://www.bibsonomy.org/bibtex/2855441f3caa06dd3370727c70b3e71cf/vngudivada},
  interhash = {4869871e5b5cfe405da307bbee87d2f9},
  intrahash = {855441f3caa06dd3370727c70b3e71cf},
  keywords = {NoSQL},
  publisher = {Manning Publications},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Making Sense of NoSQL: A guide for managers and the rest of us},
  year = 2013
}

@article{patel2001primer,
  abstract = {As a multidisciplinary field, medical informatics draws on a range of disciplines, such as computer science, information science, and the social and cognitive sciences. The cognitive sciences can provide important insights into the nature of the processes involved in human–computer interaction and help improve the design of medical information systems by providing insight into the roles that knowledge, memory, and strategies play in a variety of cognitive activities. In this paper, the authors survey literature on aspects of medical cognition and provide a set of claims that they consider to be important in medical informatics.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Patel, V. L. and Arocha, J. F. and Kaufman, D. R.},
  biburl = {https://www.bibsonomy.org/bibtex/2931896258a9e97668a30b9d88f5da087/vngudivada},
  interhash = {366f12c1c4163cb8a4eebfb76ef1260c},
  intrahash = {931896258a9e97668a30b9d88f5da087},
  journal = {J Am Med Inform Assoc},
  keywords = {CognitiveComputing MedicalInformatics},
  number = 4,
  pages = {324--343},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {{A primer on aspects of cognition for medical informatics}},
  volume = 8,
  year = 2001
}

@book{hurwitz2015cognitive,
  abstract = {This book provides detailed guidance toward building a new class of systems that learn from experience and derive insights to unlock the value of big data. It helps technologists understand cognitive computing's underlying technologies, from knowledge representation techniques and natural language processing algorithms to dynamic learning approaches based on accumulated evidence, rather than reprogramming. Case examples from financial, healthcare, and manufacturing walk readers through the design and testing of cognitive systems. Topics include: how cognitive computing evolved from promise to reality; elements of a cognitive computing system; hardware and software technologies behind cognitive computing; evaluation of your own application portfolio; cognitive computing capabilities to transform the organization.},
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Hurwitz, Judith and Kaufman, Marcia and Bowles, Adrian},
  biburl = {https://www.bibsonomy.org/bibtex/2160b2d79d4cd3ab934f87f39444618b2/vngudivada},
  interhash = {2b72a4b9a685576c69234bcfa93fdc19},
  intrahash = {160b2d79d4cd3ab934f87f39444618b2},
  keywords = {BigData BigDataAnalytics Book CognitiveComputing},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Cognitive Computing and Big Data Analytics},
  year = 2015
}

@inproceedings{laird2008extending,
  abstract = {One approach in pursuit of general intelligent agents has been to concentrate on the underlying cognitive architecture, of which Soar is a prime example. In the past, Soar has relied on a minimal number of architectural modules together with purely symbolic representations of knowledge. This paper presents the cognitive architecture approach to general intelligence and the traditional, symbolic Soar architecture. This is followed by major additions to Soar: non-symbolic representations, new learning mechanisms, and long-term memories.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {Amsterdam, The Netherlands},
  author = {Laird, John E.},
  biburl = {https://www.bibsonomy.org/bibtex/264c29481631d92e463d5506b560e5bcd/vngudivada},
  booktitle = {Proceedings of the 2008 Conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference},
  interhash = {c250d01697d81744dec54995d637ddf8},
  intrahash = {64c29481631d92e463d5506b560e5bcd},
  keywords = {Clustering CognitiveArchitecture Emotion EpisodicMemory MentalImagery ReinforcementLearning SOAR SemanticMemory},
  pages = {224--235},
  publisher = {IOS Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Extending the Soar Cognitive Architecture},
  year = 2008
}

@misc{lehman2006gentle,
  abstract = {Many intellectual disciplines contribute to the field of cognitive science: psychology, linguistics, anthropology, and artificial intelligence, to name just a few. Cognitive science itself originated in the desire to integrate expertise in these traditionally separate disciplines in order to advance our insight into cognitive phenomena, phenom ena like problem solving, decision making, language, memory, and learning. Each discipline has a history of asking certain types of questions and accepting certain types of answers. And that, according to Allen Newell, a founder of the field of artificial intelligence, is both an advantage and a problem.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Lehman, Jill Fain and Laird, John and Rosenbloom, Paul},
  biburl = {https://www.bibsonomy.org/bibtex/232c1e1f023909e85e5ee44d96bedb962/vngudivada},
  howpublished = {University of Michigan},
  interhash = {68bcec9ef498a99d6fb5cbb55a6cb5b8},
  intrahash = {32c1e1f023909e85e5ee44d96bedb962},
  keywords = {SOAR},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {A gentle introduction to Soar, an architecture for human cognition, 2006 update},
  year = 2006
}

@article{negueruela2011braincomputer,
  abstract = {Recent experiments have shown the possibility to use the brain electrical activity to directly control the movement of robots. Such a kind of brain-computer interface is a natural way to augment human capabilities by providing a new interaction link with the outside world and is particularly relevant as an aid for paralysed humans, although it also opens up new possibilities in human-robot interaction for able-bodied people. One of these new fields of application is the use of brain-computer interfaces in the space environment, where astronauts are subject to extreme conditions and could greatly benefit from direct mental teleoperation of external semi-automatic manipulators--for instance, mental commands could be sent without any output/latency delays, as it is the case for manual control in microgravity conditions. Previous studies show that there is a considerable potential for this technology onboard spacecraft.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {London, UK, UK},
  author = {Negueruela, Cristina and Broschart, Michael and Menon, Carlo and R. Mill\'{a}n, Jos{\'e}},
  biburl = {https://www.bibsonomy.org/bibtex/24b449382a1ca7f3ee26dd9d0c44898ae/vngudivada},
  doi = {10.1007/s00779-010-0322-8},
  interhash = {776143db9eff11ca9c9208bb47ac2f91},
  intrahash = {4b449382a1ca7f3ee26dd9d0c44898ae},
  journal = {Personal Ubiquitous Comput.},
  keywords = {Astronaut BCI SpaceOperation},
  month = jun,
  number = 5,
  pages = {527--537},
  publisher = {Springer-Verlag},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Brain-computer Interfaces for Space Applications},
  volume = 15,
  year = 2011
}

@inproceedings{potts2015negotiating,
  abstract = {There is a well-known preference for disjunctions X or Y to be construed so that X and Y are semantically disjoint. However, there are two felicitous usage patterns in which the speaker violates this preference in part to convey information about the language itself. First, disjunctions of terms in a one-way semantic inclusion relation, such as boat or canoe, can form part of a speaker strategy to manage lexical uncertainty surrounding the two terms, or block unwanted implicatures that the listener might draw from the general term alone. Second, disjunctions of synonymous terms like wine lover or oenophile can be used to convey definitional information. We explore both of these uses, relying on corpora to obtain a fuller picture of their motivations and their effects on the listener. In addition, we show how both these uses are predicted by a standard semantics for disjunction and a recursive probabilistic model of communication in which speakers and listeners simultaneously exchange information about the world and about the language they are using. We also use the model to begin to formally characterize the pragmatics of implicature cancelation or blocking.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {Berkeley, CA},
  author = {Potts, Christopher and Levy, Roger},
  biburl = {https://www.bibsonomy.org/bibtex/25db886a5860056ad71e0ae6e6a197389/vngudivada},
  booktitle = {Proceedings of the 41st Annual Meeting of the {B}erkeley {L}inguistics {S}ociety},
  editor = {Jurgensen, Anna E. and Sande, Hannah and Lamoureux, Spencer and Baclawski, Kenny and Zerbe, Alison},
  interhash = {d05680bff25d9d4d5f24301791024dfd},
  intrahash = {5db886a5860056ad71e0ae6e6a197389},
  keywords = {LexicalUncertainty},
  location = {Berkeley, CA},
  month = {February},
  pages = {417--445},
  publisher = {Berkeley Linguistics Society},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Negotiating Lexical Uncertainty and Speaker Expertise with Disjunction},
  year = 2015
}

@inproceedings{liu2013bioinspired,
  abstract = {Neuromorphic computing, which is inspired by the working mechanism of human brain, recently emerges as a hot research area to combat the contradiction between the limited functions of computing systems and the ever increasing variety of applications. In this work, we will introduce our research on a bio-inspired neuromorphic embedded computing engine named Centaur, which aims to achieve an ultra-high power efficiency beyond One-TeraFlops-Per-Watt by adopting the bio-inspired computation model and the advanced memristor technology. The success of Centaur design may promote the embedded system power efficiency three orders of magnitude from the current level while the small footprint and real-time re-configurability of the design allow an easy integration into MPSoCs, enabling many emerging mobile and embedded applications.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {Piscataway, NJ, USA},
  author = {Liu, Beiye and Hu, Miao and Li, Hai and Chen, Yiran and Xue, Chun (Jason)},
  biburl = {https://www.bibsonomy.org/bibtex/29646ef77546dbb63cf2730b69189d1ae/vngudivada},
  booktitle = {Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis},
  interhash = {88a9f59b544a1e49b6f888ffbf537461},
  intrahash = {9646ef77546dbb63cf2730b69189d1ae},
  keywords = {NeuromorphicComputing},
  pages = {23:1--23:1},
  publisher = {IEEE Press},
  series = {CODES+ISSS '13},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Bio-inspired Ultra Lower-power Neuromorphic Computing Engine for Embedded Systems},
  year = 2013
}

@inproceedings{monroe2015learning,
  abstract = {The Rational Speech Acts (RSA) model treats language use as a recursive process in which probabilistic speaker and listener agents reason about each other's intentions to enrich the literal semantics of their language along broadly Gricean lines. RSA has been shown to capture many kinds of conversational implicature, but it has been criticized as an unrealistic model of speakers, and it has so far required the manual specification of a semantic lexicon, preventing its use in natural language processing applications that learn lexical knowledge from data. We address these concerns by showing how to define and optimize a trained statistical classifier that uses the intermediate agents of RSA as hidden layers of representation forming a non-linear activation function. This treatment opens up new application domains and new possibilities for learning effectively from data. We validate the model on a referential expression generation task, showing that the best performance is achieved by incorporating features approximating well-established insights about natural language generation into RSA.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {Amsterdam},
  author = {Monroe, Will and Potts, Christopher},
  biburl = {https://www.bibsonomy.org/bibtex/2ddf06517ad1359db33a7c067c08d55a4/vngudivada},
  booktitle = {Proceedings of 20th {A}msterdam {C}olloquium},
  interhash = {a169ed5a5563267df1ac183a9524ebe6},
  intrahash = {ddf06517ad1359db33a7c067c08d55a4},
  keywords = {CognitiveModel},
  month = {December},
  publisher = {ILLC},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Learning in the {R}ational {S}peech {A}cts Model},
  year = 2015
}

@article{misra2010artificial,
  abstract = {This article presents a comprehensive overview of the hardware realizations of artificial neural network (ANN) models, known as hardware neural networks (HNN), appearing in academic studies as prototypes as well as in commercial use. HNN research has witnessed a steady progress for more than last two decades, though commercial adoption of the technology has been relatively slower. We study the overall progress in the field across all major ANN models, hardware design approaches, and applications. We outline underlying design approaches for mapping an ANN model onto a compact, reliable, and energy efficient hardware entailing computation and communication and survey a wide range of illustrative examples. Chip design approaches (digital, analog, hybrid, and FPGA based) at neuronal level and as neurochips realizing complete ANN models are studied. We specifically discuss, in detail, neuromorphic designs including spiking neural network hardware, cellular neural network implementations, reconfigurable FPGA based implementations, in particular, for stochastic ANN models, and optical implementations. Parallel digital implementations employing bit-slice, systolic, and SIMD architectures, implementations for associative neural memories, and RAM based implementations are also outlined. We trace the recent trends and explore potential future research directions.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Misra, Janardan and Saha, Indranil},
  biburl = {https://www.bibsonomy.org/bibtex/29220eecb98fb045f622078564b137b3d/vngudivada},
  doi = {10.1016/j.neucom.2010.03.021},
  interhash = {938ec306fcb6b64c2084867b090fe887},
  intrahash = {9220eecb98fb045f622078564b137b3d},
  journal = {Neurocomput.},
  keywords = {NeuralNetwork Survey},
  month = dec,
  number = {1-3},
  pages = {239--255},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Artificial Neural Networks in Hardware: A Survey of Two Decades of Progress},
  volume = 74,
  year = 2010
}

@article{rice2009scaling,
  abstract = {This paper presents the implementation and scaling of a neocortex inspired cognitive model on a Cray XD1. Both software and reconfigurable logic based FPGA implementations of the model are examined. This model belongs to a new class of biologically inspired cognitive models. Large scale versions of these models have the potential for significantly stronger inference capabilities than current conventional computing systems. These models have large amounts of parallelism and simple computations, thus allowing highly efficient hardware implementations. As a result, hardware-acceleration of these models can produce significant speedups over fully software implementations. Parallel software and hardware-accelerated implementations of such a model are investigated for networks of varying complexity. A scaling analysis of these networks is presented and utilized to estimate the throughput of both hardware-accelerated and software implementations of larger networks that utilize the full resources of the Cray XD1. Our results indicate that hardware-acceleration can provide average throughput gains of 75 times over software-only implementations of the networks we examined on this system.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Rice, Kenneth L. and Taha, Tarek M. and Vutsinas, Christopher N.},
  biburl = {https://www.bibsonomy.org/bibtex/234558b8008190608e6e0ea77587c00e6/vngudivada},
  doi = {10.1007/s11227-008-0195-z},
  interhash = {688cde5bd65ee33084b838f68539413a},
  intrahash = {34558b8008190608e6e0ea77587c00e6},
  journal = {The Journal of Supercomputing},
  keywords = {CognitiveModel HPC Neocortex},
  number = 1,
  pages = {21--43},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Scaling analysis of a neocortex inspired cognitive model on the Cray {XD1}},
  volume = 47,
  year = 2009
}

@book{lakoff2003metaphors,
  abstract = {The now-classic Metaphors We Live By changed our understanding of metaphor and its role in language and the mind. Metaphor, the authors explain, is a fundamental mechanism of mind, one that allows us to use what we know about our physical and social experience to provide understanding of countless other subjects. Because such metaphors structure our most basic understandings of our experience, they are "metaphors we live by"-metaphors that can shape our perceptions and actions without our ever noticing them.

In this updated edition of Lakoff and Johnson's influential book, the authors supply an afterword surveying how their theory of metaphor has developed within the cognitive sciences to become central to the contemporary understanding of how we think and how we express our thoughts in language.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Lakoff, George and Johnson, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/25267027ceb838cdc0866ef098d701760/vngudivada},
  edition = {Second},
  interhash = {a59354bc228107d6ec1d4200e2a6ef2a},
  intrahash = {5267027ceb838cdc0866ef098d701760},
  keywords = {Book Metaphor NLU},
  publisher = {University of Chicago Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Metaphors We Live},
  year = 2003
}

@article{gudivada1997information,
  abstract = {Effective search and retrieval are enabling technologies for realizing the full potential of the Web. The authors examine relevant issues, including methods for representing document content. They also compare available search tools and suggest methods for improving retrieval effectiveness},
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Gudivada, V. and Raghavan, V. and Grosky, W. and Kasanagottu, R.},
  biburl = {https://www.bibsonomy.org/bibtex/2fbcaa64990f6bf72f9a4f6c61e112008/vngudivada},
  interhash = {cad895088727025a3b23bb3c897fe0b2},
  intrahash = {fbcaa64990f6bf72f9a4f6c61e112008},
  journal = {IEEE Internet Computing},
  keywords = {IR},
  month = {October},
  pages = {58 -- 68},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Information Retrieval on the World-Wide Web},
  year = 1997
}

@book{isac2013ilanguage,
  abstract = {I-Language introduces the uninitiated to linguistics as cognitive science. In an engaging, down-to-earth style Daniela Isac and Charles Reiss give a crystal-clear demonstration of the application of the scientific method in linguistic theory. Their presentation of the research program inspired by Noam Chomsky shows how the focus of theory and research in linguistics shifted from treating language as a disembodied, human-external entity to cognitive biolinguistics - the study of language as a human cognitive system embedded within the mind/brain of each individual. The recurring theme of equivalence classes in linguistic computation ties together the presentation of material from phonology, morphology, syntax, and semantics. The same theme is used to help students understand the place of linguistics in the broader context of the cognitive sciences, by drawing on examples from vision, audition and even animal cognition.

This textbook is unique in its integration of empirical issues of linguistic analysis, engagement with philosophical questions that arise in the study of language, and treatment of the history of the field. Topics ranging from allophony to reduplication, ergativity, and negative polarity are invoked to show the implications of findings in cognitive biolinguistics for philosophical issues like reference, the mind-body problem, and nature-nurture debates.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Isac, Daniela and Reiss, Charles},
  biburl = {https://www.bibsonomy.org/bibtex/2a6d6c9e16275d147f46b40fc7ea0f158/vngudivada},
  edition = {Second},
  interhash = {6e2c69af5cbb8dd9c64810ef4fa5dc2e},
  intrahash = {a6d6c9e16275d147f46b40fc7ea0f158},
  keywords = {Book CognitiveLingustics I-Language},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {I-Language: An Introduction to Linguistics as Cognitive Science},
  year = 2013
}

@article{halevy2009unreasonable,
  abstract = {At Brown University, there is excitement of having access to the Brown Corpus, containing one million English words. Since then, we have seen several notable corpora that are about 100 times larger, and in 2006, Google released a trillion-word corpus with frequency counts for all sequences up to five words long. In some ways this corpus is a step backwards from the Brown Corpus: it's taken from unfiltered Web pages and thus contains incomplete sentences, spelling errors, grammatical errors, and all sorts of other errors. It's not annotated with carefully hand-corrected part-of-speech tags. But the fact that it's a million times larger than the Brown Corpus outweighs these drawbacks. A trillion-word corpus - along with other Web-derived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions - captures even very rare aspects of human behavior. So, this corpus could serve as the basis of a complete model for certain tasks - if only we knew how to extract the model from the data.},
  added-at = {2016-07-02T21:16:48.000+0200},
  address = {Los Alamitos, CA, USA},
  annotation = {Problems that involve interacting with humans, such as natural language understanding, have not proven to be solvable by concise, neat formulas like F = ma. Instead, the best approach appears to be to embrace the complexity of the domain and address it by harnessing the power of data: if other humans engage in the tasks and generate large amounts of unlabeled, noisy data, new algorithms can be used to build high-quality models from the data.},
  author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
  biburl = {https://www.bibsonomy.org/bibtex/2ea313c2efcaa4b17e5bed2b11693abfd/vngudivada},
  interhash = {d08a7d9e7d72d8a7f5fe1e602f79409c},
  intrahash = {ea313c2efcaa4b17e5bed2b11693abfd},
  journal = {IEEE Intelligent Systems},
  keywords = {BigData NLP},
  number = 2,
  pages = {8 - 12},
  publisher = {IEEE Computer Society},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The Unreasonable Effectiveness of Data},
  volume = 24,
  year = 2009
}

@book{sears2009humancomputer,
  abstract = {Hailed on first publication as a compendium of foundational principles and cutting-edge research, The Human-Computer Interaction Handbook has become the gold standard reference in this field. Derived from select chapters of this groundbreaking and authoritative resource, Human-Computer Interaction Fundamentals emphasizes emerging topics such as sensor based interactions, tangible interfaces, augmented cognition, cognition under stress, ubiquitous and wearable computing, and privacy and security. It puts the spotlight not only on the fundamental issues involved in the technology of human-computer interactions and but also on the users themselves. The book features visionary perspectives and developments that fundamentally transform the way in which researchers and practitioners view this discipline.},
  added-at = {2016-07-02T21:25:24.000+0200},
  address = {Boca Raton, FL},
  biburl = {https://www.bibsonomy.org/bibtex/2266faacd33ef729cb0211dc0fc1799c5/vngudivada},
  editor = {Sears, Andrew and Jacko, Julie A.},
  interhash = {75677caf496bcc44851e80170d9e62f9},
  intrahash = {266faacd33ef729cb0211dc0fc1799c5},
  keywords = {Book HCI},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Human-Computer Interaction: Fundamentals},
  year = 2009
}

@book{hey2009fourth,
  abstract = {This presentation will set out the eScience agenda by explaining the current scientific data deluge and the case for a “Fourth Paradigm” for scientific exploration. Examples of data intensive science will be used to illustrate the explosion of data and the associated new challenges for data capture, curation, analysis, and sharing. The role of cloud computing, collaboration services, and research repositories will be discussed.},
  added-at = {2016-07-02T21:16:48.000+0200},
  annotation = {This book presents the first broad look at the rapidly emerging field of data-intensive science, with the goal of influencing the worldwide scientific and computing research communities and inspiring the next generation of scientists. Increasingly, scientific breakthroughs will be powered by advanced computing capabilities that help researchers manipulate and explore massive datasets. The speed at which any given scientific discipline advances will depend on how well its researchers collaborate with one another, and with technologists, in areas of eScience such as databases, workflow management, visualization, and cloud-computing technologies. This collection of essays expands on the vision of pioneering computer scientist Jim Gray for a new, fourth paradigm of discovery based on data-intensive science and offers insights into how it can be fully realized.},
  biburl = {https://www.bibsonomy.org/bibtex/2223e036ecd3cf6228581c904031eb216/vngudivada},
  edition = {1.1},
  editor = {Hey, T. and Tansley, S. and Toll, K.},
  interhash = {05cab17278a21286dab9a21cb151a577},
  intrahash = {223e036ecd3cf6228581c904031eb216},
  keywords = {DataScience},
  month = {October},
  publisher = {Microsoft Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The Fourth Paradigm: Data-Intensive Scientific Discovery},
  year = 2009
}

@article{stewart2012neural,
  abstract = {The Neural Engineering Framework (NEF) is a general methodology that allows the building of large-scale, biologically plausible, neural models of cognition. The NEF acts as a neural compiler: once the properties of the neurons, the values to be represented, and the functions to be computed are specified, it solves for the connection weights between components that will perform the desired functions. Importantly, this works not only for feed-forward computations, but also for recurrent connections, allowing for complex dynamical systems including integrators, oscillators, Kalman filters, etc. The NEF also incorporates realistic local error-driven learning rules, allowing for the online adaptation and optimisation of responses. The NEF has been used to model visual attention, inductive reasoning, reinforcement learning and many other tasks. Recently, we used it to build Spaun, the world{\textquoteright}s largest functional brain model, using 2.5 million neurons to perform eight different cognitive tasks by interpreting visual input and producing hand-written output via a simulated 6-muscle arm.  Our open-source software Nengo was used for all of these, and is available at http://nengo.ca, along with tutorials, demos, and downloadable models.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Stewart, Terrence C.},
  biburl = {https://www.bibsonomy.org/bibtex/2cc1211f39b298f29e66b8c1a3f2470dd/vngudivada},
  interhash = {b031b1ff49e2318fa72a9fdfc2599c09},
  intrahash = {cc1211f39b298f29e66b8c1a3f2470dd},
  journal = {AISB Quarterly},
  keywords = {NeuralEngineeringFramework},
  pages = {2-7},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The Neural Engineering Framework},
  year = 2012
}

@inproceedings{wan2014learning,
  abstract = {Learning effective feature representations and similarity measures are crucial to the retrieval performance of a content-based image retrieval (CBIR) system. Despite extensive research efforts for decades, it remains one of the most challenging open problems that considerably hinders the successes of real-world CBIR systems. The key challenge has been attributed to the well-known ``semantic gap'' issue that exists between low-level image pixels captured by machines and high-level semantic concepts perceived by human. Among various techniques, machine learning has been actively investigated as a possible direction to bridge the semantic gap in the long term. Inspired by recent successes of deep learning techniques for computer vision and other applications, in this paper, we attempt to address an open problem: if deep learning is a hope for bridging the semantic gap in CBIR and how much improvements in CBIR tasks can be achieved by exploring the state-of-the-art deep learning techniques for learning feature representations and similarity measures. Specifically, we investigate a framework of deep learning with application to CBIR tasks with an extensive set of empirical studies by examining a state-of-the-art deep learning method (Convolutional Neural Networks) for CBIR tasks under varied settings. From our empirical studies, we find some encouraging results and summarize some important insights for future research.},
  added-at = {2016-07-02T21:25:24.000+0200},
  address = {New York, NY, USA},
  author = {Wan, Ji and Wang, Dayong and Hoi, Steven Chu Hong and Wu, Pengcheng and Zhu, Jianke and Zhang, Yongdong and Li, Jintao},
  biburl = {https://www.bibsonomy.org/bibtex/26f8abdc8fa98c302e4303033c0ff9fa0/vngudivada},
  booktitle = {Proceedings of the 22Nd ACM International Conference on Multimedia},
  doi = {10.1145/2647868.2654948},
  interhash = {37d0df0c9449fbcb36a9d15997c48843},
  intrahash = {6f8abdc8fa98c302e4303033c0ff9fa0},
  keywords = {CBIR ConvolutionalNeuralNetworks DeepLearning},
  pages = {157--166},
  publisher = {ACM},
  series = {MM '14},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Deep Learning for Content-Based Image Retrieval: A Comprehensive Study},
  year = 2014
}

@book{pinker2011words,
  abstract = {simple concepts we use to devise works as complex as love sonnets and tax laws. his linguistic research suggests that each of us stores a limited (though large) number of words and word-parts in memory and manipulates them with a much smaller number of rules to produce every writing and utterance.

  critique: If The Language Instinct described Pinker's view of the development of language and How the Mind Works described his views about cognition in general, this latest work details his ideas about the cognitive organization of language. And like his other books, Pinker tries to persuade the reader to agree with his assessment of thingsusing humorous examples, occasionally odd logic, hyperbole, and in this case a 290 page extended example.

Pinker believes that the brain's representation of language is rule based - morphology (such as adding -s to a noun to make it plural or -ed to a verb to make it past tense) occurs because a system in the brain applies a rule during language production. During the past twenty years or so, many cognitive scientists have begun to think that perhaps this type of morphology is not rule based at all, but instead occurs because of the specific pattern of connections in the brain. The goal of this book is to convince the reader that connectionism is wrong, and a rule based system is correct. To do this, he talks about irregular verbs; their etymology bastardization by children, idiosyncrasies, and production by non-typical populations. I never thought that irregular verbs and oddly plauralized nouns could be interesting. I was right. This topic is so much more esoteric than his other books, that even his entertaining examples could not overcome either my skepticism or my boredom. After a while you just want to hear something different. Pinker is not reporting a phenomena, and evenhandedly evaluating various explanatory theories; he is presenting one view to be dismantled, and another to be exalted as correct. But giving selective evidence could bias his readers towards his view, and I am not convinced I was given a chance to really evaluate the competing theories. I anxiously await the rebuttal by the connectionist school.

If you have read Pinker's popular books before, I can only say that this book is not at the same level. Its scope is much narrower, and its subject matter a bit more technical. That being said, if you love Pinker's way of presenting material, you will not be disappointed. If you haven't read Pinker before, I recommend that you start with one of his other books - they truly live up to their reputations.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Pinker, Steven},
  biburl = {https://www.bibsonomy.org/bibtex/24075816755083b5351c3ab2beb377722/vngudivada},
  interhash = {c6aeaf5a136a0605d515ce865e156cd7},
  intrahash = {4075816755083b5351c3ab2beb377722},
  keywords = {Book NLP},
  publisher = {Harper Perennial},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Words and Rules: The Ingredients of Language},
  year = 2011
}

@inproceedings{zadrozny2015explaining,
  abstract = {Our paper is actually two contributions in one. First, we argue that IBM's Jeopardy! playing machine needs a formal semantics. We present several arguments as we discuss the system. We also situate the work in the broader context of contemporary AI. Our second point is that the work in this area might well be done as a broad collaborative project. Hence our ''Blue Sky'' contribution is a proposal to organize a polymath-style effort aimed at developing formal tools for the study of state of the art question-answer systems, and other large scale NLP efforts whose architectures and algorithms lack a theoretical foundation.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Zadrozny, Wlodek and de Paiva, Valeria and Moss, Lawrence S.},
  biburl = {https://www.bibsonomy.org/bibtex/2183858d8e5824ac93eb98b921a3b02e6/vngudivada},
  booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  interhash = {365a4ef5d32275d5cd4a57720d086c93},
  intrahash = {183858d8e5824ac93eb98b921a3b02e6},
  keywords = {IBMWatson},
  pages = {4078 - 4082},
  publisher = {Association for the Advancement of Artificial
Intelligence},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Explaining Watson: Polymath Style},
  year = 2015
}

@incollection{gudivada2016database,
  abstract = {Special needs of Big Data applications have ushered in several new classes of systems for data storage and retrieval. Each class targets the needs of a category of Big Data application. These systems differ greatly in their data models and system architecture, approaches used for high availability and scalability, query languages and client interfaces provided. This chapter begins with a description of the emergence of Big Data and data management requirements of Big Data applications. Several new classes of database management systems have emerged recently to address the needs of Big Data applications. NoSQL is an umbrella term used to refer to these systems. Next, a taxonomy for NoSQL systems is developed and several NoSQL systems are classified under this taxonomy. Characteristics of representative systems in each class are also discussed. The chapter concludes by indicating the emerging trends of NoSQL systems and research issues.},
  added-at = {2016-07-02T21:16:48.000+0200},
  address = {Boston, MA},
  author = {Gudivada, V. and Apon, A. and Rao, D.},
  biburl = {https://www.bibsonomy.org/bibtex/22efc8814280e89f9b61c09db459dd96d/vngudivada},
  booktitle = {Big Data Storage and Visualization Techniques},
  editor = {Segall, Richard and Cook, Jeffrey and Gupta, Neha},
  interhash = {d7349d1d9eaa86da465dd36b32bda9b5},
  intrahash = {2efc8814280e89f9b61c09db459dd96d},
  keywords = {BigData NoSQL},
  pages = {in press},
  publisher = {IDG Global},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Database Systems for Big Data Storage and Retrieval},
  year = 2016
}

@article{taatgen2010present,
  abstract = {Cognitive architectures are theories of cognition that try to capture the essential representations and mechanisms that underlie cognition. Research in cognitive architectures has gradually moved from a focus on the functional capabilities of architectures to the ability to model the details of human behavior, and, more recently, brain activity. Although there are many different architectures, they share many identical or similar mechanisms, permitting possible future convergence. In judging the quality of a particular cognitive model, it is pertinent to not just judge its fit to the experimental data but also its simplicity and ability to make predictions.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Taatgen, Niels and Anderson, John R.},
  biburl = {https://www.bibsonomy.org/bibtex/29257832058168d446a2638026167039f/vngudivada},
  doi = {10.1111/j.1756-8765.2009.01063.x},
  interhash = {67e2016346229ccf033f985379abb02f},
  intrahash = {9257832058168d446a2638026167039f},
  journal = {Topics in Cognitive Science},
  keywords = {CognitiveArchitecture CognitiveModeling},
  number = 4,
  pages = {693--704},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The Past, Present, and Future of Cognitive Architectures},
  volume = 2,
  year = 2010
}

@article{kay1984sapirwhorf,
  abstract = {The history of empirical research on the Sapir-Whorf hypothesis is reviewed. A more sensitive test of the hypothesis is devised and a clear Whorfian effect is detected in the domain of color. A specific mechanism is proposed to account for this effect and a second experiment, designed to block the hypothesized mechanism, is performed. The effect disappears as predicted. The Sapir-Whorf hypothesis is reevaluated in the light of these results.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Kay, Paul and Kempton, Willett},
  biburl = {https://www.bibsonomy.org/bibtex/2cd109b63cbce21c43db9e4adf89433e1/vngudivada},
  interhash = {f4c3de652e36da82d1d9950eadb54325},
  intrahash = {cd109b63cbce21c43db9e4adf89433e1},
  journal = {American Anthropologist},
  keywords = {Sapir-WhorfHypothesis},
  pages = {65 - 79},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {What Is the Sapir-Whorf Hypothesis?},
  year = 1984
}

@inbook{snaider2011framework,
  abstract = {Intelligent software agents aiming for general intelligence are likely to be exceedingly complex systems and, as such, will be difficult to implement and to customize. Frameworks have been applied successfully in large-scale software engineering applications. A framework constitutes the skeleton of the application, capturing its generic functionality. Frameworks are powerful as they promote code reusability and significantly reduce the amount of effort necessary to develop customized applications. They are well suited for the implementation of AGI software agents. Here we describe the LIDA framework, a customizable implementation of the LIDA model of cognition. We argue that its characteristics make it suitable for wider use in developing AGI cognitive architectures.},
  added-at = {2016-07-02T21:25:24.000+0200},
  address = {Berlin, Heidelberg},
  author = {Snaider, Javier and McCall, Ryan and Franklin, Stan},
  biburl = {https://www.bibsonomy.org/bibtex/24a80a9a9799dd6c7b59fbc8ad5c552b9/vngudivada},
  booktitle = {Artificial General Intelligence: 4th International Conference, AGI 2011},
  doi = {10.1007/978-3-642-22887-2_14},
  editor = {Schmidhuber, J{\"u}rgen and Th{\'o}risson, Kristinn R. and Looks, Moshe},
  interhash = {5c2d1f371e7aa4441c96bbd36d475797},
  intrahash = {4a80a9a9799dd6c7b59fbc8ad5c552b9},
  keywords = {ArtificialGeneralIntelligence LIDA},
  pages = {133--142},
  publisher = {Springer},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The LIDA Framework as a General Tool for AGI},
  year = 2011
}

@article{hays2007scene,
  abstract = {What can you do with a million images? In this paper we present a new image completion algorithm powered by a huge database of photographs gathered from the Web. The algorithm patches up holes in images by finding similar image regions in the database that are not only seamless but also semantically valid. Our chief insight is that while the space of images is effectively infinite, the space of semantically differentiable scenes is actually not that large. For many image completion tasks we are able to find similar scenes which contain image fragments that will convincingly complete the image. Our algorithm is entirely data-driven, requiring no annotations or labelling by the user. Unlike existing image completion methods, our algorithm can generate a diverse set of image completions and we allow users to select among them. We demonstrate the superiority of our algorithm over existing image completion approaches.},
  added-at = {2016-07-02T21:16:48.000+0200},
  address = {New York, NY, USA},
  annotation = {What can you do with a million images? In this paper we present a new image completion algorithm powered by a huge database of photographs gathered from the Web. The algorithm patches up holes in images by finding similar image regions in the database that are not only seamless but also semantically valid. Our chief insight is that while the space of images is effectively infinite, the space of semantically differentiable scenes is actually not that large. For many image completion tasks we are able to find similar scenes which contain image fragments that will convincingly complete the image. Our algorithm is entirely data-driven, requiring no annotations or labeling by the user. Unlike existing image completion methods, our algorithm can generate a diverse set of results for each input image and we allow users to select among them. We demonstrate the superiority of our algorithm over existing image completion approaches.},
  author = {Hays, James and Efros, Alexei A.},
  biburl = {https://www.bibsonomy.org/bibtex/23c6922c4d51a6ac16d1d2b3246164699/vngudivada},
  interhash = {14b4c5d079014159245fbeb4691cd3e4},
  intrahash = {3c6922c4d51a6ac16d1d2b3246164699},
  journal = {ACM Trans. Graph.},
  keywords = {ImageCompletion ImageCompositing ImageDatabase},
  month = jul,
  number = 3,
  publisher = {ACM},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Scene Completion Using Millions of Photographs},
  volume = 26,
  year = 2007
}

@article{matsuda2013cognitive,
  abstract = {This paper describes an advanced learning technology used to investigate hypotheses about learning by teaching. The proposed technology is an instance of a teachable agent, called SimStudent, that learns skills (e.g., for solving linear equations) from examples and from feedback on performance. SimStudent has been integrated into an on-line, game-like environment in which students act as “tutors” and can interactively teach SimStudent by providing it with examples and feedback. We conducted three classroom “in vivo” studies to better understand how and when students learn (or fail to learn) by teaching. One of the strengths of interactive technologies is their ability to collect detailed process data on the nature and timing of student activities. The primary purpose of this paper is to provide an in-depth analysis across three studies to understand the underlying cognitive and social factors that contribute to tutor learning by making connections between outcome and process data. The results show several key cognitive and social factors that are correlated with tutor learning. The accuracy of students’ responses (i.e., feedback and hints), the quality of students’ explanations during tutoring, and the appropriateness of tutoring strategy (i.e., problem selection) all positively affected SimStudent’s learning, which further positively affected students’ learning. The results suggest that implementing adaptive help for students on how to tutor and solve problems is a crucial component for successful learning by teaching.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Matsuda, N. and Yarzebinski, E. and Keiser, V. and Raizada, R. and William, W. C. and Stylianides, G. J. and Koedinge, K. R.},
  biburl = {https://www.bibsonomy.org/bibtex/2ca07e7675e3c144e91cec34b58e49600/vngudivada},
  interhash = {35817ca7e6b42da88a6718bd602b937e},
  intrahash = {ca07e7675e3c144e91cec34b58e49600},
  journal = {Journal of Educational Psychology},
  keywords = {CognitiveTutor SimStudent},
  number = 4,
  pages = {41152-1163},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Cognitive anatomy of tutor learning: Lessons learned with {SimStudent}},
  volume = 105,
  year = 2013
}

@book{vernon2014artificial,
  abstract = {This book offers a concise and accessible introduction to the emerging field of artificial cognitive systems. Cognition, both natural and artificial, is about anticipating the need for action and developing the capacity to predict the outcome of those actions. Drawing on artificial intelligence, developmental psychology, and cognitive neuroscience, the field of artificial cognitive systems has as its ultimate goal the creation of computer-based systems that can interact with humans and serve society in a variety of ways. This primer brings together recent work in cognitive science and cognitive robotics to offer readers a solid grounding on key issues.

The book first develops a working definition of cognitive systems -- broad enough to encompass multiple views of the subject and deep enough to help in the formulation of theories and models. It surveys the cognitivist, emergent, and hybrid paradigms of cognitive science and discusses cognitive architectures derived from them. It then turns to the key issues, with chapters devoted to autonomy, embodiment, learning and development, memory and prospection, knowledge and representation, and social cognition. Ideas are introduced in an intuitive, natural order, with an emphasis on the relationships among ideas and building to an overview of the field. The main text is straightforward and succinct; sidenotes drill deeper on specific topics and provide contextual links to further reading.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Vernon, David},
  biburl = {https://www.bibsonomy.org/bibtex/2ec1195446bed5b810fadabf14bd7eafc/vngudivada},
  interhash = {9fc4e8ba4a9c0cebb974ddf19bafc7a2},
  intrahash = {ec1195446bed5b810fadabf14bd7eafc},
  keywords = {Book CognitiveSystem},
  publisher = {The MIT Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Artificial Cognitive Systems: A Primer},
  year = 2014
}

@misc{harnard2003symbol,
  abstract = {How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) "iconic representations," which are analogs of the proximal sensory projections of distal objects and events, and (2) "categorical representations," which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) "symbolic representations," grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., "An X is a Y that is Z").},
  added-at = {2016-07-02T21:16:48.000+0200},
  author = {Harnard, Stevan},
  biburl = {https://www.bibsonomy.org/bibtex/284de532d03ba1067ad811b4ac1866a66/vngudivada},
  interhash = {92745a92308bf09973ff643eb34095b5},
  intrahash = {84de532d03ba1067ad811b4ac1866a66},
  journal = {Encyclopedia of Cognitive Science},
  keywords = {SymbolGroundingProblem},
  note = {{Nature Publishing Group/Macmillan}},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The Symbol Grounding Problem},
  year = 2003
}

@article{liang2015bringing,
  abstract = {Computational semantics has long been considered a field divided between logical and statistical approaches, but this divide is rapidly eroding with the development of statistical models that learn compositional semantic theories from corpora and databases. This review presents a simple discriminative learning framework for defining such models and relating them to logical theories. Within this framework, we discuss the task of learning to map utterances to logical forms (semantic parsing) and the task of learning from denotations with logical forms as latent variables. We also consider models that use distributed (e.g., vector) representations rather than logical ones, showing that these can be considered part of the same overall framework for understanding meaning and structural complexity.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Liang, Percy and Potts, Christopher},
  biburl = {https://www.bibsonomy.org/bibtex/24d35833cf961733239a5a70e1fd8692f/vngudivada},
  interhash = {dead96084433a180b899aa3a3aa56460},
  intrahash = {4d35833cf961733239a5a70e1fd8692f},
  journal = {Annual Review of Linguistics},
  keywords = {CompositionalSemantics MachineLearning},
  number = 1,
  pages = {355--376},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Bringing Machine Learning and Compositional Semantics Together},
  volume = 1,
  year = 2015
}

@inproceedings{kormushev2010learning,
  abstract = {We present an integrated approach allowing the humanoid robot iCub to learn the skill of archery. After being instructed how to hold the bow and release the arrow, the robot learns by itself to shoot the arrow in such a way that it hits the center of the target. Two learning algorithms are proposed and compared to learn the bi-manual skill: one with Expectation-Maximization based Reinforcement Learning, and one with chained vector regression called the ARCHER algorithm. Both algorithms are used to modulate and coordinate the motion of the two hands, while an inverse kinematics controller is used for the motion of the arms. The image processing part recognizes where the arrow hits the target and is based on Gaussian Mixture Models for color-based detection of the target and the arrow's tip. The approach is evaluated on a 53-DOF humanoid robot iCub.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {Nashville, USA},
  author = {Kormushev, Petar and Calinon, S. and Saegusa, R. and Metta, G.},
  biburl = {https://www.bibsonomy.org/bibtex/2bcecc7c75052faa35626f4ca834b8f5c/vngudivada},
  booktitle = {Proc. {IEEE} Intl Conf. on Humanoid Robots ({H}umanoids)},
  interhash = {04964822defc5034a71bcc6ff9413f4a},
  intrahash = {bcecc7c75052faa35626f4ca834b8f5c},
  keywords = {Archery HumanoidRobot iCub},
  month = {December},
  pages = {417-423},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Learning the skill of archery by a humanoid robot {iCub}},
  year = 2010
}

@article{kim2015reconfigurable,
  abstract = {This article presents a brain-inspired reconfigurable digital neuromorphic processor (DNP) architecture for large-scale spiking neural networks. The proposed architecture integrates an arbitrary number of N digital leaky integrate-and-fire (LIF) silicon neurons to mimic their biological counterparts and on-chip learning circuits to realize spike-timing-dependent plasticity (STDP) learning rules. We leverage memristor nanodevices to build an N×N crossbar array to store not only multibit synaptic weight values but also network configuration data with significantly reduced area overhead. Additionally, the crossbar array is designed to be accessible both column- and row-wise to expedite the synaptic weight update process for learning. The proposed digital pulse width modulator (PWM) produces binary pulses with various durations for reading and writing the multilevel memristive crossbar. The proposed column based analog-to-digital conversion (ADC) scheme efficiently accumulates the presynaptic weights of each neuron and reduces silicon area overhead by using a shared arithmetic unit to process the LIF operations of all N neurons. With 256 silicon neurons, learning circuits and 64K synapses, the power dissipation and area of our DNP are 6.45 mW and 1.86 mm2, respectively, when implemented in a 90-nm CMOS technology. The functionality of the proposed DNP architecture is demonstrated by realizing an unsupervised-learning based character recognition system.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {New York, NY, USA},
  author = {Kim, Yongtae and Zhang, Yong and Li, Peng},
  biburl = {https://www.bibsonomy.org/bibtex/24eef17dde39f89f45eeda7cbe5a15cec/vngudivada},
  doi = {10.1145/2700234},
  interhash = {5130c8cc33ee727e1e2b33441c46cc24},
  intrahash = {4eef17dde39f89f45eeda7cbe5a15cec},
  journal = {J. Emerg. Technol. Comput. Syst.},
  keywords = {Memristor NeuromorphicProcessor SynapticCrossbarArray},
  month = apr,
  number = 4,
  pages = {38:1--38:25},
  publisher = {ACM},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {A Reconfigurable Digital Neuromorphic Processor with Memristive Synaptic Crossbar for Cognitive Computing},
  volume = 11,
  year = 2015
}

@article{silver2016mastering,
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2e3bd772f62209ea8283e242f993d3edf/vngudivada},
  doi = {10.1038/nature16961},
  interhash = {48430c7891aaf9fe2582faa8f5d076c1},
  intrahash = {e3bd772f62209ea8283e242f993d3edf},
  journal = {Nature},
  keywords = {DeepLearning GoGame},
  month = jan,
  number = 7587,
  pages = {484--489},
  publisher = {Nature Publishing Group},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {{Mastering the game of Go with deep neural networks and tree search}},
  volume = 529,
  year = 2016
}

@inproceedings{taatgen2013between,
  abstract = {One major limitation of current cognitive architectures is that models are typically constructed in an "empty" architecture, and that the knowledge specifications (typically production rules) are specific to the particular task. This means that general executive control strategies have to be implemented for each specific model, which means a lack of consistency and constraint. Alternatively, they are implemented as part of the architecture itself, which is often implausible, because strategies are learned and differ among individuals. The alternative is to assume executive control consists of strategies that can transfer from one task to another. The PRIMs theory (Taatgen 2013) provides a modeling framework for this transfer. The approach is discussed using the example of working memory control.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Taatgen, Niels A.},
  biburl = {https://www.bibsonomy.org/bibtex/2e405e785df2fbeef7073debb5e3348e2/vngudivada},
  booktitle = {AAAI Fall Symposium Series},
  interhash = {6e75238789d510bee3aa9a3bfe3ad186},
  intrahash = {e405e785df2fbeef7073debb5e3348e2},
  keywords = {CognitiveArchitecture CognitiveModel},
  pages = {95 - 101},
  publisher = {Association for the Advancement of Artificial
Intelligence},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The Gap between Architecture and Model: Strategies for Executive Control},
  year = 2013
}

@inproceedings{wen2016learning,
  abstract = {IBM TrueNorth chip uses digital spikes to perform neuromorphic computing and achieves ultrahigh execution parallelism and power efficiency. However, in TrueNorth chip, low quantization resolution of the synaptic weights and spikes significantly limits the inference (e.g., classification) accuracy of the deployed neural network model. Existing workaround, i.e., averaging the results over multiple copies instantiated in spatial and temporal domains, rapidly exhausts the hardware resources and slows down the computation. In this work, we propose a novel learning method on TrueNorth platform that constrains the random variance of each computation copy and reduces the number of needed copies. Compared to the existing learning method, our method can achieve up to 68.8% reduction of the required neuro-synaptic cores or 6.5× speedup, with even slightly improved inference accuracy.},
  added-at = {2016-07-02T21:25:24.000+0200},
  address = {New York, NY, USA},
  author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Nixon, Kent and Wu, Qing and Barnell, Mark and Li, Hai and Chen, Yiran},
  biburl = {https://www.bibsonomy.org/bibtex/21094356466dbcc1a2a4e8d49609cf6f6/vngudivada},
  booktitle = {Proceedings of the 53rd Annual Design Automation Conference},
  doi = {10.1145/2897937.2897968},
  interhash = {af0fbc83fd3f02e049a806fc753159b3},
  intrahash = {1094356466dbcc1a2a4e8d49609cf6f6},
  keywords = {NeuromorphicComputing TrueNorth},
  pages = {18:1--18:6},
  publisher = {ACM},
  series = {DAC '16},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {A New Learning Method for Inference Accuracy, Core Occupation, and Performance Co-optimization on TrueNorth Chip},
  year = 2016
}

@article{martinez2007fully,
  abstract = {We propose a new multistage procedure for a real-time brain-machine/computer interface (BCI). The developed system allows a
BCI user to navigate a small car (or any other object) on the computer screen in real time, in any of the four directions, and to stop
it if necessary. Extensive experiments with five young healthy subjects confirmed the high performance of the proposed online BCI
system. The modular structure, high speed, and the optimal frequency band characteristics of the BCI platform are features which
allow an extension to a substantially higher number of commands in the near future.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {New York, USA},
  author = {Martinez, Pablo and Bakardjian, Hovagim and Cichocki, Andrzej},
  biburl = {https://www.bibsonomy.org/bibtex/299407d24a8bb096a625b61f9fa38caef/vngudivada},
  doi = {10.1155/2007/94561},
  interhash = {4a2e6050352689d7bcabadcbc0ebed41},
  intrahash = {99407d24a8bb096a625b61f9fa38caef},
  journal = {Intell. Neuroscience},
  keywords = {BCI},
  month = jan,
  pages = {1 - 9},
  publisher = {Hindawi Publishing Corp.},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Fully Online Multicommand Brain-computer Interface with Visual Neurofeedback Using SSVEP Paradigm},
  volume = 2007,
  year = 2007
}

@article{miller1995wordnet,
  abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {New York, NY, USA},
  author = {Miller, George A.},
  biburl = {https://www.bibsonomy.org/bibtex/276ce362b2a79a7da4e2cb28666fcaeef/vngudivada},
  doi = {10.1145/219717.219748},
  interhash = {a71daa7f928392f119792e0165543533},
  intrahash = {76ce362b2a79a7da4e2cb28666fcaeef},
  journal = {Commun. ACM},
  keywords = {WordNet},
  month = nov,
  number = 11,
  pages = {39--41},
  publisher = {ACM},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {WordNet: A Lexical Database for English},
  volume = 38,
  year = 1995
}

@book{neisser2014cognitive,
  abstract = {First published in 1967, this seminal volume by Ulric Neisser was the first attempt at a comprehensive and accessible survey of Cognitive Psychology; as such, it provided the field with its first true textbook.

Its chapters are organized so that they began with stimulus information that came 'inward' through the organs of sense, through its many transformations and reconstructions, and finally through to its eventual use in thought and memory.

The volume inspired numerous students enter the field of cognitive psychology and some of the today's leading and most respected cognitive psychologists cite Neisser's book as the reason they embarked on their careers.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Neisser, Ulric},
  biburl = {https://www.bibsonomy.org/bibtex/2956a53c3045d4e8c8fb9af57d9274e97/vngudivada},
  interhash = {0f691594931f4b74219291e9ca693e2f},
  intrahash = {956a53c3045d4e8c8fb9af57d9274e97},
  keywords = {Book CognitivePsychology},
  publisher = {Psychology Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Cognitive Psychology, Classic Edition},
  year = 2014
}

@book{purves2011neuroscience,
  abstract = {The book does an excellent job of bridging between molecular and molar levels of analysis, between basic and clinical science, and between the big picture and discrete detail. In other words, it keeps you grounded and allows you to understand how basic processes relate to cognition, emotion, and behavior, as well as to various forms of dysfunction.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Purves, Dale and Augustine, George J. and Fitzpatrick, David and Hall, William C. and LaMantia, Anthony-Samuel and White, Leonard E.},
  biburl = {https://www.bibsonomy.org/bibtex/25bb7849a781e9dba95a40f1594e1cf5c/vngudivada},
  edition = {Fifth},
  interhash = {051ce93413f9d7e2157c57b7725b4fa1},
  intrahash = {5bb7849a781e9dba95a40f1594e1cf5c},
  keywords = {Book NeuroScience},
  publisher = {Sinauer Associates},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Neuroscience},
  year = 2011
}

@book{ryza2015advanced,
  abstract = {In the second edition of this practical book, four Cloudera data scientists present a set of self-contained patterns for performing large-scale data analysis with Spark. The authors bring Spark, statistical methods, and real-world data sets together to teach you how to approach analytics problems by example. Updated for Spark 2.1, this edition acts as an introduction to these techniques and other best practices in Spark programming.

You'll start with an introduction to Spark and its ecosystem, and then dive into patterns that apply common techniques--including classification, clustering, collaborative filtering, and anomaly detection--to fields such as genomics, security, and finance.

If you have an entry-level understanding of machine learning and statistics, and you program in Java, Python, or Scala, you'll find the book's patterns useful for working on your own data applications.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Ryza, Sandy and Laserson, Uri and Owen, Sean and Wills, Josh},
  biburl = {https://www.bibsonomy.org/bibtex/21b6da9d94091273fdcc3e26894924ee3/vngudivada},
  interhash = {d74bddff9f40c08fd316b73070996f02},
  intrahash = {1b6da9d94091273fdcc3e26894924ee3},
  keywords = {Analytics DataScience Spark},
  publisher = {O'Reilly Media},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Advanced Analytics with Spark: Patterns for Learning from Data at Scale},
  year = 2015
}

@book{white2015hadoop,
  abstract = {Get ready to unlock the power of your data. With the fourth edition of this comprehensive guide, you’ll learn how to build and maintain reliable, scalable, distributed systems with Apache Hadoop. This book is ideal for programmers looking to analyze datasets of any size, and for administrators who want to set up and run Hadoop clusters.

    Using Hadoop 2 exclusively, author Tom White presents new chapters on YARN and several Hadoop-related projects such as Parquet, Flume, Crunch, and Spark. You will learn about recent changes to Hadoop, and explore new case studies on Hadoop's role in healthcare systems and genomics data processing.

    Learn fundamental components such as MapReduce, HDFS, and YARN

    Explore MapReduce in depth, including steps for developing applications with it

    Set up and maintain a Hadoop cluster running HDFS and MapReduce on YARN

    Learn two data formats: Avro for data serialization and Parquet for nested data

    Use data ingestion tools such as Flume (for streaming data) and Sqoop (for bulk data transfer)

    Understand how high-level data processing tools like Pig, Hive, Crunch, and Spark work with Hadoop

    Learn the HBase distributed database and the ZooKeeper distributed configuration service},
  added-at = {2016-07-02T21:25:24.000+0200},
  address = {Sabastopol, California},
  author = {White, Tom},
  biburl = {https://www.bibsonomy.org/bibtex/238d4792bc89b9be52649a4c9c77a61e3/vngudivada},
  edition = {Fourth},
  interhash = {21fb318b1d517bf5d77efbd6251f0684},
  intrahash = {38d4792bc89b9be52649a4c9c77a61e3},
  keywords = {Book DataScience Hadoop},
  publisher = {O'Reilly Media},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Hadoop: The Definitive Guide},
  year = 2015
}

@article{laughlin2003communication,
  abstract = {Brains perform with remarkable efficiency, are capable of prodigious computation, and are marvels of communication. We are beginning to understand some of the geometric, biophysical, and energy constraints that have governed the evolution of cortical networks. To operate efficiently within these constraints, nature has optimized the structure and function of cortical networks with design principles similar to those used in electronic networks. The brain also exploits the adaptability of biological systems to reconfigure in response to changing needs.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {New York, N.Y.},
  author = {Laughlin, Simon B. and Sejnowski, Terrence J.},
  biburl = {https://www.bibsonomy.org/bibtex/2be5a339ca1dd3e81b21f31d026046f74/vngudivada},
  interhash = {e70f5649cd467a2ec18476fb911e0508},
  intrahash = {be5a339ca1dd3e81b21f31d026046f74},
  journal = {Science},
  keywords = {NeuralNetwork},
  number = 5641,
  pages = {1870 - 1874},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Communication in Neuronal Networks},
  volume = 301,
  year = 2003
}

@article{langley2009cognitive,
  abstract = {In this paper, we examine the motivations for research on cognitive architectures and review some candidates that have been explored in the literature. After this, we consider the capabilities that a cognitive architecture should support, some properties that it should exhibit related to representation, organization, performance, and learning, and some criteria for evaluating such architectures at the systems level. In closing, we discuss some open issues that should drive future research in this important area.},
  added-at = {2016-07-02T21:21:33.000+0200},
  address = {Amsterdam, The Netherlands, The Netherlands},
  author = {Langley, Pat and Laird, John E. and Rogers, Seth},
  biburl = {https://www.bibsonomy.org/bibtex/2efbdb8586e2191908d1d1dac965fe725/vngudivada},
  doi = {10.1016/j.cogsys.2006.07.004},
  interhash = {0593ef5c2f21053b1e4fc3a57d22a31f},
  intrahash = {efbdb8586e2191908d1d1dac965fe725},
  journal = {Cogn. Syst. Res.},
  keywords = {CognitiveArchitecture CognitiveProcesse IntelligentSystem},
  month = jun,
  number = 2,
  pages = {141--160},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Cognitive Architectures: Research Issues and Challenges},
  volume = 10,
  year = 2009
}

@article{koedinger1997intelligent,
  abstract = {This paper reports on a large-scale experiment introducing and evaluating intelligent tutoring in an urban High School setting. Critical to the success of this project has been a client-centered design approach that has matched our client's expertise in curricular objectives and classroom teaching with our expertise in artificial intelligence and cognitive psychology. The Pittsburgh Urban Mathematics Project (PUMP) has produced an algebra curriculum that is centrally focused on mathematical analysis of real world situations and the use of computational tools. We have built an intelligent tutor, called PAT, that supports this curriculum and has been made a regular part of 9th grade Algebra in 3 Pittsburgh schools. In the 1993-94 school year, we evaluated the effect of the PUMP curriculum and PAT tutor use. On average the 470 students in experimental classes outperformed students in comparison classes by 15% on standardized tests and 100% on tests targeting the PUMP objectives. This study provides further evidence that laboratory tutoring systems can be scaled up and made to work, both technically and pedagogically, in real and unforgiving settings like urban high schools.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Koedinger, Kenneth R. and Anderson, John R. and Hadley, William H. and Mark, Mary A.},
  biburl = {https://www.bibsonomy.org/bibtex/29e1e03ceb3f51dbb8cd4c78ba3529103/vngudivada},
  interhash = {845274b33efc1afc85a660603dd8ba5c},
  intrahash = {9e1e03ceb3f51dbb8cd4c78ba3529103},
  journal = {International Journal of Artificial Intelligence in Education},
  keywords = {CognitiveTutor},
  pages = {30--43},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Intelligent Tutoring Goes to School in the Big City},
  year = 1997
}

@article{noor2014potential,
  abstract = {Cognitive computing and cognitive technologies are game changers for future engineering systems, as well as for engineering practice and training. They are major drivers for knowledge automation work, and the creation of cognitive products with higher levels of intelligence than current smart products. This paper gives a brief review of cognitive computing and some of the cognitive engineering systems activities. The potential of cognitive technologies is outlined, along with a brief description of future cognitive environments, incorporating cognitive assistants - specialized proactive intelligent software agents designed to follow and interact with humans and other cognitive assistants across the environments. The cognitive assistants engage, individually or collectively, with humans through a combination of adaptive multimodal interfaces, and advanced visualization and navigation techniques. The realization of future cognitive environments requires the development of a cognitive innovation ecosystem for the engineering workforce. The continuously expanding major components of the ecosystem include integrated knowledge discovery and exploitation facilities (incorporating predictive and prescriptive big data analytics); novel cognitive modeling and visual simulation facilities; cognitive multimodal interfaces; and cognitive mobile and wearable devices. The ecosystem will provide timely, engaging, personalized / collaborative, learning and effective decision making. It will stimulate creativity and innovation, and prepare the participants to work in future cognitive enterprises and develop new cognitive products of increasing complexity.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Noor, Ahmed K.},
  biburl = {https://www.bibsonomy.org/bibtex/258b99433fc892de7e5c2dc18359eefe1/vngudivada},
  doi = {DOI: 10.1515/eng-2015-0008},
  interhash = {c300d58be0d2be336fd937714d413d94},
  intrahash = {58b99433fc892de7e5c2dc18359eefe1},
  journal = {pen Engineering},
  keywords = {CognitiveComputing CognitiveSystem},
  number = 1,
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Potential of Cognitive Computing and Cognitive Systems},
  volume = 5,
  year = 2014
}

@article{mcclelland2015cognitive,
  abstract = {Cognitive neuroscience explores the neural basis of cognition, including perception, attention, language understanding, memory, problem solving, and decision-making. The field draws on findings on how neurons process and represent information, and on ideas about how learning may occur through the modification of properties of neurons and their connections. It is clear that there is specialization of function in the brain, yet brain areas appear to work together, interactively, to support emergent cognitive functions. The article discusses these points, reviews research methods used in this field, and touches on open questions, as well as the future of cognitive neuroscience.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {McClelland, James L and Ralph, Matthew AL},
  biburl = {https://www.bibsonomy.org/bibtex/2f0da70e283b328967002db65ee76f613/vngudivada},
  interhash = {090755d92fbe0f24603089fa296f1a3d},
  intrahash = {f0da70e283b328967002db65ee76f613},
  journal = {International Encyclopedia of the Social \& Behavioral Sciences},
  keywords = {CognitiveNeuroscience},
  note = {Second Edition},
  pages = {95–102},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Cognitive Neuroscience},
  volume = 4,
  year = 2015
}

@article{wu2014cognitive,
  abstract = {Current research on Internet of Things (IoT) mainly focuses on how to enable general objects to see, hear, and smell the physical world for themselves, and make them connected to share the observations. In this paper, we argue that only connected is not enough, beyond that, general objects should have the capability to learn, think, and understand both physical and social worlds by themselves. This practical need impels us to develop a new paradigm, named Cognitive Internet of Things (CIoT), to empower the current IoT with a `brain' for high-level intelligence. Specifically, we first present a comprehensive definition for CIoT, primarily inspired by the effectiveness of human cognition. Then, we propose an operational framework of CIoT, which mainly characterizes the interactions among five fundamental cognitive tasks: perception-action cycle, massive data analytics, semantic derivation and knowledge discovery, intelligent decision-making, and on-demand service provisioning. Furthermore, we provide a systematic tutorial on key enabling techniques involved in the cognitive tasks. In addition, we also discuss the design of proper performance metrics on evaluating the enabling techniques. Last but not least, we present the research challenges and open issues ahead. Building on the present work and potentially fruitful future studies, CIoT has the capability to bridge the physical world (with objects, resources, etc.) and the social world (with human demand, social behavior, etc.), and enhance smart resource allocation, automatic network operation, and intelligent service provisioning.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Wu, Qihui and Ding, Guoru and Xu, Yuhua and Feng, Shuo and Du, Zhiyong and Wang, Jinlong and Long, Keping},
  biburl = {https://www.bibsonomy.org/bibtex/20d7003aeba8fff03202c9bcc9e0af3ad/vngudivada},
  interhash = {9912577df25b05a086e847399ad36fe6},
  intrahash = {0d7003aeba8fff03202c9bcc9e0af3ad},
  journal = {CoRR},
  keywords = {CognitiveIoT},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Cognitive Internet of Things: {A} New Paradigm beyond Connection},
  url = {http://arxiv.org/abs/1403.2498},
  volume = {abs/1403.2498},
  year = 2014
}

@article{rosenbloom2013sigma,
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Rosenbloom, Paul S.},
  biburl = {https://www.bibsonomy.org/bibtex/258cc49c1de072da07b3e615b35bd4d27/vngudivada},
  interhash = {77cf7f85b72ff98702c446d4e3adfbe6},
  intrahash = {58cc49c1de072da07b3e615b35bd4d27},
  journal = {AISB Quarterly},
  keywords = {CognitiveArchitecture},
  month = may,
  pages = {4--13},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The {Sigma} cognitive architecture and system},
  year = 2013
}

@book{nkambou2010advances,
  abstract = {The idea for this book on Intelligent Tutoring Systems (ITS) was sparked by the success of the ITS08 international conference. The number of presentations and their quality bore witness to the vitality and maturity of the field, and the enthusiasm of the participants held out a promise of sustainability and innovative research. Long life to ITS research! The book is divided into five parts. The introductory chapters to these parts, which summarize foundations, developments, strengths and weaknesses in each of the areas covered, are addressed to all readers. For those who want more in-depth knowledge, we give the floor to researchers who present their work, their results, and their view of what the future holds. It is our hope that all readers will find the book informative and thought-provoking.},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Nkambou, Roger and Bourdeau, Jacqueline and Mizoguchi, Riichiro},
  biburl = {https://www.bibsonomy.org/bibtex/2ad2870ecb6f96adaa3e36d939696c230/vngudivada},
  interhash = {31bdbae33ae91828c6fa92e04c4181d8},
  intrahash = {ad2870ecb6f96adaa3e36d939696c230},
  keywords = {CognitiveTutor ITS},
  publisher = {Springer},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Advances in Intelligent Tutoring Systems},
  year = 2010
}

@inbook{stephan2007knowledge,
  abstract = {In Artificial Intelligence, knowledge representation studies the formalisation of knowledge and its processing within machines. Techniques of automated reasoning allow a computer system to draw conclusions from knowledge represented in a machine-interpretable form. Recently, ontologies have evolved in computer science as computational artefacts to provide computer systems with a conceptual yet computational model of a particular domain of interest. In this way, computer systems can base decisions on reasoning about domain knowledge, similar to humans. This chapter gives an overview on basic knowledge representation aspects and on ontologies as used within computer systems. After introducing ontologies in terms of their appearance, usage and classification, it addresses concrete ontology languages that are particularly important in the context of the Semantic Web. The most recent and predominant ontology languages and formalisms are presented in relation to each other and a selection of them is discussed in more detail.},
  added-at = {2016-07-02T21:25:24.000+0200},
  address = {Berlin, Heidelberg},
  author = {Stephan, Grimm and Pascal, Hitzler and Andreas, Abecker},
  biburl = {https://www.bibsonomy.org/bibtex/2a9d34c56545f9daaad771419faa7b9b3/vngudivada},
  booktitle = {Semantic Web Services: Concepts, Technologies, and Applications},
  doi = {10.1007/3-540-70894-4_3},
  editor = {Studer, Rudi and Grimm, Stephan and Abecker, Andreas},
  interhash = {b23ed78d8687bde69fdb6e15611f5906},
  intrahash = {a9d34c56545f9daaad771419faa7b9b3},
  keywords = {Ontology},
  pages = {51--105},
  publisher = {Springer},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Knowledge Representation and Ontologies},
  year = 2007
}

@article{ling2015finding,
  abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
  added-at = {2016-07-02T21:21:33.000+0200},
  author = {Ling, Wang and Lu{\'{\i}}s, Tiago and Marujo, Lu{\'{\i}}s and Astudillo, Ram{\'{o}}n Fernandez and Amir, Silvio and Dyer, Chris and Black, Alan W. and Trancoso, Isabel},
  biburl = {https://www.bibsonomy.org/bibtex/2c90897a49f048834e2a5fb78e7cefbb3/vngudivada},
  interhash = {79c2751f76e22c205f44122fe9f3893d},
  intrahash = {c90897a49f048834e2a5fb78e7cefbb3},
  journal = {CoRR},
  keywords = {POS WordRepresentation},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation},
  url = {http://arxiv.org/abs/1508.02096},
  volume = {abs/1508.02096},
  year = 2015
}

@article{schatsky2015cognitive,
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Schatsky, David and Muraskin, Craig and Gurumurthy, Ragu},
  biburl = {https://www.bibsonomy.org/bibtex/2e62367a2b1a4cf1452c76d43a91a63d7/vngudivada},
  interhash = {73a8fed489b253fc81a5f50d3b8269e0},
  intrahash = {e62367a2b1a4cf1452c76d43a91a63d7},
  journal = {Deloitte Review},
  keywords = {CognitiveComputing},
  number = 16,
  pages = {113 - 129},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Cognitive technologies: The real opportunities for business},
  year = 2015
}

@article{ibm2016watson,
  added-at = {2016-07-02T21:34:27.000+0200},
  author = {IBM},
  biburl = {https://www.bibsonomy.org/bibtex/21104831c1ec683ae0912a0ace731e53c/vngudivada},
  interhash = {31c1676295d76c85f40ee8483bb517d9},
  intrahash = {1104831c1ec683ae0912a0ace731e53c},
  keywords = {IBMWatson},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {IBM Watson Developer Cloud (WDC)},
  url = {http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/doc/getting_started/},
  urldate = {2016-06-27},
  year = 2016
}

@article{cheng1994neural,
  abstract = {This paper informs a statistical readership about Artificial Neural Networks (ANNs), points out some of the links with statistical methodology and encourages cross-disciplinary research in the directions most likely to bear fruit. The areas of statistical interest are briefly outlined, and a series of examples indicates the flavor of ANN models. We then treat various topics in more depth. In each case, we describe the neural network architectures and training rules and provide a statistical commentary. The topics treated in this way are perceptrons (from single-unit to multilayer versions), Hopfield-type recurrent networks (including probabilistic versions strongly related to statistical physics and Gibbs distributions) and associative memory networks trained by so-called unsuperviszd learning rules. Perceptrons are shown to have strong associations with discriminant analysis and regression, and unsupervised networks with cluster analysis. The paper concludes with some thoughts on the future of the interface between neural networks and statistics.},
  added-at = {2016-07-08T04:43:10.000+0200},
  author = {Cheng, Bing and Titterington, D. M.},
  biburl = {https://www.bibsonomy.org/bibtex/23e7a0e100556b34bdf87980c285571b5/vngudivada},
  interhash = {20ae118c35f12a3116ca95488cb3e50d},
  intrahash = {3e7a0e100556b34bdf87980c285571b5},
  journal = {Statistical Science},
  keywords = {NeuralNetwork Statistics},
  month = feb,
  number = 1,
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {{Neural Networks: A Review from a Statistical Perspective}},
  volume = 9,
  year = 1994
}

@inproceedings{presutti2012knowledge,
  abstract = {We have implemented a novel approach for robust ontology design from natural language texts by combining Discourse Representation Theory (DRT), linguistic frame semantics, and ontology design patterns. We show that DRT-based frame detection is feasible by conducting a comparative evaluation of our approach and existing tools. Furthermore, we define a mapping between DRT and RDF/OWL for the production of quality linked data and ontologies, and present FRED, an online tool for converting text into internally well-connected and linked-data-ready ontologies in web-service-acceptable time.},
  added-at = {2016-07-10T18:04:57.000+0200},
  author = {Presutti, Valentina and Draicchio, Francesco and Gangemi, Aldo},
  biburl = {https://www.bibsonomy.org/bibtex/2d46f6226b815d4d4a3b025e478d487f1/vngudivada},
  booktitle = {Proceedings of the 18th International Conference on Knowledge Engineering and Knowledge Management},
  doi = {10.1007/978-3-642-33876-2_12},
  interhash = {2d59a36bbeb4a3bf96b52a3a2bc9a700},
  intrahash = {d46f6226b815d4d4a3b025e478d487f1},
  keywords = {DiscourseRepresentation NLU OntologyDesign SemanticWeb},
  location = {Galway City, Ireland},
  pages = {114--129},
  publisher = {Springer-Verlag},
  series = {EKAW'12},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Knowledge Extraction Based on Discourse Representation Theory and Linguistic Frames},
  year = 2012
}

@article{neuroware2016neurocam,
  added-at = {2016-07-02T21:33:17.000+0200},
  author = {Neuroware},
  biburl = {https://www.bibsonomy.org/bibtex/20c4e2a6671196d8a4b67c395353b6387/vngudivada},
  interhash = {e3ebb0e504b313ee2dde0891764a39fe},
  intrahash = {0c4e2a6671196d8a4b67c395353b6387},
  keywords = {Neurocam},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Neurocam - a head mounted camera that detects human emotions},
  url = {http://neurowear.com/projects_detail/neurocam.html},
  urldate = {2016-06-26},
  year = 2016
}

@article{thagard2009cognitive,
  abstract = {{Contrary to common views that philosophy is extraneous to cognitive science, this paper argues that philosophy has a crucial role to play in cognitive science with respect to generality and normativity. General questions include the nature of theories and explanations, the role of computer simulation in cognitive theorizing, and the relations among the different fields of cognitive science. Normative questions include whether human thinking should be Bayesian, whether decision making should maximize expected utility, and how norms should be established. These kinds of general and normative questions make philosophical reflection an important part of progress in cognitive science. Philosophy operates best, however, not with a priori reasoning or conceptual analysis, but rather with empirically informed reflection on a wide range of findings in cognitive science.}},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Thagard, Paul},
  biburl = {https://www.bibsonomy.org/bibtex/2b2e492d236ad71bbc39b30bf24f47ffa/vngudivada},
  doi = {10.1111/j.1756-8765.2009.01016.x},
  interhash = {fa4dd4a30b952c297721d4a92edbe2e0},
  intrahash = {b2e492d236ad71bbc39b30bf24f47ffa},
  journal = {Topics in Cognitive Science},
  keywords = {CognitiveScience},
  month = apr,
  number = 2,
  pages = {237--254},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {{Why Cognitive Science Needs Philosophy and Vice Versa}},
  volume = 1,
  year = 2009
}

@inbook{graimann2010braincomputer,
  added-at = {2016-07-03T16:28:43.000+0200},
  address = {Berlin, Heidelberg},
  author = {Graimann, Bernhard and Allison, Brendan and Pfurtscheller, Gert},
  biburl = {https://www.bibsonomy.org/bibtex/257dac658e38fbf051f2a7266f99ea30c/vngudivada},
  booktitle = {Brain-Computer Interfaces: Revolutionizing Human-Computer Interaction},
  doi = {10.1007/978-3-642-02091-9_1},
  editor = {Graimann, Bernhard and Pfurtscheller, Gert and Allison, Brendan},
  interhash = {12e0cbcde850d9b3fc5dd524de567346},
  intrahash = {57dac658e38fbf051f2a7266f99ea30c},
  isbn = {978-3-642-02091-9},
  keywords = {BCI HCI},
  pages = {1--27},
  publisher = {Springer Berlin Heidelberg},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Brain--Computer Interfaces: A Gentle Introduction},
  year = 2010
}

@article{tensorflow2016source,
  added-at = {2016-07-02T21:38:52.000+0200},
  author = {{TensorFlow}},
  biburl = {https://www.bibsonomy.org/bibtex/21c8e57da52853e277a678c5a4d6dc7a0/vngudivada},
  interhash = {7dd4482cf402c8f8bafe48af2a7bd115},
  intrahash = {1c8e57da52853e277a678c5a4d6dc7a0},
  keywords = {TensorFlow},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {An open source software library for numerical computation using data flow graphs},
  url = {https://www.tensorflow.org/},
  urldate = {2016-06-27},
  year = 2016
}

@article{trafton2005enabling,
  abstract = {We propose that an important aspect of human-robot interaction is perspective-taking. We show how perspective-taking occurs in a naturalistic environment (astronauts working on a collaborative project) and present a cognitive architecture for performing perspective-taking called Polyscheme. Finally, we show a fully integrated system that instantiates our theoretical framework within a working robot system. Our system successfully solves a series of perspective-taking problems and uses the same frames of references that astronauts do to facilitate collaborative problem solving with a person},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Trafton, J. G. and Cassimatis, N. L. and Bugajska, M. D. and Brock, D. P. and Mintz, F. E. and Schultz, A. C.},
  biburl = {https://www.bibsonomy.org/bibtex/2d8aaeb9592efcc49e05a23a1c53de137/vngudivada},
  interhash = {cfa0e365c2a1e5b3bbd41e3b94e7be01},
  intrahash = {d8aaeb9592efcc49e05a23a1c53de137},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
  keywords = {HumanRobotInteraction PerspectiveTaking},
  month = {July},
  number = 4,
  pages = {460-470},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Enabling effective human-robot interaction using perspective-taking in robots},
  volume = 35,
  year = 2005
}

@inproceedings{seo2015solving,
  abstract = {This paper introduces GEOS, the first automated system to solve unaltered SAT geometry questions by combining text understanding and diagram interpretation. We model the problem of understanding geometry questions as submodular optimization, and identify a formal problem description likely to be compatible with both the question text and diagram. GEOS then feeds the description to a geometric solver that attempts to determine the correct answer. In our experiments, GEOS achieves a 49% score on official SAT questions, and a score of 61% on practice questions.1 Finally, we show that by integrating textual and visual information, GEOS boosts the accuracy of dependency and semantic parsing of the question text.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Seo, Min Joon and Hajishirzi, Hannaneh and Farhadi, Ali and Etzioni, Oren and Malcolm, Clint},
  biburl = {https://www.bibsonomy.org/bibtex/211fd6b4c7f233f8e95896a97c5cc8c94/vngudivada},
  booktitle = {EMNLP},
  editor = {Màrquez, Lluís and Callison-Burch, Chris and Su, Jian and Pighin, Daniele and Marton, Yuval},
  interhash = {a6a6e2a38082752335b5870b77176c03},
  intrahash = {11fd6b4c7f233f8e95896a97c5cc8c94},
  keywords = {CognitiveComputing GeometryProblem},
  pages = {1466-1476},
  publisher = {The Association for Computational Linguistics},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Solving Geometry Problems: Combining Text and Diagram Interpretation.},
  year = 2015
}

@phdthesis{yosef2016uaida,
  abstract = {Recognizing and disambiguating entities such as people, organizations, events or places in natural language text are essential steps for many linguistic tasks such as information extraction and text categorization. A variety of named entity disambiguation methods have been proposed, but most of them focus on Wikipedia as a sole knowledge resource. This focus does not fit all application scenarios, and customization to the respective application domain is crucial. This dissertation addresses the problem of building an easily customizable system for named entity disambiguation. The first contribution is the development of a universal and flexible architecture that supports plugging in different knowledge resources. The second contribution is utilizing the flexible architecture to develop two domain-specific disambiguation systems. The third contribution is the design of a complete pipeline for building disambiguation systems for languages other than English that have poor annotated resources such as Arabic. The fourth contribution is a novel approach that performs fine-grained type classification of names in natural language text.},
  added-at = {2016-07-10T18:43:55.000+0200},
  author = {Yosef, Mohamed Amir},
  biburl = {https://www.bibsonomy.org/bibtex/241bd952b9c863a26df5330fd4868ff33/vngudivada},
  ee = {http://d-nb.info/1083894722},
  interhash = {c2f61f4a33bcaf7ee22077340bec87fe},
  intrahash = {41bd952b9c863a26df5330fd4868ff33},
  keywords = {NamedEntityRecognition},
  school = {Saarland University},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {U-AIDA: a customizable system for named entity recognition, classification, and disambiguation.},
  year = 2016
}

@article{merolla2014million,
  abstract = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.Science, this issue p. 668 Inspired by the brains structure, we have developed an efficient, scalable, and flexible non-von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
  added-at = {2016-07-09T23:14:11.000+0200},
  author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
  biburl = {https://www.bibsonomy.org/bibtex/2a9c9d9af79498c8e8d4da81d6d2d7363/vngudivada},
  description = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.},
  doi = {10.1126/science.1254642},
  interhash = {8a7070c39194f7da0f78f71b498284a5},
  intrahash = {a9c9d9af79498c8e8d4da81d6d2d7363},
  journal = {Science},
  keywords = {NueromorphicComputing TrueNorth cognitivecomputing},
  number = 6197,
  pages = {668--673},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {A million spiking-neuron integrated circuit with a scalable communication network and interface},
  volume = 345,
  year = 2014
}

@article{wigner1960unreasonable,
  abstract = {There is a story about two friends, who were classmates in high school, talking about their jobs. One of them became a statistician and was working on population trends. He showed a reprint to his former classmate. The reprint started, as usual, with the Gaussian distribution and the statistician explained to his former classmate the meaning of the symbols for the actual population, for the average population, and so on. His classmate was a bit incredulous and was not quite sure whether the statistician was pulling his leg. “How can you know that?” was his query. “And what is this symbol here?” “Oh,” said the statistician, “this is π.” “What is that?” “The ratio of the circumference of the circle to its diameter.” “Well, now you are pushing your joke too far,” said the classmate, “surely the population has nothing to do with the circumference of the circle.”},
  added-at = {2016-07-02T21:25:24.000+0200},
  annotation = {``Mathematics, rightly viewed, possesses not only truth, but supreme beauty, a beauty cold and austere, like that of sculpture, without appeal to any part of our weaker nature, without the gorgeous trappings of painting or music, yet sublimely pure, and capable of a stern perfection such as only the greatest art can show. The true spirit of delight, the exaltation, the sense of being more than Man, which is the touchstone of the highest excellence, is to be found in mathematics as surely as in poetry.'' --- Bertrand Russell, Study of Mathematics},
  author = {Wigner, Eugene},
  biburl = {https://www.bibsonomy.org/bibtex/2334f0f54a2d6aa6283d7419c58dc089b/vngudivada},
  interhash = {36ae900671a9a6529c85108f008ae982},
  intrahash = {334f0f54a2d6aa6283d7419c58dc089b},
  journal = {Communications in Pure and Applied Mathematics},
  keywords = {DataScience},
  month = {February},
  number = 1,
  pages = {1 - 14},
  publisher = {John Wiley \& Sons, Inc.},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The Unreasonable Effectiveness of Mathematics in the Natural Sciences},
  volume = 13,
  year = 1960
}

@article{williams2016brain,
  abstract = {Efficiently allocating shared resources in computer systems is critical to optimizing execution. Recently, a number of market-based solutions have been proposed to attack this problem. Some of them provide provable theoretical bounds to efficiency and/or fairness losses under market equilibrium. However, they are limited to markets with potentially important constraints, such as enforcing equal budget for all players, or curve-fitting players' utility into a specific function type. Moreover, they do not generally provide an intuitive "knob" to control efficiency vs. fairness. In this paper, we introduce two new metrics, Market Utility Range (MUR) and Market Budget Range (MBR), through which we provide for the first time theoretical bounds on efficiency and fairness of market equilibria under arbitrary budget assignments. We leverage this result and propose ReBudget, an iterative budget re-assignment algorithm that can be used to control efficiency vs. fairness at run-time. We apply our algorithm to a multi-resource allocation problem in multicore chips. Our evaluation using detailed execution-driven simulations shows that our budget re-assignment technique is intuitive, effective, and efficient. To protect existing systems from more advanced rowhammer attacks, we develop a software-based defense, ANVIL, which thwarts all known rowhammer attacks on existing systems. ANVIL detects rowhammer attacks by tracking the locality of DRAM accesses using existing hardware performance counters. Our detector identifies the rows being frequently accessed (i.e., the aggressors), then selectively refreshes the nearby victim rows to prevent hammering. Experiments running on real hardware with the SPEC2006 benchmarks show that ANVIL has less than a 1% false positive rate and an average slowdown of 1%. ANVIL is low-cost and robust, and our experiments indicate that it is an effective approach for protecting existing and future systems from even advanced rowhammer attacks. This paper proposes a novel architecture that breaks the serialization of hardware queues and enables the queued processors to perform lock-free synchronization in parallel. The architecture, called CASPAR, is able to (1) execute the CASes in the queued-up processors in parallel through eager forwarding of expected values, and (2) validate the CASes in parallel and dequeue groups of processors at a time. The result is highly-scalable synchronization. We evaluate CASPAR with simulations of a 64-core chip. Compared to existing proposals with hardware queues, CASPAR improves the throughput of kernels by 32% on average, and reduces the execution time of the sections considered in lock-free versions of applications by 47% on average. This makes these sections 2.5x faster than in the original applications.},
  added-at = {2016-07-02T21:25:24.000+0200},
  address = {New York, NY, USA},
  author = {Williams, R. Stanley},
  biburl = {https://www.bibsonomy.org/bibtex/28f18781725752d07b99ed9a461047a3c/vngudivada},
  doi = {10.1145/2954680.2872417},
  interhash = {2456fb9e39292c9bad3a1a9daf38bd80},
  intrahash = {8f18781725752d07b99ed9a461047a3c},
  journal = {SIGOPS Oper. Syst. Rev.},
  keywords = {CorticalArchitecture NeuromorphicComputing},
  month = mar,
  number = 2,
  pages = {295--295},
  publisher = {ACM},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Brain Inspired Computing},
  volume = 50,
  year = 2016
}

@book{samani2015cognitive,
  abstract = {The kimono-clad android robot that recently made its debut as the new greeter at the entrance of Tokyo's Mitsukoshi department store is just one example of the rapid advancements being made in the field of robotics. Cognitive robotics is an approach to creating artificial intelligence in robots by enabling them to learn from and respond to real-world situations, as opposed to pre-programming the robot with specific responses to every conceivable stimulus. Presenting the contributions of international experts from various disciplines within the field, Cognitive Robotics provides novel material and discusses advanced approaches in the field of intelligent robotics. It explains the various aspects of the topic to provide readers with a solid foundation on the subject. This edited collection presents theoretical research in cognitive robotics. It takes a multidisciplinary approach that considers the artificial intelligence, physical, chemical, philosophical, psychological, social, cultural, and ethical aspects of this rapidly emerging field. The editor is a prominent researcher whose Lovotics research into emotional bonds with robots is widely recognized. Supplying an accessible introduction to cognitive robotics, the book considers computational intelligence for cognitive robotics based on informationally structured space. It examines how people respond to robots and what makes robots psychologically appealing to humans. The book contextualizes concepts in the history of studies on intelligence theories and includes case studies of different types of robots in action. Although ideal for robotics researchers and professionals, this book is also suitable for use as a supporting textbook in advanced robotics courses at both the undergraduate and graduate levels.},
  added-at = {2016-07-02T21:25:24.000+0200},
  author = {Samani, Hooman},
  biburl = {https://www.bibsonomy.org/bibtex/2869e3a79ed868616820de4ca1d828078/vngudivada},
  interhash = {d78fad864a7e379fc37eadb3b6a23b1f},
  intrahash = {869e3a79ed868616820de4ca1d828078},
  keywords = {Book CognitiveRobotics},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Cognitive Robotics},
  year = 2015
}

@inproceedings{feng2012towards,
  abstract = {The increasing use of statistical data analysis in enterprise applications has created an arms race among database vendors to offer ever more sophisticated in-database analytics. One challenge in this race is that each new statistical technique must be implemented from scratch in the RDBMS, which leads to a lengthy and complex development process. We argue that the root cause for this overhead is the lack of a unified architecture for in-database analytics. Our main contribution in this work is to take a step towards such a unified architecture. A key benefit of our unified architecture is that performance optimizations for analytics techniques can be studied generically instead of an ad hoc, per-technique fashion. In particular, our technical contributions are theoretical and empirical studies of two key factors that we found impact performance: the order data is stored, and parallelization of computations on a single-node multicore RDBMS. We demonstrate the feasibility of our architecture by integrating several popular analytics techniques into two commercial and one open-source RDBMS. Our architecture requires changes to only a few dozen lines of code to integrate a new statistical technique. We then compare our approach with the native analytics tools offered by the commercial RDBMSes on various analytics tasks, and validate that our approach achieves competitive or higher performance, while still achieving the same quality.},
  acmid = {2213874},
  added-at = {2016-07-08T18:32:30.000+0200},
  address = {New York, NY, USA},
  author = {Feng, Xixuan and Kumar, Arun and Recht, Benjamin and R{\'e}, Christopher},
  biburl = {https://www.bibsonomy.org/bibtex/2c2b007d3f660aee93918adefeb3eaff4/vngudivada},
  booktitle = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
  doi = {10.1145/2213836.2213874},
  interhash = {38a12359d677f33faf1e45f2005bd0ee},
  intrahash = {c2b007d3f660aee93918adefeb3eaff4},
  keywords = {Analytics RDBMS},
  pages = {325--336},
  publisher = {ACM},
  series = {SIGMOD '12},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Towards a Unified Architecture for in-RDBMS Analytics},
  url = {http://doi.acm.org/10.1145/2213836.2213874},
  year = 2012
}

@inproceedings{feyisetan2014quickandclean,
  abstract = {In this paper, we address the problem of finding Named Entities in very large micropost datasets. We propose methods to generate a sample of representative microposts by discovering tweets that are likely to refer to new entities. Our approach is able to significantly speed-up the semantic analysis process by discarding retweets, tweets without pre-identifiable entities, as well similar and redundant tweets, while retaining information content.

We apply the approach on a corpus of 1:4 billion microposts, using the IE services of AlchemyAPI, Calais, and Zemanta to identify more than 700,000 unique entities. For the evaluation we compare runtime and number of entities extracted based on the full and the downscaled version of a micropost set. We are able to demonstrate that for datasets of more than 10 million tweets we can achieve a reduction in size of more than 80% while maintaining up to 60% coverage on unique entities cumulatively discovered by the three IE tools.

We publish the resulting Twitter metadata as Linked Data using SIOC and an extension of the NERD core ontology.},
  acmid = {2660527},
  added-at = {2016-07-10T19:46:17.000+0200},
  address = {New York, NY, USA},
  author = {Feyisetan, Oluwaseyi and Simperl, Elena and Tinati, Ramine and Luczak-Roesch, Markus and Shadbolt, Nigel},
  biburl = {https://www.bibsonomy.org/bibtex/29de83c8af8345214c7c71e9b18a1ab3c/vngudivada},
  booktitle = {Proceedings of the 10th International Conference on Semantic Systems},
  doi = {10.1145/2660517.2660527},
  interhash = {816bec9f26a78ca24e687a355adb22ca},
  intrahash = {9de83c8af8345214c7c71e9b18a1ab3c},
  keywords = {MicroBlog NamedEntityRecognition},
  pages = {5--12},
  publisher = {ACM},
  series = {SEM '14},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Quick-and-clean Extraction of Linked Data Entities from Microblogs},
  year = 2014
}

@article{russakovsky2015imagenet,
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.

This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  added-at = {2016-07-08T05:28:36.000+0200},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  biburl = {https://www.bibsonomy.org/bibtex/2839a6078815bee8b4ce1eba6c2192f3d/vngudivada},
  doi = {10.1007/s11263-015-0816-y},
  interhash = {b5e39d2e78c8ddc08e55b83ebccd842a},
  intrahash = {839a6078815bee8b4ce1eba6c2192f3d},
  journal = {International Journal of Computer Vision (IJCV)},
  keywords = {sys:relevantfor:ecu-cc-research ImageNet LargeScaleVisualRecognitionChallenge},
  number = 3,
  pages = {211-252},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {{ImageNet Large Scale Visual Recognition Challenge}},
  volume = 115,
  year = 2015
}

@inproceedings{afergan2014using,
  abstract = {Passive brain-computer interfaces, in which implicit input is derived from a user's changing brain activity without conscious effort from the user, may be one of the most promising applications of brain-computer interfaces because they can improve user performance without additional effort on the user's part. I seek to use physiological signals that correlate to particular brain states in order to adapt an interface while the user behaves normally. My research aims to develop strategies to adapt the interface to the user and the user's cognitive state using functional near-infrared spectroscopy (fNIRS), a non-invasive, lightweight brain-sensing technique. While passive brain-computer interfaces are currently being developed and researchers have shown their utility, there has been little effort to develop a framework or hierarchy for adaptation strategies.},
  acmid = {2661166},
  added-at = {2016-07-03T16:06:52.000+0200},
  address = {New York, NY, USA},
  author = {Afergan, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/2491cbb455f571c10e1c17f44dbec905e/vngudivada},
  booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
  doi = {10.1145/2658779.2661166},
  interhash = {22dae4b301feb99eaf252f35d386cbfd},
  intrahash = {491cbb455f571c10e1c17f44dbec905e},
  isbn = {978-1-4503-3068-8},
  keywords = {BCI},
  location = {Honolulu, Hawaii, USA},
  numpages = {4},
  pages = {13--16},
  publisher = {ACM},
  series = {UIST'14 Adjunct},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Using Brain-computer Interfaces for Implicit Input},
  url = {http://doi.acm.org/10.1145/2658779.2661166},
  year = 2014
}

@inproceedings{li2013gptext,
  abstract = {Many companies keep large amounts of text data inside of relational databases. Several challenges exist in using state-of-the-art systems to perform analysis on such datasets. First, expensive big data transfer cost must be paid up front to move data between databases and analytics systems. Second, many popular text analytics packages do not scale up to production sized datasets. In this paper, we introduce GPText, Greenplum parallel statistical text analysis framework that addresses the above problems by supporting statistical inference and learning algorithms natively in a massively parallel processing database system. GPText seamlessly integrates the Solr search engine and applies statistical algorithms such as k-means and LDA using MADLib, an open source library for scalable in-database analytics which can be installed on Post-greSQL and Greenplum. In addition, GPText also developed and contributed a linear-chain conditional random field (CRF) module to MADLib to enable information extraction tasks such as part-of-speech tagging and named entity recognition. We show the performance and scalability of the parallel CRF implementation. Finally, we describe an eDiscovery application built on the GPText framework.},
  added-at = {2016-07-08T18:56:43.000+0200},
  address = {New York, NY, USA},
  author = {Li, Kun and Grant, Christan and Wang, Daisy Zhe and Khatri, Sunny and Chitouras, George},
  biburl = {https://www.bibsonomy.org/bibtex/2ab8160e14434dfc6a2adcac313146db3/vngudivada},
  booktitle = {Proceedings of the Second Workshop on Data Analytics in the Cloud},
  doi = {10.1145/2486767.2486774},
  interhash = {6b201ec52e07e5b48ac3e58959364396},
  intrahash = {ab8160e14434dfc6a2adcac313146db3},
  keywords = {GPText RDBMS TextAnalytics},
  numpages = {5},
  pages = {31--35},
  publisher = {ACM},
  series = {DanaC '13},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {GPText: Greenplum Parallel Statistical Text Analysis Framework},
  url = {http://doi.acm.org/10.1145/2486767.2486774},
  year = 2013
}

@article{turner2016digital,
  added-at = {2016-07-02T21:40:15.000+0200},
  author = {Turner, Vernon},
  biburl = {https://www.bibsonomy.org/bibtex/2b9c3350e6b1d5009053436d254781675/vngudivada},
  interhash = {95700ca05dd7acb4c28821c95ebaee4e},
  intrahash = {b9c3350e6b1d5009053436d254781675},
  keywords = {IoT},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The Digital Universe of Opportunities: Rich Data and the Increasing Value of the Internet of Things},
  url = {http://www.emc.com/leadership/digital-universe/2014iview/digital-universe-of-opportunities-vernon-turner.htm},
  urldate = {2016-06-27},
  year = 2016
}

@article{hellerstein2012madlib,
  abstract = {MADlib is a free, open-source library of in-database analytic methods. It provides an evolving suite of SQL-based algorithms for machine learning, data mining and statistics that run at scale within a database engine, with no need for data import/export to other tools. The goal is for MADlib to eventually serve a role for scalable database systems that is similar to the CRAN library for R: a community repository of statistical methods, this time written with scale and parallelism in mind.

In this paper we introduce the MADlib project, including the background that led to its beginnings, and the motivation for its open-source nature. We provide an overview of the library's architecture and design patterns, and provide a description of various statistical methods in that context. We include performance and speedup results of a core design pattern from one of those methods over the Greenplum parallel DBMS on a modest-sized test cluster. We then report on two initial efforts at incorporating academic research into MADlib, which is one of the project's goals.

MADlib is freely available at http://madlib.net, and the project is open for contributions of both new methods, and ports to additional database platforms.},
  acmid = {2367510},
  added-at = {2016-07-08T19:00:21.000+0200},
  author = {Hellerstein, Joseph M. and R{\'e}, Christoper and Schoppmann, Florian and Wang, Daisy Zhe and Fratkin, Eugene and Gorajek, Aleksander and Ng, Kee Siong and Welton, Caleb and Feng, Xixuan and Li, Kun and Kumar, Arun},
  biburl = {https://www.bibsonomy.org/bibtex/2ed58c2ea372c166b6272724e2b80ce67/vngudivada},
  doi = {10.14778/2367502.2367510},
  interhash = {559eb24a8697876cdd509f5cfff9e917},
  intrahash = {ed58c2ea372c166b6272724e2b80ce67},
  journal = {Proc. VLDB Endow.},
  keywords = {Analytics MADlib RDBMS SQL},
  month = aug,
  number = 12,
  pages = {1700--1711},
  publisher = {VLDB Endowment},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The MADlib Analytics Library: Or MAD Skills, the SQL},
  url = {http://dx.doi.org/10.14778/2367502.2367510},
  volume = 5,
  year = 2012
}

@article{li2013spoken,
  abstract = {Spoken language recognition refers to the automatic process through which we determine or verify the identity of the language spoken in a speech sample. We study a computational framework that allows such a decision to be made in a quantitative manner. In recent decades, we have made tremendous progress in spoken language recognition, which benefited from technological breakthroughs in related areas, such as signal processing, pattern recognition, cognitive science, and machine learning. In this paper, we attempt to provide an introductory tutorial on the fundamentals of the theory and the state-of-the-art solutions, from both phonological and computational aspects. We also give a comprehensive review of current trends and future research directions using the language recognition evaluation (LRE) formulated by the National Institute of Standards and Technology (NIST) as the case studies.},
  added-at = {2016-07-03T17:36:45.000+0200},
  author = {Li, Haizhou and Ma, Bin and Lee, Kong-Aik},
  biburl = {https://www.bibsonomy.org/bibtex/2fba6c8300b2213980df40d5cfa5620b6/vngudivada},
  interhash = {82fa9688319ecf2cbdcf9322bc28d60d},
  intrahash = {fba6c8300b2213980df40d5cfa5620b6},
  journal = {Proceedings of the IEEE},
  keywords = {AcousticFeatures LRE Language LanguageRecognitionEvaluation PhonotacticFeatures Spoken SpokenLanguageRecognition VectorSpaceModeling},
  number = 5,
  pages = {1136--1159},
  privnote = {This paper provides an introductory tutorial on the fundamentals and the state-of-the-art solutions to automatic spoken language recognition, from both phonological and computational perspectives. It also gives a comprehensive review of current trends and future research directions.},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Spoken Language Recognition: From Fundamentals to Practice},
  volume = 101,
  year = 2013
}

@article{denicola2016lightweight,
  abstract = {UPON Lite focuses on users, typically domain experts without ontology expertise, minimizing the role of ontology engineers},
  added-at = {2016-07-10T19:57:42.000+0200},
  address = {New York, NY, USA},
  author = {De Nicola, Antonio and Missikoff, Michele},
  biburl = {https://www.bibsonomy.org/bibtex/2a9487d87608db96146d684c5a4e48417/vngudivada},
  doi = {10.1145/2818359},
  interhash = {20a0b841e159d8ed9fa546a1b08d43f9},
  intrahash = {a9487d87608db96146d684c5a4e48417},
  journal = {Commun. ACM},
  keywords = {OntologyDesign},
  month = feb,
  number = 3,
  pages = {79--86},
  publisher = {ACM},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {A Lightweight Methodology for Rapid Ontology Engineering},
  volume = 59,
  year = 2016
}

@article{nips2016neural,
  added-at = {2016-07-02T21:36:14.000+0200},
  author = {NIPS},
  biburl = {https://www.bibsonomy.org/bibtex/27a78ff6f9e850d5dcf596bb761eadaff/vngudivada},
  interhash = {ab249bbba69f7713dbffc034890f2f00},
  intrahash = {7a78ff6f9e850d5dcf596bb761eadaff},
  keywords = {NIPS NeuralComputing},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Neural Information Processing Systems Annual Conference},
  url = {https://nips.cc/},
  urldate = {2016-05-10},
  year = 2016
}

@techreport{stodder2014eight,
  abstract = {Organizations that aspire to be data driven cannot afford to limit users’ data analysis to structured data: that is, the alphanumeric data types that have been carefully defined, modeled, and stored in standard spreadsheets, relational databases, and data warehouses. In the course of business processes, operations, and customer interactions, enterprises generate far more than just structured data—in particular, most are swimming in massive amounts of text.


Drawn from both internal and external sources, text files form a potential gold mine of insights that could help organizations reduce costs, improve customer relationships, speed response to events, and innovate with products and services. Business intelligence (BI) and data warehousing (DW) systems are adept at delivering the numbers, but if users do not have access to unstructured and semi-structured text such as customer comments, field personnel notes, and social media, they will be blind to contextual information that could help answer questions about the numbers. Exploratory analysis of text could reveal emerging trends that do not show up in BI reports.


Text analytics covers a range of technologies and practices for analyzing text, extracting relevant information, and transforming sources by applying structure so that analysis can be repeated and adjusted over time. Software solutions combine techniques from natural language processing, statistics, and machine learning. One of the goals of text analytics is to accurately extract entities, facts, concepts, themes, and sentiment. With the popularity of social media, many enterprises are interested in performing sentiment analysis. However, interest is expanding across industries to other uses, such as in healthcare, where text analytics helps doctors understand the full context of patient symptoms and engage in evidence-based medicine.


Making use of modern data visualization, text analytics solutions enable users to explore text on desktops and mobile devices. Business users, not just specialized text analysts, can use the solutions to derive value—for example, by aggregating findings from text with their analysis of structured data. Following TDWI’s previous Checklist Report on this topic, How to Gain Insight from Text, this Checklist discusses the relevance of text analytics for improving user experiences with the semi-structured and unstructured textual big data now at their disposal.},
  added-at = {2016-07-07T04:13:48.000+0200},
  address = 2014,
  author = {Stodder, David},
  biburl = {https://www.bibsonomy.org/bibtex/2320dff0362a096e1f0fa367d660b5335/vngudivada},
  institution = {TDWI},
  interhash = {c621cb0d1cb5f1b6c9ffb3d1dfd0fc1c},
  intrahash = {320dff0362a096e1f0fa367d660b5335},
  keywords = {sys:relevantfor:ecu-cc-research TextAnalytics},
  month = may,
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Eight Steps for Using Analytics to Gain Value from Text and Unstructured Content},
  year = 2014
}

@article{esser2016convolutional,
  abstract = {Deep networks are now able to achieve human-level performance on a broad spectrum of recognition tasks. Independently, neuromorphic computing has now demonstrated unprecedented energy-efficiency through a new chip architecture based on spiking neurons, low precision synapses, and a scalable communication network. Here, we demonstrate that neuromorphic computing, despite its novel architectural primitives, can implement deep convolution networks that i) approach state-of-the-art classification accuracy across 8 standard datasets, encompassing vision and speech, ii) perform inference while preserving the hardware's underlying energy-efficiency and high throughput, running on the aforementioned datasets at between 1200 and 2600 frames per second and using between 25 and 275 mW (effectively > 6000 frames / sec / W) and iii) can be specified and trained using backpropagation with the same ease-of-use as contemporary deep learning. For the first time, the algorithmic power of deep learning can be merged with the efficiency of neuromorphic processors, bringing the promise of embedded, intelligent, brain-inspired computing one step closer.},
  added-at = {2016-07-09T23:08:03.000+0200},
  author = {Esser, Steven K. and Merolla, Paul A. and Arthur, John V. and Cassidy, Andrew S. and Appuswamy, Rathinakumar and Andreopoulos, Alexander and Berg, David J. and McKinstry, Jeffrey L. and Melano, Timothy and Barch, Davis R. and di Nolfo, Carmelo and Datta, Pallab and Amir, Arnon and Taba, Brian and Flickner, Myron D. and Modha, Dharmendra S.},
  biburl = {https://www.bibsonomy.org/bibtex/2ee6239833297c9a0ad04ee6f167fead8/vngudivada},
  interhash = {2c8ba97d0cc9e5ab8eeb77e0ea88240b},
  intrahash = {ee6239833297c9a0ad04ee6f167fead8},
  journal = {CoRR},
  keywords = {ConvolutionalNetworks DeepLearning NeuromorphicComputing TrueNorth},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing},
  url = {http://arxiv.org/abs/1603.08270},
  volume = {abs/1603.08270},
  year = 2016
}

@article{lake2015humanlevel,
  abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
  added-at = {2016-07-08T05:34:55.000+0200},
  author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
  biburl = {https://www.bibsonomy.org/bibtex/2ce6ede6e8d4024bf6d4a10efd4662f58/vngudivada},
  description = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look “right” as judged by Turing-like tests of the model's output in comparison to what real humans produce.},
  doi = {10.1126/science.aab3050},
  interhash = {1bd92043546d4c7c2ea239bcd1675986},
  intrahash = {ce6ede6e8d4024bf6d4a10efd4662f58},
  journal = {Science},
  keywords = {sys:relevantfor:ecu-cc-research BayesianProgramLearning Captcha ConceptLearning},
  number = 6266,
  pages = {1332 - 1338},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Human-level concept learning through probabilistic program induction},
  url = {http://science.sciencemag.org/content/350/6266/1332},
  volume = 350,
  year = 2015
}

@techreport{seligman2012provenance,
  abstract = {We describe the needs for data provenance in a large-scale analytic environment to support financial systemic risk analysis. Government financial regulators need to make sense of the outputs of thousands to tens of thousands of simulation runs invoked by a large analytic staff; automatic capture of data provenance (dataset sources and processing steps) supports analysts without adding to their workloads. We present an architecture for automated provenance capture from both simulations and data transformation tools. Finally, we describe a prototype implementation and next steps.},
  added-at = {2016-07-10T13:33:04.000+0200},
  author = {Seligman, Len and Brady, Shaun and Blaustein, Barbara and Mutchler, Paula and Chapman, Adriane and Worrell, Charles},
  biburl = {https://www.bibsonomy.org/bibtex/2341dc19da45afdb2392ae4f85f0fd99b/vngudivada},
  institution = {The MITRE Corporation},
  interhash = {c9fe371ce9d07800b49b74142c8650cb},
  intrahash = {341dc19da45afdb2392ae4f85f0fd99b},
  keywords = {CaseStudy DataProvenance},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Data Provenance and Financial Systemic Risk: A Case Study},
  url = {https://www.mitre.org/sites/default/files/pdf/12_3756.pdf},
  year = 2012
}

@article{syntaxnet2016source,
  added-at = {2016-07-02T21:37:44.000+0200},
  author = {SyntaxNet},
  biburl = {https://www.bibsonomy.org/bibtex/2ba369363812b947ef8b560f887bc7492/vngudivada},
  interhash = {51b6be20b183463341f77c15ebbf3a79},
  intrahash = {ba369363812b947ef8b560f887bc7492},
  keywords = {NLU TensorFlow},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {An open source neural network framework for TensorFlow for developing Natural Language Understanding (NLU) systems},
  url = {https://github.com/tensorflow/models/tree/master/syntaxnet},
  urldate = {2016-06-27},
  year = 2016
}

@article{goodfellow2014multidigit,
  abstract = {Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. We employ the DistBelief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over 96% accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving 97.84% accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over 90% accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. We report a 99.8% accuracy on the hardest category of reCAPTCHA. Our evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators.},
  added-at = {2016-07-08T06:10:46.000+0200},
  author = {Goodfellow, Ian J. and Bulatov, Yaroslav and Ibarz, Julian and Arnoud, Sacha and Shet, Vinay},
  biburl = {https://www.bibsonomy.org/bibtex/2f8471d5ce0b3fe89c72c1b58768202ad/vngudivada},
  interhash = {6dfccceb1ea4dc8296160f3b022e2879},
  intrahash = {f8471d5ce0b3fe89c72c1b58768202ad},
  journal = {arXiv.org},
  keywords = {sys:relevantfor:ecu-cc-research Captcha ConvolutionalNetworks DeepLearning},
  month = {April},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks},
  url = {https://arxiv.org/pdf/1312.6082.pdf},
  year = 2014
}

@article{pfurtscheller2010hybrid,
  abstract = {Nowadays, everybody knows what a hybrid car is. A hybrid car normally has two engines to enhance energy efficiency and reduce CO2 output. Similarly, a hybrid brain-computer interface (BCI) is composed of two BCIs, or at least one BCI and another system. A hybrid BCI, like any BCI, must fulfill the following four criteria: (i) the device must rely on signals recorded directly from the brain; (ii) there must be at least one recordable brain signal that the user can intentionally modulate to effect goal-directed behaviour; (iii) real time processing; and (iv) the user must obtain feedback. This paper introduces hybrid BCIs that have already been published or are in development. We also introduce concepts for future work. We describe BCIs that classify two EEG patterns: one is the event-related (de)synchronisation (ERD, ERS) of sensorimotor rhythms, and the other is the steady-state visual evoked potential (SSVEP). Hybrid BCIs can either process their inputs simultaneously, or operate two systems sequentially, where the first system can act as a “brain switch”. For example, we describe a hybrid BCI that simultaneously combines ERD and SSVEP BCIs. We also describe a sequential hybrid BCI, in which subjects could use a brain switch to control an SSVEP-based hand orthosis. Subjects who used this hybrid BCI exhibited about half the false positives encountered while using the SSVEP BCI alone. A brain switch can also rely on hemodynamic changes measured through near-infrared spectroscopy (NIRS). Hybrid BCIs can also use one brain signal and a different type of input. This additional input can be an electrophysiological signal such as the heart rate, or a signal from an external device such as an eye tracking system.},
  added-at = {2016-07-03T16:22:48.000+0200},
  author = {Pfurtscheller, Gert and Allison, Brendan Z and Bauernfeind, G{\"u}nther and Brunner, Clemens and Solis Escalante, Teodoro and Scherer, Reinhold and Zander, Thorsten O and Mueller-Putz, Gernot and Neuper, Christa and Birbaumer, Niels},
  biburl = {https://www.bibsonomy.org/bibtex/26e219de2a8fcbd7d3460c9253bac7ba0/vngudivada},
  interhash = {4b25100afa7927b5df9adf3c37ef5234},
  intrahash = {6e219de2a8fcbd7d3460c9253bac7ba0},
  journal = {Frontiers in neuroscience},
  keywords = {BCI HybridBCI},
  pages = 3,
  publisher = {Frontiers},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The hybrid BCI},
  volume = 4,
  year = 2010
}

@incollection{speck2014ensemble,
  abstract = {A considerable portion of the information on the Web is still only available in unstructured form. Implementing the vision of the Semantic Web thus requires transforming this unstructured data into structured data. One key step during this process is the recognition of named entities. Previous works suggest that ensemble learning can be used to improve the performance of named entity recognition tools. However, no comparison of the performance of existing supervised machine learning approaches on this task has been presented so far. We address this research gap by presenting a thorough evaluation of named entity recognition based on ensemble learning. To this end, we combine four different state-of-the approaches by using 15 different algorithms for ensemble learning and evaluate their performance on five different datasets. Our results suggest that ensemble learning can reduce the error rate of state-of-the-art named entity recognition systems by 40\%, thereby leading to over 95\% f-score in our best run.},
  added-at = {2016-07-10T18:19:14.000+0200},
  author = {Speck, Ren\'e and {Ngonga Ngomo}, Axel-Cyrille},
  biburl = {https://www.bibsonomy.org/bibtex/268497c7ece6c365c254615c68bba8da0/vngudivada},
  booktitle = {The Semantic Web -- ISWC 2014},
  interhash = {9532d9191e0ac308063a61cead34d85b},
  intrahash = {68497c7ece6c365c254615c68bba8da0},
  keywords = {EnsembleLearning NLU NamedEntityRecognition},
  pages = {519-534},
  privnote = {http://aksw.org/Projects/FOX.html},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {Ensemble Learning for Named Entity Recognition},
  url = {http://svn.aksw.org/papers/2014/ISWC_EL4NER/public.pdf},
  volume = 8796,
  year = 2014
}

@book{hall2016evolution,
  abstract = {Machine learning is a hot topic in business. Even data-driven organizations that have spent years developing successful data analysis platforms, with many accurate statistical models in place, are now looking into this decades-old discipline. But how can companies turn hyped opportunities for machine learning into real business value?

This report examines the growing momentum of machine learning in the analytics landscape, the challenges machine learning presents to businesses, and examples of how organizations are actively seeking to incorporate modern machine learning techniques into their production data infrastructures. Authors Patrick Hall, Wen Phan, and Katie Whitson look at two companies in depth—one in healthcare and one in finance—that are seeing the real impact of machine learning.},
  added-at = {2016-07-08T15:47:27.000+0200},
  author = {Hall, Patrick and Phan, Wen and Whitson, Katie},
  biburl = {https://www.bibsonomy.org/bibtex/247b0e5c90faa916367a126b69335d553/vngudivada},
  interhash = {4b09df03fee0f4e39228effd135ee3d3},
  intrahash = {47b0e5c90faa916367a126b69335d553},
  keywords = {Analytics Book SAS},
  publisher = {O'Reilly Media, Inc.},
  timestamp = {2019-03-25T17:14:02.000+0100},
  title = {The Evolution of Analytics: Opportunities and Challenges for Machine Learning in Business},
  year = 2016
}

@article{devlin1988architecture,
  abstract = {The transaction-processing environment in which companies maintain their operational databases was the original target for computerization and is now well understood. On the other hand, access to company information on a large scale by an end user for reporting and data analysis is relatively new. Within IBM, the computerization of informational systems is progressing, driven by business needs and by the availability of improved tools for accessing the company data. It is now apparent that an architecture is needed to draw together the various strands of informational system activity within the company. IBM Europe, Middle East, and Africa (E/ME/A) has adopted an architecture called the E/ME/A Business Information System (EBIS) architecture as the strategic direction for informational systems. EBIS proposes an integrated warehouse of company data based firmly in the relational database environment. End-user access to this warehouse is simplified by a consistent set of tools provided by an end-user interface and supported by a business data directory that describes the information available in user terms. This paper describes the background and components of the architecture of EBIS},
  added-at = {2016-07-10T23:20:23.000+0200},
  author = {Devlin, B. A. and Murphy, P. T.},
  biburl = {https://www.bibsonomy.org/bibtex/20f8e0e7b9e5627f3ad755c200b048416/vngudivada},
  doi = {10.1147/sj.271.0060},
  interhash = {5286b5b3473ab6a9b253d0e37557f930},
  intrahash = {0f8e0e7b9e5627f3ad755c200b048416},
  journal = {IBM Systems Journal},
  keywords = {DataAnalytics DataWarehouse},
  number = 1,
  pages = {60 - 80},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {An architecture for a business and information system},
  volume = 27,
  year = 1988
}

@book{bostorm2016superintelligence,
  abstract = {The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains.

If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence.

But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation?

To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence.

This profoundly ambitious and original book picks its way carefully through a vast tract of forbiddingly difficult intellectual terrain. Yet the writing is so lucid that it somehow makes it all seem easy. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.},
  added-at = {2016-07-14T21:24:36.000+0200},
  author = {Bostorm, Nick},
  biburl = {https://www.bibsonomy.org/bibtex/2ba1d690ecec62259c80996f86d320f0e/vngudivada},
  edition = {First},
  interhash = {fa4837e0aff339df0fc1e78f6ca74dba},
  intrahash = {ba1d690ecec62259c80996f86d320f0e},
  keywords = {Book CognitiveScience Superintelligence},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Superintelligence: Paths, Dangers, Strategies},
  url = {https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834/ref=sr_1_5?s=books&ie=UTF8&qid=1468523432&sr=1-5&keywords=Information+Theory%2C+Inference+and+Learning+Algorithms},
  year = 2016
}

@book{domingos2015master,
  abstract = {Algorithms increasingly run our lives. They find books, movies, jobs, and dates for us, manage our investments, and discover new drugs. More and more, these algorithms work by learning from the trails of data we leave in our newly digital world. Like curious children, they observe us, imitate, and experiment. And in the world’s top research labs and universities, the race is on to invent the ultimate learning algorithm: one capable of discovering any knowledge from data, and doing anything we want, before we even ask.

Machine learning is the automation of discovery—the scientific method on steroids—that enables intelligent robots and computers to program themselves. No field of science today is more important yet more shrouded in mystery. Pedro Domingos, one of the field’s leading lights, lifts the veil for the first time to give us a peek inside the learning machines that power Google, Amazon, and your smartphone. He charts a course through machine learning’s five major schools of thought, showing how they turn ideas from neuroscience, evolution, psychology, physics, and statistics into algorithms ready to serve you. Step by step, he assembles a blueprint for the future universal learner—the Master Algorithm—and discusses what it means for you, and for the future of business, science, and society.

If data-ism is today’s rising philosophy, this book will be its bible. The quest for universal learning is one of the most significant, fascinating, and revolutionary intellectual developments of all time. A groundbreaking book, The Master Algorithm is the essential guide for anyone and everyone wanting to understand not just how the revolution will happen, but how to be at its forefront.

In the world's top research labs and universities, the race is on to invent the ultimate learning algorithm: one capable of discovering any knowledge from data, and doing anything we want, before we even ask. In The Master Algorithm, Pedro Domingos lifts the veil to give us a peek inside the learning machines that power Google, Amazon, and your smartphone. He assembles a blueprint for the future universal learner-the Master Algorithm-and discusses what it will mean for business, science, and society. If data-ism is today's philosophy, this book is its bible.},
  added-at = {2016-07-14T21:29:27.000+0200},
  author = {Domingos, Pedro},
  biburl = {https://www.bibsonomy.org/bibtex/2b48e9f824f83e98a132511b244ed8ad0/vngudivada},
  interhash = {3d8bc19852ad9cab7580e2e00322a9c2},
  intrahash = {b48e9f824f83e98a132511b244ed8ad0},
  keywords = {Book CognitiveScience MasterAlgorithm},
  publisher = {Basic Books},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World},
  year = 2015
}

@book{mitzenmacher2005probability,
  abstract = {Assuming only an elementary background in discrete mathematics, this textbook is an excellent introduction to the probabilistic techniques and paradigms used in the development of probabilistic algorithms and analyses. It includes random sampling, expectations, Markov's and Chevyshev's inequalities, Chernoff bounds, balls and bins models, the probabilistic method, Markov chains, MCMC, martingales, entropy, and other topics. The book is designed to accompany a one- or two-semester course for graduate students in computer science and applied mathematics.},
  added-at = {2016-07-14T17:57:37.000+0200},
  author = {Mitzenmacher, Michael and Upfal, Eli},
  biburl = {https://www.bibsonomy.org/bibtex/2b50738dff07b44f218777a1555f5ad3c/vngudivada},
  interhash = {1274cccb3b3b72d9337d38c248623fc5},
  intrahash = {b50738dff07b44f218777a1555f5ad3c},
  isbn = {978-0-521-83540-4},
  keywords = {Book Probability},
  pages = {I-XVI, 1-352},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Probability and computing - randomized algorithms and probabilistic analysis.},
  year = 2005
}

@techreport{halper2016operationalizing,
  abstract = {This TDWI Best Practices Report focuses on how organizations can and are operationalizing analytics to derive business value. It provides in-depth survey analysis of current strategies and future trends for embedded analytics across both organizational and technical dimensions, including organizational culture, infrastructure, data and processes. It looks at challenges and how organizations are overcoming them, and offers recommendations and best practices for successfully operationalizing analytics in the organization.},
  added-at = {2016-07-11T04:43:35.000+0200},
  author = {Halper, Fern},
  biburl = {https://www.bibsonomy.org/bibtex/2bc75d9a4cbbedf9890d1ce7b8111bae2/vngudivada},
  institution = {TDWI},
  interhash = {1c8ba7962fbf6de78707838a57f67d9c},
  intrahash = {bc75d9a4cbbedf9890d1ce7b8111bae2},
  keywords = {EmbeddedAnalytics},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Operationalizing and Embedding Analytics for Action},
  year = 2016
}

@book{kurzweil2013create,
  abstract = {This book presents a provocative exploration of the most important project in human-machine civilization: reverse-engineering the brain to understand precisely how it works and using that knowledge to create even more intelligent machines. Kursweil discusses how the brain functions, how the mind emerges, brain computer interfaces, and the implications of vastly increasing the powers of our intelligence to address the world's problems.},
  added-at = {2016-07-14T21:51:00.000+0200},
  address = {New York, N.Y.},
  author = {Kurzweil, Ray},
  biburl = {https://www.bibsonomy.org/bibtex/239cf19fc5d8f7d64be882c04fb28dfe6/vngudivada},
  interhash = {d5d58b2b4d2024d593d3de4fa7988935},
  intrahash = {39cf19fc5d8f7d64be882c04fb28dfe6},
  keywords = {Book CognitiveScience},
  publisher = {Penguin Books},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {How to create a mind: the secret of human thought revealed},
  year = 2013
}

@article{rys2011scalable,
  abstract = {How do large-scale sites and applications remain SQL-based?},
  acmid = {1953141},
  added-at = {2016-07-14T19:55:25.000+0200},
  address = {New York, NY, USA},
  author = {Rys, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/23c98d04d2ed34915ee87f47062034238/vngudivada},
  description = {Scalable SQL},
  doi = {10.1145/1953122.1953141},
  interhash = {264c46ed58eb311bc808685fd451df66},
  intrahash = {3c98d04d2ed34915ee87f47062034238},
  issn = {0001-0782},
  issue_date = {June 2011},
  journal = {Commun. ACM},
  keywords = {SQL},
  month = jun,
  number = 6,
  numpages = {6},
  pages = {48--53},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Scalable SQL},
  url = {http://doi.acm.org/10.1145/1953122.1953141},
  volume = 54,
  year = 2011
}

@book{ellenberg2015wrong,
  abstract = {Overview:  The math we learn in school can seem like a dull set of rules, laid down by the ancients and not to be questioned.  In How Not to Be Wrong, Jordan Ellenberg shows us how terribly limiting this view is:  Math isn't confined to abstract incidents that never occur in real life, but rather touches everything we do-the whole world is shot through with it.  Math allows us to see the hidden structures underneath the messy and chaotic surface of our world.  It's a science of not being wrong, hammered out by centuries of hard work and argument.  Armed with the tools of mathematics, we can see through to the true meaning of information we take for granted:  How early should you get to the airport?  What does "public opinion" really represent?  Why do tall parents have shorter children?  Who really won Florida in 2000?  And how likely are you, really, to develop cancer?  How Not to Be Wrong presents the surprising revelations behind all of these questions and many more, using the mathematician's method of analyzing life and exposing the hard-won insights of the academic community to the layman-minus the jargon.  Ellenberg chases mathematical threads through a vast range of time and space, from the everyday to the cosmic, encountering, among other things, baseball, Reaganomics, daring lottery schemes, Voltaire, the replicability crisis in psychology, Italian Renaissance painting, artificial languages, the development of non-Euclidean geometry, the coming obesity apocalypse, Antonin Scalia's views on crime and punishment, the psychology of slime molds, what Facebook can and can't figure out about you, and the existence of God.  Ellenberg pulls from history as well as from the latest theoretical developments to provide those not trained in math with the knowledge they need.  Math, as Ellenberg says, is "an atomic-powered prosthesis that you attach to your common sense, vastly multiplying its reach and strength."  With the tools of mathematics in hand, you can understand the world in a deeper, more meaningful way. How Not to Be Wrong will show you how.},
  added-at = {2016-07-14T21:45:20.000+0200},
  author = {Ellenberg, Jordan},
  biburl = {https://www.bibsonomy.org/bibtex/2fe1e40d50f90720e1554749f208db272/vngudivada},
  interhash = {397de1601a0d2366122edc4158b01135},
  intrahash = {fe1e40d50f90720e1554749f208db272},
  isbn = {0143127535 9780143127536},
  keywords = {Book Learning MathematicalThinking},
  note = {https://www.youtube.com/watch?v=kZTKuMBJP7Y},
  refid = {892041314},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {How not to be wrong : the power of mathematical thinking},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780143127536},
  year = 2015
}

@inproceedings{harnarinesingh2013investigating,
  abstract = {Assistive technologies predominantly employ devices such as virtual keyboards and wheelchair controllers to enable handicapped individuals to interact with their environment. In the absence of assistive devices, a handicapped individual's only recourse is to rely on an attendant for assistance in performing daily activities. Assistive technologies therefore enable disabled person to regain a measure of independence. However, the state-of-the-art assistive technologies are limited in their abilities. There has been limited work on assistive writing agents.

A previous study investigated robot-based writing using a 5-axis industrial robot [8]. This paper investigates the feasibility of a robot-based writing agent using a lower capability3-axis device. This work is en route to the interfacing to a Brain-Computer Interface (BCI) for BCI-controlled robot writing. A full alphanumerical English character set was developed and implemented using the system. The generated characters were legible. Minor errors were evident in trajectory execution due to controller transience and frayed marker tips. However, these errors do not significantly demerit the legibility of the characters.The 3-axis robot was therefore able to generate legible text like the previous study [8] but with a lower capability device.},
  added-at = {2016-07-11T01:07:56.000+0200},
  address = {New York, NY, USA},
  author = {Harnarinesingh, Randy E. S. and Syan, Chanan S.},
  biburl = {https://www.bibsonomy.org/bibtex/25cdbfbdbca1fae090caa3c1fbbc48efa/vngudivada},
  booktitle = {Proceedings of the Second International Conference on Innovative Computing and Cloud Computing},
  doi = {10.1145/2556871.2556885},
  interhash = {417a2b71cf04729c3b19f561254f1618},
  intrahash = {5cdbfbdbca1fae090caa3c1fbbc48efa},
  keywords = {AssistiveTechnology BCI Kinematics RobotWriting},
  pages = {60:60--60:65},
  publisher = {ACM},
  series = {ICCC '13},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Investigating the Feasibility of a Robot-Based Writing Agent},
  year = 2013
}

@book{ross2010introduction,
  abstract = {Sheldon Ross's classic bestseller, Introduction to Probability Models, has been used extensively by professionals and as the primary text for a first undergraduate course in applied probability. It introduces elementary probability theory and stochastic processes, and shows how probability theory can be applied fields such as engineering, computer science, management science, the physical and social sciences, and operations research.

The hallmark features of this renowned text remain in this eleventh edition: superior writing style; excellent exercises and examples covering the wide breadth of coverage of probability topic; and real-world applications in engineering, science, business and economics. The 65% new chapter material includes coverage of finite capacity queues, insurance risk models, and Markov chains, as well as updated data.

Updated data, and a list of commonly used notations and equations, instructor's solutions manual
Offers new applications of probability models in biology and new material on Point Processes, including the Hawkes process
Introduces elementary probability theory and stochastic processes, and shows how probability theory can be applied in fields such as engineering, computer science, management science, the physical and social sciences, and operations research
Covers finite capacity queues, insurance risk models, and Markov chains
Contains compulsory material for new Exam 3 of the Society of Actuaries including several sections in the new exams
Appropriate for a full year course, this book is written under the assumption that students are familiar with calculus},
  added-at = {2016-07-14T19:10:31.000+0200},
  author = {Ross, Sheldon M.},
  biburl = {https://www.bibsonomy.org/bibtex/219d23a577492e905786ef5a5057afa77/vngudivada},
  edition = {Tenth},
  interhash = {dbb353abe60a75bd4184d3b1bc1a94fa},
  intrahash = {19d23a577492e905786ef5a5057afa77},
  keywords = {Book Probability},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Introduction to Probability Models},
  year = 2010
}

@book{alon2000probabilistic,
  abstract = {One of the most powerful and popular tools used in combinatorics is the probabilistic method. Describes current algorithmic techniques, applying both the classical method and the modern tools it uses. Along with a detailed description of the techniques used in probabilistic arguments, it includes basic methods which utilize expectation and variance plus recent applications of martingales and correlation inequalities. Examines discrepancy and random graphs and covers such topics as theoretical computer science, computational geometry, derandomization of randomized algorithms and more. A study of various topics using successful probabilistic techniques is included along with an Open Problems Appendix by Paul Erdos, the founder of the probabilistic method.},
  added-at = {2016-07-14T18:57:28.000+0200},
  author = {Alon, Noga and Spencer, Joel H.},
  biburl = {https://www.bibsonomy.org/bibtex/26ff5d5af091d01c218dedef40f5d8cae/vngudivada},
  edition = {Second},
  interhash = {d90e6bb1e4b0ee465af1b53427869d9d},
  intrahash = {6ff5d5af091d01c218dedef40f5d8cae},
  keywords = {Book Probability},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {The Probabilistic Method},
  year = 2000
}

@article{burges1998tutorial,
  abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
  acmid = {593463},
  added-at = {2016-07-14T14:34:52.000+0200},
  address = {Hingham, MA, USA},
  author = {Burges, Christopher J. C.},
  biburl = {https://www.bibsonomy.org/bibtex/2287ff185646e020c301c2cc0ae9ed8a0/vngudivada},
  doi = {10.1023/A:1009715923555},
  interhash = {2d37bd146ae27e7251296b0b7f1f4a61},
  intrahash = {287ff185646e020c301c2cc0ae9ed8a0},
  issn = {1384-5810},
  issue_date = {June 1998},
  journal = {Data Min. Knowl. Discov.},
  keywords = {SVM},
  month = jun,
  number = 2,
  numpages = {47},
  pages = {121--167},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {A Tutorial on Support Vector Machines for Pattern Recognition},
  url = {http://dx.doi.org/10.1023/A:1009715923555},
  volume = 2,
  year = 1998
}

@book{motwani1995randomized,
  abstract = {A randomized algorithm is one that makes random choices during its execution. The behavior of such an algorithm may thus be random even on a fixed input. The design and analysis of a randomized algorithm focus on establishing that it is likely to behave well on every input; the likelihood in such a statement depends only on the probabilistic choices made by the algorithm during execution and not on any assumptions about the input. It is especially important to distinguish a randomized algorithm from the average-case analysis of algorithms, where one analyzes an algorithm assuming that its input is drawn from a fixed probability distribution. With a randomized algorithm, in contrast, no assumption is made about the input.},
  added-at = {2016-07-14T18:53:57.000+0200},
  author = {Motwani, Rajeev and Raghavan, Prabhakar},
  biburl = {https://www.bibsonomy.org/bibtex/2ecbae4479b6b1e985eaf30b53b508224/vngudivada},
  interhash = {2afda1a92c9624462acd34e4ca58535b},
  intrahash = {ecbae4479b6b1e985eaf30b53b508224},
  keywords = {Book Probability RandomizedAlgorithms},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Randomized Algorithms},
  year = 1995
}

@book{koller2009probabilistic,
  abstract = {Most tasks require a person or an automated system to reason -- to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality.

Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs.},
  added-at = {2016-07-14T20:46:26.000+0200},
  address = {Cambridge, MA},
  author = {Koller, Daphne and Friedman, Nir},
  biburl = {https://www.bibsonomy.org/bibtex/2f63abd84ef2f028756f5e53a843af284/vngudivada},
  edition = {First},
  interhash = {0c61213fa4c06778a14fb33e04705fb5},
  intrahash = {f63abd84ef2f028756f5e53a843af284},
  isbn = {978-0262013192},
  keywords = {Book GraphicalModel Probability},
  publisher = {The MIT Press},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Probabilistic Graphical Models: Principles and Techniques},
  year = 2009
}

@article{cai2003svmprot,
  abstract = {Prediction of protein function is of significance in studying biological processes. One approach for function prediction is to classify a protein into functional family. Support vector machine (SVM) is a useful method for such classification, which may involve proteins with diverse sequence distribution. We have developed a web-based software, SVMProt, for SVM classification of a protein into functional family from its primary sequence. SVMProt classification system is trained from representative proteins of a number of functional families and seed proteins of Pfam curated protein families. It currently covers 54 functional families and additional families will be added in the near future. The computed accuracy for protein family classification is found to be in the range of 69.1–99.6\%. SVMProt shows a certain degree of capability for the classification of distantly related proteins and homologous proteins of different function and thus may be used as a protein function prediction tool that complements sequence alignment methods. SVMProt can be accessed at http://jing.cz3.nus.edu.sg/cgi-bin/svmprot.cgi},
  added-at = {2016-07-11T16:59:38.000+0200},
  author = {Cai, C. Z. and Han, L. Y. and Ji, Z. L. and Chen, X. and Chen, Y. Z.},
  biburl = {https://www.bibsonomy.org/bibtex/250054e347402cb73e94b0517bed9bda9/vngudivada},
  interhash = {bcf7e70e1c20901b3d90afc2e09c45dc},
  intrahash = {50054e347402cb73e94b0517bed9bda9},
  journal = {Nucleic Acids Res.},
  keywords = {ML ProteinClassification SVM},
  month = {July},
  number = 13,
  pages = {3692--3697},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {{S}{V}{M}-{P}rot: {W}eb-based support vector machine software for functional classification of a protein from its primary sequence},
  volume = 31,
  year = 2003
}

@book{barber2012bayesian,
  abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
  added-at = {2016-07-14T20:27:13.000+0200},
  address = {Cambridge, UK},
  author = {Barber, David},
  biburl = {https://www.bibsonomy.org/bibtex/26c2ee9824a5146c12cf8c25156bdf257/vngudivada},
  edition = {First},
  interhash = {09a3db9ff3df9e0e2cb5928621ca3f7e},
  intrahash = {6c2ee9824a5146c12cf8c25156bdf257},
  isbn = {978-0521518147},
  keywords = {BayesianLearning Book ML},
  privnote = {This book is easier to follow than Probabilistic Graphical Models: Principles and Techniques by Daphne Kollerand and Nir Friedman. The latter is also a good book.},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Bayesian Reasoning and Machine Learning},
  year = 2012
}

@book{harispe2015semantic,
  abstract = {Artificial Intelligence federates numerous scientific fields in the aim of developing machines able to assist human operators performing complex treatments.most of which demand high cognitive skills (e.g. learning or decision processes). Central to this quest is to give machines the ability to estimate the likeness or similarity between things in the way human beings estimate the similarity between stimuli. In this context, this book focuses on semantic measures: approaches designed for comparing semantic entities such as units of language, e.g. words, sentences, or concepts and instances defined into knowledge bases. The aim of these measures is to assess the similarity or relatedness of such semantic entities by taking into account their semantics, i.e. their meaning.intuitively, the words tea and coffee, which both refer to stimulating beverage, will be estimated to be more semantically similar than the words toffee (confection) and coffee, despite that the last pair has a higher syntactic similarity. The two state-of-the-art approaches for estimating and quantifying semantic similarities/relatedness of semantic entities are presented in detail: the first one relies on corpora analysis and is based on Natural Language Processing techniques and semantic models while the second is based on more or less formal, computer-readable and workable forms of knowledge such as semantic networks, thesauri or ontologies. Semantic measures are widely used today to compare units of language, concepts, instances or even resources indexed by them (e.g., documents, genes). They are central elements of a large variety of Natural Language Processing applications and knowledge-based treatments, and have therefore naturally been subject to intensive and interdisciplinary research efforts during last decades. Beyond a simple inventory and categorization of existing measures, the aim of this monograph is to convey novices as well as researchers of these domains toward a better understanding of semantic similarity estimation and more generally semantic measures. To this end, we propose an in-depth characterization of existing proposals by discussing their features, the assumptions on which they are based and empirical results regarding their performance in particular applications. By answering these questions and by providing a detailed discussion on the foundations of semantic measures, our aim is to give the reader key knowledge required to: (i) select the more relevant methods according to a particular usage context, (ii) understand the challenges offered to this field of study, (iii) distinguish room of improvements for state-of-the-art approaches and (iv) stimulate creativity toward the development of new approaches. In this aim, several definitions, theoretical and practical details, as well as concrete applications are presented.},
  added-at = {2016-07-14T23:18:14.000+0200},
  author = {Harispe, Sébastien and Ranwez, Sylvie and Janaqi, Stefan and Montmain, Jacky},
  biburl = {https://www.bibsonomy.org/bibtex/25660248e6389f3f2a8de9a2ecdb3b09c/vngudivada},
  interhash = {6d29ac5ba1d28ab8bda41c6fefdd6f8c},
  intrahash = {5660248e6389f3f2a8de9a2ecdb3b09c},
  isbn = {9781627054478 1627054472 1627054464 9781627054461},
  keywords = {NLP Ontology SynthesisLecture},
  refid = {911245957},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Semantic similarity from natural language and ontology analysis},
  url = {http://dx.doi.org/10.2200/S00639ED1V01Y201504HLT027},
  year = 2015
}

@book{kubler2009dependency,
  abstract = {Dependency-based methods for syntactic parsing have become increasingly popular in natural language processing in recent years. This book gives a thorough introduction to the methods that are most widely used today. After an introduction to dependency grammar and dependency parsing, followed by a formal characterization of the dependency parsing problem, the book surveys the three major classes of parsing models that are in current use: transition-based, graph-based, and grammar-based models. It continues with a chapter on evaluation and one on the comparison of different methods, and it closes with a few words on current trends and future prospects of dependency parsing. The book presupposes a knowledge of basic concepts in linguistics and computer science, as well as some knowledge of parsing methods for constituency-based representations.},
  added-at = {2016-07-14T23:38:18.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Kübler, Sandra and McDonald, Ryan and Nivre, Joakim},
  biburl = {https://www.bibsonomy.org/bibtex/2a109800cfdc072da6d47544ced069138/vngudivada},
  interhash = {b4cf00e3891d1e6aceb333a0b9775561},
  intrahash = {a109800cfdc072da6d47544ced069138},
  isbn = {9781598295979 1598295977},
  keywords = {DependencyParsing SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {299803264},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Dependency parsing},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881034},
  year = 2009
}

@article{palmer2010semantic,
  abstract = {This book is aimed at providing an overview of several aspects of semantic role labeling. Chapter 1 begins with linguistic background on the definition of semantic roles and the controversies surrounding them. Chapter 2 describes how the theories have led to structured lexicons such as FrameNet, VerbNet and the PropBank Frame Files that in turn provide the basis for large scale semantic annotation of corpora. This data has facilitated the development of automatic semantic role labeling systems based on supervised machine learning techniques. Chapter 3 presents the general principles of applying both supervised and unsupervised machine learning to this task, with a description of the standard stages and feature choices, as well as giving details of several specific systems. Recent advances include the use of joint inference to take advantage of context sensitivities, and attempts to improve performance by closer integration of the syntactic parsing task with semantic role labeling. Chapter 3 also discusses the impact the granularity of the semantic roles has on system performance. Having outlined the basic approach with respect to English, Chapter 4 goes on to discuss applying the same techniques to other languages, using Chinese as the primary example. Although substantial training data is available for Chinese, this is not the case for many other languages, and techniques for projecting English role labels onto parallel corpora are also presented.},
  added-at = {2016-07-14T23:37:13.000+0200},
  address = {San Rafael, California},
  author = {Palmer, Martha Stone and Gildea, Daniel and Xue, Nianwen},
  biburl = {https://www.bibsonomy.org/bibtex/2e5b988e77f9f11c1aac64b812e5aa212/vngudivada},
  doi = {10.2200/S00239ED1V01Y200912HLT006},
  interhash = {d2fb58d86c09a8e8eb348b04c142e3bf},
  intrahash = {e5b988e77f9f11c1aac64b812e5aa212},
  isbn = {9781598298321},
  journal = {Synthesis Lectures on Human Language Technologies},
  keywords = {SemanticRoleLabeling SynthesisLecture},
  number = 1,
  pages = {1 -- 103},
  publisher = {Morgan \& Claypool},
  refid = {506483341},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Semantic role labeling},
  volume = 3,
  year = 2010
}

@book{brynjolfsson2014second,
  abstract = {A revolution is under way. In recent years, Google's autonomous cars have logged thousands of miles on American highways and IBM's Watson trounced the best human Jeopardy! players. Digital technologies -- with hardware, software, and networks at their core -- will in the near future diagnose diseases more accurately than doctors can, apply enormous data sets to transform retailing, and accomplish many tasks once considered uniquely human. In The Second Machine Age MIT's Erik Brynjolfsson and Andrew McAfee -- two thinkers at the forefront of their field -- reveal the forces driving the reinvention of our lives and our economy. As the full impact of digital technologies is felt, we will realize immense bounty in the form of dazzling personal technology, advanced infrastructure, and near-boundless access to the cultural items that enrich our lives. Amid this bounty will also be wrenching change. Professions of all kinds, from lawyers to truck drivers, will be forever upended. Companies will be forced to transform or die. Recent economic indicators reflect this shift: fewer people are working, and wages are falling even as productivity and profits soar. Drawing on years of research and up-to-the-minute trends, Brynjolfsson and McAfee identify the best strategies for survival and offer a new path to prosperity. These include revamping education so that it prepares people for the next economy instead of the last one, designing new collaborations that pair brute processing power with human ingenuity, and embracing policies that make sense in a radically transformed landscape. A fundamentally optimistic book, The Second Machine Age will alter how we think about issues of technological, societal, and economic progress. - Publisher. This book takes a look into the future of business, work, and the economy in a digital world. In recent years, computers have learned to diagnose diseases, drive cars, and win at Jeopardy!. Advances like these have created unprecedented economic bounty, but in their wake median income has stagnated and the share of the population with jobs has fallen. In this book the authors reveal the technological forces driving this reinvention of the economy and chart a path toward future prosperity. They describe how humans will have to keep pace with machines in order to become prosperous in the future and identify strategies and policies for business and individuals to use to combine digital processing power with human ingenuity. -- From book jacket.},
  added-at = {2016-07-14T21:55:18.000+0200},
  author = {Brynjolfsson, Erik and McAfee, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/24ab0f6af000c7f18beada0697653ce17/vngudivada},
  interhash = {76b369c308f4525ea032109e8f0bff07},
  intrahash = {4ab0f6af000c7f18beada0697653ce17},
  isbn = {9780393239355 0393239357 9780393350647 0393350649},
  keywords = {Book CognitiveComputing},
  refid = {867423744},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {The second machine age : work, progress, and prosperity in a time of brilliant technologies},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780393350647},
  year = 2014
}

@book{ling2012crafting,
  abstract = {What is it like to be a researcher or a scientist? For young people, including graduate students and junior faculty members in universities, how can they identify good ideas for research? How do they conduct solid research to verify and realize their new ideas? How can they formulate their ideas and research results into high quality articles, and publish them in highly competitive journals and conferences? What are effective ways to supervise graduate students so that they can establish themselves quickly in their research careers? In this book, Ling and Yang answer these questions in a step-by-step manner with specific and concrete examples from their first-hand research experience.},
  added-at = {2016-07-14T22:45:37.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Ling, Charles X. and Yang, Qiang},
  biburl = {https://www.bibsonomy.org/bibtex/2301881636e5e053a9732bd7b61627bec/vngudivada},
  interhash = {ffb5ecce9a4cfa8b8eb6fc88a35a09ab},
  intrahash = {301881636e5e053a9732bd7b61627bec},
  isbn = {9781608458110 1608458113},
  keywords = {Research SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {795404077},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Crafting your research future a guide to successful master's and Ph. D. degrees in science \& engineering},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=919699},
  year = 2012
}

@book{zhu2009introduction,
  abstract = {Semi-supervised learning is a learning paradigm concerned with the study of how computers and natural systems such as humans learn in the presence of both labeled and unlabeled data. Traditionally, learning has been studied either in the unsupervised paradigm (e.g., clustering, outlier detection) where all the data is unlabeled, or in the supervised paradigm (e.g., classification, regression) where all the data is labeled. The goal of semi-supervised learning is to understand how combining labeled and unlabeled data may change the learning behavior, and design algorithms that take advantage of such a combination. Semi-supervised learning is of great interest in machine learning and data mining because it can use readily available unlabeled data to improve supervised learning tasks when the labeled data is scarce or expensive. Semi-supervised learning also shows potential as a quantitative tool to understand human category learning, where most of the input is self-evidently unlabeled. In this introductory book, we present some popular semi-supervised learning models, including self-training, mixture models, co-training and multiview learning, graph-based methods, and semisupervised support vector machines. For each model, we discuss its basic mathematical formulation. The success of semi-supervised learning depends critically on some underlying assumptions. We emphasize the assumptions made by each model and give counterexamples when appropriate to demonstrate the limitations of the different models. In addition, we discuss semi-supervised learning for cognitive psychology. Finally, we give a computational learning theoretic perspective on semisupervised learning, and we conclude the book with a brief discussion of open questions in the field.},
  added-at = {2016-07-15T04:12:36.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Zhu, Xiaojin and Goldberg, A. B.},
  biburl = {https://www.bibsonomy.org/bibtex/2523867daeee1de03faa7aeaf85a80ba8/vngudivada},
  interhash = {b7bbcd90364051d64ea01112c64f2513},
  intrahash = {523867daeee1de03faa7aeaf85a80ba8},
  isbn = {9781598295481 1598295489},
  keywords = {SemiSupervisedLearning SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {428541480},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Introduction to semi-supervised learning},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881022},
  year = 2009
}

@book{james2013introduction,
  abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
  added-at = {2016-07-14T22:18:44.000+0200},
  address = {New York, NY},
  author = {James, Gareth},
  biburl = {https://www.bibsonomy.org/bibtex/2d572fa9420de6412364250e0eb551228/vngudivada},
  interhash = {4a17ec5eda95a576dc1b14320be4579d},
  intrahash = {d572fa9420de6412364250e0eb551228},
  isbn = {978-1461471370},
  keywords = {Book ML StatisticalLearning},
  publisher = {Springer},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {An introduction to statistical learning: with applications in R},
  url = {http://dx.doi.org/10.1007/978-1-4614-7138-7},
  year = 2013
}

@book{carenini2011methods,
  abstract = {Due to the Internet Revolution, human conversational data--in written forms--are accumulating at a phenomenal rate. At the same time, improvements in speech technology enable many spoken conversations to be transcribed. Individuals and organizations engage in email exchanges, face-to-face meetings, blogging, texting and other social media activities. The advances in natural language processing provide ample opportunities for these "informal documents" to be analyzed and mined, thus creating numerous new and valuable applications. This book presents a set of computational methods to extract information from conversational data, and to provide natural language summaries of the data. The book begins with an overview of basic concepts, such as the differences between extractive and abstractive summaries, and metrics for evaluating the effectiveness of summarization and various extraction tasks. It also describes some of the benchmark corpora used in the literature. The book introduces extraction and mining methods for performing subjectivity and sentiment detection, topic segmentation and modeling, and the extraction of conversational structure. It also describes frameworks for conducting dialogue act recognition, decision and action item detection, and extraction of thread structure. There is a specific focus on performing all these tasks on conversational data, such as meeting transcripts (which exemplify synchronous conversations) and emails (which exemplify asynchronous conversations). Very recent approaches to deal with blogs, discussion forums and microblogs (e.g., Twitter) are also discussed. The second half of this book focuses on natural language summarization of conversational data. It gives an overview of several extractive and abstractive summarizers developed for emails, meetings, blogs and forums. It also describes attempts for building multi-modal summarizers. Last but not least, the book concludes with thoughts on topics for further development.},
  added-at = {2016-07-15T00:55:24.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Carenini, Giuseppe and Murray, Gabriel and Ng, Raymond Tak-yan},
  biburl = {https://www.bibsonomy.org/bibtex/24aa793ad7723b3e4ea31c093b2b5e55b/vngudivada},
  interhash = {bfa4ed032051017a8271a4ab766bc4dd},
  intrahash = {4aa793ad7723b3e4ea31c093b2b5e55b},
  isbn = {9781608453917 160845391X 1608453901 9781608453900},
  keywords = {DataMining TextSummarization SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {744268237},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Methods for mining and summarizing text conversations},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881197},
  year = 2011
}

@book{leacock2014automated,
  abstract = {It has been estimated that over a billion people are using or learning English as a second or foreign language, and the numbers are growing not only for English but for other languages as well. These language learners provide a burgeoning market for tools that help identify and correct learners' writing errors. Unfortunately, the errors targeted by typical commercial proofreading tools do not include those aspects of a second language that are hardest to learn. This volume describes the types of constructions English language learners find most difficult; constructions containing prepositions, articles, and collocations. It provides an overview of the automated approaches that have been developed to identify and correct these and other classes of learner errors in a number of languages. Error annotation and system evaluation are particularly important topics in grammatical error detection because there are no commonly accepted standards. Chapters in the book describe the options available to researchers, recommend best practices for reporting results, and present annotation and evaluation schemes. The final chapters explore recent innovative work that opens new directions for research. It is the authors' hope that this volume will continue to contribute to the growing interest in grammatical error detection by encouraging researchers to take a closer look at the field and its many challenging problems.},
  added-at = {2016-07-14T23:21:17.000+0200},
  author = {Leacock, Claudia and Chodorow, Martin and Gamon, Michael and Tetreault, Joel},
  biburl = {https://www.bibsonomy.org/bibtex/25f5db7f38fa3756516d368fa3ea9c2c8/vngudivada},
  interhash = {4def6bdc061e65904821c8d289ca0b19},
  intrahash = {5f5db7f38fa3756516d368fa3ea9c2c8},
  keywords = {GrammaticalError LanguageLearning SynthesisLecture},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Automated grammatical error detection for language learners},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1651106},
  year = 2014
}

@book{augsten2014similarity,
  abstract = {State-of-the-art database systems manage and process a variety of complex objects, including strings and trees. For such objects equality comparisons are often not meaningful and must be replaced by similarity comparisons. This book describes the concepts and techniques to incorporate similarity into database systems. We start out by discussing the properties of strings and trees, and identify the edit distance as the de facto standard for comparing complex objects. Since the edit distance is computationally expensive, token-based distances have been introduced to speed up edit distance computations. The basic idea is to decompose complex objects into sets of tokens that can be compared efficiently. Token-based distances are used to compute an approximation of the edit distance and prune expensive edit distance calculations. A key observation when computing similarity joins is that many of the object pairs, for which the similarity is computed, are very different from each other. Filters exploit this property to improve the performance of similarity joins. A filter preprocesses the input data sets and produces a set of candidate pairs. The distance function is evaluated on the candidate pairs only. We describe the essential query processing techniques for filters based on lower and upper bounds. For token equality joins we describe prefix, size, positional and partitioning filters, which can be used to avoid the computation of small intersections that are not needed since the similarity would be too low.},
  added-at = {2016-07-15T01:34:55.000+0200},
  author = {Augsten, Nikolaus and Böhlen, Michael H.},
  biburl = {https://www.bibsonomy.org/bibtex/2cd7ec5cbdc04595c58268630c8cc7038/vngudivada},
  interhash = {014901db372220ed1cc44cc241e4bdb9},
  intrahash = {cd7ec5cbdc04595c58268630c8cc7038},
  isbn = {9781627050296 1627050299},
  keywords = {SimilarityJoin SynthesisLecture},
  refid = {866563916},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Similarity joins in relational database systems},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1566026},
  year = 2014
}

@book{evans2006cognitive,
  abstract = {An authoritative general introduction to cognitive linguistics, this book provides up-to-date coverage of all areas of the field and sets in context recent developments within cognitive semantics and cognitive approaches to grammar.},
  added-at = {2016-07-14T22:16:32.000+0200},
  address = {Mahwah, N.J.},
  author = {Evans, Vyvyan and Green, Melanie},
  biburl = {https://www.bibsonomy.org/bibtex/281cc25011a61a70779b83de641f88462/vngudivada},
  interhash = {2b6048bead8ecbbdec369994f50f7621},
  intrahash = {81cc25011a61a70779b83de641f88462},
  isbn = {0805860142 9780805860146 0805860134 9780805860139},
  keywords = {Book CognitiveLinguistics},
  publisher = {L. Erlbaum},
  refid = {69171708},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Cognitive linguistics : an introduction},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780805860146},
  year = 2006
}

@book{mamoulis2012spatial,
  abstract = {Spatial database management deals with the storage, indexing, and querying of data with spatial features, such as location and geometric extent. Many applications require the efficient management of spatial data, including Geographic Information Systems, Computer Aided Design, and Location Based Services. The goal of this book is to provide the reader with an overview of spatial data management technology, with an emphasis on indexing and search techniques. It first introduces spatial data models and queries and discusses the main issues of extending a database system to support spatial data. It presents indexing approaches for spatial data, with a focus on the R-tree. Query evaluation and optimization techniques for the most popular spatial query types (selections, nearest neighbor search, and spatial joins) are portrayed for data in Euclidean spaces and spatial networks. The book concludes by demonstrating the ample application of spatial data management technology on a wide range of related application domains: management of spatio-temporal data and high-dimensional feature vectors, multi-criteria ranking, data mining and OLAP, privacy-preserving data publishing, and spatial keyword search.},
  added-at = {2016-07-15T01:13:36.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Mamoulis, Nikos},
  biburl = {https://www.bibsonomy.org/bibtex/247924a46dbbcfa5c11d927df70c279e4/vngudivada},
  interhash = {a4b8b363e87f1e11679489cf8ee1ce60},
  intrahash = {47924a46dbbcfa5c11d927df70c279e4},
  isbn = {9781608458332 1608458334 1608458326 9781608458325},
  keywords = {SpatialDataManagement SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {767844616},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Spatial data management},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881292},
  year = 2012
}

@book{alishahi2011computational,
  abstract = {Human language acquisition has been studied for centuries, but using computational modeling for such studies is a relatively recent trend. However, computational approaches to language learning have become increasingly popular, mainly due to advances in developing machine learning techniques, and the availability of vast collections of experimental data on child language learning and child-adult interaction. Many of the existing computational models attempt to study the complex task of learning a language under cognitive plausibility criteria (such as memory and processing limitations that humans face), and to explain the developmental stages observed in children. By simulating the process of child language learning, computational models can show us which linguistic representations are learnable from the input that children have access to, and which mechanisms yield the same patterns of behaviour that children exhibit during this process. In doing so, computational modeling provides insight into the plausible mechanisms involved in human language acquisition, and inspires the development of better language models and techniques. This book provides an overview of the main research questions in the field of human language acquisition. It reviews the most commonly used computational frameworks, methodologies and resources for modeling child language learning, and the evaluation techniques used for assessing these computational models. The book is aimed at cognitive scientists who want to become familiar with the available computational methods for investigating problems related to human language acquisition, as well as computational linguists who are interested in applying their skills to the study of child language acquisition. Different aspects of language learning are discussed in separate chapters, including the acquisition of the individual words, the general regularities which govern word and sentence form, and the associations between form and meaning. For each of these aspects, the challenges of the task are discussed and the relevant empirical findings on children are summarized. Furthermore, the existing computational models that attempt to simulate the task under study are reviewed, and a number of case studies are presented.},
  added-at = {2016-07-14T23:50:57.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Alishahi, Afra},
  biburl = {https://www.bibsonomy.org/bibtex/202149ef2a71aeaf42eb4db938bbdf61f/vngudivada},
  interhash = {d2c07834695b894ec94211a17b600756},
  intrahash = {02149ef2a71aeaf42eb4db938bbdf61f},
  isbn = {9781608453405 1608453405},
  keywords = {LanguageAcquisition SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {680315233},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Computational modeling of human language acquisition},
  url = {http://site.ebrary.com/id/10530787},
  year = 2011
}

@book{ilyas2011probabilistic,
  abstract = {Ranking queries are widely used in data exploration, data analysis and decision making scenarios. While most of the currently proposed ranking techniques focus on deterministic data, several emerging applications involve data that are imprecise or uncertain. Ranking uncertain data raises new challenges in query semantics and processing, making conventional methods inapplicable. Furthermore, the interplay between ranking and uncertainty models introduces new dimensions for ordering query results that do not exist in the traditional settings. This lecture describes new formulations and processing techniques for ranking queries on uncertain data. The formulations are based on marriage of traditional ranking semantics with possible worlds semantics under widely-adopted uncertainty models. In particular, we focus on discussing the impact of tuple-level and attribute-level uncertainty on the semantics and processing techniques of ranking queries. Under the tuple-level uncertainty model, we describe new processing techniques leveraging the capabilities of relational database systems to recognize and handle data uncertainty in score-based ranking. Under the attribute-level uncertainty model, we describe new probabilistic ranking models and a set of query evaluation algorithms, including sampling-based techniques. We also discuss supporting rank join queries on uncertain data, and we show how to extend current rank join methods to handle uncertainty in scoring attributes.},
  added-at = {2016-07-15T01:10:35.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Ilyas, Ihab F. and Soliman, Mohamed A.},
  biburl = {https://www.bibsonomy.org/bibtex/2310f85e82bbbd170a78fea0dc24649e2/vngudivada},
  interhash = {1117110291d90af9a5529d87822be0aa},
  intrahash = {310f85e82bbbd170a78fea0dc24649e2},
  isbn = {9781608455683 1608455688 160845567X 9781608455676},
  keywords = {ProbabilisticRanking RDBMS SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {713369505},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Probabilistic ranking techniques in relational databases},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=3017308},
  year = 2011
}

@book{chen2014information,
  abstract = {Research on social networks has exploded over the last decade. To a large extent, this has been fueled by the spectacular growth of social media and online social networking sites, which continue growing at a very fast pace, as well as by the increasing availability of very large social network datasets for purposes of research. A rich body of this research has been devoted to the analysis of the propagation of information, influence, innovations, infections, practices and customs through networks. Can we build models to explain the way these propagations occur? How can we validate our models against any available real datasets consisting of a social network and propagation traces that occurred in the past? These are just some questions studied by researchers in this area. Information propagation models find applications in viral marketing, outbreak detection, finding key blog posts to read in order to catch important stories, finding leaders or trendsetters, information feed ranking, etc. A number of algorithmic problems arising in these applications have been abstracted and studied extensively by researchers under the garb of influence maximization.},
  added-at = {2016-07-15T00:56:53.000+0200},
  author = {Chen, Wei and Lakshmanan, Laks V. S. and Castillo, Carlos},
  biburl = {https://www.bibsonomy.org/bibtex/21c29369a807e8dc4defc307f4cae258b/vngudivada},
  interhash = {298f1df78540459f306ff1b91389a116},
  intrahash = {1c29369a807e8dc4defc307f4cae258b},
  isbn = {9781627051163 1627051163 1627051155 9781627051156},
  keywords = {InfluencePropagation SocialNetworks SynthesisLecture},
  refid = {862937428},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Information and influence propagation in social networks},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1543318},
  year = 2014
}

@book{dragut2012query,
  abstract = {There are millions of searchable data sources on the web and to a large extent their contents can only be reached through their own query interfaces. There is an enormous interest in making the data in these sources easily accessible. There are primarily two general approaches to achieve this objective. The first is to surface the contents of these sources from the deep web and add the contents to the index of regular search engines. The second is to integrate the searching capabilities of these sources and support integrated access to them. In this book, we introduce the state-of-the-art techniques for extracting, understanding, and integrating the query interfaces of deep web data sources. These techniques are critical for producing an integrated query interface for each domain. The interface serves as the mediator for searching all data sources in the concerned domain. While query interface integration is only relevant for the deep web integration approach, the extraction and understanding of query interfaces are critical for both deep web exploration approaches. This book aims to provide in-depth and comprehensive coverage of the key technologies needed to create high quality integrated query interfaces automatically. The following technical issues are discussed in detail in this book: query interface modeling, query interface extraction, query interface clustering, query interface matching, query interface attribute integration, and query interface integration.},
  added-at = {2016-07-15T01:45:28.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Dragut, Eduard Constantin and Meng, Weiyi and Yu, C. T.},
  biburl = {https://www.bibsonomy.org/bibtex/2eff8fbadc782a056c92c6d0fb2164428/vngudivada},
  interhash = {4d630f91674570894cfba5d0fc45511d},
  intrahash = {eff8fbadc782a056c92c6d0fb2164428},
  isbn = {9781608458950 1608458954 1608458946 9781608458943},
  keywords = {DeepWebQuery SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {799359868},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Deep web query interface understanding and integration},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=947422},
  year = 2012
}

@book{fan2012foundations,
  abstract = {Data quality is one of the most important problems in data management. A database system typically aims to support the creation, maintenance, and use of large amount of data, focusing on the quantity of data. However, real-life data are often dirty: inconsistent, duplicated, inaccurate, incomplete, or stale. Dirty data in a database routinely generate misleading or biased analytical results and decisions, and lead to loss of revenues, credibility and customers. With this comes the need for data quality management. In contrast to traditional data management tasks, data quality management enables the detection and correction of errors in the data, syntactic or semantic, in order to improve the quality of the data and hence, add value to business processes.},
  added-at = {2016-07-15T01:03:09.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Fan, Wenfei and Geerts, Floris},
  biburl = {https://www.bibsonomy.org/bibtex/23b71d3f96d7ca113383a006119a86e7a/vngudivada},
  interhash = {28301cacff06c9239471a86c563b0529},
  intrahash = {3b71d3f96d7ca113383a006119a86e7a},
  isbn = {9781608457786 1608457788 160845777X 9781608457779},
  keywords = {DataQuality SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {806466412},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Foundations of data quality management},
  url = {http://site.ebrary.com/id/10591109},
  year = 2012
}

@book{nastase2013semantic,
  abstract = {People make sense of a text by identifying the semantic relations which connect the entities or concepts described by that text. A system which aspires to human-like performance must also be equipped to identify, and learn from, semantic relations in the texts it processes. Understanding even a simple sentence such as "Opportunity and Curiosity find similar rocks on Mars" requires recognizing relations (rocks are located on Mars, signalled by the word on) and drawing on already known relations (Opportunity and Curiosity are instances of the class of Mars rovers). A language-understanding system should be able to find such relations in documents and progressively build a knowledge base or even an ontology. Resources of this kind assist continuous learning and other advanced language-processing tasks such as text summarization, question answering and machine translation.},
  added-at = {2016-07-14T23:42:22.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Nastase, Vivi},
  biburl = {https://www.bibsonomy.org/bibtex/2a35e7fe22205c0c04d144da31cbcde9a/vngudivada},
  interhash = {bd9d47e157df984f237b172c398266c0},
  intrahash = {a35e7fe22205c0c04d144da31cbcde9a},
  isbn = {9781608459803 1608459802},
  keywords = {SemanticRelation SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {844063325},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Semantic relations between nominals},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1183970},
  year = 2013
}

@book{tiedemann2011bitext,
  abstract = {This book provides an overview of various techniques for the alignment of bitexts. It describes general concepts and strategies that can be applied to map corresponding parts in parallel documents on various levels of granularity. Bitexts are valuable linguistic resources for many different research fields and practical applications. The most predominant application is machine translation, in particular, statistical machine translation. However, there are various other threads that can be followed which may be supported by the rich linguistic knowledge implicitly stored in parallel resources. Bitexts have been explored in lexicography, word sense disambiguation, terminology extraction, computer-aided language learning and translation studies to name just a few. The book covers the essential tasks that have to be carried out when building parallel corpora starting from the collection of translated documents up to sub-sentential alignments. In particular, it describes various approaches to document alignment, sentence alignment, word alignment and tree structure alignment. It also includes a list of resources and a comprehensive review of the literature on alignment techniques.},
  added-at = {2016-07-14T23:52:07.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Tiedemann, Jörg},
  biburl = {https://www.bibsonomy.org/bibtex/265b1f90280ea22743b4870a4226d08e0/vngudivada},
  interhash = {5bf5ad4db49d2610a39aa93f1304268e},
  intrahash = {65b1f90280ea22743b4870a4226d08e0},
  isbn = {9781608455119 1608455114 9781608455102 1608455106},
  keywords = {BitextAlignment SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {742535715},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Bitext alignment},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881222},
  year = 2011
}

@book{cimiano2014ontologybased,
  abstract = {For humans, understanding a natural language sentence or discourse is so effortless that we hardly ever think about it. For machines, however, the task of interpreting natural language, especially grasping meaning beyond the literal content, has proven extremely difficult and requires a large amount of background knowledge. This book focuses on the interpretation of natural language with respect to specific domain knowledge captured in ontologies. the main contribution is an approach that puts ontologies at the center of the interpretation process. This means that ontologies not only provide a formalization of domain knowledge necessary for interpretation but also support and guide the construction of meaning representations. We start with an introduction to ontologies and demonstrate how linguistic information can be attached to them by means of the ontology lexicon model lemon. These lexica then serve as basis for the automatic generation of grammars, which we use to compositionally construct meaning representations that conform with the vocabulary of an underlying ontology. As a result, the level of representational granularity is not driven by language but by the semantic distinctions made in the underlying ontology and thus by distinctions that are relevant in the context of a particular domain. We highlight some of the challenges involved in the construction of ontology-based meaning representations, and show how ontologies can be exploited for ambiguity resolution and the interpretation of temporal expressions. Finally, we present a question answering system that combines all tools and techniques introduced throughout the book in a real-world application, and sketch how the presented approach can scale to larger, multi-domain scenarios in the context of the Semantic Web.},
  added-at = {2016-07-14T23:27:24.000+0200},
  author = {Cimiano, Philipp and Unger, Andrea Christina and McCrae, John},
  biburl = {https://www.bibsonomy.org/bibtex/2fc9c383e7d9b9b1b820893f86bc238c5/vngudivada},
  interhash = {64732ef69813b3c2af2e033f783a7785},
  intrahash = {fc9c383e7d9b9b1b820893f86bc238c5},
  isbn = {9781608459902 160845990X},
  keywords = {NLP Ontology SynthesisLecture},
  refid = {877885206},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Ontology-based interpretation of natural language},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1663901},
  year = 2014
}

@book{leytonbrown2008essentials,
  abstract = {Game theory is the mathematical study of interaction among independent, self-interested agents. The audience for game theory has grown dramatically in recent years, and now spans disciplines as diverse as political science, biology, psychology, economics, linguistics, sociology, and computer science, among others. What has been missing is a relatively short introduction to the field covering the common basis that anyone with a professional interest in game theory is likely to require. Such a text would minimize notation, ruthlessly focus on essentials, and yet not sacrifice rigor. This Synthesis Lecture aims to fill this gap by providing a concise and accessible introduction to the field. It covers the main classes of games, their representations, and the main concepts used to analyze them.},
  added-at = {2016-07-15T04:07:30.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Leyton-Brown, Kevin and Shoham, Yoav},
  biburl = {https://www.bibsonomy.org/bibtex/255cd763b1fcf0d61ffc1bfe9d8f3497c/vngudivada},
  interhash = {e514316fb1ad3316c5afef20d8360b21},
  intrahash = {55cd763b1fcf0d61ffc1bfe9d8f3497c},
  isbn = {9781598295931 1598295934 9781598295948 1598295942},
  keywords = {GameTheory SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {231624172},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Essentials of game theory: a concise, multidisciplinary introduction},
  url = {http://site.ebrary.com/id/10515554},
  year = 2008
}

@book{schafer2013corpus,
  abstract = {The World Wide Web constitutes the largest existing source of texts written in a great variety of languages. A feasible and sound way of exploiting this data for linguistic research is to compile a static corpus for a given language. There are several advantages of this approach: (i) Working with such corpora obviates the problems encountered when using Internet search engines in quantitative linguistic research (such as non-transparent ranking algorithms). (ii) Creating a corpus from web data is virtually free. (iii) The size of corpora compiled from the WWW may exceed by several orders of magnitudes the size of language resources offered elsewhere. (iv) The data is locally available to the user, and it can be linguistically post-processed and queried with the tools preferred by her/him. This book addresses the main practical tasks in the creation of web corpora up to giga-token size. Among these tasks are the sampling process (i. e., web crawling) and the usual cleanups including boilerplate removal and removal of duplicated content. Linguistic processing and problems with linguistic processing coming from the different kinds of noise in web corpora are also covered. Finally, the authors show how web corpora can be evaluated and compared to other corpora (such as traditionally compiled corpora).},
  added-at = {2016-07-14T23:24:11.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Schäfer, Roland and Bildhauer, Felix},
  biburl = {https://www.bibsonomy.org/bibtex/24cd34bf4ffd6211788e8aa47fc4f3d9a/vngudivada},
  interhash = {ede4d4ffe5f1f704d85d7010f99fc97a},
  intrahash = {4cd34bf4ffd6211788e8aa47fc4f3d9a},
  isbn = {9781608459841 1608459845 1608459837 9781608459834 9781627053129 1627053123},
  keywords = {Corpus NLP SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {855857901},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Web corpus construction},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1323256},
  year = 2013
}

@book{dekking2005modern,
  abstract = {"A Modern Introduction to Probability and Statistics has numerous quick exercises to give direct feedback to the students. In addition the book contains over 350 exercises, half of which have answers, of which half have full solutions. The only pre-requisite for the book is a first course in calculus; the text covers standard statistics and probability material, and develops beyond traditional parametric models to the Poisson process, and on to useful modern methods such as the bootstrap." "This will be a key text for undergraduates in Computer Science, Physics, Mathematics, Chemistry, Biology and Business Studies who are studying a mathematical statistics course, and also for more intensive engineering statistics courses for undergraduates in all engineering subjects."--Jacket.},
  added-at = {2016-07-14T22:06:37.000+0200},
  address = {London},
  author = {Dekking, Michel},
  biburl = {https://www.bibsonomy.org/bibtex/2db5741781265a272bf99045722cec621/vngudivada},
  interhash = {31c9bb0188ff18b4ddf57cf54a2c24ba},
  intrahash = {db5741781265a272bf99045722cec621},
  isbn = {9781852338961 1852338962 9781846281686 1846281687 9781849969529 1849969523},
  keywords = {Book Probability Statistics},
  publisher = {Springer},
  refid = {262680588},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {A modern introduction to probability and statistics : understanding why and how},
  url = {http://site.ebrary.com/id/10228809},
  year = 2005
}

@book{li2011learning,
  abstract = {Learning to rank refers to machine learning techniques for training the model in a ranking task. Learning to rank is useful for many applications in information retrieval, natural language processing, and data mining. Intensive studies have been conducted on the problem recently and significant progress has been made. This lecture gives an introduction to the area including the fundamental problems, existing approaches, theories, applications, and future work.},
  added-at = {2016-07-14T23:12:52.000+0200},
  address = {San Rafael, California},
  author = {Li, Hang},
  biburl = {https://www.bibsonomy.org/bibtex/2fb8b6ab93405aa9b0662f8408ec08f2d/vngudivada},
  doi = {10.2200/S00348ED1V01Y201104HLT012},
  interhash = {423cada1edb33233898582ab57dd6270},
  intrahash = {fb8b6ab93405aa9b0662f8408ec08f2d},
  keywords = {IR NLP Ranking SynthesisLecture},
  publisher = {Morgan \& Claypool},
  series = {Synthesis Lectures on Human Language Technologies},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Learning to rank for information retrieval and natural language processing},
  year = 2011
}

@book{meng2011advanced,
  abstract = {Among the search tools currently on the Web, search engines are the most well known thanks to the popularity of major search engines such as Google and Yahoo! While extremely successful, these major search engines do have serious limitations. This book introduces large-scale metasearch engine technology, which has the potential to overcome the limitations of the major search engines. Essentially, a metasearch engine is a search system that supports unified access to multiple existing search engines by passing the queries it receives to its component search engines and aggregating the returned results into a single ranked list. A large-scale metasearch engine has thousands or more component search engines. While metasearch engines were initially motivated by their ability to combine the search coverage of multiple search engines, there are also other benefits such as the potential to obtain better and fresher results and to reach the DeepWeb. The following major components of large-scale metasearch engines will be discussed in detail in this book: search engine selection, search engine incorporation, and result merging. Highly scalable and automated solutions for these components are emphasized. The authors make a strong case for the viability of the large-scale metasearch engine technology as a competitive technology for Web search.},
  added-at = {2016-07-15T01:41:35.000+0200},
  address = {San Rafael, California},
  author = {Meng, Weiyi and Yu, Clement T.},
  biburl = {https://www.bibsonomy.org/bibtex/2788eda6a96d09d653fbb59cd45d5b7ba/vngudivada},
  doi = {10.2200/S00307ED1V01Y201011DTM011},
  interhash = {cd822477425d07612adf7950e16cd099},
  intrahash = {788eda6a96d09d653fbb59cd45d5b7ba},
  isbn = {9781608451937},
  keywords = {MetasearchEngine SynthesisLecture},
  publisher = {Morgan \& Claypool},
  refid = {707877337},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Advanced metasearch engine technology},
  year = 2011
}

@book{teubner2013processing,
  abstract = {Computer architectures are quickly changing toward heterogeneous many-core systems. Such a trend opens up interesting opportunities but also raises immense challenges since the efficient use of heterogeneous many-core systems is not a trivial problem. In this paper, we explore how to program data processing operators on top of field-programmable gate arrays (FPGAs). FPGAs are very versatile in terms of how they can be used and can also be added as additional processing units in standard CPU sockets. In the paper, we study how data processing can be accelerated using an FPGA. Our results indicate that efficient usage of FPGAs involves non-trivial aspects such as having the right computation model (an asynchronous sorting network in this case); a careful implementation that balances all the design constraints in an FPGA; and the proper integration strategy to link the FPGA to the rest of the system. Once these issues are properly addressed, our experiments show that FPGAs exhibit performance figures competitive with those of modern general-purpose CPUs while offering significant advantages in terms of power consumption and parallel stream evaluation.},
  added-at = {2016-07-15T01:39:22.000+0200},
  author = {Teubner, Jens and Woods, Louis},
  biburl = {https://www.bibsonomy.org/bibtex/20d126fab6d2b59922caa21b4e13ffcdf/vngudivada},
  doi = {10.2200/S00514ED1V01Y201306DTM035},
  eprint = {http://dx.doi.org/10.2200/S00514ED1V01Y201306DTM035},
  interhash = {0d5f5bef877afcc9b6b382653f81bc37},
  intrahash = {0d126fab6d2b59922caa21b4e13ffcdf},
  journal = {Synthesis Lectures on Data Management},
  keywords = {FPGA SynthesisLecture},
  number = 2,
  pages = {1-118},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Data Processing on FPGAs},
  url = {http://dx.doi.org/10.2200/S00514ED1V01Y201306DTM035},
  volume = 5,
  year = 2013
}

@book{jamison2013making,
  abstract = {This book discusses the ways in which engineering educators are responding to the challenges that confront their profession. On the one hand, there is an overarching sustainability challenge: the need for engineers to relate to the problems brought to light in the debates about environmental protection, resource depletion, and climate change. There are also a range of societal challenges that are due to the permeation of science and technology into ever more areas of our societies and everyday lives, and finally, there are the intrinsic scientific and technological challenges stemming from the emergence of new fields of "technosciences" that mix science and technology in new combinations.},
  added-at = {2016-07-14T22:42:32.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Jamison, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/282fa2e4424d034d14792a566019fc70e/vngudivada},
  interhash = {418508468264ab95246db7311d2f1d8d},
  intrahash = {82fa2e4424d034d14792a566019fc70e},
  isbn = {9781627051606 1627051600 1627051597 9781627051590},
  keywords = {GreenEngineer Sustainability SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {836054473},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {The making of green engineers sustainable development and the hybrid imagination},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1160997},
  year = 2013
}

@book{loo2012declarative,
  abstract = {Declarative Networking is a programming methodology that enables developers to concisely specify network protocols and services, which are directly compiled to a dataflow framework that executes the specifications. Declarative networking proposes the use of a declarative query language for specifying and implementing network protocols, and employs a dataflow framework at runtime for communication and maintenance of network state. The primary goal of declarative networking is to greatly simplify the process of specifying, implementing, deploying and evolving a network design. In addition, declarative networking serves as an important step towards an extensible, evolvable network architecture that can support flexible, secure and efficient deployment of new network protocols.},
  added-at = {2016-07-15T01:27:24.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Loo, Boon Thau and Zhou, Wenchao},
  biburl = {https://www.bibsonomy.org/bibtex/22fcf6fbcb20b57dbaebfeab5b6b81278/vngudivada},
  interhash = {f551971e4b3c9994fa4a1b5f70c8341f},
  intrahash = {2fcf6fbcb20b57dbaebfeab5b6b81278},
  isbn = {9781608456024 1608456021},
  keywords = {DeclarativeNetworking SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {777009154},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Declarative networking},
  url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=440531},
  year = 2012
}

@book{kaynar2011theory,
  abstract = {This monograph presents the Timed Input/Output Automaton (TIOA) modeling framework, a basic mathematical framework to support description and analysis of timed (computing) systems. Timed systems are systems in which desirable correctness or performance properties of the system depend on the timing of events, not just on the order of their occurrence. Timed systems are employed in a wide range of domains including communications, embedded systems, real-time operating systems, and automated control. Many applications involving timed systems have strong safety, reliability, and predictability requirements, which make it important to have methods for systematic design of systems and rigorous analysis of timing-dependent behavior.},
  added-at = {2016-07-14T22:40:44.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Kaynar, Dilsun K.},
  biburl = {https://www.bibsonomy.org/bibtex/2bb59bb7a7f94a6541a3b77460d603d2f/vngudivada},
  interhash = {b51376e42d9cb43c57e0158c41443f9b},
  intrahash = {bb59bb7a7f94a6541a3b77460d603d2f},
  isbn = {9781608450039 1608450031},
  keywords = {Automata SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {707877341},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {The theory of timed I/O automata},
  url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=440382},
  year = 2011
}

@book{wong2010introduction,
  abstract = {This book introduces Chinese language-processing issues and techniques to readers who already have a basic background in natural language processing (NLP). Since the major difference between Chinese and Western languages is at the word level, the book primarily focuses on Chinese morphological analysis and introduces the concept, structure, and interword semantics of Chinese words. The following topics are covered: a general introduction to Chinese NLP; Chinese characters, morphemes, and words and the characteristics of Chinese words that have to be considered in NLP applications; Chinese word segmentation; unknown word detection; word meaning and Chinese linguistic resources; interword semantics based on word collocation and NLP techniques for collocation extraction.},
  added-at = {2016-07-14T23:40:04.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Wong, Kam-Fai},
  biburl = {https://www.bibsonomy.org/bibtex/2634e801637f4f8252dfe69da4248f9d1/vngudivada},
  interhash = {507a64a197b996f93bbb3462a1432ba9},
  intrahash = {634e801637f4f8252dfe69da4248f9d1},
  isbn = {9781598299335 1598299336},
  keywords = {ChineseNLP SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {472468557},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Introduction to Chinese natural language processing},
  url = {http://site.ebrary.com/id/10515567},
  year = 2010
}

@book{settles2012active,
  abstract = {The key idea behind active learning is that a machine learning algorithm can perform better with less training if it is allowed to choose the data from which it learns. An active learner may pose "queries," usually in the form of unlabeled data instances to be labeled by an "oracle" (e.g., a human annotator) that already understands the nature of the problem. This sort of approach is well-motivated in many modern machine learning and data mining applications, where unlabeled data may be abundant or easy to come by, but training labels are difficult, time-consuming, or expensive to obtain. This book is a general introduction to active learning. It outlines several scenarios in which queries might be formulated, and details many query selection algorithms which have been organized into four broad categories, or "query selection frameworks."We also touch on some of the theoretical foundations of active learning, and conclude with an overview of the strengths and weaknesses of these approaches in practice, including a summary of ongoing work to address these open challenges and opportunities.},
  added-at = {2016-07-15T04:13:56.000+0200},
  address = {San Rafael, California},
  author = {Settles, Burr},
  biburl = {https://www.bibsonomy.org/bibtex/217d1001b6fef5289d36375c38a774b2d/vngudivada},
  interhash = {109a5d54ab3e8fe26fbbebb89a67891f},
  intrahash = {17d1001b6fef5289d36375c38a774b2d},
  isbn = {9781608457267},
  keywords = {ActiveLearning SynthesisLecture},
  publisher = {Morgan \& Claypool},
  refid = {799360189},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Active learning},
  year = 2012
}

@book{ng2013perspectives,
  abstract = {In the 1980s, traditional Business Intelligence (BI) systems focused on the delivery of reports that describe the state of business activities in the past, such as for questions like "How did our sales perform during the last quarter?" A decade later, there was a shift to more interactive content that presented how the business was performing at the present time, answering questions like "How are we doing right now?" Today the focus of BI users are looking into the future. "Given what I did before and how I am currently doing this quarter, how will I do next quarter?" Furthermore, fuelled by the demands of Big Data, BI systems are going through a time of incredible change. Predictive analytics, high volume data, unstructured data, social data, mobile, consumable analytics, and data visualization are all examples of demands and capabilities that have become critical within just the past few years, and are growing at an unprecedented pace.},
  added-at = {2016-07-15T01:15:20.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Ng, Raymond Tak-yan},
  biburl = {https://www.bibsonomy.org/bibtex/2cbcc0b2f57b3540a526b9dcb101fd45f/vngudivada},
  interhash = {812c892e788c1924ee976d688c0d52d1},
  intrahash = {cbcc0b2f57b3540a526b9dcb101fd45f},
  isbn = {9781627050944 1627050949},
  keywords = {BusinessIntelligence DataAnalytics SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {844080530},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Perspectives on business intelligence},
  url = {http://site.ebrary.com/id/10705995},
  year = 2013
}

@book{wilcock2009introduction,
  abstract = {Linguistic annotation and text analytics are active areas of research and development, with academic conferences and industry events such as the Linguistic Annotation Workshops and the annual Text Analytics Summits. This book provides a basic introduction to both fields, and aims to show that good linguistic annotations are the essential foundation for good text analytics. After briefly reviewing the basics of XML, with practical exercises illustrating in-line and stand-off annotations, a chapter is devoted to explaining the different levels of linguistic annotations. The reader is encouraged to create example annotations using the WordFreak linguistic annotation tool. The next chapter shows how annotations can be created automatically using statistical NLP tools, and compares two sets of tools, the OpenNLP and Stanford NLP tools. The second half of the book describes different annotation formats and gives practical examples of how to interchange annotations between different formats using XSLT transformations. The two main text analytics architectures, GATE and UIMA, are then described and compared, with practical exercises showing how to configure and customize them. The final chapter is an introduction to text analytics, describing the main applications and functions including named entity recognition, coreference resolution and information extraction, with practical examples using both open source and commercial tools. Copies of the example files, scripts, and stylesheets used in the book are available from the companion website, located at http://sites.morganclaypool.com/wilcock.},
  added-at = {2016-07-14T23:46:12.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Wilcock, Graham},
  biburl = {https://www.bibsonomy.org/bibtex/2211bc15a3d1f17bc6698663ce2e2f8e0/vngudivada},
  interhash = {7936bb10604f6fe63ea8e1d0334e755a},
  intrahash = {211bc15a3d1f17bc6698663ce2e2f8e0},
  isbn = {9781598297393 1598297392},
  keywords = {LinguisticAnnotation TextAnalytics SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {426825849},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Introduction to linguistic annotation and text analytics},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881060},
  year = 2009
}

@book{dagan2013recognizing,
  abstract = {In the last few years, a number of NLP researchers have developed and participated in the task of Recognizing Textual Entailment (RTE). This task encapsulates Natural Language Understanding capabilities within a very simple interface: recognizing when the meaning of a text snippet is contained in the meaning of a second piece of text. This simple abstraction of an exceedingly complex problem has broad appeal partly because it can be conceived also as a component in other NLP applications, from Machine Translation to Semantic Search to Information Extraction. It also avoids commitment to any sp.},
  added-at = {2016-07-14T23:25:59.000+0200},
  address = {San Rafael, California},
  author = {Dagan, Ido and Roth, Dan and Sammons, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/2ee2e0706d2eb3dbc231d399ee7503a92/vngudivada},
  doi = {10.2200/S00509ED1V01Y201305HLT023},
  interhash = {6100af0c3a279933d84b3ba8d40437a5},
  intrahash = {ee2e0706d2eb3dbc231d399ee7503a92},
  isbn = {9781598298352},
  keywords = {IR NLP SynthesisLecture TextualEntailment},
  publisher = {Morgan \& Claypool},
  refid = {854520068},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Recognizing Textual Entailment Models and Applications},
  year = 2013
}

@book{gebser2013answer,
  abstract = {Answer Set Programming (ASP) is a declarative problem solving approach, initially tailored to modeling problems in the area of Knowledge Representation and Reasoning (KRR). More recently, its attractive combination of a rich yet simple modeling language with high-performance solving capacities has sparked interest in many other areas even beyond KRR. This book presents a practical introduction to ASP, aiming at using ASP languages and systems for solving application problems. Starting from the essential formal foundations, it introduces ASP's solving technology, modeling language and methodology, while illustrating the overall solving process by practical examples.},
  added-at = {2016-07-15T04:15:03.000+0200},
  address = {[San Rafael]},
  author = {Gebser, Martin and Kaminski, Roland and Kaufmann, Benjamin},
  biburl = {https://www.bibsonomy.org/bibtex/2c0e7b88a7666c76cdce0ea696f78b445/vngudivada},
  interhash = {9c7391b4bb531b89bbe3bfac7bfd1284},
  intrahash = {c0e7b88a7666c76cdce0ea696f78b445},
  isbn = {9781608459728 1608459721 1608459713 9781608459711},
  keywords = {AnswerSet SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {823389942},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Answer set solving in practice},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1104604},
  year = 2013
}

@book{catarci2010usercentered,
  abstract = {This lecture covers several core issues in user-centered data management, including how to design usable interfaces that suitably support database tasks, and relevant approaches to visual querying, information visualization, and visual data mining. Novel interaction paradigms, e.g., mobile and interfaces that go beyond the visual dimension, are also discussed.},
  added-at = {2016-07-15T01:19:09.000+0200},
  address = {San Rafael},
  author = {Catarci, Tiziana and Dix, Alan and Kimani, Stephen and Santucci, Giuseppe},
  biburl = {https://www.bibsonomy.org/bibtex/2b4a1604a2cf7d44e48e87a37b3e44eb9/vngudivada},
  interhash = {f0a90786dc32a0b1544d920bd86a5042},
  intrahash = {b4a1604a2cf7d44e48e87a37b3e44eb9},
  isbn = {9781608452828 1608452824},
  keywords = {DataManagement UserCentric SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {647801440},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {User-centered data management},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881178},
  year = 2010
}

@book{wolpaw2012braincomputer,
  abstract = {In the last 15 years, a recognizable surge in the field of Brain Computer Interface (BCI) research and development has emerged. This emergence has sprung from a variety of factors. For one, inexpensive computer hardware and software is now available and can support the complex high-speed analyses of brain activity that is essential is BCI. Another factor is the greater understanding of the central nervous system including the abundance of new information on the nature and functional correlates of brain signals and improved methods for recording these signals in both the short-term and long-term. And the third, and perhaps most significant factor, is the new recognition of the needs and abilities of people disabled by disorders such as cerebral palsy, spinal cord injury, stroke, amyotrophic lateral sclerosis (ALS), multiple sclerosis, and muscular dystrophies. The severely disabled are now able to live for many years and even those with severely limited voluntary muscle control can now be given the most basic means of communication and control because of the recent advances in the technology, research, and applications of BCI. This book is intended to provide an introduction to and summary of essentially all major aspects of BCI research and development. Its goal is to be a comprehensive, balanced, and coordinated presentation of the field's key principles, current practice, and future prospects.},
  added-at = {2016-07-15T02:15:22.000+0200},
  address = {New York, NY},
  author = {Wolpaw, Jonathan R. and Wolpaw, Elizabeth Winter},
  biburl = {https://www.bibsonomy.org/bibtex/27ff6ab0f6cf0436c1137807d5bb2692c/vngudivada},
  interhash = {adb5d75c24ae78f2b410d7405e5a5a6a},
  intrahash = {7ff6ab0f6cf0436c1137807d5bb2692c},
  keywords = {BCI Book},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Brain-computer interfaces : principles and practice},
  year = 2012
}

@book{ganti2013cleaning,
  abstract = {Data warehouses consolidate various activities of a business and often form the backbone for generating reports that support important business decisions. Errors in data tend to creep in for a variety of reasons. Some of these reasons include errors during input data collection and errors while merging data collected independently across different databases. These errors in data warehouses often result in erroneous upstream reports, and could impact business decisions negatively. Therefore, one of the critical challenges while maintaining large data warehouses is that of ensuring the quality of data in the data warehouse remains high. The process of maintaining high data quality is commonly referred to as data cleaning. In this book, we first discuss the goals of data cleaning. Often, the goals of data cleaning are not well defined and could mean different solutions in different scenarios. Toward clarifying these goals, we abstract out a common set of data cleaning tasks that often need to be addressed. This abstraction allows us to develop solutions for these common data cleaning tasks. We then discuss a few popular approaches for developing such solutions. In particular, we focus on an operator-centric approach for developing a data cleaning platform. The operator-centric approach involves the development of customizable operators that could be used as building blocks for developing common solutions. This is similar to the approach of relational algebra for query processing. The basic set of operators can be put together to build complex queries. Finally, we discuss the development of custom scripts which leverage the basic data cleaning operators along with relational operators to implement effective solutions for data cleaning tasks.},
  added-at = {2016-07-15T01:08:08.000+0200},
  address = {San Rafael, California},
  author = {Ganti, Venkatesh and Das Sarma, Anish},
  biburl = {https://www.bibsonomy.org/bibtex/253f36117d285bd18fefc1137e94a4584/vngudivada},
  interhash = {fae217b6e95c6a934736e286b4900eb0},
  intrahash = {53f36117d285bd18fefc1137e94a4584},
  isbn = {978-1608456789},
  keywords = {DataCleaning SynthesisLecture},
  month = sep,
  publisher = {Morgan \& Claypool},
  refid = {860909369},
  series = {Synthesis Lectures on Data Management },
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Data cleaning: a practical perspective},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1365101},
  volume = 36,
  year = 2013
}

@book{naumann2010introduction,
  abstract = {With the ever increasing volume of data, data quality problems abound. Multiple, yet different representations of the same real-world objects in data, duplicates, are one of the most intriguing data quality problems. The effects of such duplicates are detrimental; for instance, bank customers can obtain duplicate identities, inventory levels are monitored incorrectly, catalogs are mailed multiple times to the same household, etc. Automatically detecting duplicates is difficult: First, duplicate representations are usually not identical but slightly differ in their values. Second, in principle all pairs of records should be compared, which is infeasible for large volumes of data. This lecture examines closely the two main components to overcome these difficulties: (i) Similarity measures are used to automatically identify duplicates when comparing two records. Well-chosen similarity measures improve the effectiveness of duplicate detection. (ii) Algorithms are developed to perform on very large volumes of data in search for duplicates. Well-designed algorithms improve the efficiency of duplicate detection. Finally, we discuss methods to evaluate the success of duplicate detection.},
  added-at = {2016-07-15T01:00:46.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Naumann, Felix and Herschel, Melanie},
  biburl = {https://www.bibsonomy.org/bibtex/2b65bcabb97a0cf4586ac61d995fe3687/vngudivada},
  interhash = {ace63548736b2ee253386c2a3f9b23c7},
  intrahash = {b65bcabb97a0cf4586ac61d995fe3687},
  isbn = {9781608452217 1608452212},
  keywords = {DuplicateDetection SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {607131943},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {An introduction to duplicate detection},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881167},
  year = 2010
}

@book{greco2012incomplete,
  abstract = {The chase has long been used as a central tool to analyze dependencies and their effect on queries. It has been applied to different relevant problems in database theory such as query optimization, query containment and equivalence, dependency implication, and database schema design. Recent years have seen a renewed interest in the chase as an important tool in several database applications, such as data exchange and integration, query answering in incomplete data, and many others. It is well known that the chase algorithm might be non-terminating and thus, in order for it to find practical applicability, it is crucial to identify cases where its termination is guaranteed. Another important aspect to consider when dealing with the chase is that it can introduce null values into the database, thereby leading to incomplete data. Thus, in several scenarios where the chase is used the problem of dealing with data dependencies and incomplete data arises. This book discusses fundamental issues concerning data dependencies and incomplete data with a particular focus on the chase and its applications in different database areas. We report recent results about the crucial issue of identifying conditions that guarantee the chase termination. Different database applications where the chase is a central tool are discussed with particular attention devoted to query answering in the presence of data dependencies and database schema design.},
  added-at = {2016-07-15T01:16:26.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Greco, Sergio and Molinaro, Cristian and Spezzano, Francesca},
  biburl = {https://www.bibsonomy.org/bibtex/2d421059309081cec8838627b1efb7aff/vngudivada},
  interhash = {cbed61ec021b87b2507875aa85e78e18},
  intrahash = {d421059309081cec8838627b1efb7aff},
  isbn = {9781608459278 1608459276 1608459268 9781608459261},
  keywords = {DataDependencies RDBMS SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {806466421},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Incomplete data and data dependencies in relational databases},
  url = {http://site.ebrary.com/id/10591110},
  year = 2012
}

@book{chen2013query,
  abstract = {Due to measurement errors, transmission lost, or injected noise for privacy protection, uncertainty exists in the data of many real applications. However, query processing techniques for deterministic data cannot be directly applied to uncertain data because they do not have mechanisms to handle the data uncertainty. Therefore, efficient and effective manipulation of uncertain data is a practical yet challenging research topic. In this book, we start from the data models for imprecise and uncertain data, move on to defining different semantics for queries on uncertain data, and finally discuss the advanced query processing techniques for various probabilistic queries in uncertain databases. The book serves as a comprehensive guideline for query processing over uncertain databases.},
  added-at = {2016-07-15T01:32:02.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Chen, Lei and Lian, Xiang},
  biburl = {https://www.bibsonomy.org/bibtex/2beed1fb64a2dc90bccdf7727166ef79d/vngudivada},
  interhash = {81f7f4f8492fad318b91a50b61877275},
  intrahash = {beed1fb64a2dc90bccdf7727166ef79d},
  isbn = {9781608458936 1608458938 160845892X 9781608458929},
  keywords = {QueryProcessing UncertainDatabase SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {824172444},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Query processing over uncertain databases},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1010023},
  year = 2013
}

@book{smith2011linguistic,
  abstract = {A major part of natural language processing now depends on the use of text data to build linguistic analyzers. We consider statistical, computational approaches to modeling linguistic structure. We seek to unify across many approaches and many kinds of linguistic structures. Assuming a basic understanding of natural language processing and/or machine learning, we seek to bridge the gap between the two fields. Approaches to decoding (i.e., carrying out linguistic structure prediction) and supervised and unsupervised learning of models that predict discrete structures as outputs are the focus. We also survey natural language processing problems to which these methods are being applied, and we address related topics in probabilistic inference, optimization, and experimental methodology.},
  added-at = {2016-07-14T23:47:30.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Smith, Noah Ashton},
  biburl = {https://www.bibsonomy.org/bibtex/26506d1f5c65a9e7c60d3d743bb0702a5/vngudivada},
  interhash = {b0b5657837044eb015bed038835f3db1},
  intrahash = {6506d1f5c65a9e7c60d3d743bb0702a5},
  isbn = {9781608454068 1608454061},
  keywords = {LinguisticStructure SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {742535678},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Linguistic structure prediction},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881199},
  year = 2011
}

@book{agrawal2013management,
  abstract = {Cloud computing has emerged as a successful paradigm of service-oriented computing and has revolutionized the way computing infrastructure is used. This success has seen a proliferation in the number of applications that are being deployed in various cloud platforms. There has also been an increase in the scale of the data generated as well as consumed by such applications. Scalable database management systems form a critical part of the cloud infrastructure. The attempt to address the challenges posed by the management of big data has led to a plethora of systems. This book aims to clarify some of the important concepts in the design space of scalable data management in cloud computing infrastructures. Some of the questions that this book aims to answer are: the appropriate systems for a specific set of application requirements, the research challenges in data management for the cloud, and what is novel in the cloud for database researchers? We also aim to address one basic question: whether cloud computing poses new challenges in scalable data management or it is just a reincarnation of old problems? We provide a comprehensive background study of state-of-the-art systems for scalable data management and analysis. We also identify important aspects in the design of different systems and the applicability and scope of these systems. A thorough understanding of current solutions and a precise characterization of the design space are essential for clearing the "cloudy skies of data management" and ensuring the success of DBMSs in the cloud, thus emulating the success enjoyed by relational databases in traditional enterprise settings.},
  added-at = {2016-07-15T00:49:44.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Agrawal, Divyakant and Das, Sudipto and El Abbadi, Amr},
  biburl = {https://www.bibsonomy.org/bibtex/274c1f7a5806756162a6a3fe7c83ae1cb/vngudivada},
  interhash = {ca50a2105cbb85d34466df4745305e67},
  intrahash = {74c1f7a5806756162a6a3fe7c83ae1cb},
  isbn = {9781608459254 160845925X 1608459241 9781608459247},
  keywords = {CloudDBMS SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {824172665},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Data management in the cloud challenges and opportunities},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1104603},
  year = 2013
}

@book{barsky2012fulltext,
  abstract = {Nowadays, textual databases are among the most rapidly growing collections of data. Some of these collections contain a new type of data that differs from classical numerical or textual data. These are long sequences of symbols, not divided into well-separated small tokens (words). The most prominent among such collections are databases of biological sequences, which are experiencing today an unprecedented growth rate. Starting in 2008, the "1000 Genomes Project" has been launched with the ultimate goal of collecting sequences of additional 1,500 Human genomes, 500 each of European, African, and East Asian origin. This will produce an extensive catalog of Human genetic variations. The size of just the raw sequences in this catalog would be about 5 terabytes.},
  added-at = {2016-07-15T01:09:21.000+0200},
  address = {San Rafael, Calif.},
  author = {Barsky, Marina and Stege, Ulrike and Thomo, Alex-Imir},
  biburl = {https://www.bibsonomy.org/bibtex/271fa3585e17ff1f5987c62bc026d34df/vngudivada},
  interhash = {2aea713103a55076bc0dbb2297a13587},
  intrahash = {71fa3585e17ff1f5987c62bc026d34df},
  isbn = {9781608457960 1608457966},
  keywords = {FullTextIndexing SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {772525452},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Full-text (substring) indexes in external memory},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881286},
  year = 2012
}

@book{tay2014analytical,
  abstract = {This book is an introduction to analytical performance modeling for computer systems, i.e., writing equations to describe their performance behavior. It is accessible to readers who have taken college-level courses in calculus and probability, networking and operating systems. This is not a training manual for becoming an expert performance analyst. Rather, the objective is to help the reader construct simple models for analyzing and understanding the systems that they are interested in. Describing a complicated system abstractly with mathematical equations requires a careful choice of assumptions and approximations. They make the model tractable, but they must not remove essential characteristics of the system, nor introduce spurious properties. To help the reader understand the choices and their implications, this book discusses the analytical models for 30 research papers. These papers cover a broad range of topics: processors and disks, routers and crawling, databases and multimedia, worms and wireless, multicore and cloud, etc. An appendix provides many questions for readers to exercise their understanding of the models in these papers.},
  added-at = {2016-07-14T22:47:38.000+0200},
  author = {Tay, Y. C.},
  biburl = {https://www.bibsonomy.org/bibtex/21150d28aee66f5874fa671f3ad5701c8/vngudivada},
  interhash = {da4cf831f7fe67ad7de7d5be6e36c6a1},
  intrahash = {1150d28aee66f5874fa671f3ad5701c8},
  isbn = {9781627052702 1627052704 1627052690 9781627052696},
  keywords = {PerformanceModeling SynthesisLecture},
  refid = {862936282},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Analytical performance modeling for computer systems},
  url = {http://site.ebrary.com/id/10789227},
  year = 2014
}

@book{piotrowski2012natural,
  abstract = {More and more historical texts are becoming available in digital form. Digitization of paper documents is motivated by the aim of preserving cultural heritage and making it more accessible, both to laypeople and scholars. As digital images cannot be searched for text, digitization projects increasingly strive to create digital text, which can be searched and otherwise automatically processed, in addition to facsimiles. Indeed, the emerging field of digital humanities heavily relies on the availability of digital text for its studies. Together with the increasing availability of historical texts in digital form, there is a growing interest in applying natural language processing (NLP) methods and tools to historical texts. However, the specific linguistic properties of historical texts--the lack of standardized orthography in particular--pose special challenges for NLP.},
  added-at = {2016-07-14T23:30:16.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Piotrowski, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/235af306f4ce43776500f64f0263bb1e2/vngudivada},
  interhash = {a7bc815f086a5c3cb6464e9ad2d61b99},
  intrahash = {35af306f4ce43776500f64f0263bb1e2},
  isbn = {9781608459476 1608459470},
  keywords = {HistoricalText NLP SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {812207537},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Natural language processing for historical texts},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=919654},
  year = 2012
}

@book{golab2010stream,
  abstract = {In this lecture many applications process high volumes of streaming data, among them Internet traffic analysis, financial tickers, and transaction log mining. In general, a data stream is an unbounded data set that is produced incrementally over time, rather than being available in full before its processing begins. In this lecture, we give an overview of recent research in stream processing, ranging from answering simple queries on high-speed streams to loading real-time data feeds into a streaming warehouse for off-line analysis. We will discuss two types of systems for end-to-end stream processing: Data Stream Management Systems (DSMSs) and Streaming Data Warehouses (SDWs).A traditional database management system typically processes a stream of ad-hoc queries over relatively static data. In contrast, a DSMS evaluates static (long-running) queries on streaming data, making a single pass over the data and using limited working memory. In the first part of this lecture, we will discuss research problems in DSMSs, such as continuous query languages, non-blocking query operators that continually react to new data, and continuous query optimization. The second part covers SDWs, which combine the real-time response of a DSMS by loading new data as soon as they arrive with a data warehouse's ability to manage Terabytes of historical data on secondary storage.},
  added-at = {2016-07-15T01:43:09.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Golab, Lukasz and Özsu, M. Tamer},
  biburl = {https://www.bibsonomy.org/bibtex/2d68384da931d756294ddf99183a48330/vngudivada},
  interhash = {cdca43d9d218def54250bec5dd6f63ed},
  intrahash = {d68384da931d756294ddf99183a48330},
  isbn = {9781608452736 1608452735 1608452727 9781608452729},
  keywords = {DataStreamManagement SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {647796599},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Data stream management},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881176},
  year = 2010
}

@book{liu2012sentiment,
  abstract = {Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes, and emotions from written language. It is one of the most active research areas in natural language processing and is also widely studied in data mining, Web mining, and text mining. In fact, this research has spread outside of computer science to the management sciences and social sciences due to its importance to business and society as a whole. The growing importance of sentiment analysis coincides with the growth of social media such as reviews, forum discussions, blogs, micro-blogs, Twitter, and social networks. For the first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis.},
  added-at = {2016-07-14T23:31:54.000+0200},
  address = {San Rafael, Calif.},
  author = {Liu, Bing},
  biburl = {https://www.bibsonomy.org/bibtex/205391d3739b495b8f8066801bceb73a3/vngudivada},
  interhash = {38baa4d6a430731ea8c610d69321c856},
  intrahash = {05391d3739b495b8f8066801bceb73a3},
  isbn = {9781608458851 1608458857},
  keywords = {OpinionMining SentimentAnalysis SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {795403192},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Sentiment analysis and opinion mining},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=931999},
  year = 2012
}

@book{domingos2009markov,
  abstract = {Most subfields of computer science have an interface layer via which applications communicate with the infrastructure, and this is key to their success (e.g., the Internet in networking, the relational model in databases, etc.). So far this interface layer has been missing in AI. First-order logic and probabilistic graphical models each have some of the necessary features, but a viable interface layer requires combining both. Markov logic is a powerful new language that accomplishes this by attaching weights to first-order formulas and treating them as templates for features of Markov random fields. Most statistical models in wide use are special cases of Markov logic, and first-order logic is its infinite-weight limit. Inference algorithms for Markov logic combine ideas from satisfiability, Markov chain Monte Carlo, belief propagation, and resolution. Learning algorithms make use of conditional likelihood, convex optimization, and inductive logic programming. Markov logic has been successfully applied to problems in information extraction and integration, natural language processing, robot mapping, social networks, computational biology, and others, and is the basis of the open-source Alchemy system.},
  added-at = {2016-07-15T04:08:33.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Domingos, Pedro and Lowd, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/2455150e8f989941854c987d7e3661942/vngudivada},
  interhash = {22dec55d710183f0c73b4b2677db94a4},
  intrahash = {455150e8f989941854c987d7e3661942},
  isbn = {9781598296938 1598296930},
  keywords = {MarkovLogiv SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {428525464},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Markov logic: an interface layer for artificial intelligence},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881056},
  year = 2009
}

@book{szepesvari2010algorithms,
  abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective. What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming. We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
  added-at = {2016-07-15T04:09:42.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Szepesvári, Csaba},
  biburl = {https://www.bibsonomy.org/bibtex/2c616e3f744b3a6ed58f2d04e875c3e72/vngudivada},
  interhash = {46c07bb11e2b5fe91663d461efbc2af1},
  intrahash = {c616e3f744b3a6ed58f2d04e875c3e72},
  isbn = {9781608454938 1608454932},
  keywords = {ReinforcementLearning SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {647995927},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Algorithms for reinforcement learning},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881218},
  year = 2010
}

@book{geffner2013concise,
  abstract = {Planning is the model-based approach to autonomous behavior where the agent behavior is derived automatically from a model of the actions, sensors, and goals. The main challenges in planning are computational as all models, whether featuring uncertainty and feedback or not, are intractable in the worst case when represented in compact form. In this book, we look at a variety of models used in AI planning, and at the methods that have been developed for solving them. The goal is to provide a modern and coherent view of planning that is precise, concise, and mostly self-contained, without being shallow. For this, we make no attempt at covering the whole variety of planning approaches, ideas, and applications, and focus on the essentials. The target audience of the book are students and researchers interested in autonomous behavior and planning from an AI, engineering, or cognitive science perspective.},
  added-at = {2016-07-14T22:32:32.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Geffner, Hector and Bonet, Blai},
  biburl = {https://www.bibsonomy.org/bibtex/2870a0977ffe427b9b2bab527a3f50b7b/vngudivada},
  interhash = {90862c7c318efca6e35089a258abcc72},
  intrahash = {870a0977ffe427b9b2bab527a3f50b7b},
  isbn = {9781608459704 1608459705 1608459691 9781608459698},
  keywords = {AutomatedPlanning SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {853278184},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {A concise introduction to models and methods for automated planning},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1250667},
  year = 2013
}

@book{suciu2011probabilistic,
  abstract = {Probabilistic databases are databases where the value of some attributes or the presence of some records are uncertain and known only with some probability. Applications in many areas such as information extraction, RFID and scientific data management, data cleaning, data integration, and financial risk assessment produce large volumes of uncertain data, which are best modeled and processed by a probabilistic database.},
  added-at = {2016-07-15T00:53:14.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Suciu, Dan},
  biburl = {https://www.bibsonomy.org/bibtex/22e2727a8250566f4f5405351f51c1b7e/vngudivada},
  interhash = {bcc4ec8dc7df9c2a1739c6adb3559451},
  intrahash = {2e2727a8250566f4f5405351f51c1b7e},
  isbn = {9781608456819 1608456811},
  keywords = {ProbabbilisticDBMS Probability SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {742535665},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Probabilistic databases},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881260},
  year = 2011
}

@book{bertossi2011database,
  abstract = {Integrity constraints are semantic conditions that a database should satisfy in order to be an appropriate model of external reality. In practice, and for many reasons, a database may not satisfy those integrity constraints, and for that reason it is said to be inconsistent. However, and most likely a large portion of the database is still semantically correct, in a sense that has to be made precise. After having provided a formal characterization of consistent data in an inconsistent database, the natural problem emerges of extracting that semantically correct data, as query answers.},
  added-at = {2016-07-15T01:29:55.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Bertossi, Leopoldo},
  biburl = {https://www.bibsonomy.org/bibtex/262ff1d9d8312a7b0908191d2ff699f93/vngudivada},
  interhash = {84b94e3c64e1d3e54098a68184707e87},
  intrahash = {62ff1d9d8312a7b0908191d2ff699f93},
  isbn = {9781608457632 160845763X},
  keywords = {Database QueryAnswering Repair SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {752688343},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Database repairing and consistent query answering},
  url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=440457},
  year = 2011
}

@book{arenas2010relational,
  abstract = {Data exchange is the problem of finding an instance of a target schema, given an instance of a source schema and a specification of the relationship between the source and the target. Such a target instance should correctly represent information from the source instance under the constraints imposed by the target schema, and it should allow one to evaluate queries on the target instance in a way that is semantically consistent with the source data. Data exchange is an old problem that re-emerged as an active research topic recently, due to the increased need for exchange of data in various formats, often in e-business applications. In this lecture, we give an overview of the basic concepts of data exchange in both relational and XML contexts. We give examples of data exchange problems, and we introduce the main tasks that need to addressed. We then discuss relational data exchange, concentrating on issues such as relational schema mappings, materializing target instances (including canonical solutions and cores), query answering, and query rewriting. After that, we discuss metadata management, i.e., handling schema mappings themselves. We pay particular attention to operations on schema mappings, such as composition and inverse. Finally, we describe both data exchange and metadata management in the context of XML. We use mappings based on transforming tree patterns, and we show that they lead to a host of new problems that did not arise in the relational case, but they need to be addressed for XML. These include consistency issues for mappings and schemas, as well as imposing tighter restrictions on mappings and queries to achieve tractable query answering in data exchange.},
  added-at = {2016-07-15T01:26:21.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Arenas, Marcelo},
  biburl = {https://www.bibsonomy.org/bibtex/2f73fe76fdb9d1f3bb1612d2a64f976be/vngudivada},
  interhash = {f342954c2247d61bdee24de1973903af},
  intrahash = {f73fe76fdb9d1f3bb1612d2a64f976be},
  isbn = {9781608454129 1608454126},
  keywords = {DataExchange XML SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {707877273},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Relational and XML data exchange},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881201},
  year = 2010
}

@book{dietrich2011fundamentals,
  abstract = {Object-oriented databases were originally developed as an alternative to relational database technology for the representation, storage, and access of non-traditional data forms that were increasingly found in advanced applications of database technology. After much debate regarding object oriented versus relational database technology, object-oriented extensions were eventually incorporated into relational technology to create object-relational databases. Both object-oriented databases and object-relational databases, collectively known as object databases, provide inherent support for object features, such as object identity, classes, inheritance hierarchies, and associations between classes using object references.},
  added-at = {2016-07-15T01:02:03.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Dietrich, Suzanne Wagner and Urban, Susan and Dietrich, Suzanne Wagner},
  biburl = {https://www.bibsonomy.org/bibtex/20dc2507c0ab05343ed234a2663729848/vngudivada},
  interhash = {a3fd465f6c0123b313e39b803fb51d33},
  intrahash = {0dc2507c0ab05343ed234a2663729848},
  isbn = {9781608454778 1608454770},
  keywords = {OODBMS ORDBMS SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {696253558},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Fundamentals of object databases object-oriented and object-relational design},
  url = {http://site.ebrary.com/id/10530771},
  year = 2011
}

@book{yu2010keyword,
  abstract = {It has become highly desirable to provide users with flexible ways to query/search information over databases as simple as keyword search like Google search. This book surveys the recent developments on keyword search over databases, and focuses on finding structural information among objects in a database using a set of keywords. Such structural information to be returned can be either trees or subgraphs representing how the objects, that contain the required keywords, are interconnected in a relational database or in an XML database. The structural keyword search is completely different from finding documents that contain all the user-given keywords. The former focuses on the interconnected object structures, whereas the latter focuses on the object content. The book is organized as follows. In Chapter 1, we highlight the main research issues on the structural keyword search in different contexts. In Chapter 2, we focus on supporting structural keyword search in a relational database management system using the SQL query language. We concentrate on how to generate a set of SQL queries that can find all the structural information among records in a relational database completely, and how to evaluate the generated set of SQL queries efficiently. In Chapter 3,we discuss graph algorithms for structural keyword search by treating an entire relational database as a large data graph. In Chapter 4, we discuss structural keyword search in a large tree-structuredXMLdatabase. In Chapter 5,we highlight several interesting research issues regarding keyword search on databases. The book can be used as either an extended survey for people who are interested in the structural keyword search or a reference book for a postgraduate course on the related topics.},
  added-at = {2016-07-15T01:21:57.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Yu, Jeffrey Xu and Qin, Lu and Chang, Lijun},
  biburl = {https://www.bibsonomy.org/bibtex/2d930cd3ff0d2f0bb55d1b2799f16e071/vngudivada},
  interhash = {d0dcd355a9c0c9b97d627c1755730cf2},
  intrahash = {d930cd3ff0d2f0bb55d1b2799f16e071},
  isbn = {9781608451968 1608451968},
  keywords = {DBMS KeywordSearch SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {495436787},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Keyword search in databases},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881164},
  year = 2010
}

@book{friedenberg2012cognitive,
  abstract = {This third edition contains expanded information on many new topics including formal systems, empirical philosophy, optogenetics, von Economo neurons, domain-general vs. domain-specific mechanisms in language acquisition and IBM’s Watson artificial intelligence program. There is a new section on emotions, evolution and psychological disorders and on neural and synaptic mechanisms of emotion, among others. Chapter introductions provide a better preview of what is to come and new tables and figures enhance existing course content.},
  added-at = {2016-07-14T22:14:38.000+0200},
  address = {Thousand Oaks, Calif.},
  author = {Friedenberg, Jay and Silverman, Gordon},
  biburl = {https://www.bibsonomy.org/bibtex/21bbbbfb99ad76161f6a844b1275baa84/vngudivada},
  interhash = {a33ea821f570be225b87f630adc13cb4},
  intrahash = {1bbbbfb99ad76161f6a844b1275baa84},
  isbn = {9781412977616 1412977614},
  keywords = {Book CognitiveScience},
  publisher = {SAGE},
  refid = {707887027},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Cognitive science : an introduction to the study of mind},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=1412977614},
  year = 2012
}

@book{gunduzoguducu2011recommendation,
  abstract = {One of the application areas of data mining is the World Wide Web (WWW or Web), which serves as a huge, widely distributed, global information service for every kind of information such as news, advertisements, consumer information, financial management, education, government, e-commerce, health services, and many other information services. The Web also contains a rich and dynamic collection of hyperlink information, Web page access and usage information, providing sources for data mining. The amount of information on the Web is growing rapidly, as well as the number of Web sites and Web pages per Web site. Consequently, it has become more difficult to find relevant and useful information for Web users. Web usage mining is concerned with guiding the Web users to discover useful knowledge and supporting them for decision-making. In that context, predicting the needs of a Web user as she visits Web sites has gained importance. The requirement for predicting user needs in order to guide the user in a Web site and improve the usability of the Web site can be addressed by recommending pages to the user that are related to the interest of the user at that time. This monograph gives an overview of the research in the area of discovering and modeling the users' interest in order to recommend related Web pages. The Web page recommender systems studied in this monograph are categorized according to the data mining algorithms they use for recommendation.},
  added-at = {2016-07-15T01:17:37.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Gündüz-Ögüdücü, Sule},
  biburl = {https://www.bibsonomy.org/bibtex/2b98ecf25b88bc5b5ac8a16f3835a0538/vngudivada},
  interhash = {2fafbea0dd96622ef82bc3ae4abc2af1},
  intrahash = {b98ecf25b88bc5b5ac8a16f3835a0538},
  isbn = {9781608452484 1608452484},
  keywords = {SynthesisLecture WebPageRecommendationModel},
  publisher = {Morgan & Claypool},
  refid = {707877332},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Web page recommendation models theory and algorithms},
  url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=440364},
  year = 2011
}

@book{zhai2009statistical,
  abstract = {As online information grows dramatically, search engines such as Google are playing a more and more important role in our lives. Critical to all search engines is the problem of designing an effective retrieval model that can rank documents accurately for a given query. This has been a central research problem in information retrieval for several decades. In the past ten years, a new generation of retrieval models, often referred to as statistical language models, has been successfully applied to solve many different information retrieval problems. Compared with the traditional models such as the vector space model, these new models have a more sound statistical foundation and can leverage statistical estimation to optimize retrieval parameters. They can also be more easily adapted to model nontraditional and complex retrieval problems. Empirically, they tend to achieve comparable or better performance than a traditional model with less effort on parameter tuning. This book systematically reviews the large body of literature on applying statistical language models to information retrieval with an emphasis on the underlying principles, empirically effective language models, and language models developed for non-traditional retrieval tasks. All the relevant literature has been synthesized to make it easy for a reader to digest the research progress achieved so far and see the frontier of research in this area. The book also offers practitioners an informative introduction to a set of practically useful language models that can effectively solve a variety of retrieval problems. No prior knowledge about information retrieval is required, but some basic knowledge about probability and statistics would be useful for fully digesting all the details.},
  added-at = {2016-07-14T23:41:18.000+0200},
  address = {San Rafael, California},
  author = {Zhai, ChengXiang},
  biburl = {https://www.bibsonomy.org/bibtex/276d002e07250bae4e3ccdc6e7015c796/vngudivada},
  doi = {10.2200/S00158ED1V01Y200811HLT001},
  interhash = {87ad79dd4a772cbd09cb109b1f0f214c},
  intrahash = {76d002e07250bae4e3ccdc6e7015c796},
  isbn = {9781598295917},
  keywords = {IR LanguageModeling SynthesisLecture},
  publisher = {Morgan \& Claypool},
  refid = {299778162},
  series = {Synthesis Lectures on Human Language Technologies},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Statistical language models for information retrieval},
  year = 2009
}

@book{pacitti2012techniques,
  abstract = {As an alternative to traditional client-server systems, Peer-to-Peer (P2P) systems provide major advantages in terms of scalability, autonomy and dynamic behavior of peers, and decentralization of control. Thus, they are well suited for large-scale data sharing in distributed environments. Most of the existing P2P approaches for data sharing rely on either structured networks (e.g., DHTs) for efficient indexing, or unstructured networks for ease of deployment, or some combination. However, these approaches have some limitations, such as lack of freedom for data placement in DHTs, and high latency and high network traffic in unstructured networks. To address these limitations, gossip protocols which are easy to deploy and scale well, can be exploited. In this book, we will give a overview of these different P2P techniques and architectures, discuss their trade-offs and illustrate their use for decentralizing several large-scale data sharing applications.},
  added-at = {2016-07-15T00:51:19.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Pacitti, Esther and Akbarinia, Reza and El-Dick, Manal},
  biburl = {https://www.bibsonomy.org/bibtex/2205b66f3aa1cea6099b1ee51579fbf00/vngudivada},
  interhash = {dd444358f854699c5c7f58662f5de57a},
  intrahash = {205b66f3aa1cea6099b1ee51579fbf00},
  isbn = {9781608458233 1608458237},
  keywords = {P2P SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {793207556},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {P2P techniques for decentralized applications},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=919779},
  year = 2012
}

@book{gupta2011managing,
  abstract = {With the proliferation of citizen reporting, smart mobile devices, and social media, an increasing number of people are beginning to generate information about events they observe and participate in. A significant fraction of this information contains multimedia data to share the experience with their audience. A systematic information modeling and management framework is necessary to capture this widely heterogeneous, schemaless, potentially humongous information produced by many different people. This book is an attempt to examine the modeling, storage, querying, and applications of such an event management system in a holistic manner. It uses a semantic-web style graph-based view of events, and shows how this event model, together with its query facility, can be used toward emerging applications like semi-automated storytelling.},
  added-at = {2016-07-15T01:04:53.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Gupta, Amarnath and Jain, Ramesh},
  biburl = {https://www.bibsonomy.org/bibtex/29ca371fc956db082d073642a0aecfd27/vngudivada},
  interhash = {a0f70b53d0031b53fc83d4c45961273b},
  intrahash = {9ca371fc956db082d073642a0aecfd27},
  isbn = {9781608453528 1608453529 9781608453511 1608453510},
  keywords = {EventInformationManagement SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {746231224},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Managing event information modeling, retrieval, and applications},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881187},
  year = 2011
}

@book{ferrari2010access,
  abstract = {Access control is one of the fundamental services that any Data Management System should provide. Its main goal is to protect data from unauthorized read and write operations. This is particularly crucial in today's open and interconnected world, where each kind of information can be easily made available to a huge user population, and where a damage or misuse of data may have unpredictable consequences that go beyond the boundaries where data reside or have been generated. This book provides an overview of the various developments in access control for data management systems. Discretionary, mandatory, and role-based access control will be discussed, by surveying the most relevant proposals and analyzing the benefits and drawbacks of each paradigm in view of the requirements of different application domains. Access control mechanisms provided by commercial Data Management Systems are presented and discussed. Finally, the last part of the book is devoted to discussion of some of the most challenging and innovative research trends in the area of access control, such as those related to the Web 2.0 revolution or to the Database as a Service paradigm. This book is a valuable reference for an heterogeneous audience. It can be used as either an extended survey for people who are interested in access control or as a reference book for senior undergraduate or graduate courses in data security with a special focus on access control. It is also useful for technologists, researchers, managers, and developers who want to know more about access control and related emerging trends.},
  added-at = {2016-07-15T01:25:10.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Ferrari, Elena},
  biburl = {https://www.bibsonomy.org/bibtex/245025bc5df82ad4fd603a05aacf0c995/vngudivada},
  interhash = {681ceabd990792f262b31d17c9202338},
  intrahash = {45025bc5df82ad4fd603a05aacf0c995},
  isbn = {9781608453764 1608453766 1608453758 9781608453757},
  keywords = {AccessControl DBMS SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {646518543},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Access control in data management systems},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881193},
  year = 2010
}

@book{purves2012neuroscience,
  abstract = {This classic textbook guides students through the challenges and excitement of the rapidly changing field of neuroscience. Accessible for both medical students and undergraduate neuroscience students, the 5th edition has been updated throughout to reflect the latest developments.},
  added-at = {2016-07-14T22:04:38.000+0200},
  address = {Sunderland, Mass.},
  author = {Purves, Dale},
  biburl = {https://www.bibsonomy.org/bibtex/29e7673bded6e5d731450cbe9c6d5b45c/vngudivada},
  interhash = {be4305632ccd66b5553321b206970e56},
  intrahash = {9e7673bded6e5d731450cbe9c6d5b45c},
  isbn = {9780878936953 0878936955 9780878936465 0878936467},
  keywords = {Book NeuroScience},
  publisher = {Sinauer Associates},
  refid = {754389847},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Neuroscience},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780878936953},
  year = 2012
}

@book{lin2010dataintensive,
  abstract = {Our world is being revolutionized by data-driven methods: access to large amounts of data has generated new insights and opened exciting new opportunities in commerce, science, and computing applications. Processing the enormous quantities of data necessary for these advances requires large clusters, making distributed computing paradigms more crucial than ever. MapReduce is a programming model for expressing distributed computations on massive datasets and an execution framework for large-scale data processing on clusters of commodity servers. The programming model provides an easy-to-understand abstraction for designing scalable algorithms, while the execution framework transparently handles many system-level details, ranging from scheduling to synchronization to fault tolerance. This book focuses on MapReduce algorithm design, with an emphasis on text processing algorithms common in natural language processing, information retrieval, and machine learning. We introduce the notion of MapReduce design patterns, which represent general reusable solutions to commonly occurring problems across a variety of problem domains. This book not only intends to help the reader "think in MapReduce", but also discusses limitations of the programming model as well.},
  added-at = {2016-07-14T23:48:46.000+0200},
  address = {San Rafael, California},
  author = {Lin, Jimmy and Dyer, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/24c76cc6f1c2614c835b7e2eed70de5a8/vngudivada},
  doi = {10.2200/S00274ED1V01Y201006HLT007},
  interhash = {59cdbabd0acd852c0b3042904e05dacf},
  intrahash = {4c76cc6f1c2614c835b7e2eed70de5a8},
  isbn = {9781608453436},
  keywords = {MapReduce SynthesisLecture TextProcessing},
  publisher = {Morgan \& Claypool Publishers},
  refid = {631796396},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Data-intensive text processing with MapReduce},
  year = 2010
}

@book{mausam2012planning,
  abstract = {Markov Decision Processes (MDPs) are widely popular in Artificial Intelligence for modeling sequential decision-making scenarios with probabilistic dynamics. They are the framework of choice when designing an intelligent agent that needs to act for long periods of time in an environment where its actions could have uncertain outcomes. MDPs are actively researched in two related subareas of AI, probabilistic planning and reinforcement learning. Probabilistic planning assumes known models for the agent's goals and domain dynamics, and focuses on determining how the agent should behave to achieve its objectives. On the other hand, reinforcement learning additionally learns these models based on the feedback the agent gets from the environment. This book provides a concise introduction to the use of MDPs for solving probabilistic planning problems, with an emphasis on the algorithmic perspective. It covers the whole spectrum of the field, from the basics to state-of-the-art optimal and approximation algorithms. We first describe the theoretical foundations of MDPs and the fundamental solution techniques for them. We then discuss modern optimal algorithms based on heuristic search and the use of structured representations. A major focus of the book is on the numerous approximation schemes for MDPs that have been developed in the AI literature. These include determinization-based approaches, sampling techniques, heuristic functions, dimensionality reduction, and hierarchical representations. Finally, we briefly introduce several extensions of the standard MDP classes that model and solve even more complex planning problems.},
  added-at = {2016-07-15T04:16:16.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Mausam and Kolobov, Andrey},
  biburl = {https://www.bibsonomy.org/bibtex/2c00b9da0c8317531621ecc985f1da33d/vngudivada},
  interhash = {c59b24bf5809d97caa7f214cf575f73a},
  intrahash = {c00b9da0c8317531621ecc985f1da33d},
  isbn = {9781608458875 1608458873},
  keywords = {MarkovDecisionProcess Planning SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {799364832},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Planning with Markov decision processes an AI perspective},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=956874},
  year = 2012
}

@book{nie2010crosslanguage,
  abstract = {Search for information is no longer exclusively limited within the native language of the user, but is more and more extended to other languages. This gives rise to the problem of cross-language information retrieval (CLIR), whose goal is to find relevant information written in a different language to a query. In addition to the problems of monolingual information retrieval (IR), translation is the key problem in CLIR: one should translate either the query or the documents from a language to another. However, this translation problem is not identical to full-text machine translation (MT): the goal is not to produce a human-readable translation, but a translation suitable for finding relevant documents. Specific translation methods are thus required. The goal of this book is to provide a comprehensive description of the specific problems arising in CLIR, the solutions proposed in this area, as well as the remaining problems. The book starts with a general description of the monolingual IR and CLIR problems. Different classes of approaches to translation are then presented: approaches using an MT system, dictionary-based translation and approaches based on parallel and comparable corpora. In addition, the typical retrieval effectiveness using different approaches is compared. It will be shown that translation approaches specifically designed for CLIR can rival and outperform high-quality MT systems. Finally, the book offers a look into the future that draws a strong parallel between query expansion in monolingual IR and query translation in CLIR, suggesting that many approaches developed in monolingual IR can be adapted to CLIR. The book can be used as an introduction to CLIR. Advanced readers can also find more technical details and discussions about the remaining research challenges in the future. It is suitable to new researchers who intend to carry out research on CLIR.},
  added-at = {2016-07-14T23:57:00.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Nie, Jian-Yun},
  biburl = {https://www.bibsonomy.org/bibtex/2ef244aa58162a966a3e4d5bc448cefb3/vngudivada},
  interhash = {be1120e448417f7f94392891a86e692f},
  intrahash = {ef244aa58162a966a3e4d5bc448cefb3},
  isbn = {9781598298642 159829864X},
  keywords = {CrossLanguageIR SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {647822495},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Cross-language information retrieval},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881088},
  year = 2010
}

@book{toman2011fundamentals,
  abstract = {Query compilation is the problem of translating user requests formulated over purely conceptual and domain specific ways of understanding data, commonly called logical designs, to efficient executable programs called query plans. Such plans access various concrete data sources through their low-level often iterator-based interfaces. An appreciation of the concrete data sources, their interfaces and how such capabilities relate to logical design is commonly called a physical design. This book is an introduction to the fundamental methods underlying database technology that solves the problem of query compilation. The methods are presented in terms of first-order logic which serves as the vehicle for specifying physical design, expressing user requests and query plans, and understanding how query plans implement user requests.},
  added-at = {2016-07-15T01:24:05.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Toman, David and Weddell, Grant E.},
  biburl = {https://www.bibsonomy.org/bibtex/2abc2b92bdc006fecde52ce7957f6aae5/vngudivada},
  interhash = {e0e1b87afabd81fc998e43890f781a4d},
  intrahash = {abc2b92bdc006fecde52ce7957f6aae5},
  isbn = {9781608452798 1608452794 9781608452781 1608452786},
  keywords = {PhysicalDesign QueryCompilation SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {743246512},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Fundamentals of physical design and query compilation},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881177},
  year = 2011
}

@book{jokinen2010spoken,
  abstract = {Considerable progress has been made in recent years in the development of dialogue systems that support robust and efficient human-machine interaction using spoken language. Spoken dialogue technology allows various interactive applications to be built and used for practical purposes, and research focuses on issues that aim to increase the system's communicative competence by including aspects of error correction, cooperation, multimodality, and adaptation in context.},
  added-at = {2016-07-14T23:33:14.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Jokinen, Kristiina and McTear, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/206f5848548c539b80df4121d802bc65a/vngudivada},
  interhash = {848686b9357088e5d24f3273d2e80be6},
  intrahash = {06f5848548c539b80df4121d802bc65a},
  keywords = {Language Speech Spoken SpokenDialogSystem SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Spoken dialogue systems},
  year = 2010
}

@book{wong2010privacypreserving,
  abstract = {Privacy preservation has become a major issue in many data analysis applications. When a data set is released to other parties for data analysis, privacy-preserving techniques are often required to reduce the possibility of identifying sensitive information about individuals. For example, in medical data, sensitive information can be the fact that a particular patient suffers from HIV. In spatial data, sensitive information can be a specific location of an individual. In web surfing data, the information that a user browses certain websites may be considered sensitive. Consider a dataset containing some sensitive information is to be released to the public. In order to protect sensitive information, the simplest solution is not to disclose the information. However, this would be an overkill since it will hinder the process of data analysis over the data from which we can find interesting patterns. Moreover, in some applications, the data must be disclosed under the government regulations. Alternatively, the data owner can first modify the data such that the modified data can guarantee privacy and, at the same time, the modified data retains sufficient utility and can be released to other parties safely. This process is usually called as privacy-preserving data publishing. In this monograph, we study how the data owner can modify the data and how the modified data can preserve privacy and protect sensitive information.},
  added-at = {2016-07-15T01:22:57.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Wong, Raymond Chi-Wing and Fu, Ada},
  biburl = {https://www.bibsonomy.org/bibtex/2e734e753101f1c1a9110115b19bad0cd/vngudivada},
  interhash = {7a3dbe24e94c9693692183b1b216f764},
  intrahash = {e734e753101f1c1a9110115b19bad0cd},
  isbn = {9781608452170 1608452174},
  keywords = {DataPrivacy SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {601237943},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Privacy-preserving data publishing : an overview},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881166},
  year = 2010
}

@book{taleb2010black,
  abstract = {Examines the role of the unexpected, discussing why improbable events are not anticipated or understood properly, and how humans rationalize the black swan phenomenon to make it appear less random.},
  added-at = {2016-07-14T21:59:17.000+0200},
  address = {New York},
  author = {Taleb, Nassim Nicholas},
  biburl = {https://www.bibsonomy.org/bibtex/2e95a3ab924a114a2e2d6ca9324cf4eb7/vngudivada},
  interhash = {eaf8b1dccb7bec4487915dc849a33a4a},
  intrahash = {e95a3ab924a114a2e2d6ca9324cf4eb7},
  isbn = {081297381X 9780812973815},
  keywords = {Biology Book},
  publisher = {Random House Trade Paperbacks},
  refid = {213400968},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {The black swan: the impact of the highly improbable},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780812973815},
  year = 2010
}

@book{bender2013linguistic,
  abstract = {Many NLP tasks have at their core a subtask of extracting the dependencies--who did what to whom--from natural language sentences. This task can be understood as the inverse of the problem solved in different ways by diverse human languages, namely, how to indicate the relationship between different parts of a sentence. Understanding how languages solve the problem can be extremely useful in both feature design and error analysis in the application of machine learning to NLP. Likewise, understanding cross-linguistic variation can be important for the design of MT systems and other multilingual applications. The purpose of this book is to present in a succinct and accessible fashion information about the morphological and syntactic structure of human languages that can be useful in creating more linguistically sophisticated, more language-independent, and thus more successful NLP systems.},
  added-at = {2016-07-14T23:22:58.000+0200},
  address = {San Rafael, California},
  author = {Bender, Emily M.},
  biburl = {https://www.bibsonomy.org/bibtex/2ac8278c05b34e7274f2fa13bd85dc7b9/vngudivada},
  interhash = {c6ff03b95798ed351ef3c7d98cbf91cd},
  intrahash = {ac8278c05b34e7274f2fa13bd85dc7b9},
  isbn = {978-1627050128},
  keywords = {Linguistics Morphology Syntax SynthesisLecture},
  publisher = {Morgan \& Claypool},
  refid = {853273078},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Linguistic fundamentals for natural language processing: 100 essentials from morphology and syntax},
  year = 2013
}

@book{dechter2013reasoning,
  abstract = {Graphical models (e.g., Bayesian and constraint networks, influence diagrams, and Markov decision processes) have become a central paradigm for knowledge representation and reasoning in both artificial intelligence and computer science in general. These models are used to perform many reasoning tasks, such as scheduling, planning and learning, diagnosis and prediction, design, hardware and software verification, and bioinformatics. These problems can be stated as the formal tasks of constraint satisfaction and satisfiability, combinatorial optimization, and probabilistic inference.},
  added-at = {2016-07-15T04:11:16.000+0200},
  author = {Dechter, Rina},
  biburl = {https://www.bibsonomy.org/bibtex/2c45bf10d1f8feded50063f9331add639/vngudivada},
  interhash = {9a7cc8fdf7e14f9c5dfa01db51c95e1e},
  intrahash = {c45bf10d1f8feded50063f9331add639},
  isbn = {9781627051989 1627051988},
  keywords = {GraphicalModel SynthesisLecture},
  refid = {867318552},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Reasoning with probabilistic and deterministic graphical models: exact algorithms},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1585293},
  year = 2013
}

@book{bertino2012protection,
  abstract = {As data represent a key asset for today's organizations, the problem of how to protect this data from theft and misuse is at the forefront of these organizations' minds. Even though today several data security techniques are available to protect data and computing infrastructures, many such techniques--such as firewalls and network security tools--are unable to protect data from attacks posed by those working on an organization's "inside." These "insiders" usually have authorized access to relevant information systems, making it extremely challenging to block the misuse of information while still allowing them to do their jobs. This book discusses several techniques that can provide effective protection against attacks posed by people working on the inside of an organization. Chapter 1 introduces the notion of insider threat and reports some data about data breaches due to insider threats. Chapter 2 covers authentication and access control techniques, and Chapter 3 shows how these general security techniques can be extended and used in the context of protection from insider threats. Chapter 4 addresses anomaly detection techniques that are used to determine anomalies in data accesses by insiders. These anomalies are often indicative of potential insider data attacks and therefore play an important role in protection from these attacks. Security information and event management (SIEM) tools and fine-grained auditing are discussed in Chapter 5. These tools aim at collecting, analyzing, and correlating--in real-time--any information and event that may be relevant for the security of an organization. As such, they can be a key element in finding a solution to such undesirable insider threats. Chapter 6 goes on to provide a survey of techniques for separation-of-duty (SoD). SoD is an important principle that, when implemented in systems and tools, can strengthen data protection from malicious insiders. However, to date, very few approaches have been proposed for implementing SoD in systems. In Chapter 7, a short survey of a commercial product is presented, which provides different techniques for protection from malicious users with system privileges--such as a DBA in database management systems. Finally, in Chapter 8, the book concludes with a few remarks and additional research directions.},
  added-at = {2016-07-15T01:11:50.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Bertino, Elisa},
  biburl = {https://www.bibsonomy.org/bibtex/20084d8c27375a7a4a1f62458468940d2/vngudivada},
  interhash = {5f552f10346df71917740b013045b9fe},
  intrahash = {0084d8c27375a7a4a1f62458468940d2},
  isbn = {9781608457694 1608457699},
  keywords = {DataSecurity SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {799363515},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Data protection from insider threats},
  url = {http://site.ebrary.com/id/10583910},
  year = 2012
}

@book{sogaard2013semisupervised,
  abstract = {Annotation},
  added-at = {2016-07-14T23:36:03.000+0200},
  address = {San Rafael},
  author = {Søgaard, Anders},
  biburl = {https://www.bibsonomy.org/bibtex/2aa92ca80ebd77ff3de43e81bd13b0bba/vngudivada},
  interhash = {9f26d865a7f1b40c0a8660081954e64a},
  intrahash = {aa92ca80ebd77ff3de43e81bd13b0bba},
  isbn = {1608459861 9781608459865},
  keywords = {DomainAdaptation SemiSupervisedLearning SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {852835763},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Semi-supervised learning and domain adaptation in nlp},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1207304_0},
  year = 2013
}

@book{jensen2010multidimensional,
  abstract = {The present book's subject is multidimensional data models and data modeling concepts as they are applied in real data warehouses. The book aims to present the most important concepts within this subject in a precise and understandable manner. The book's coverage of fundamental concepts includes data cubes and their elements, such as dimensions, facts, and measures and their representation in a relational setting; it includes architecture-related concepts; and it includes the querying of multidimensional databases. The book also covers advanced multidimensional concepts that are considered to be particularly important. This coverage includes advanced dimension-related concepts such as slowly changing dimensions, degenerate and junk dimensions, outriggers, parent-child hierarchies, and unbalanced, non-covering, and non-strict hierarchies. The book offers a principled overview of key implementation techniques that are particularly important to multidimensional databases, including materialized views, bitmap indices, join indices, and star join processing. The book ends with a chapter that presents the literature on which the book is based and offers further readings for those readers who wish to engage in more in-depth study of specific aspects of the book's subject.},
  added-at = {2016-07-15T00:58:24.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Jensen, Christian S. and Pedersen, Torben Bach and Thomsen, Christian},
  biburl = {https://www.bibsonomy.org/bibtex/2271da809f0f96cdc493f77781351dc6e/vngudivada},
  interhash = {15e35f6f10f20ec4f39e5494988c6945},
  intrahash = {271da809f0f96cdc493f77781351dc6e},
  isbn = {9781608455386 1608455386 1608455378 9781608455379},
  keywords = {MultidimensionalDBMS SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {664723898},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Multidimensional databases and data warehousing},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881228},
  year = 2010
}

@book{aberer2011peertopeer,
  abstract = {This lecture introduces systematically into the problem of managing large data collections in peer-to-peer systems. Search over large datasets has always been a key problem in peer-to-peer systems and the peer-to-peer paradigm has incited novel directions in the field of data management. This resulted in many novel peer-to-peer data management concepts and algorithms, for supporting data management tasks in a wider sense, including data integration, document management and text retrieval. The lecture covers four different types of peer-to-peer data management systems that are characterized by the type of data they manage and the search capabilities they support. The first type are structured peer-to-peer data management systems which support structured query capabilities for standard data models. The second type are peer-to-peer data integration systems for querying of heterogeneous databases without requiring a common global schema. The third type are peer-to-peer document retrieval systems that enable document search based both on the textual content and the document structure. Finally, we introduce semantic overlay networks, which support similarity search on information represented in hierarchically organized and multi-dimensional semantic spaces. Topics that go beyond data representation and search are summarized at the end of the lecture.},
  added-at = {2016-07-15T01:28:26.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Aberer, Karl},
  biburl = {https://www.bibsonomy.org/bibtex/22f531c6fafd86f997de2cb1c0710a005/vngudivada},
  interhash = {a6ce0ae8a2537d3cd4b94e83e56d5dba},
  intrahash = {2f531c6fafd86f997de2cb1c0710a005},
  isbn = {9781608457205 1608457206 1608457192 9781608457199},
  keywords = {DataManagement P2P SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {726897639},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Peer-to-peer data management},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881270},
  year = 2011
}

@book{stede2012discourse,
  abstract = {Discourse Processing here is framed as marking up a text with structural descriptions on several levels, which can serve to support many language-processing or text-mining tasks. We first explore some ways of assigning structure on the document level: the logical document structure as determined by the layout of the text, its genre-specific content structure, and its breakdown into topical segments. Then the focus moves to phenomena of local coherence. We introduce the problem of coreference and look at methods for building chains of coreferring entities in the text. Next, the notion of coherence relation is introduced as the second important factor of local coherence. We study the role of connectives and other means of signaling such relations in text, and then return to the level of larger textual units, where tree or graph structures can be ascribed by recursively assigning coherence relations. Taken together, these descriptions can inform text summarization, information extraction, discourse-aware sentiment analysis, question answering, and the like.},
  added-at = {2016-07-14T23:49:48.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Stede, Manfred},
  biburl = {https://www.bibsonomy.org/bibtex/29447a7fa98a90ae74e3adda4016eef32/vngudivada},
  interhash = {0b080656196fbdddfc3c8b92c4fa1643},
  intrahash = {9447a7fa98a90ae74e3adda4016eef32},
  isbn = {9781608457359 1608457354},
  keywords = {DiscourseProcessing SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {767731873},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Discourse processing},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881273},
  year = 2012
}

@book{mani2013computational,
  abstract = {The field of narrative (or story) understanding and generation is one of the oldest in natural language processing (NLP) and artificial intelligence (AI), which is hardly surprising, since storytelling is such a fundamental and familiar intellectual and social activity. In recent years, the demands of interactive entertainment and interest in the creation of engaging narratives with life-like characters have provided a fresh impetus to this field. This book provides an overview of the principal problems, approaches, and challenges faced today in modeling the narrative structure of stories. The book introduces classical narratological concepts from literary theory and their mapping to computational approaches. It demonstrates how research in AI and NLP has modeled character goals, causality, and time using formalisms from planning, case-based reasoning, and temporal reasoning, and discusses fundamental limitations in such approaches. It proposes new representations for embedded narratives and fictional entities, for assessing the pace of a narrative, and offers an empirical theory of audience response. These notions are incorporated into an annotation scheme called NarrativeML. The book identifies key issues that need to be addressed, including annotation methods for long literary narratives, the representation of modality and habituality, and characterizing the goals of narrators. It also suggests a future characterized by advanced text mining of narrative structure from large-scale corpora and the development of a variety of useful authoring aids. This is the first book to provide a systematic foundation that integrates together narratology, AI, and computational linguistics. It can serve as a narratology primer for computer scientists and an elucidation of computational narratology for literary theorists. It is written in a highly accessible manner and is intended for use by a broad scientific audience that includes linguists (computational and formal semanticists), AI researchers, cognitive scientists, computer scientists, game developers, and narrative theorists.},
  added-at = {2016-07-14T23:43:37.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Mani, Inderjeet},
  biburl = {https://www.bibsonomy.org/bibtex/235632038a3406e0d4fa172f01963f601/vngudivada},
  interhash = {3e352ec9f6c97f9b82ccdf6a15909505},
  intrahash = {35632038a3406e0d4fa172f01963f601},
  isbn = {9781608459827 1608459829 1608459810 9781608459810},
  keywords = {NarraiveModeling SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {824174878},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Computational modeling of narrative},
  url = {http://site.ebrary.com/id/10645232},
  year = 2013
}

@book{deutch2012business,
  abstract = {While classic data management focuses on the data itself, research on Business Processes also considers the context in which this data is generated and manipulated, namely the processes, users, and goals that this data serves. This provides the analysts a better perspective of the organizational needs centered around the data. As such, this research is of fundamental importance. Much of the success of database systems in the last decade is due to the beauty and elegance of the relational model and its declarative query languages, combined with a rich spectrum of underlying evaluation and optimization techniques, and efficient implementations. Much like the case for traditional database research, elegant modeling and rich underlying technology are likely to be highly beneficiary for the Business Process owners and their users; both can benefit from easy formulation and analysis of the processes. While there have been many important advances in this research in recent years, there is still much to be desired: specifically, there have been many works that focus on the processes behavior (flow), and many that focus on its data, but only very few works have dealt with both the state-of-the-art in a database approach to Business Process modeling and analysis, the progress towards a holistic flow-and-data framework for these tasks, and highlight the current gaps and research directions.},
  added-at = {2016-07-15T01:33:13.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Deutch, Daniel and Milo, Tova},
  biburl = {https://www.bibsonomy.org/bibtex/2232cfffe9c4577e60c47a557e10986a4/vngudivada},
  interhash = {6526c29a7f3e2ec44338319773693cdc},
  intrahash = {232cfffe9c4577e60c47a557e10986a4},
  isbn = {9781608459032 1608459039},
  keywords = {BusinessProcess DBMS SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {801682228},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Business processes a database perspective},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=981417},
  year = 2012
}

@book{genesereth2013introduction,
  abstract = {This book is a gentle but rigorous introduction to Formal Logic. It is intended primarily for use at the college level. However, it can also be used for advanced secondary school students, and it can be used at the start of graduate school for those who have not yet seen the material. The approach to teaching logic used here emerged from more than 20 years of teaching logic to students at Stanford University and from teaching logic to tens of thousands of others via online courses on the World Wide Web. The approach differs from that taken by other books in logic in two essential ways, one having to do with content, the other with form. Like many other books on logic, this one covers logical syntax and semantics and proof theory plus induction. However, unlike other books, this book begins with Herbrand semantics rather than the more traditional Tarskian semantics. This approach makes the material considerably easier for students to understand and leaves them with a deeper understanding of what logic is all about.},
  added-at = {2016-07-14T22:39:25.000+0200},
  author = {Genesereth, Michael R. and Kao, Eric},
  biburl = {https://www.bibsonomy.org/bibtex/2ae77dfa5b12d18cd37d4a69d6554b8b1/vngudivada},
  interhash = {8f86e26853548bcd4fba7b41bac8381e},
  intrahash = {ae77dfa5b12d18cd37d4a69d6554b8b1},
  isbn = {9781627052481 1627052488},
  keywords = {Logic SynthesisLecture},
  refid = {858583601},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Introduction to logic},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1365094},
  year = 2013
}

@book{kemme2010database,
  abstract = {Database replication is widely used for fault-tolerance, scalability and performance. The failure of one database replica does not stop the system from working as available replicas can take over the tasks of the failed replica. Scalability can be achieved by distributing the load across all replicas, and adding new replicas should the load increase. Finally, database replication can provide fast local access, even if clients are geographically distributed clients, if data copies are located close to clients.},
  added-at = {2016-07-15T01:06:20.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Kemme, Bettina and Jiménez-Peris, R. and Patiño-Martínez, M.},
  biburl = {https://www.bibsonomy.org/bibtex/2037f1b70fdbe7ebc987735a6063293c8/vngudivada},
  interhash = {99261e04911840b8b48035a5e2b0378c},
  intrahash = {037f1b70fdbe7ebc987735a6063293c8},
  isbn = {9781608453825 1608453820},
  keywords = {DataReplication SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {707877270},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Database replication},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881194},
  year = 2010
}

@book{vlassis2007concise,
  abstract = {Multiagent systems is an expanding field that blends classical fields like game theory and decentralized control with modern fields like computer science and machine learning. This monograph provides a concise introduction to the subject, covering the theoretical foundations as well as more recent developments in a coherent and readable manner.},
  added-at = {2016-07-14T22:34:51.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Vlassis, Nikos},
  biburl = {https://www.bibsonomy.org/bibtex/2fc524022ba0f55f27fbedf7dbd5b659f/vngudivada},
  interhash = {d03dfe6e07240620f3bcddd491427aee},
  intrahash = {fc524022ba0f55f27fbedf7dbd5b659f},
  isbn = {1598295276 9781598295276},
  keywords = {DAI MultiagentSystem SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {156727279},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {A concise introduction to multiagent systems and distributed artificial intelligence},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881015},
  year = 2007
}

@book{gal2011uncertain,
  abstract = {Schema matching is the task of providing correspondences between concepts describing the meaning of data in various heterogeneous, distributed data sources. Schema matching is one of the basic operations required by the process of data and schema integration, and thus has a great effect on its outcomes, whether these involve targeted content delivery, view integration, database integration, query rewriting over heterogeneous sources, duplicate data elimination, or automatic streamlining of workflow activities that involve heterogeneous data sources. Although schema matching research has been ongoing for over 25 years, more recently a realization has emerged that schema matchers are inherently uncertain. Since 2003, work on the uncertainty in schema matching has picked up, along with research on uncertainty in other areas of data management. This lecture presents various aspects of uncertainty in schema matching within a single unified framework. We introduce basic formulations of uncertainty and provide several alternative representations of schema matching uncertainty. Then, we cover two common methods that have been proposed to deal with uncertainty in schema matching, namely ensembles, and top-K matchings, and analyze them in this context. We conclude with a set of real-world applications.},
  added-at = {2016-07-15T01:20:04.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Gal, Avigdor},
  biburl = {https://www.bibsonomy.org/bibtex/2971cdbd5f4173e4415d7b4500e0f80c7/vngudivada},
  interhash = {8507b3c96c428e6642795c888af1222b},
  intrahash = {971cdbd5f4173e4415d7b4500e0f80c7},
  isbn = {9781608454341 1608454347 1608454339 9781608454334},
  keywords = {SchemaMatching SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {708649889},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Uncertain schema matching},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881204},
  year = 2011
}

@book{pang2012query,
  abstract = {In data publishing, the owner delegates the role of satisfying user queries to a third-party publisher. As the servers of the publisher may be untrusted or susceptible to attacks, we cannot assume that they would always process queries correctly, hence there is a need for users to authenticate their query answers. This book introduces various notions that the research community has studied for defining the correctness of a query answer. In particular, it is important to guarantee the completeness, authenticity and minimality of the answer, as well as its freshness. We present authentication mechanisms for a wide variety of queries in the context of relational and spatial databases, text retrieval, and data streams. We also explain the cryptographic protocols from which the authentication mechanisms derive their security properties.},
  added-at = {2016-07-14T22:29:01.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Pang, HweeHwa and Tan, Kian-Lee},
  biburl = {https://www.bibsonomy.org/bibtex/235fa3d529a8726aa5ed975ce7b5defe6/vngudivada},
  interhash = {b3d02a17382aa9e7c2fde83fbc64c633},
  intrahash = {35fa3d529a8726aa5ed975ce7b5defe6},
  isbn = {9781608457663 1608457664},
  keywords = {QueryValidation SQL},
  publisher = {Morgan & Claypool Publishers},
  refid = {779514204},
  timestamp = {2019-03-25T17:13:44.000+0100},
  title = {Query answer authentication},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881281},
  year = 2012
}

@article{collins2012creating,
  abstract = {In Ireland, while detailed information is available regarding hospital attendance, little is known regarding general (family) practice attendance. However, it is conservatively estimated that there are almost nine times as many general practice encounters than there are hospital encounters each year in Ireland. This represents a very significant gap in health information. Indeed, general practice has been shown in other countries to be an important and rich source of information about the health of the population, their behaviors and their utilization of health services. Funded by the Health Information and Quality Authority (HIQA), the Irish College of General Practitioners (ICGP) undertook a feasibility study of diagnostic coding of routinely entered patient data and the creation of a national general practice morbidity and epidemiological database (GPMED project). This article outlines the process of data quality issue management undertaken. The study’s findings suggest that the quality of data collection and reporting structures available in general practice throughout Ireland at the outset of this project were not adequate to permit the creation of a database of sufficient quality for service planning and policy or epidemiological research. Challenges include the dearth of a minimum standard of data recorded in consultations by GPs and the absence of the digital data recording and exporting infrastructure within Irish patient management software systems. In addition, there is at present a lack of recognition regarding the value of such data for patient management and service planning---including importantly, data collectors who do not fully accept the merit of maintaining data, which has a direct consequence for data quality. The work of this project has substantial implications for the data available to the health sector in Ireland and contributes to the knowledge base internationally regarding general practice morbidity data.},
  acmid = {2378018},
  added-at = {2016-07-23T20:27:32.000+0200},
  address = {New York, NY, USA},
  articleno = {2},
  author = {Collins, Claire and Janssens, Kelly},
  biburl = {https://www.bibsonomy.org/bibtex/2960f4b0310aa2e16279547fa12ac3d28/vngudivada},
  doi = {10.1145/2378016.2378018},
  interhash = {6b4fd7928fa4cadb2ac6cc1bf6da71e3},
  intrahash = {960f4b0310aa2e16279547fa12ac3d28},
  issn = {1936-1955},
  issue_date = {October 2012},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality EpidemiologicalData},
  month = oct,
  number = 1,
  numpages = {9},
  pages = {2:1--2:9},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Creating a General (Family) Practice Epidemiological Database in Ireland - Data Quality Issue Management},
  url = {http://doi.acm.org/10.1145/2378016.2378018},
  volume = 4,
  year = 2012
}

@article{muller2012improving,
  abstract = {In many domains, data cleaning is hampered by our limited ability to specify a comprehensive set of integrity constraints to assist in identification of erroneous data. An alternative approach to improve data quality is to exploit different data sources that contain information about the same set of objects. Such overlapping sources highlight hot-spots of poor data quality through conflicting data values and immediately provide alternative values for conflict resolution. In order to derive a dataset of high quality, we can merge the overlapping sources based on a quality assessment of the conflicting values. The quality of the resulting dataset, however, is highly dependent on our ability to asses the quality of conflicting values effectively. The main objective of this article is to introduce methods that aid the developer of an integrated system over overlapping, but contradicting sources in the task of improving the quality of data. Value conflicts between contradicting sources are often systematic, caused by some characteristic of the different sources. Our goal is to identify such systematic differences and outline data patterns that occur in conjunction with them. Evaluated by an expert user, the regularities discovered provide insights into possible conflict reasons and help to assess the quality of inconsistent values. The contributions of this article are two concepts of systematic conflicts: contradiction patterns and minimal update sequences. Contradiction patterns resemble a special form of association rules that summarize characteristic data properties for conflict occurrence. We adapt existing association rule mining algorithms for mining contradiction patterns. Contradiction patterns, however, view each class of conflicts in isolation, sometimes leading to largely overlapping patterns. Sequences of set-oriented update operations that transform one data source into the other are compact descriptions for all regular differences among the sources. We consider minimal update sequences as the most likely explanation for observed differences between overlapping data sources. Furthermore, the order of operations within the sequences point out potential dependencies between systematic differences. Finding minimal update sequences, however, is beyond reach in practice. We show that the problem already is NP-complete for a restricted set of operations. In the light of this intractability result, we present heuristics that lead to convincing results for all examples we considered.},
  acmid = {2107538},
  added-at = {2016-07-23T21:38:54.000+0200},
  address = {New York, NY, USA},
  articleno = {15},
  author = {M\"{u}ller, Heiko and Freytag, Johann-Christoph and Leser, Ulf},
  biburl = {https://www.bibsonomy.org/bibtex/29ddbcfd0d2034157134c645bd428b872/vngudivada},
  doi = {10.1145/2107536.2107538},
  interhash = {ab602e9b10879683e9eec58616bcc294},
  intrahash = {9ddbcfd0d2034157134c645bd428b872},
  issn = {1936-1955},
  issue_date = {February 2012},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality},
  month = mar,
  number = 4,
  numpages = {38},
  pages = {15:1--15:38},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Improving Data Quality by Source Analysis},
  url = {http://doi.acm.org/10.1145/2107536.2107538},
  volume = 2,
  year = 2012
}

@article{gennari2015challenges,
  abstract = {Annotation mistakes with temporal relations are rather widespread and present also in gold standards. Current methods for controlling the quality of annotations are limited: they usually check the local context of the annotation, or suggest annotations similar to those already available. This paper overview current challenges for quality temporal annotations, and it proposes solutions.},
  acmid = {2736699},
  added-at = {2016-07-23T20:08:24.000+0200},
  address = {New York, NY, USA},
  articleno = {9},
  author = {Gennari, Rosella and Tonelli, Sara and Vittorini, Pierpaolo},
  biburl = {https://www.bibsonomy.org/bibtex/2cd0ccf5c52561995453e0e1ebf8139cb/vngudivada},
  doi = {10.1145/2736699},
  interhash = {dded6729ad541aec8decc2146b872634},
  intrahash = {cd0ccf5c52561995453e0e1ebf8139cb},
  issn = {1936-1955},
  issue_date = {July 2015},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality TemporalData},
  month = jun,
  number = {2-3},
  numpages = {4},
  pages = {9:1--9:4},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Challenges in Quality of Temporal Data \&Mdash; Starting with Gold Standards},
  url = {http://doi.acm.org/10.1145/2736699},
  volume = 6,
  year = 2015
}

@book{wellman2011trading,
  abstract = {Automated trading in electronic markets is one of the most common and consequential applications of autonomous software agents. Design of effective trading strategies requires thorough understanding of how market mechanisms operate, and appreciation of strategic issues that commonly manifest in trading scenarios. Drawing on research in auction theory and artificial intelligence, this book presents core principles of strategic reasoning that apply to market situations. The author illustrates trading strategy choices through examples of concrete market environments, such as eBay, as well as abstract market models defined by configurations of auctions and traders. Techniques for addressing these choices constitute essential building blocks for the design of trading strategies for rich market applications. The lecture assumes no prior background in game theory or auction theory, or artificial intelligence.},
  added-at = {2016-07-15T05:00:04.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Wellman, Michael P.},
  biburl = {https://www.bibsonomy.org/bibtex/2efb64d45765f9ee2bb3eff92f01898eb/vngudivada},
  interhash = {be0e815d9bff363b7948d719d8a2a1cc},
  intrahash = {efb64d45765f9ee2bb3eff92f01898eb},
  isbn = {9781598296068 159829606X 9781598296051 1598296051},
  keywords = {SynthesisLecture TradingAgents},
  publisher = {Morgan & Claypool Publishers},
  refid = {744673472},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Trading agents},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881036},
  year = 2011
}

@inproceedings{lucas2010corporate,
  abstract = {It is now assumed that poor quality data is costing large amounts of money to corporations all over the world. Although research on methods and techniques for data quality assessment and improvement have begun in the early nineties of the past century and being currently abundant and innovative, it is noted that the academic and professional communities virtually have no dialogue, which turns out to be harmful to both of them. The challenge of promoting the relevance in information systems research, without compromising the necessary rigor, is still present in the various disciplines of information systems scientific area, including the data quality one. In this paper we present “data as a corporate asset” as a business philosophy, and a framework for the concepts related to that philosophy, derived from the academic and professional literature. According to this framework, we present, analyze and discuss a single explanatory case study, developed in a fixed and mobile telecommunications company, operating in one of the European Union Countries. The results show that, in the absence of data stewardship roles, data quality problems become more of an "IT problem" than typically is considered in the literature, owing to Requirements Analysis Teams of the IS Development Units, to become a “quality negotiator” between the various stakeholders. Other findings are their bottom-up approach to data quality management, their biggest focus on motivating employees through innovative forms of communication, which appears to be a critical success factor (CSF) for data quality management, as well as the importance of a data quality champion leadership.},
  added-at = {2016-07-23T21:21:50.000+0200},
  author = {Lucas, Ana},
  biburl = {https://www.bibsonomy.org/bibtex/26409d9c60a300de74333d43457111c95/vngudivada},
  booktitle = {5th Iberian Conference on Information Systems and Technologies},
  interhash = {3ad0589202b322ec3f8c2aa1da7285a0},
  intrahash = {6409d9c60a300de74333d43457111c95},
  keywords = {DataQuality DataQualityManagement},
  month = jun,
  pages = {1-7},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Corporate Data Quality Management: From Theory to Practice},
  year = 2010
}

@inproceedings{kandel2012profiler,
  abstract = {Data quality issues such as missing, erroneous, extreme and duplicate values undermine analysis and are time-consuming to find and fix. Automated methods can help identify anomalies, but determining what constitutes an error is context-dependent and so requires human judgment. While visualization tools can facilitate this process, analysts must often manually construct the necessary views, requiring significant expertise. We present Profiler, a visual analysis tool for assessing quality issues in tabular data. Profiler applies data mining methods to automatically flag problematic data and suggests coordinated summary visualizations for assessing the data in context. The system contributes novel methods for integrated statistical and visual analysis, automatic view suggestion, and scalable visual summaries that support real-time interaction with millions of data points. We present Profiler's architecture --- including modular components for custom data types, anomaly detection routines and summary visualizations --- and describe its application to motion picture, natural disaster and water quality data sets.},
  acmid = {2254659},
  added-at = {2016-07-23T22:40:04.000+0200},
  address = {New York, NY, USA},
  author = {Kandel, Sean and Parikh, Ravi and Paepcke, Andreas and Hellerstein, Joseph M. and Heer, Jeffrey},
  biburl = {https://www.bibsonomy.org/bibtex/24897fb4ab35f9171b5729d002cb5d9b0/vngudivada},
  booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
  doi = {10.1145/2254556.2254659},
  interhash = {bbb24890cfb3ecb4447c41415e42b148},
  intrahash = {4897fb4ab35f9171b5729d002cb5d9b0},
  isbn = {978-1-4503-1287-5},
  keywords = {DataQuality DataQualityAssessment StatisticalAnalysis Visualization},
  location = {Capri Island, Italy},
  numpages = {8},
  pages = {547--554},
  publisher = {ACM},
  series = {AVI '12},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Profiler: Integrated Statistical Analysis and Visualization for Data Quality Assessment},
  url = {http://doi.acm.org/10.1145/2254556.2254659},
  year = 2012
}

@book{law2011human,
  abstract = {Human computation is a new and evolving research area that centers around harnessing human intelligence to solve computational problems that are beyond the scope of existing Artificial Intelligence (AI) algorithms. With the growth of the Web, human computation systems can now leverage the abilities of an unprecedented number of people via the Web to perform complex computation. There are various genres of human computation applications that exist today. Games with a purpose (e.g., the ESP Game) specifically target online gamers who generate useful data (e.g., image tags) while playing an enjoyable game. Crowdsourcing marketplaces (e.g., Amazon Mechanical Turk) are human computation systems that coordinate workers to perform tasks in exchange for monetary rewards. In identity verification tasks, users perform computation in order to gain access to some online content; an example is reCAPTCHA, which leverages millions of users who solve CAPTCHAs every day to correct words in books that optical character recognition (OCR) programs fail to recognize with certainty. This book is aimed at achieving four goals: (1) defining human computation as a research area; (2) providing a comprehensive review of existing work; (3) drawing connections to a wide variety of disciplines, including AI, Machine Learning, HCI, Mechanism/Market Design and Psychology, and capturing their unique perspectives on the core research questions in human computation; and (4) suggesting promising research directions for the future.},
  added-at = {2016-07-15T04:38:06.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Law, Edith L. M. and Von Ahn, Luis},
  biburl = {https://www.bibsonomy.org/bibtex/24200620c052c8c1ededcd5f961b609ed/vngudivada},
  interhash = {84d48e806c27ac623cfff81f756412fd},
  intrahash = {4200620c052c8c1ededcd5f961b609ed},
  isbn = {9781608455171 1608455173 9781608455164 1608455165},
  keywords = {CrowdSourcing HumanComputation SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {743307093},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Human computation},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881224},
  year = 2011
}

@inproceedings{agnihotri2016educational,
  abstract = {Enormous amount of educational data has been accumulated through Massive Open Online Courses (MOOCs), as well as commercial and non-commercial learning platforms. This is in addition to the educational data released by US government since 2012 to facilitate disruption in education by making data freely available. The high volume, variety and velocity of collected data necessitate use of big data tools and storage systems such as distributed databases for storage and Apache Spark for analysis. This tutorial will introduce researchers and faculty to real-world applications involving data mining and predictive analytics in learning sciences. In addition, the tutorial will introduce statistics required to validate and accurately report results. Topics will cover how big data is being used to transform education. Specifically, we will demonstrate how exploratory data analysis, data mining, predictive analytics, machine learning, and visualization techniques are being applied to educational big data to improve learning and scale insights driven from millions of student's records. The tutorial will be held over a half day and will be hands on with pre-posted material. Due to the interdisciplinary nature of work, the tutorial appeals to researchers from a wide range of backgrounds including big data, predictive analytics, learning sciences, educational data mining, and in general, those interested in how big data analytics can transform learning. As a prerequisite, attendees are required to have familiarity with at least one programming language.},
  acmid = {2883857},
  added-at = {2016-07-16T23:53:37.000+0200},
  address = {New York, NY, USA},
  author = {Agnihotri, Lalitha and Mojarad, Shirin and Lewkow, Nicholas and Essa, Alfred},
  biburl = {https://www.bibsonomy.org/bibtex/274251e133e0765693ff6435d43ca8377/vngudivada},
  booktitle = {Proceedings of the Sixth International Conference on Learning Analytics \& Knowledge},
  doi = {10.1145/2883851.2883857},
  interhash = {f465f571c8020f9abf8c285c57595d0f},
  intrahash = {74251e133e0765693ff6435d43ca8377},
  isbn = {978-1-4503-4190-5},
  keywords = {ApacheSpark EDM Tutorial},
  location = {Edinburgh, United Kingdom},
  numpages = {2},
  pages = {507--508},
  publisher = {ACM},
  series = {LAK '16},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Educational Data Mining with Python and Apache Spark: A Hands-on Tutorial},
  url = {http://doi.acm.org/10.1145/2883851.2883857},
  year = 2016
}

@article{qin2016things,
  abstract = {Abstract With the recent advances in radio-frequency identification (RFID), low-cost wireless sensor devices, and Web technologies, the Internet of Things (IoT) approach has gained momentum in connecting everyday objects to the Internet and facilitating machine-to-human and machine-to-machine communication with the physical world. IoT offers the capability to connect and integrate both digital and physical entities, enabling a whole new class of applications and services, but several significant challenges need to be addressed before these applications and services can be fully realized. A fundamental challenge centers around managing IoT data, typically produced in dynamic and volatile environments, which is not only extremely large in scale and volume, but also noisy and continuous. This paper reviews the main techniques and state-of-the-art research efforts in IoT from data-centric perspectives, including data stream processing, data storage models, complex event processing, and searching in IoT. Open research issues for IoT data management are also discussed. },
  added-at = {2016-07-21T13:24:51.000+0200},
  author = {Qin, Yongrui and Sheng, Quan Z. and Falkner, Nickolas J.G. and Dustdar, Schahram and Wang, Hua and Vasilakos, Athanasios V.},
  biburl = {https://www.bibsonomy.org/bibtex/2f7a8a60d7850b23adb96e52fbe5af742/vngudivada},
  doi = {http://dx.doi.org/10.1016/j.jnca.2015.12.016},
  interhash = {11038039d5e5b0507062f0942fcd7dd0},
  intrahash = {f7a8a60d7850b23adb96e52fbe5af742},
  issn = {1084-8045},
  journal = {Journal of Network and Computer Applications },
  keywords = {IoT Survey},
  pages = {137 - 153},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {When things matter: A survey on data-centric internet of things },
  url = {http://www.sciencedirect.com/science/article/pii/S1084804516000606},
  volume = 64,
  year = 2016
}

@article{even2009assessment,
  abstract = {Quantitative assessment of data quality is critical for identifying the presence of data defects and the extent of the damage due to these defects. Quantitative assessment can help define realistic quality improvement targets, track progress, evaluate the impacts of different solutions, and prioritize improvement efforts accordingly. This study describes a methodology for quantitatively assessing both impartial and contextual data quality in large datasets. Impartial assessment measures the extent to which a dataset is defective, independent of the context in which that dataset is used. Contextual assessment, as defined in this study, measures the extent to which the presence of defects reduces a dataset’s utility, the benefits gained by using that dataset in a specific context. The dual assessment methodology is demonstrated in the context of Customer Relationship Management (CRM), using large data samples from real-world datasets. The results from comparing the two assessments offer important insights for directing quality maintenance efforts and prioritizing quality improvement solutions for this dataset. The study describes the steps and the computation involved in the dual-assessment methodology and discusses the implications for applying the methodology in other business contexts and data environments.},
  acmid = {1659228},
  added-at = {2016-07-23T21:26:00.000+0200},
  address = {New York, NY, USA},
  articleno = {15},
  author = {Even, Adir and Shankaranarayanan, G.},
  biburl = {https://www.bibsonomy.org/bibtex/261a23edc3052d09fcd45753746c9351a/vngudivada},
  doi = {10.1145/1659225.1659228},
  interhash = {33091012157769738efa735bcaa69bfc},
  intrahash = {61a23edc3052d09fcd45753746c9351a},
  issn = {1936-1955},
  issue_date = {December 2009},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality},
  month = dec,
  number = 3,
  numpages = {29},
  pages = {15:1--15:29},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Dual Assessment of Data Quality in Customer Databases},
  url = {http://doi.acm.org/10.1145/1659225.1659228},
  volume = 1,
  year = 2009
}

@inproceedings{guerragarcia2011capturing,
  abstract = {The number and complexity of Web applications which are part of Business Intelligence (BI) applications had grown exponentially in recent years. The amount of data used in these applications has consequently also grown. Managing data with an acceptable level of quality is paramount to success in any organizational business process. In order to raise and maintain the adequate levels of Data Quality (DQ) it is indispensable for Web applications to be able to satisfy specific DQ requirements. In order to achieve this goal, DQ requirements should be captured and introduced into the development process together with the other software requirements needed in the applications. However, in the field of Web application development, and to the best of our knowledge, no proposals exist with regard to the way in which to manage specific DQ software requirements. This paper considers the MDA (Model Driven Architecture) approach and, principally, the benefits provided by Model Driven Web Engineering (MDWE) in order to put forward a proposal for two artifacts. These two artifacts are a metamodel and a UML profile for the management of Data Quality Software Requirements for Web Applications (DQ_WebRE).},
  acmid = {1966892},
  added-at = {2016-07-23T18:45:18.000+0200},
  address = {New York, NY, USA},
  author = {Guerra-Garc\'{\i}a, C{\'e}sar and Caballero, Ismael and Piattini, Mario},
  biburl = {https://www.bibsonomy.org/bibtex/2332b6ef9ae56ed27a50c9c2b6f22b317/vngudivada},
  booktitle = {Proceedings of the 2Nd International Workshop on Business intelligencE and the WEB},
  doi = {10.1145/1966883.1966892},
  interhash = {94658cb270cf6e06dd65b1eb6f28ef48},
  intrahash = {332b6ef9ae56ed27a50c9c2b6f22b317},
  isbn = {978-1-4503-0610-2},
  keywords = {DataQuality DataQualityRequirements},
  location = {Uppsala, Sweden},
  numpages = {8},
  pages = {28--35},
  publisher = {ACM},
  series = {BEWEB '11},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Capturing Data Quality Requirements for Web Applications by Means of DQ_WebRE},
  url = {http://doi.acm.org/10.1145/1966883.1966892},
  year = 2011
}

@book{subramanya2014graphbased,
  abstract = {While labeled data is expensive to prepare, ever increasing amounts of unlabeled data is becoming widely available. In order to adapt to this phenomenon, several semi-supervised learning (SSL) algorithms, which learn from labeled as well as unlabeled data, have been developed. In a separate line of work, researchers have started to realize that graphs provide a natural way to represent data in a variety of domains. Graph-based SSL algorithms, which bring together these two lines of work, have been shown to outperform the state-of-the-art in many applications in speech processing, computer vision, natural language processing, and other areas of Artificial Intelligence. Recognizing this promising and emerging area of research, this synthesis lecture focuses on graph based SSL algorithms (e.g., label propagation methods). Our hope is that after reading this book, the reader will walk away with the following: (1) an in-depth knowledge of the current state-of- the-art in graph-based SSL algorithms, and the ability to implement them; (2) the ability to decide on the suitability of graph-based SSL methods for a problem; and (3) familiarity with different applications where graph-based SSL methods have been successfully applied.},
  added-at = {2016-07-15T04:19:51.000+0200},
  author = {Subramanya, Amarnag and Talukdar, Partha Pratim},
  biburl = {https://www.bibsonomy.org/bibtex/2ec5d8e2aa1648cee7bf1ae6e8392f571/vngudivada},
  interhash = {f6678b14cac1031f0d76a28f243be7b9},
  intrahash = {ec5d8e2aa1648cee7bf1ae6e8392f571},
  isbn = {9781627052023 162705202X 1627052011 9781627052016},
  keywords = {SemiSupervisedLearning SynthesisLecture},
  refid = {887483734},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Graph-based semi-supervised learning},
  url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=915652},
  year = 2014
}

@inproceedings{gomes2007quality,
  abstract = {The importance of metadata has been broadly referred in the last years, mainly in the field of data warehousing and decision support systems. Contemporarily, in the adjacent field of data quality, several approaches and tools have been set out for the purpose of data profiling and cleaning. However, little effort has been made in order to formally specify metrics and techniques for data quality in a structured way. As a matter of fact, little relevance has been assigned to metadata regarding data quality and data cleaning issues. This paper aims at filling this gap, proposing a conceptual metamodel for data quality and cleaning, both applicable to operational and data warehousing contexts. The presented metadata model is integrated with OMG's CWM, offering a possible extension of this standard toward data quality.},
  acmid = {1274459},
  added-at = {2016-07-23T17:43:04.000+0200},
  address = {Darlinghurst, Australia, Australia},
  author = {Gomes, Pedro and Farinha, Jos{\'e} and Trigueiros, Maria Jos{\'e}},
  biburl = {https://www.bibsonomy.org/bibtex/2dcebc1ddafb68a9d0a4d075a96afacf9/vngudivada},
  booktitle = {Proceedings of the Fourth Asia-Pacific Conference on Comceptual Modelling - Volume 67},
  interhash = {2d227e0ac4269fa6242f5950bb8aa411},
  intrahash = {dcebc1ddafb68a9d0a4d075a96afacf9},
  isbn = {1-920-68285-X},
  keywords = {DataQuality MetaModel},
  location = {Ballarat, Australia},
  numpages = {10},
  pages = {17--26},
  publisher = {Australian Computer Society, Inc.},
  series = {APCCM '07},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {A Data Quality Metamodel Extension to CWM},
  url = {http://dl.acm.org/citation.cfm?id=1274453.1274459},
  year = 2007
}

@misc{madnick2011editorial,
  abstract = {Data quality remains a persistent problem in practice and a challenge for research. In this study we focus on the four dimensions of data quality noted as the most important to information consumers, namely accuracy, completeness, consistency, and timeliness.},
  added-at = {2016-07-23T21:32:17.000+0200},
  author = {Madnick, Stuart E. and Lee, Yang W.},
  biburl = {https://www.bibsonomy.org/bibtex/29ee8dfa5d08bc351cf930117a13f976f/vngudivada},
  interhash = {f3a3bd860cda2a1c9e398d3c116ac41f},
  intrahash = {9ee8dfa5d08bc351cf930117a13f976f},
  keywords = {DataQuality JDIQ},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Editorial: In Search of Novel Ideas and Solutions with a Broader Context of Data Quality in Mind},
  year = 2011
}

@article{embury2009incorporating,
  abstract = {The range of information now available in queryable repositories opens up a host of possibilities for new and valuable forms of data analysis. Database query languages such as SQL and XQuery offer a concise and high-level means by which such analyses can be implemented, facilitating the extraction of relevant data subsets into either generic or bespoke data analysis environments. Unfortunately, the quality of data in these repositories is often highly variable. The data is still useful, but only if the consumer is aware of the data quality problems and can work around them. Standard query languages offer little support for this aspect of data management. In principle, however, it should be possible to embed constraints describing the consumer’s data quality requirements into the query directly, so that the query evaluator can take over responsibility for enforcing them during query processing. Most previous attempts to incorporate information quality constraints into database queries have been based around a small number of highly generic quality measures, which are defined and computed by the information provider. This is a useful approach in some application areas but, in practice, quality criteria are more commonly determined by the user of the information not by the provider. In this article, we explore an approach to incorporating quality constraints into database queries where the definition of quality is set by the user and not the provider of the information. Our approach is based around the concept of a quality view, a configurable quality assessment component into which domain-specific notions of quality can be embedded. We examine how quality views can be incorporated into XQuery, and draw from this the language features that are required in general to embed quality views into any query language. We also propose some syntactic sugar on top of XQuery to simplify the process of querying with quality constraints.},
  acmid = {1577846},
  added-at = {2016-07-23T21:42:13.000+0200},
  address = {New York, NY, USA},
  articleno = {11},
  author = {Embury, Suzanne M. and Missier, Paolo and Sampaio, Sandra and Greenwood, R. Mark and Preece, Alun D.},
  biburl = {https://www.bibsonomy.org/bibtex/26fa8cacc8984cbcbffecd9e82a9bb8d2/vngudivada},
  doi = {10.1145/1577840.1577846},
  interhash = {07e8f85f8a6fdd76dc5a888de3791540},
  intrahash = {6fa8cacc8984cbcbffecd9e82a9bb8d2},
  issn = {1936-1955},
  issue_date = {September 2009},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality DatabaseQuery},
  month = sep,
  number = 2,
  numpages = {31},
  pages = {11:1--11:31},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Incorporating Domain-Specific Information Quality Constraints into Database Queries},
  url = {http://doi.acm.org/10.1145/1577840.1577846},
  volume = 1,
  year = 2009
}

@inproceedings{cappiello2004quality,
  abstract = {The quality of data is often defined as "fitness for use", i.e., the ability of a data collection to meet user requirements. The assessment of data quality dimensions should consider the degree to which data satisfy users' needs. User expectations are clearly related to the selected services and at the same time a service can have different characteristics depending on the type of user that accesses it. The data quality assessment process has to consider both aspects and, consequently, select a suitable evaluation function to obtain a correct interpretation of results. This paper proposes a model that ties the assessment phase to user requirements. Multichannel information systems are considered as an example to show the applicability of the proposed model.},
  acmid = {1012465},
  added-at = {2016-07-23T20:32:36.000+0200},
  address = {New York, NY, USA},
  author = {Cappiello, Cinzia and Francalanci, Chiara and Pernici, Barbara},
  biburl = {https://www.bibsonomy.org/bibtex/22c55e5c9a951d3e2fe63dc99151d41d3/vngudivada},
  booktitle = {Proceedings of the 2004 International Workshop on Information Quality in Information Systems},
  doi = {10.1145/1012453.1012465},
  interhash = {5ad3d497a7d664b3ef14ff2fc04c8b80},
  intrahash = {2c55e5c9a951d3e2fe63dc99151d41d3},
  isbn = {1-58113-902-0},
  keywords = {DataQuality DataQualityAssessment},
  location = {Paris, France},
  numpages = {6},
  pages = {68--73},
  publisher = {ACM},
  series = {IQIS '04},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data Quality Assessment from the User's Perspective},
  url = {http://doi.acm.org/10.1145/1012453.1012465},
  year = 2004
}

@article{fan2015quality,
  abstract = {Data quantity and data quality, like two sides of a coin, are equally important to data management. This paper provides an overview of recent advances in the study of data quality, from theory to practice. We also address challenges introduced by big data to data quality management.},
  acmid = {2854008},
  added-at = {2016-07-23T21:12:03.000+0200},
  address = {New York, NY, USA},
  author = {Fan, Wenfei},
  biburl = {https://www.bibsonomy.org/bibtex/2aa44ad47f2d2330c3c4a052de8f2d29c/vngudivada},
  doi = {10.1145/2854006.2854008},
  interhash = {e3ad20e1bf1ac913764fa8a029f77103},
  intrahash = {aa44ad47f2d2330c3c4a052de8f2d29c},
  issn = {0163-5808},
  issue_date = {September 2015},
  journal = {SIGMOD Rec.},
  keywords = {BigData DataQuality},
  month = dec,
  number = 3,
  numpages = {12},
  pages = {7--18},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data Quality: From Theory to Practice},
  url = {http://doi.acm.org/10.1145/2854006.2854008},
  volume = 44,
  year = 2015
}

@misc{madnick2009editorial,
  added-at = {2016-07-23T21:29:15.000+0200},
  author = {Madnick, Stuart E. and Lee, Yang W.},
  biburl = {https://www.bibsonomy.org/bibtex/2f7d1ed95f4c7041ed44c376626a21067/vngudivada},
  interhash = {4e69a46ad2d05bc6e34eb9207f5c9214},
  intrahash = {f7d1ed95f4c7041ed44c376626a21067},
  keywords = {DataQuality JDIQ},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Editorial for the Inaugural Issue of the ACM Journal of Data and Information Quality},
  year = 2009
}

@book{genesereth2010integration,
  abstract = {Data integration is a critical problem in our increasingly interconnected but inevitably heterogeneous world. There are numerous data sources available in organizational databases and on public information systems like the World Wide Web. Not surprisingly, the sources often use different vocabularies and different data structures, being created, as they are, by different people, at different times, for different purposes. The goal of data integration is to provide programmatic and human users with integrated access to multiple, heterogeneous data sources, giving each user the illusion of a single, homogeneous database designed for his or her specific need. The good news is that, in many cases, the data integration process can be automated. This book is an introduction to the problem of data integration and a rigorous account of one of the leading approaches to solving this problem, viz., the relational logic approach. Relational logic provides a theoretical framework for discussing data integration. Moreover, in many important cases, it provides algorithms for solving the problem in a computationally practical way. In many respects, relational logic does for data integration what relational algebra did for database theory several decades ago. A companion web site provides interactive demonstrations of the algorithms.},
  added-at = {2016-07-15T04:27:14.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Genesereth, Michael R.},
  biburl = {https://www.bibsonomy.org/bibtex/2ee66319e906b40e49d18c0a70a34c007/vngudivada},
  interhash = {5b281cde97a1eaf6f742169caaa5c2b0},
  intrahash = {ee66319e906b40e49d18c0a70a34c007},
  isbn = {9781598297423 1598297422},
  keywords = {DataIntegration SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {607096667},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data integration: the relational logic approach},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881061},
  year = 2010
}

@article{martin2014methodology,
  abstract = {Data integration aims to combine heterogeneous information sources and to provide interfaces for accessing the integrated resource. Data integration is a collaborative task that may involve many people with different degrees of experience, knowledge of the application domain, and expectations relating to the integrated resource. It may be difficult to determine and control the quality of an integrated resource due to these factors. In this article, we propose a data integration methodology that has embedded within it iterative quality assessment and improvement of the integrated resource. We also propose an architecture for the realisation of this methodology. The quality assessment is based on an ontology representation of different users’ quality requirements and of the main elements of the integrated resource. We use description logic as the formal basis for reasoning about users’ quality requirements and for validating that an integrated resource satisfies these requirements. We define quality factors and associated metrics which enable the quality of alternative global schemas for an integrated resource to be assessed quantitively, and hence the improvement which results from the refinement of a global schema following our methodology to be measured. We evaluate our approach through a large-scale real-life case study in biological data integration in which an integrated resource is constructed from three autononous proteomics data sources.},
  acmid = {2567663},
  added-at = {2016-07-23T18:03:35.000+0200},
  address = {New York, NY, USA},
  articleno = {17},
  author = {Martin, Nigel and Poulovassilis, Alexandra and Wang, Jianing},
  biburl = {https://www.bibsonomy.org/bibtex/245deb99df2f96ad62ef8adec33e7cc54/vngudivada},
  doi = {10.1145/2567663},
  interhash = {a404002a995bfa9c23a46795b7a2dab4},
  intrahash = {45deb99df2f96ad62ef8adec33e7cc54},
  issn = {1936-1955},
  issue_date = {May 2014},
  journal = {J. Data and Information Quality},
  keywords = {DataIntegration DataQuality},
  month = may,
  number = 4,
  numpages = {40},
  pages = {17:1--17:40},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {A Methodology and Architecture Embedding Quality Assessment in Data Integration},
  url = {http://doi.acm.org/10.1145/2567663},
  volume = 4,
  year = 2014
}

@article{boubiche2015question,
  abstract = {The need to query information content available in various formats including structured and unstructured data (text in natural language, semi-structured Web documents, structured RDF data in the semantic Web, etc.) has become increasingly important. Thus, Question Answering Systems (QAS) are essential to satisfy this need. QAS aim at satisfying users who are looking to answer a specific question in natural language. In this paper we survey various QAS. We give also statistics and analysis. This can clear the way and help researchers to choose the appropriate solution to their issue. They can see the insufficiency, so that they can propose new systems for complex queries. They can also adapt or reuse QAS techniques for specific research issues.},
  added-at = {2016-07-16T02:01:12.000+0200},
  author = {Boubiche, Djallel Eddine and Hidoussi, Faouzi and Cruz, Homero Toral and Bouziane, Abdelghani and Bouchiha, Djelloul and Doumi, Noureddine and Malki, Mimoun},
  biburl = {https://www.bibsonomy.org/bibtex/2a67ac69bd1543693f9354d3ee927bf1e/vngudivada},
  doi = {http://dx.doi.org/10.1016/j.procs.2015.12.005},
  interhash = {911d83a07088ea04e0e1ae1a39ad7251},
  intrahash = {a67ac69bd1543693f9354d3ee927bf1e},
  issn = {1877-0509},
  journal = {Procedia Computer Science},
  keywords = {QASystem Survey},
  pages = {366 - 375},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Question Answering Systems: Survey and Trends},
  url = {http://www.sciencedirect.com/science/article/pii/S1877050915034663},
  volume = 73,
  year = 2015
}

@book{campbell2011learning,
  abstract = {Support Vectors Machines have become a well established tool within machine learning. They work well in practice and have now been used across a wide range of applications from recognizing handwritten digits, to face identification, text categorisation, bioinformatics and database marketing. In this book we give an introductory overview of this subject. We start with a simple Support Vector Machine for performing binary classification before considering multi-class classification and learning in the presence of noise. We show that this framework can be extended to many other scenarios such as prediction with real-valued outputs, novelty detection and the handling of complex output structures such as parse trees. Finally, we give an overview of the main types of kernels which are used in practice and how to learn and make predictions from multiple types of input data.},
  added-at = {2016-07-15T04:48:06.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Campbell, Colin and Ying, Yiming},
  biburl = {https://www.bibsonomy.org/bibtex/2bdd724c7fceba86358ecf4c8b87c7e29/vngudivada},
  interhash = {f15ce0ff537a28ce4d9765044c71cc5b},
  intrahash = {bdd724c7fceba86358ecf4c8b87c7e29},
  isbn = {9781608456178 160845617X},
  keywords = {Learning SVM SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {708649879},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Learning with support vector machines},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881246},
  year = 2011
}

@inproceedings{francis2013modeldriven,
  abstract = {Accurate and timely place-based information from multiple sources is essential for making informed social protection decisions and rapid interventions. Developing solutions to the challenges presented by multi-disciplanary data integration provides a rationale, and mechanisms, to realize the broader goals of Linked Data. The Spatial Identifier Reference Framework (SIRF) combines principles of indentifiers and Linked Data to link place names to related data. Unlike generic placename databases, SIRF uses semantic web technologies to describe relationships between sources of place names and exposes the provenance of identifiers to disambiuguate and explain them. This paper will describe how SIRF uses explicit information models of the spatial datasets from which it builds an index of spatial identifiers in use. Within the SIRF infrastructure the spatial identifiers are harvested from geospatial data sets and published as Web based identifiers (Uniform Resource Identifiers- URIs). These URIs may be used to access multiple forms of data and metadata for the identified feature, including accessing provenance metadata and direct links back to the source datasets. Formal models using the "Application Schema" profile defined by the ISO TC 211 General Feature Model drive a repeatable harvesting process and are directly published as part of the provenance metadata. Mappings between the source and common models, used to drive transformations of harvested data into the common index are also presented together with an explanation of their role. Modelled properties, linked to vocabulary mappings, also expposed by web services, to provide a complete Web-accessible provenance of both the source and the interpretation used.},
  acmid = {2479863},
  added-at = {2016-07-23T22:56:49.000+0200},
  address = {New York, NY, USA},
  author = {Francis, Will and Atkinson, Rob and Box, Paul and Rankine, Terry and Woodman, Stuart and Kostanski, Laura},
  biburl = {https://www.bibsonomy.org/bibtex/2605b6afd3f0aa0cfd81e47c2566ec2d4/vngudivada},
  booktitle = {Proceedings of the Seventh International Conference on Knowledge Capture},
  doi = {10.1145/2479832.2479863},
  interhash = {59d3ba1ada6d5127455b5382d77d39b4},
  intrahash = {605b6afd3f0aa0cfd81e47c2566ec2d4},
  isbn = {978-1-4503-2102-0},
  keywords = {GeospatialData Provenance},
  location = {Banff, Canada},
  numpages = {4},
  pages = {121--124},
  publisher = {ACM},
  series = {K-CAP '13},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Model-driven Data Harvesting to Publish Provenance for Geospatial References},
  url = {http://doi.acm.org/10.1145/2479832.2479863},
  year = 2013
}

@misc{cassidy2016cognitive,
  abstract = {Marching along the DARPA SyNAPSE roadmap, IBM unveils a trilogy of innovations towards the True North cognitive computing system inspired by the brain's function andefficiency. Judiciously balancing the dual objectives of functional capability and implementation/operational cost, we develop asimple, digital, reconfigurable, versatile spiking neuron modelthat supports one-to-one equivalence between hardware andsimulation and is implementable using only 1272 ASIC gates. Starting with the classic leaky integrate-and-fire neuron, we add: (a) configurable and reproducible stochasticity to the input, thestate, and the output; (b) four leak modes that bias the internal state dynamics; (c) deterministic and stochastic thresholds; and(d) six reset modes for rich finite-state behavior. The model sup-ports a wide variety of computational functions and neural codes. We capture 50+ neuron behaviors in a library for hierarchical composition of complex computations and behaviors. Although designed with cognitive algorithms and applications in mind,serendipitously, the neuron model can qualitatively replicate the biologically-relevant behaviors of a dynamical neuron model.},
  added-at = {2016-07-18T02:36:25.000+0200},
  author = {Cassidy, Andrew S. and Merolla, Paul and Arthur, John V. and Esser, Steve K. and Jackson, Bryan and Alvarez-Icaza, Rodrigo and Datta, Pallab and Sawaday, Jun and Wong, Theodore M. and Feldman, Vitaly and Amir, Arnon and Rubinx, Daniel Ben-Dayan and Akopyan, Filipp and McQuinn, Emmett and Risk, William P. and Modha, Dharmendra S.},
  biburl = {https://www.bibsonomy.org/bibtex/2b5643a31c1a4849d4bfc6e043ca92f17/vngudivada},
  interhash = {4ec556a34fb1e19242a26c1cb62c8956},
  intrahash = {b5643a31c1a4849d4bfc6e043ca92f17},
  keywords = {CognitiveComputing NeuromorphicProcessor NeurosynapticCore TrueNorth},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Cognitive Computing Building Block: A Versatile and EfficientDigital Neuron Model for Neurosynaptic Cores},
  year = 2016
}

@book{rossi2011short,
  abstract = {Computational social choice is an expanding field that merges classical topics like economics and voting theory with more modern topics like artificial intelligence, multiagent systems, and computational complexity. This book provides a concise introduction to the main research lines in this field, covering aspects such as preference modelling, uncertainty reasoning, social choice, stable matching, and computational aspects of preference aggregation and manipulation. The book is centered around the notion of preference reasoning, both in the single-agent and the multi-agent setting. It presents the main approaches to modeling and reasoning with preferences, with particular attention to two popular and powerful formalisms, soft constraints and CP-nets. The authors consider preference elicitation and various forms of uncertainty in soft constraints. They review the most relevant results in voting, with special attention to computational social choice. Finally, the book considers preferences in matching problems. The book is intended for students and researchers who may be interested in an introduction to preference reasoning and multi-agent preference aggregation, and who want to know the basic notions and results in computational social choice.},
  added-at = {2016-07-15T04:28:38.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Rossi, Francesca and Venable, Kristen Brent and Walsh, Toby},
  biburl = {https://www.bibsonomy.org/bibtex/226e277a3e09d9f36bd285d811b471504/vngudivada},
  interhash = {935fe591b0fe9fb30194efb76d7d284c},
  intrahash = {26e277a3e09d9f36bd285d811b471504},
  isbn = {9781608455874 1608455874 9781608455867 1608455866},
  keywords = {Preference SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {744661246},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {A short introduction to preferences between artificial intelligence and social choice},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881236},
  year = 2011
}

@inproceedings{yahya2012natural,
  abstract = {The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources.

Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the question translation and the resulting query answering.},
  added-at = {2016-07-16T01:56:15.000+0200},
  address = {Stroudsburg, PA, USA},
  author = {Yahya, Mohamed and Berberich, Klaus and Elbassuoni, Shady and Ramanath, Maya and Tresp, Volker and Weikum, Gerhard},
  biburl = {https://www.bibsonomy.org/bibtex/2078d1c2803a403b0b2237bcdd2f44481/vngudivada},
  booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  interhash = {437b1b59414b6e83309d0de13b716ea4},
  intrahash = {078d1c2803a403b0b2237bcdd2f44481},
  keywords = {LinkedData NaturalLanguageQuestionGeneration},
  numpages = {12},
  pages = {379--390},
  publisher = {Association for Computational Linguistics},
  series = {EMNLP-CoNLL '12},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Natural Language Questions for the Web of Data},
  url = {http://dl.acm.org/citation.cfm?id=2390948.2390995},
  year = 2012
}

@book{bartak2014introduction,
  abstract = {Solving challenging computational problems involving time has been a critical component in the development of artificial intelligence systems almost since the inception of the field. This book provides a concise introduction to the core computational elements of temporal reasoning for use in AI systems for planning and scheduling, as well as systems that extract temporal information from data. It presents a survey of temporal frameworks based on constraints, both qualitative and quantitative, as well as of major temporal consistency techniques. The book also introduces the reader to more recent extensions to the core model that allow AI systems to explicitly represent temporal preferences and temporal uncertainty. This book is intended for students and researchers interested in constraint-based temporal reasoning. It provides a self-contained guide to the different representations of time, as well as examples of recent applications of time in AI systems.},
  added-at = {2016-07-15T04:52:41.000+0200},
  author = {Barták, Roman and Morris, Robert A. and Venable, Kristen Brent},
  biburl = {https://www.bibsonomy.org/bibtex/267b9e8f7de7397dc941a331a93ebb03d/vngudivada},
  interhash = {936490bf5e8fe5edef1fb6c336fdd691},
  intrahash = {67b9e8f7de7397dc941a331a93ebb03d},
  isbn = {9781608459681 1608459683},
  keywords = {TemporalReasoning SynthesisLecture},
  refid = {873082078},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {An introduction to constraint-based temporal reasoning},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1651105},
  year = 2014
}

@book{hexmoor2013essential,
  abstract = {From driving, flying, and swimming, to digging for unknown objects in space exploration, autonomous robots take on varied shapes and sizes. In part, autonomous robots are designed to perform tasks that are too dirty, dull, or dangerous for humans. With nontrivial autonomy and volition, they may soon claim their own place in human society. These robots will be our allies as we strive for understanding our natural and man-made environments and build positive synergies around us. Although we may never perfect replication of biological capabilities in robots, we must harness the inevitable emergence of robots that synchronizes with our own capacities to live, learn, and grow. This book is a snapshot of motivations and methodologies for our collective attempts to transform our lives and enable us to cohabit with robots that work with and for us. It reviews and guides the reader to seminal and continual developments that are the foundations for successful paradigms. It attempts to demystify the abilities and limitations of robots. It is a progress report on the continuing work that will fuel future endeavors.},
  added-at = {2016-07-15T04:44:59.000+0200},
  address = {San Rafael, Calif. (1537 Fourth Street, San Rafael, CA 94901 USA)},
  author = {Hexmoor, Henry},
  biburl = {https://www.bibsonomy.org/bibtex/2653fe5e06605440cc9ccc63d88dfe7ed/vngudivada},
  interhash = {26a9c2fcae06e18a810ac0a8c3ba75aa},
  intrahash = {653fe5e06605440cc9ccc63d88dfe7ed},
  isbn = {9781627050593 1627050590 1627050582 9781627050586},
  keywords = {AutonomousRobot SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {848834547},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Essential principles for autonomous robotics},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1214380},
  year = 2013
}

@inproceedings{seyler2015generating,
  abstract = {We propose an approach to generate natural language questions from knowledge graphs such as DBpedia and YAGO. We stage this in the setting of a quiz game. Our approach, though, is general enough to be applicable in other settings. Given a topic of interest (e.g., Soccer) and a difficulty (e.g., hard), our approach selects a query answer, generates a SPARQL query having the answer as its sole result, before verbalizing the question.},
  acmid = {2742722},
  added-at = {2016-07-16T02:09:39.000+0200},
  address = {New York, NY, USA},
  author = {Seyler, Dominic and Yahya, Mohamed and Berberich, Klaus},
  biburl = {https://www.bibsonomy.org/bibtex/239f49f3f6bfddc63be57628ea0223720/vngudivada},
  booktitle = {Proceedings of the 24th International Conference on World Wide Web},
  doi = {10.1145/2740908.2742722},
  interhash = {53e23642f6f35aafa9922a2af01e4632},
  intrahash = {39f49f3f6bfddc63be57628ea0223720},
  isbn = {978-1-4503-3473-0},
  keywords = {KnowledgeGraph QuestionGeneration},
  location = {Florence, Italy},
  numpages = {2},
  pages = {113--114},
  publisher = {ACM},
  series = {WWW '15 Companion},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Generating Quiz Questions from Knowledge Graphs},
  url = {http://doi.acm.org/10.1145/2740908.2742722},
  year = 2015
}

@book{genesereth2014general,
  abstract = {General game players are computer systems able to play strategy games based solely on formal game descriptions supplied at runtime. (In other words, they don't know the rules until the game starts.) Unlike specialized game players, such as Deep Blue, general game players cannot rely on algorithms designed in advance for specific games; they must discover such algorithms themselves. General game playing expertise depends on intelligence on the part of the game player and not just intelligence of the programmer of the game player. GGP is an interesting application in its own right. It is intellectually engaging and more than a little fun. But it is much more than that. It provides a theoretical framework for modeling discrete dynamic systems and defining rationality in a way that takes into account problem representation and complexities like incompleteness of information and resource bounds. It has practical applications in areas where these features are important, e.g., in business and law. More fundamentally, it raises questions about the nature of intelligence and serves as a laboratory in which to evaluate competing approaches to artificial intelligence. This book is an elementary introduction to General Game Playing (GGP). (1) It presents the theory of General Game Playing and leading GGP technologies. (2) It shows how to create GGP programs capable of competing against other programs and humans. (3) It offers a glimpse of some of the real-world applications of General Game Playing.},
  added-at = {2016-07-15T04:24:12.000+0200},
  author = {Genesereth, Michael R. and Thielscher, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2976d994e2441d97bd67cd37373fc91c3/vngudivada},
  interhash = {7d088488d97c05ad921a146632d01301},
  intrahash = {976d994e2441d97bd67cd37373fc91c3},
  isbn = {9781627052566 1627052569},
  keywords = {GamePlaying SynthesisLecture},
  refid = {877885350},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {General game playing},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1663905},
  year = 2014
}

@article{glowalla2014processdriven,
  abstract = {Data quality is critical to organizational success. In order to improve and sustain data quality in the long term, process-driven data quality management (PDDQM) seeks to redesign processes that create or modify data. Consequently, process modeling is mandatory for PDDQM. Current research examines process modeling languages with respect to representational capabilities. However, there is a gap, since process modeling languages for PDDQM are not considered. We address this research gap by providing a synthesis of the varying applications of process modeling languages for PDDQM. We conducted a keyword-based literature review in conferences as well as 74 highranked information systems and computer science journals, reviewing 1,555 articles from 1995 onwards. For practitioners, it is possible to integrate the quality perspective within broadly applied process models. For further research, we derive representational requirements for PDDQM that should be integrated within existing process modeling languages. However, there is a need for further representational analysis to examine the adequacy of upcoming process modeling languages. New or enhanced process modeling languages may substitute for PDDQM-specific process modeling languages and facilitate development of a broadly applicable and accepted process modeling language for PDDQM.},
  acmid = {2629568},
  added-at = {2016-07-23T17:38:12.000+0200},
  address = {New York, NY, USA},
  articleno = {7},
  author = {Glowalla, Paul and Sunyaev, Ali},
  biburl = {https://www.bibsonomy.org/bibtex/251c871742b69ccabc1f70415b32d637c/vngudivada},
  doi = {10.1145/2629568},
  interhash = {d8bb9030ba8ccadbbaa9bc3666841f08},
  intrahash = {51c871742b69ccabc1f70415b32d637c},
  issn = {1936-1955},
  issue_date = {August 2014},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality ProcessModelingLanguage},
  month = sep,
  number = {1-2},
  numpages = {30},
  pages = {7:1--7:30},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Process-driven Data Quality Management: A Critical Review on the Application of Process Modeling Languages},
  url = {http://doi.acm.org/10.1145/2629568},
  volume = 5,
  year = 2014
}

@misc{esser2016cognitive,
  abstract = {Marching along the DARPA SyNAPSE roadmap, IBM unveils a trilogy of innovations towards the TrueNorthcognitive computing system inspired by the brain's function and efficiency. The non-von Neumann nature of the TrueNorth archi-tecture necessitates a novel approach to efficient system design. To this end, we have developed a set of abstractions, algorithms,and applications that are natively efficient for TrueNorth. First, we developed repeatedly-used abstractions that span neural codes(such as binary, rate, population, and time-to-spike), long-rangeconnectivity, and short-range connectivity. Second, we implemented ten algorithms that include convolution networks, spectralcontent estimators, liquid state machines, restricted Boltzmann machines, hidden Markov models, looming detection, temporalpattern matching, and various classifiers. Third, we demonstrates even applications that include speaker recognition, music com-poser recognition, digit recognition, sequence prediction, collision avoidance, optical flow, and eye detection. Our results showcase the parallelism, versatility, rich connectivity, spatio-temporality, and multi-modality of the TrueNorth architecture as well as compositionality of the corelet programming paradigm and the flexibility of the underlying neuron model.},
  added-at = {2016-07-18T02:45:26.000+0200},
  author = {Esser, Steve K. and Andreopoulos, Alexander and Appuswamy, Rathinakumar and Datta, Pallab and Barch, Davis and Amir, Arnon and Arthur, John and Cassidy, Andrew and Flickner, Myron and Merolla, Paul and Chandra, Shyamal and Basilico, Nicola and Carpin, Stefano and Zimmerman, Tom and Zee, Frank and Alvarez-Icaza, Rodrigo and Kusnitz, Jeffrey A. and Wong, Theodore M. and Risk, William P. and McQuinn, Emmett and Nayak, Tapan K. and Singh, Raghavendra and Modha, Dharmendra S.},
  biburl = {https://www.bibsonomy.org/bibtex/211a65507f90f519454280f677e92fdfb/vngudivada},
  interhash = {f969c386e38a642a7797fc1b07771edc},
  intrahash = {11a65507f90f519454280f677e92fdfb},
  keywords = {CognitiveComputing NuerosynapticCore TrueNorth},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Cognitive Computing Systems: Algorithms and Applications forNetworks of Neurosynaptic Cores},
  year = 2016
}

@book{myatt2014making,
  abstract = {A proven go-to guide for data analysis, Making Sense of Data I: A Practical Guide to Exploratory Data Analysis and Data Mining, Second Edition focuses on basic data analysis approaches that are necessary to make timely and accurate decisions in a diverse range of projects. Based on the authors' practical experience in implementing data analysis and data mining, the new edition provides clear explanations that guide readers from almost every field of study. In order to facilitate the needed steps when handling a data analysis or data mining project, a step-by-step approach aids professionals in carefully analyzing data and implementing results, leading to the development of smarter business decisions. },
  added-at = {2016-07-22T13:21:01.000+0200},
  author = {Myatt, Glenn J. and Johnson, Wayne P.},
  biburl = {https://www.bibsonomy.org/bibtex/24d6b1aa6bd08fface23907dc7d4df672/vngudivada},
  edition = {Second},
  interhash = {cc214134101a9db6943ffaec41bdfb43},
  intrahash = {4d6b1aa6bd08fface23907dc7d4df672},
  isbn = {9781118422014 1118422015 9781118422106 1118422104},
  keywords = {Book DataMining ExploratoryDataAnalysis MachineLearning Statistics},
  refid = {875056210},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Making sense of data I : a practical guide to exploratory data analysis and data mining},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1729064},
  year = 2014
}

@book{raedt2016statistical,
  abstract = {An intelligent agent interacting with the real world will encounter individual people, courses, test results, drugs prescriptions, chairs, boxes, etc., and needs to reason about properties of these individuals and relations among them as well as cope with uncertainty. Uncertainty has been studied in probability theory and graphical models, and relations have been studied in logic, in particular in the predicate calculus and its extensions. This book examines the foundations of combining logic and probability into what are called relational probabilistic models. It introduces representations, inference, and learning techniques for probability, logic, and their combinations. The book focuses on two representations in detail: Markov logic networks, a relational extension of undirected graphical models and weighted first-order predicate calculus formula, and Problog, a probabilistic extension of logic programs that can also be viewed as a Turing-complete relational extension of Bayesian networks.},
  added-at = {2016-07-15T04:34:25.000+0200},
  author = {Raedt, Luc de and Kersting, Kristian and Natarajan, Sriraam and Poole, David L.},
  biburl = {https://www.bibsonomy.org/bibtex/2082c7bef5a38a651d92548d589fa3e80/vngudivada},
  interhash = {19c6469606f6c1250f7139475d22b09c},
  intrahash = {082c7bef5a38a651d92548d589fa3e80},
  isbn = {9781627058421 1627058427},
  keywords = {AI Logic Probability SynthesisLecture},
  refid = {946774679},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Statistical relational artificial intelligence: logic, probability, and computation},
  url = {http://dx.doi.org/10.2200/S00692ED1V01Y201601AIM032},
  year = 2016
}

@inproceedings{ahadi2016students,
  abstract = {The computing education community has studied extensively the errors of novice programmers. In contrast, little attention has been given to student's mistake in writing SQL statements. This paper represents the first large scale quantitative analysis of the student's syntactic mistakes in writing different types of SQL queries. Over 160 thousand snapshots of SQL queries were collected from over 2000 students across eight years. We describe the most common types of syntactic errors that students make. We also describe our development of an automatic classifier with an overall accuracy of 0.78 for predicting student performance in writing SQL queries.},
  acmid = {2844640},
  added-at = {2016-07-18T03:58:54.000+0200},
  address = {New York, NY, USA},
  author = {Ahadi, Alireza and Behbood, Vahid and Vihavainen, Arto and Prior, Julia and Lister, Raymond},
  biburl = {https://www.bibsonomy.org/bibtex/2dc045ef58cd80b01446ca36623e9ec15/vngudivada},
  booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
  doi = {10.1145/2839509.2844640},
  interhash = {18d829198eaa4bf93be2f8399d958ea0},
  intrahash = {dc045ef58cd80b01446ca36623e9ec15},
  isbn = {978-1-4503-3685-7},
  keywords = {LearningAnalytics SQL SemanticMistake},
  location = {Memphis, Tennessee, USA},
  numpages = {6},
  pages = {401--406},
  publisher = {ACM},
  series = {SIGCSE '16},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Students' Syntactic Mistakes in Writing Seven Different Types of SQL Queries and Its Application to Predicting Students' Success},
  url = {http://doi.acm.org/10.1145/2839509.2844640},
  year = 2016
}

@book{chernova2014robot,
  abstract = {Learning from Demonstration (LfD) explores techniques for learning a task policy from examples provided by a human teacher. The field of LfD has grown into an extensive body of literature over the past 30 years, with a wide variety of approaches for encoding human demonstrations and modeling skills and tasks. Additionally, we have recently seen a focus on gathering data from nonexpert human teachers (i.e., domain experts but not robotics experts). In this book, we provide an introduction to the field with a focus on the unique technical challenges associated with designing robots that learn from naive human teachers. We begin, in the introduction, with a unification of the various terminology seen in the literature as well as an outline of the design choices one has in designing an LfD system. Chapter 2 gives a brief survey of the psychology literature that provides insights from human social learning that are relevant to designing robotic social learners. Chapter 3 walks through an LfD interaction, surveying the design choices one makes and state of the art approaches in prior work. First, is the choice of input, how the human teacher interacts with the robot to provide demonstrations. Next, is the choice of modeling technique. Currently, there is a dichotomy in the field between approaches that model low-level motor skills and those that model high-level tasks composed of primitive actions. We devote a chapter to each of these. Chapter 7 is devoted to interactive and active learning approaches that allow the robot to refine an existing task model. And finally, Chapter 8 provides best practices for evaluation of LfD systems, with a focus on how to approach experiments with human subjects in this domain.},
  added-at = {2016-07-15T04:39:58.000+0200},
  author = {Chernova, Sonia and Thomaz, Andrea L.},
  biburl = {https://www.bibsonomy.org/bibtex/2376f5f44e4ce0055c222f731acb775d5/vngudivada},
  interhash = {8d876b1b34d11c294785d30f453b962e},
  intrahash = {376f5f44e4ce0055c222f731acb775d5},
  isbn = {9781627052009 1627052003 1627051996 9781627051996},
  keywords = {RobotLearning SynthesisLecture},
  refid = {880357617},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Robot learning from human teachers},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1686815},
  year = 2014
}

@inproceedings{mehmood2009quality,
  abstract = {Data quality has emerged as an important and challenging topic in recent years. This article addresses the conceptual model quality as it has been widely accepted that better conceptual models produce better information systems and thus implicitly improve the data quality. Conceptual Models are designed as part of the analysis phase and serve as a communicating mediator between the users and the development team. Consequently, their understandability is a real challenge to avoid the propagation of inaccurate interpretation of the user requirements to the underlying system design and implementation. In this paper, we propose an adaptive quality model. We illustrate its usefulness by describing how it can be used to model and evaluate the understandability of conceptual models. Our quality evaluation is enriched with corrective actions provided to the designer, leading to a guidance modeling process. A first validation based on a survey is proposed.},
  acmid = {1651421},
  added-at = {2016-07-23T21:06:35.000+0200},
  address = {New York, NY, USA},
  author = {Mehmood, Kashif and Si-Said Cherfi, Samira and Comyn-Wattiau, Isabelle},
  biburl = {https://www.bibsonomy.org/bibtex/2d696a5cc32930ba0c746b0428f578b08/vngudivada},
  booktitle = {Proceedings of the First International Workshop on Model Driven Service Engineering and Data Quality and Security},
  doi = {10.1145/1651415.1651421},
  interhash = {1003662531f4533a65f0016fcdb9f61e},
  intrahash = {d696a5cc32930ba0c746b0428f578b08},
  isbn = {978-1-60558-816-2},
  keywords = {ConceptualModel DataQuality},
  location = {Hong Kong, China},
  numpages = {4},
  pages = {29--32},
  publisher = {ACM},
  series = {MoSE+DQS '09},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data Quality Through Model Quality: A Quality Model for Measuring and Improving the Understandability of Conceptual Models},
  url = {http://doi.acm.org/10.1145/1651415.1651421},
  year = 2009
}

@booklet{tdwiorg2014quality,
  added-at = {2016-07-20T23:47:40.000+0200},
  author = {tdwi.org},
  biburl = {https://www.bibsonomy.org/bibtex/20b967c20b59266e2514c6b0f87a2a00f/vngudivada},
  interhash = {ed0c3a76577fca57f22963497d6aadf1},
  intrahash = {0b967c20b59266e2514c6b0f87a2a00f},
  keywords = {DataQuality},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data Quality Challenges and Priorities},
  year = 2014
}

@book{chalkiadakis2012computational,
  abstract = {Cooperative game theory is a branch of (micro-)economics that studies the behavior of self-interested agents in strategic settings where binding agreements among agents are possible. Our aim in this book is to present a survey of work on the computational aspects of cooperative game theory. We begin by formally defining transferable utility games in characteristic function form, and introducing key solution concepts such as the core and the Shapley value. We then discuss two major issues that arise when considering such games from a computational perspective: identifying compact representations for games, and the closely related problem of efficiently computing solution concepts for games. We survey several formalisms for cooperative games that have been proposed in the literature, including, for example, cooperative games defined on networks, as well as general compact representation schemes such as MC-nets and skill games. As a detailed case study, we consider weighted voting games: a widely-used and practically important class of cooperative games that inherently have a natural compact representation. We investigate the complexity of solution concepts for such games, and generalizations of them. We briefly discuss games with non-transferable utility and partition function games. We then overview algorithms for identifying welfare-maximizing coalition structures and methods used by rational agents to form coalitions (even under uncertainty), including bargaining algorithms. We conclude by considering some developing topics, applications, and future research directions.},
  added-at = {2016-07-15T04:21:07.000+0200},
  address = {[San Rafael]},
  author = {Chalkiadakis, Georgios and Elkind, Edith and Wooldridge, Michael J.},
  biburl = {https://www.bibsonomy.org/bibtex/2b0b5bb0bff7ae95363809bb21ab60b33/vngudivada},
  interhash = {fdc4dd707a3df83db5e0bc41e2ec06d8},
  intrahash = {b0b5bb0bff7ae95363809bb21ab60b33},
  isbn = {9781608456529 1608456528},
  keywords = {GameTheory SynthesisLecture},
  publisher = {Morgan & Claypool},
  refid = {805098145},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Computational aspects of cooperative game theory},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9781608456529},
  year = 2012
}

@book{grauman2011visual,
  abstract = {The visual recognition problem is central to computer vision research. From robotics to information retrieval, many desired applications demand the ability to identify and localize categories, places, and objects. This tutorial overviews computer vision algorithms for visual object recognition and image classification. We introduce primary representations and learning approaches, with an emphasis on recent advances in the field. The target audience consists of researchers or students working in AI, robotics, or vision who would like to understand what methods and representations are available for these problems. This lecture summarizes what is and isn't possible to do reliably today, and overviews key concepts that could be employed in systems requiring visual categorization.},
  added-at = {2016-07-15T05:02:17.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Grauman, Kristen Lorraine and Leibe, Bastian},
  biburl = {https://www.bibsonomy.org/bibtex/2b97f37a9b9d512bbf9dc29f09fe472df/vngudivada},
  interhash = {ff1ddf27fe320ce0fcd751b23c72188a},
  intrahash = {b97f37a9b9d512bbf9dc29f09fe472df},
  isbn = {9781598299694 1598299697 1598299689 9781598299687},
  keywords = {ObjectRecognition SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {720114130},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Visual object recognition},
  url = {http://site.ebrary.com/id/10530798},
  year = 2011
}

@article{mcnaull2012information,
  abstract = {Demographic aging, as a result of people living for longer, has put an increased burden on health and social care provision across most of the economies of the developed and developing world. In order to cope with the greater numbers of older people, together with increasing prevalence of chronic diseases, governments are looking to new ways to provide care and support to older people and their care providers. A growing trend is where health and social care providers are moving towards the use of assisted living technologies to provide care and assistance in the home. In this article, the research area of Ambient Assisted Living (AAL) systems is examined and the data, information and the higher-level contextual knowledge quality issues in relation to these systems, is discussed. Lack of quality control may result in an AAL system providing assistance and support based upon incorrect data, information and knowledge inputs, and this may have a detrimental effect on the person making use of the system. We propose a model whereby contextual knowledge gained during the AAL system’s reasoning cycle can be fed back to aid in further quality checking at the various architectural layers, and a realistic AAL scenario is provided to support this. Future research should be conducted in these areas, with the requirement of building quality criteria into the design and implementation of AAL systems.},
  acmid = {2378020},
  added-at = {2016-07-23T20:29:54.000+0200},
  address = {New York, NY, USA},
  articleno = {4},
  author = {McNaull, James and Augusto, Juan Carlos and Mulvenna, Maurice and McCullagh, Paul},
  biburl = {https://www.bibsonomy.org/bibtex/2b316bd2a7de7084b0c0208c429986490/vngudivada},
  doi = {10.1145/2378016.2378020},
  interhash = {6b9d07f24a699c71eb5aff61fc8af863},
  intrahash = {b316bd2a7de7084b0c0208c429986490},
  issn = {1936-1955},
  issue_date = {October 2012},
  journal = {J. Data and Information Quality},
  keywords = {AssistiveTechnology DataQuality},
  month = oct,
  number = 1,
  numpages = {15},
  pages = {4:1--4:15},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data and Information Quality Issues in Ambient Assisted Living Systems},
  url = {http://doi.acm.org/10.1145/2378016.2378020},
  volume = 4,
  year = 2012
}

@inproceedings{brown2014investigating,
  abstract = {Educators often form opinions on which programming mistakes novices make most often - for example, in Java: "they always confuse equality with assignment", or "they always call methods with the wrong types". These opinions are generally based solely on personal experience. We report a study to determine if programming educators form a consensus about which Java programming mistakes are the most common. We used the Blackbox data set to check whether the educators' opinions matched data from over 100,000 students - and checked whether this agreement was mediated by educators' experience. We found that educators formed only a weak consensus about which mistakes are most frequent, that their rankings bore only a moderate correspondence to the students in the Blackbox data, and that educators' experience had no effect on this level of agreement. These results raise questions about claims educators make regarding which errors students are most likely to commit.},
  acmid = {2632343},
  added-at = {2016-07-18T04:04:09.000+0200},
  address = {New York, NY, USA},
  author = {Brown, Neil C.C. and Altadmri, Amjad},
  biburl = {https://www.bibsonomy.org/bibtex/2bde47bfcdd6c5b5de8bdf4c7739a6419/vngudivada},
  booktitle = {Proceedings of the Tenth Annual Conference on International Computing Education Research},
  doi = {10.1145/2632320.2632343},
  interhash = {f8e29d4ef2ad98e1717fb87039c8e3b0},
  intrahash = {bde47bfcdd6c5b5de8bdf4c7739a6419},
  isbn = {978-1-4503-2755-8},
  keywords = {CS1 NoviceProgrammer},
  location = {Glasgow, Scotland, United Kingdom},
  numpages = {8},
  pages = {43--50},
  publisher = {ACM},
  series = {ICER '14},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Investigating Novice Programming Mistakes: Educator Beliefs vs. Student Data},
  url = {http://doi.acm.org/10.1145/2632320.2632343},
  year = 2014
}

@inproceedings{lamprianidis2014extraction,
  abstract = {Our work focuses around a Web application that retrieves user-generated geospatial content from multiple popular Web sources, and applies schema mapping and entity matching techniques to obtain an integrated dataset. Moreover, density-based clustering of the obtained data is performed to reveal of interest for various data categories. An analysis and overview of the underlying data are also provided by computing various statistics that are visualized in a series of charts. Further data exploration and navigation is enabled via keyword search and faceted browsing. This demonstration covers all the steps of the process, from selecting an area and the sources for data collection, to visualizing and navigating the integrated results.},
  acmid = {2666367},
  added-at = {2016-07-23T23:06:33.000+0200},
  address = {New York, NY, USA},
  author = {Lamprianidis, George and Skoutas, Dimitrios and Papatheodorou, George and Pfoser, Dieter},
  biburl = {https://www.bibsonomy.org/bibtex/25d08fdfcd20e4b13373f2c2cca8bd904/vngudivada},
  booktitle = {Proceedings of the 22Nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  doi = {10.1145/2666310.2666367},
  interhash = {7810a396eaeb361af3e8174c86b7dcda},
  intrahash = {5d08fdfcd20e4b13373f2c2cca8bd904},
  isbn = {978-1-4503-3131-9},
  keywords = {CitizenScience CrowdSourcing DataIntegration GeospatialData},
  location = {Dallas, Texas},
  numpages = {4},
  pages = {553--556},
  publisher = {ACM},
  series = {SIGSPATIAL '14},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Extraction, Integration and Exploration of Crowdsourced Geospatial Content from Multiple Web Sources},
  url = {http://doi.acm.org/10.1145/2666310.2666367},
  year = 2014
}

@book{thielscher2008action,
  abstract = {Artificial systems that think and behave intelligently are one of the most exciting and challenging goals of Artificial Intelligence. Action Programming is the art and science of devising high-level control strategies for autonomous systems which employ a mental model of their environment and which reason about their actions as a means to achieve their goals. Applications of this programming paradigm include autonomous software agents, mobile robots with high-level reasoning capabilities, and General Game Playing. These lecture notes give an in-depth introduction to the current state-of-the-art in action programming. The main topics are knowledge representation for actions, procedural action programming, planning, agent logic programs, and reactive, behavior-based agents. The only prerequisite for understanding the material in these lecture notes is some general programming experience and basic knowledge of classical first-order logic.},
  added-at = {2016-07-15T04:54:28.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Thielscher, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/21ee269123e3999484ad2815deeeb2c40/vngudivada},
  interhash = {bcd2f762b7637a88f8797a020974468f},
  intrahash = {1ee269123e3999484ad2815deeeb2c40},
  isbn = {9781598295450 1598295454},
  keywords = {ActionProgrammingLanguage SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {235377841},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Action programming languages},
  url = {http://site.ebrary.com/id/10515724},
  year = 2008
}

@inproceedings{kalyanam2013idata,
  abstract = {With the advent of XSEDE, the national cyberinfrastructure has evolved from a set of traditional HPC resources to a broader range of digital services. Science gateways, which serve as portals to scientific applications, have also evolved as researchers are dealing with rapidly expanding scientific datasets and the increasingly complex workflows. More and more gateways are being developed to support integrated services for running data-driven applications on HPC resources such as those on XSEDE. To facilitate this type of workflow, there is a pressing need for web-based data management systems that are easy to use, support data upload, sharing, access and management, and can be integrated with advanced computation and storage resources. More importantly such systems need to be accessible by users from the broad research and education communities. In this paper, we describe the design and implementation of iData, a web-based community data publishing and sharing system. iData supports both generic file-based data collections and several commonly used environmental data collection formats including time series, GIS vector and raster data. Integrated data processing, visualization and filtering capabilities are provided for these data formats. Currently iData can be downloaded and deployed in a HUBzero-based gateway, and we plan to make it available for non-HUBzero platforms in the future. We present two examples in which iData has been successfully used to support research collaboration in driNET and GEOSHARE projects.},
  acmid = {2484813},
  added-at = {2016-07-23T23:01:27.000+0200},
  address = {New York, NY, USA},
  articleno = {41},
  author = {Kalyanam, Rajesh and Zhao, Lan and Song, Carol X. and Wong, Yuet Ling and Lee, Jaewoo and Villoria, Nelson B.},
  biburl = {https://www.bibsonomy.org/bibtex/2d63ee923f0f592d89d735d88de559012/vngudivada},
  booktitle = {Proceedings of the Conference on Extreme Science and Engineering Discovery Environment: Gateway to Discovery},
  doi = {10.1145/2484762.2484813},
  interhash = {a338a1c3bdf31e1777e8fbb988d46ece},
  intrahash = {d63ee923f0f592d89d735d88de559012},
  isbn = {978-1-4503-2170-9},
  keywords = {DataDrivenScience DataSharing GeospatialData},
  location = {San Diego, California, USA},
  numpages = {6},
  pages = {41:1--41:6},
  publisher = {ACM},
  series = {XSEDE '13},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {iData: A Community Geospatial Data Sharing Environment to Support Data-driven Science},
  url = {http://doi.acm.org/10.1145/2484762.2484813},
  year = 2013
}

@article{sha2015quality,
  acmid = {2740965},
  added-at = {2016-07-23T20:34:36.000+0200},
  address = {New York, NY, USA},
  articleno = {8},
  author = {Sha, Kewei and Zeadally, Sherali},
  biburl = {https://www.bibsonomy.org/bibtex/20f029b872ab94bb4922a6a82b43a5d2e/vngudivada},
  doi = {10.1145/2740965},
  interhash = {d2bfd21b5eb6ffac6c5062d3f1deb7cc},
  intrahash = {0f029b872ab94bb4922a6a82b43a5d2e},
  issn = {1936-1955},
  issue_date = {July 2015},
  journal = {J. Data and Information Quality},
  keywords = {CyberPhisicalSystem DataQuality},
  month = jun,
  number = {2-3},
  numpages = {4},
  pages = {8:1--8:4},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data Quality Challenges in Cyber-Physical Systems},
  url = {http://doi.acm.org/10.1145/2740965},
  volume = 6,
  year = 2015
}

@article{barnaghi2015challenges,
  abstract = {Smart cities use multimodal information coming from heterogeneous sources, including various types of the Internet of Things (IoT) data such as traffic, weather, pollution, and noise data. The smart city data usually have different quality of information (QoI). QoI of each data source mainly depends on three factors: (1) errors in measurements or precision of the data collection devices, (2) noise in the environment and quality of data communication and processing (including network-dependent quality of service parameters), and (3) granularity of the observations and measurements in both spatial and temporal dimensions. Furthermore, various environments have different requirements that will determine the efficacy of using the data in the smart city applications; some systems have energy restrictions; and some wireless networks could rely on low bandwidth or intermittent connectivity. Most smart city applications also have to deal with huge volumes of data, with high velocity, dynamicity, and a variety of types of data. The QoI issues become more challenging when various data with different QoI are going to be integrated into an application to extract higher-level information and/or to provide actionable information to other services and applications. In some of the current smart city frameworks, the underlying information model is based on semantic descriptions that provide an annotation model to support interoperability between different sources of information (e.g., SmartSantander , Aarhus). These models can help to represent the QoI for each data source; however, the models and annotated data are often represented as static descriptions, making them unsuitable for dynamic smart city data in which the quality can change over time.},
  acmid = {2747881},
  added-at = {2016-07-23T20:05:36.000+0200},
  address = {New York, NY, USA},
  articleno = {6},
  author = {Barnaghi, Payam and Bermudez-Edo, Maria and T\"{o}njes, Ralf},
  biburl = {https://www.bibsonomy.org/bibtex/2076854828a7a42f2ba74047082e89a31/vngudivada},
  doi = {10.1145/2747881},
  interhash = {d349c5f51390c147bb0787358598d578},
  intrahash = {076854828a7a42f2ba74047082e89a31},
  issn = {1936-1955},
  issue_date = {July 2015},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality SmartCity},
  month = jun,
  number = {2-3},
  numpages = {4},
  pages = {6:1--6:4},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Challenges for Quality of Data in Smart Cities},
  url = {http://doi.acm.org/10.1145/2747881},
  volume = 6,
  year = 2015
}

@techreport{nature2016challenges,
  added-at = {2016-07-23T20:19:41.000+0200},
  author = {Nature, Third},
  biburl = {https://www.bibsonomy.org/bibtex/2e7d750824b77d74d8b6c74445640ae9a/vngudivada},
  interhash = {5372297fc0036cd23eb64c2d536694a3},
  intrahash = {e7d750824b77d74d8b6c74445640ae9a},
  keywords = {BigData DataQuality},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {The Challenges of Big Data & Approaches to Data Quality: Using big data to examine and discover the value in data for accurate analytics},
  year = 2016
}

@inproceedings{gamble2011quality,
  abstract = {In science, quality is paramount. As scientists increasingly look to the Web to share and discover scientific data, there is a growing need to support the scientist in assessing the quality of that data. However, quality is an ambiguous and overloaded term. In order to support the scientific user in discovering useful data we have systematically examined the nature of "quality" by exploiting three, prevalent properties of scientific data sets: (1) that data quality is commonly defined objectively; (2) the provenance and lineage in its production has a well understood role; and (3) "fitness-for-use" is a definition of utility rather than quality or trust, where the quality and trust-worthiness of the data and the entities that produced that data inform its utility. Our study is presented in two stages. First we review existing information quality dimensions and detail an assessment-oriented classification. We introduce definitions for quality, trust and utility in terms of the entities required in their assessment; producer, provider, consumer, process, artifact and quality standard. Next we detail a novel and experimental approach to assessment by modelling the causal relationships between quality, trust, and utility dimensions through the construction of decision networks informed by provenance graphs. To ground and motivate our discussion throughout we draw on the European Bioinformatics Institute's Gene Ontology Annotations database. We present an initial demonstration of our approach with an example for ranking results from the Gene Ontology Annotation database using an emerging objective quality measure, the Gene Ontology Annotation Quality score.},
  acmid = {2527048},
  added-at = {2016-07-23T22:46:14.000+0200},
  address = {New York, NY, USA},
  articleno = {15},
  author = {Gamble, Matthew and Goble, Carole},
  biburl = {https://www.bibsonomy.org/bibtex/2652d77424c758b592914745b4ff1c212/vngudivada},
  booktitle = {Proceedings of the 3rd International Web Science Conference},
  doi = {10.1145/2527031.2527048},
  interhash = {c04e8c72d167d02f3cfc3d871216fffa},
  intrahash = {652d77424c758b592914745b4ff1c212},
  isbn = {978-1-4503-0855-7},
  keywords = {DataQuality ScientificData WebData},
  location = {Koblenz, Germany},
  numpages = {8},
  pages = {15:1--15:8},
  publisher = {ACM},
  series = {WebSci '11},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Quality, Trust, and Utility of Scientific Data on the Web: Towards a Joint Model},
  url = {http://doi.acm.org/10.1145/2527031.2527048},
  year = 2011
}

@inproceedings{lassoued2010gqbox,
  abstract = {In order to measure and assess the quality of GIS, there exist a sparse offer of tools, providing specific functions with their own interest but are not sufficient to deal with broader user's requirements. Interoperability of these tools remains a technical challenge because of the heterogeneity of their models and access patterns. On the other side, quality analysts require more and more integration facilities that allow them to consolidate and aggregate multiple quality measures acquired from different observations or data sources, in using/combining seamlessly different quality tools. Clearly, there is a gap between users's requirements and the spatial data quality market. This demo paper will illustrate GQBox, a geographic quality (tool)box. GQBox supplies a standards-based generic meta model that supports the definition of quality goals and metrics, and it provides a service-based infrastructure that allows interoperability among several quality tools.},
  acmid = {1869884},
  added-at = {2016-07-23T22:52:38.000+0200},
  address = {New York, NY, USA},
  author = {Lassoued, Yassine and Bouadjenek, Mohamed Reda and Boucelma, Omar and Lemos, Fernando and Bouzeghoub, Mokrane},
  biburl = {https://www.bibsonomy.org/bibtex/25555ac3c4a115e81b38b4e4cd3aa2d03/vngudivada},
  booktitle = {Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  doi = {10.1145/1869790.1869884},
  interhash = {6637de2ebf3598517550bc4e1f80b521},
  intrahash = {5555ac3c4a115e81b38b4e4cd3aa2d03},
  isbn = {978-1-4503-0428-3},
  keywords = {DataQuality DataQualityAssessment GeospatialData},
  location = {San Jose, California},
  numpages = {2},
  pages = {534--535},
  publisher = {ACM},
  series = {GIS '10},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {GQBox: Geospatial Data Quality Assessment},
  url = {http://doi.acm.org/10.1145/1869790.1869884},
  year = 2010
}

@inproceedings{klein2007incorporating,
  abstract = {Sensors are increasingly embedded into physical products in order to capture data about their conditions and usage for decision making in business applications. However, a major issue for such applications is the limited quality of the captured data due to inherently restricted precision and performance of the sensors. Moreover, the data quality is further decreased by data processing to meet resource constraints in streaming environments and ultimately influences business decisions. The issue of how to efficiently provide applications with information about data quality (DQ) is still an open research problem. In my Ph.D. thesis, I address this problem by developing a system to provide business applications with accurate information on data quality. Furthermore, the system will be able to incorporate and guarantee user-defined data quality levels. In this paper, I will present the major results from my research so far. This includes a novel jumping-window-based approach for the efficient transfer of data quality information as well as a flexible metamodel for storage and propagation of data quality. The comprehensive analysis of common data processing operators w.r.t. their impact on data quality allows a fruitful knowledge evaluation and thus diminishes incorrect business decisions.},
  acmid = {1316888},
  added-at = {2016-07-23T21:43:51.000+0200},
  address = {New York, NY, USA},
  author = {Klein, Anja},
  biburl = {https://www.bibsonomy.org/bibtex/2d9f36937996a33d00ef48ebfc6d4fd4b/vngudivada},
  booktitle = {Proceedings of the ACM First Ph.D. Workshop in CIKM},
  doi = {10.1145/1316874.1316888},
  interhash = {e696b9b20ba12a976b51c476dbd88b49},
  intrahash = {d9f36937996a33d00ef48ebfc6d4fd4b},
  isbn = {978-1-59593-832-9},
  keywords = {DataQuality DataStream SensorData},
  location = {Lisbon, Portugal},
  numpages = {8},
  pages = {77--84},
  publisher = {ACM},
  series = {PIKM '07},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Incorporating Quality Aspects in Sensor Data Streams},
  url = {http://doi.acm.org/10.1145/1316874.1316888},
  year = 2007
}

@article{negri1991formal,
  abstract = {The semantics of SQL queries is formally defined by stating a set of rules that determine a syntax-driven translation of an SQL query to a formal model. The target model, called Extended Three Valued Predicate Calculus (E3VPC), is largely based on a set of well-known mathematical concepts. The rules which allow the transformation of a general E3VPC expression to a Canonical Form, which can be manipulated using traditional, two-valued predicate calculus are also given; in this way, problems like equivalence analysis of SQL queries are completely solved. Finally, the fact that reasoning about the equivalence of SQL queries using two-valued predicate calculus, without taking care of the real SQL semantics can lead to errors is shown, and the reasons for this are analyzed.},
  acmid = {111212},
  added-at = {2016-07-18T04:12:20.000+0200},
  address = {New York, NY, USA},
  author = {Negri, M. and Pelagatti, G. and Sbattella, L.},
  biburl = {https://www.bibsonomy.org/bibtex/20e112579936732ee8392874ecd8bc9b4/vngudivada},
  doi = {10.1145/111197.111212},
  interhash = {928ecbe07cd920dff635141d7f4df5bf},
  intrahash = {0e112579936732ee8392874ecd8bc9b4},
  issn = {0362-5915},
  issue_date = {Sept. 1991},
  journal = {ACM Trans. Database Syst.},
  keywords = {FormalSemantics SQL},
  month = sep,
  number = 3,
  numpages = {22},
  pages = {513--534},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Formal Semantics of SQL Queries},
  url = {http://doi.acm.org/10.1145/111197.111212},
  volume = 16,
  year = 1991
}

@inproceedings{dekeyser2007computer,
  abstract = {Structured Query Language (SQL) is the dominant language for querying relational databases today, and is an essential topic in introductory database courses in higher education. Even though the language is syntactically simple, relatively concise, and highly structured, students experience many difficulties while learning to express queries in SQL. In recent years a small number of software tools have been proposed to help students learn to write query statements and to assess their querying skills.

In this paper we compare and evaluate existing tools mainly from the perspective of database theory and practice, but also from a pedagogical perspective. Addressing the deficiencies and opportunities uncovered by the evaluation, we then introduce SQLify, a new tool that extends the current state of the art by incorporating semantic feedback, enhanced automatic assessment based on database theory, and peer review to arrive at a richer learning experience for students, as well as consistent assessment results and reduced marking for instructors.},
  added-at = {2016-07-18T04:34:17.000+0200},
  address = {Darlinghurst, Australia, Australia},
  author = {Dekeyser, Stijn and de Raadt, Michael and Lee, Tien Yu},
  biburl = {https://www.bibsonomy.org/bibtex/25267ecb55a395b4e8429c5f8d17915b0/vngudivada},
  booktitle = {Proceedings of the Eighteenth Conference on Australasian Database - Volume 63},
  interhash = {08c867ad1e7547e4a229b580a1705162},
  intrahash = {5267ecb55a395b4e8429c5f8d17915b0},
  keywords = {QueryEquivalence SQL},
  numpages = {10},
  pages = {53--62},
  publisher = {Australian Computer Society, Inc.},
  series = {ADC '07},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Computer Assisted Assessment of SQL Query Skills},
  url = {http://dl.acm.org/citation.cfm?id=1273730.1273737},
  year = 2007
}

@inproceedings{holmqvist2012tracker,
  abstract = {Data quality is essential to the validity of research results and to the quality of gaze interaction. We argue that the lack of standard measures for eye data quality makes several aspects of manufacturing and using eye trackers, as well as researching eye movements and vision, more difficult than necessary. Uncertainty regarding the comparability of research results is a considerable impediment to progress in the field. In this paper, we illustrate why data quality matters and review previous work on how eye data quality has been measured and reported. The goal is to achieve a common understanding of what data quality is and how it can be defined, measured, evaluated, and reported.},
  acmid = {2168563},
  added-at = {2016-07-23T21:34:23.000+0200},
  address = {New York, NY, USA},
  author = {Holmqvist, Kenneth and Nystr\"{o}m, Marcus and Mulvey, Fiona},
  biburl = {https://www.bibsonomy.org/bibtex/29a88b802d8f62684821801a3800951a1/vngudivada},
  booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  doi = {10.1145/2168556.2168563},
  interhash = {195a5ccb1d1fd04b430d87b84a59226d},
  intrahash = {9a88b802d8f62684821801a3800951a1},
  isbn = {978-1-4503-1221-9},
  keywords = {DataQuality EyeTrackerData},
  location = {Santa Barbara, California},
  numpages = {8},
  pages = {45--52},
  publisher = {ACM},
  series = {ETRA '12},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Eye Tracker Data Quality: What It is and How to Measure It},
  url = {http://doi.acm.org/10.1145/2168556.2168563},
  year = 2012
}

@inproceedings{chen2014performance,
  abstract = {The growth of spatial big data has been explosive thanks to cost-effective and ubiquitous positioning technologies, and the generation of data from multiple sources in multi-forms. Such emerging spatial data has high potential to create new insights and values for our life through spatial analytics. However, spatial data analytics faces two major challenges. First, spatial data is both data-and compute-intensive due to the massive amounts of data and the multi-dimensional nature, which requires high performance spatial computing infrastructure and methods. Second, spatial big data sources are often isolated, for example, OpenStreetMap, census data and Twitter tweets are independent data sources. This leads to incompleteness of information and sometimes limited data accuracy, thus limited values from the data. Integrating spatial big data analytics by consolidating multiple data sources provides significant potential for data quality improvement in terms of completeness and accuracy, and much increased values derived from the data. In this paper, we present our vision of a high performance integrated spatial big data analytics framework. We provide a scalable spatial query based data integration engine with MapReduce, and demonstrate integrated spatial data analytics through a few use cases in our preliminary work. We then present our future plan on integrated spatial big data analytics for improving public health research and applications.},
  acmid = {2676538},
  added-at = {2016-07-23T23:08:59.000+0200},
  address = {New York, NY, USA},
  author = {Chen, Xin and Vo, Hoang and Aji, Ablimit and Wang, Fusheng},
  biburl = {https://www.bibsonomy.org/bibtex/241e75c9ab132b082e5219fd4fe1d8c83/vngudivada},
  booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
  doi = {10.1145/2676536.2676538},
  interhash = {768409e02df129e1be611cac605a91cf},
  intrahash = {41e75c9ab132b082e5219fd4fe1d8c83},
  isbn = {978-1-4503-3132-6},
  keywords = {HPC SpatialData SpatialDataAnalytics},
  location = {Dallas, Texas},
  numpages = {4},
  pages = {11--14},
  publisher = {ACM},
  series = {BigSpatial '14},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {High Performance Integrated Spatial Big Data Analytics},
  url = {http://doi.acm.org/10.1145/2676536.2676538},
  year = 2014
}

@article{wand1996anchoring,
  abstract = {Poor data quality can have a severe impact on the overall effectiveness of an organization. In order to design information systems that deliver good quality of data, the notion of data quality has to be well-understood. However, there is still no consensus on what constitutes a good set of data quality dimensions and on appropriate definitions for each dimension.

We propose an ontologically-based approach to define data quality dimensions based on the role of an information systems as a representation of a real-world system. The dimensions are derived from possible failures of the representation. The analysis leads to four intrinsic dimensions of data quality: completeness, lack of ambiguity, meaningfulness, and correctness. We discuss the relationships of these dimensions to those cited in the literature and briefly present some implications of the analysis to information systems design.},
  acmid = {240479},
  added-at = {2016-07-23T20:49:13.000+0200},
  address = {New York, NY, USA},
  author = {Wand, Yair and Wang, Richard Y.},
  biburl = {https://www.bibsonomy.org/bibtex/2847b2c7354c35387e95f71108ecc1a6e/vngudivada},
  doi = {10.1145/240455.240479},
  interhash = {d2a5c38d4f5e5c84eb71deac5e548655},
  intrahash = {847b2c7354c35387e95f71108ecc1a6e},
  issn = {0001-0782},
  issue_date = {Nov. 1996},
  journal = {Commun. ACM},
  keywords = {DataQuality Ontology},
  month = nov,
  number = 11,
  numpages = {10},
  pages = {86--95},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Anchoring Data Quality Dimensions in Ontological Foundations},
  url = {http://doi.acm.org/10.1145/240455.240479},
  volume = 39,
  year = 1996
}

@inproceedings{freudiger2014privacy,
  abstract = {In a data-driven economy that struggles to cope with the volume and diversity of information, data quality assessment has become a necessary precursor to data analytics. Real-world data often contains inconsistencies, conflicts and errors. Such dirty data increases processing costs and has a negative impact on analytics. Assessing the quality of a dataset is especially important when a party is considering acquisition of data held by an untrusted entity. In this scenario, it is necessary to consider privacy risks of the stakeholders. This paper examines challenges in privacy-preserving data quality assessment. A two-party scenario is considered, consisting of a client that wishes to test data quality and a server that holds the dataset. Privacy-preserving protocols are presented for testing important data quality metrics: completeness, consistency, uniqueness, timeliness and validity. For semi-honest parties, the protocols ensure that the client does not discover any information about the data other than the value of the quality metric. The server does not discover the parameters of the client's query, the specific attributes being tested and the computed value of the data quality metric. The proposed protocols employ additively homomorphic encryption in conjunction with condensed data representations such as counting hash tables and histograms, serving as efficient alternatives to solutions based on private set intersection.},
  acmid = {2663885},
  added-at = {2016-07-23T22:36:36.000+0200},
  address = {New York, NY, USA},
  author = {Freudiger, Julien and Rane, Shantanu and Brito, Alejandro E. and Uzun, Ersin},
  biburl = {https://www.bibsonomy.org/bibtex/2903bba684de0f4a19e588d4d1a22e00a/vngudivada},
  booktitle = {Proceedings of the 2014 ACM Workshop on Information Sharing \&\#38; Collaborative Security},
  doi = {10.1145/2663876.2663885},
  interhash = {92715fb4540818cb1f809f2823128f92},
  intrahash = {903bba684de0f4a19e588d4d1a22e00a},
  isbn = {978-1-4503-3151-7},
  keywords = {DataQuality DataSharing Privacy},
  location = {Scottsdale, Arizona, USA},
  numpages = {9},
  pages = {21--29},
  publisher = {ACM},
  series = {WISCS '14},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Privacy Preserving Data Quality Assessment for High-Fidelity Data Sharing},
  url = {http://doi.acm.org/10.1145/2663876.2663885},
  year = 2014
}

@inbook{tejay2006quality,
  abstract = {Data is an important asset used for various organizational activities. Poor data quality could have severe implications for information systems security in organizations. In this paper, data is viewed as embodied in the concept of signs. This paper identifies dimensions of data quality by using semiotics as a theoretical basis. We argue that the nature and scope of data quality dimensions changes as we move between different semiotic levels. An understanding of these changes is essential for ensuring information systems security.},
  added-at = {2016-07-23T20:44:29.000+0200},
  address = {Boston, MA},
  author = {Tejay, Gurvirender and Dhillon, Gurpreet and Chin, Amita Goyal},
  biburl = {https://www.bibsonomy.org/bibtex/2341ed6291c363764fcd2db71c7b29784/vngudivada},
  booktitle = {Security Management, Integrity, and Internal Control in Information Systems: IFIP TC-11 WG 11.1 {\&} WG 11.5 Joint Working Conference},
  doi = {10.1007/0-387-31167-X_2},
  editor = {Dowland, Paul and Furnell, Steve and Thuraisingham, Bhavani and Wang, X. Sean},
  interhash = {a3188ea5824ab1cdffe55a7c196c9de1},
  intrahash = {341ed6291c363764fcd2db71c7b29784},
  isbn = {978-0-387-31167-8},
  keywords = {DataQuality InformationSecurity},
  pages = {21--39},
  publisher = {Springer US},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data Quality Dimensions for Information Systems Security: A Theoretical Exposition (Invited Paper)},
  url = {http://dx.doi.org/10.1007/0-387-31167-X_2},
  year = 2006
}

@book{hoiem2011representations,
  abstract = {One of the grand challenges of artificial intelligence is to enable computers to interpret 3D scenes and objects from imagery. This book organizes and introduces major concepts in 3D scene and object representation and inference from still images, with a focus on recent efforts to fuse models of geometry and perspective with statistical machine learning.},
  added-at = {2016-07-15T04:43:24.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Hoiem, Derek and Savarese, Silvio},
  biburl = {https://www.bibsonomy.org/bibtex/221270cd740c053482d66ec8572f74e19/vngudivada},
  interhash = {ea51e073c8a1e05be301fa85d3f3f91b},
  intrahash = {21270cd740c053482d66ec8572f74e19},
  isbn = {9781608457298 160845729X},
  keywords = {3D ObjectRecognition SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {752335498},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Representations and techniques for 3D object recognition and scene interpretation},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881271},
  year = 2011
}

@inproceedings{sneed2011process,
  abstract = {Abstract: This industrial report stems from practical experience in assessing the quality of customer databases. The process it describes unites three automated audits, - an audit of the database schema, an audit of the database structure and an audit of the database content. The audit of the database schema checks for design smells and rule violations. The audit of the database structure measures the size, complexity and quality of the database model. The audit of the database content processes the data itself to uncover invalid data values, missing records and redundant records. The purpose of these audits is to assess the quality of the database and to determine whether a data reengineering or data clean-up project is required.},
  acmid = {2024599},
  added-at = {2016-07-23T18:32:54.000+0200},
  address = {New York, NY, USA},
  author = {Sneed, Harry M. and Majnar, Rudolf},
  biburl = {https://www.bibsonomy.org/bibtex/251fb417f1e204f603d4bf5d5e2843585/vngudivada},
  booktitle = {Proceedings of the 8th International Workshop on Software Quality},
  doi = {10.1145/2024587.2024599},
  interhash = {e790fb7a20c22a534ff2999598bd9b39},
  intrahash = {51fb417f1e204f603d4bf5d5e2843585},
  isbn = {978-1-4503-0851-9},
  keywords = {DataQuality DataQualityAssessment},
  location = {Szeged, Hungary},
  numpages = {8},
  pages = {50--57},
  publisher = {ACM},
  series = {WoSQ '11},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {A Process for Assessing Data Quality},
  url = {http://doi.acm.org/10.1145/2024587.2024599},
  year = 2011
}

@techreport{experian2016state,
  abstract = {The perception of data across organizations is changing. No longer is data just viewed as a secondary component of business.

Today information contained within a database is viewed by senior management and many departments as a critical factor in decision making, customer interaction and service delivery. In fact, 93 percent of companies believe data is essential to their marketing success. However, data quality inaccuracy leaves organizations at risk.

Unfortunately, the level of global inaccurate contact data has increased from 17 percent to 22 percent, up five percent in just 12 months.

With the increasing volume of information collected through a variety of channels, there is more room for human error. This combined with the prevalence of segmented, departmental approaches to data accuracy is preventing stakeholders from analyzing, improving and controlling data problems.

This year's study revealed that 66 percent of companies lack a coherent, centralized approach to data quality. But, ad-hoc approaches are dividing resources and further segmenting information.

Poor data quality is having a negative effect on budgets, marketing efforts and most importantly, customer satisfaction. Organizations that are not able to control the quality of their data are unable to effectively communicate with their customer base.

Data quality is the foundation for any data-driven effort and in order to succeed in the year ahead, organizations will need to look at prioritizing data accuracy and accessibility.},
  added-at = {2016-07-23T20:17:10.000+0200},
  author = {Experian},
  biburl = {https://www.bibsonomy.org/bibtex/21d0dd58f0bed563f6ef25d4fe42a9338/vngudivada},
  interhash = {142238bdeec7c0f54d326254711a93e6},
  intrahash = {1d0dd58f0bed563f6ef25d4fe42a9338},
  keywords = {DataQuality Experian},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {The state of data quality: An Experian Data Quality white paper},
  year = 2016
}

@article{cao2013normal,
  abstract = {The efficient operation of Enterprise Resource Planning (ERP) systems largely depends on data quality. ERP can improve data quality and information sharing within an organization. It can also pose challenges to data quality. While it is well known that data quality is important in ERP systems, most existing research has focused on identifying the factors affecting the implementation and the business values of ERP. With normal accident theory as a theoretical lens, we examine data quality problems in ERP using a case study of a large, fast-growing multinational manufacturer headquartered in China. Our findings show that organizations that have successfully implemented ERP can still experience certain data quality problems. We identify major data quality problems in data production, storage and maintenance, and utilization processes. We also analyze the causes of these data quality problems by linking them to certain characteristics of ERP systems within an organizational context. Our analysis shows that problems resulting from the tight coupling effects and the complexity of ERP-enabled manufacturing systems can be inevitable. This study will help researchers and practitioners formulate data management strategies that are effective in the presence of certain “normal” data quality problems.},
  acmid = {2458519},
  added-at = {2016-07-23T22:32:43.000+0200},
  address = {New York, NY, USA},
  articleno = {11},
  author = {Cao, Lan and Zhu, Hongwei},
  biburl = {https://www.bibsonomy.org/bibtex/269f38331ab7fcf0baf1b45869465cf66/vngudivada},
  doi = {10.1145/2458517.2458519},
  interhash = {c0b0451e074938c1f189266cf2885bd6},
  intrahash = {69f38331ab7fcf0baf1b45869465cf66},
  issn = {1936-1955},
  issue_date = {May 2013},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality ERP},
  month = may,
  number = 3,
  numpages = {26},
  pages = {11:1--11:26},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Normal Accidents: Data Quality Problems in ERP-enabled Manufacturing},
  url = {http://doi.acm.org/10.1145/2458517.2458519},
  volume = 4,
  year = 2013
}

@inproceedings{shepperd2011quality,
  abstract = {In this keynote I explore what exactly do we mean by data quality, techniques to assess data quality and the very significant challenges that poor data quality can pose. I believe we neglect data quality at our peril since - whether we like it or not - our research results are founded upon data and our assumptions that data quality issues do not confound our results. A systematic review of the literature suggests that it is a minority practice to even explicitly discuss data quality. I therefore suggest that this topic should become a higher priority amongst empirical software engineering researchers.},
  acmid = {1985376},
  added-at = {2016-07-23T20:36:30.000+0200},
  address = {New York, NY, USA},
  author = {Shepperd, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/20c57848f2c83600fe5f4075c371d04b9/vngudivada},
  booktitle = {Proceedings of the 2Nd International Workshop on Emerging Trends in Software Metrics},
  doi = {10.1145/1985374.1985376},
  interhash = {b2d29696b9fb627da1ec05651e37cd0d},
  intrahash = {0c57848f2c83600fe5f4075c371d04b9},
  isbn = {978-1-4503-0593-8},
  keywords = {DataQuality SoftwareMetrics},
  location = {Waikiki, Honolulu, HI, USA},
  numpages = {4},
  pages = {1--4},
  publisher = {ACM},
  series = {WETSoM '11},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data Quality: Cinderella at the Software Metrics Ball?},
  url = {http://doi.acm.org/10.1145/1985374.1985376},
  year = 2011
}

@article{mhuner2011product,
  abstract = {A number of business requirements (e.g. compliance with regulatory and legal provisions, diffusion of global standards, supply chain integration) are forcing consumer goods manufacturers to increase their efforts to provide product data (e.g. product identifiers, dimensions) at business-to-business interfaces timely and accurately. The quality of such data is a critical success factor for efficient and effective cross-company collaboration. If compliance relevant data (e.g. dangerous goods indicators) is missing or false, consumer goods manufacturers risk being fined and see their company's image damaged. Or if logistics data (e.g. product dimensions, gross weight) is inaccurate or provided not in time, business with key account trading partners is endangered. To be able to manage the risk of business critical data defects, companies must be able to a) identify such data defects, and b) specify and use metrics that allow to monitor the data's quality. As scientific research on both these issues has come up with only few results so far, this case study explores the process of identifying business critical product data defects at German consumer goods manufacturing company Beiersdorf AG. Despite advanced data quality management structures such defects still occur and can result in complaints, service level impairment and avoidable costs. The case study analyzes product data use and maintenance in Beiersdorf's ecosystem, identifies typical product data defects, and proposes a set of data quality metrics for monitoring those defects.},
  added-at = {2016-07-23T23:53:47.000+0200},
  author = {M. H{\"u}ner, Kai and Schierning, Andreas and Otto, Boris and {\"O}sterle, Hubert},
  biburl = {https://www.bibsonomy.org/bibtex/294a9e536bf46b238c3eab1eb77248117/vngudivada},
  interhash = {0a10b7c930c4494508da956f1eba468b},
  intrahash = {94a9e536bf46b238c3eab1eb77248117},
  journal = {Electronic Markets},
  keywords = {DataQuality SupplyChain},
  number = 2,
  pages = {141--154},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Product data quality in supply chains: the case of Beiersdorf},
  url = {http://dx.doi.org/10.1007/s12525-011-0059-x},
  volume = 21,
  year = 2011
}

@inproceedings{morana2014geobench,
  abstract = {In the last decade, a large market for location-based services and geospatial applications has emerged. Multiple cartographic providers propose their visualization tool for displaying points of interests. However, the data describing spatial entities is often incomplete and contradictory from one provider to another, thus limiting further applications such as geospatial data mining. Recent works in entity matching tackle this issue by discovering correspondences between spatial entities that refer to the same real world location. To evaluate and compare these works, we propose GeoBench, a tool which facilitates the building of a benchmark for spatial entity matching.},
  acmid = {2666362},
  added-at = {2016-07-23T23:04:33.000+0200},
  address = {New York, NY, USA},
  author = {Morana, Anthony and Morel, Thomas and Berjawi, Bilal and Duchateau, Fabien},
  biburl = {https://www.bibsonomy.org/bibtex/2e31252c20d78ce1abb5aae2b2060dcc2/vngudivada},
  booktitle = {Proceedings of the 22Nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  doi = {10.1145/2666310.2666362},
  interhash = {87ce8580134388e41b4ba8af66892a0d},
  intrahash = {e31252c20d78ce1abb5aae2b2060dcc2},
  isbn = {978-1-4503-3131-9},
  keywords = {EntityMatching GeospatialData RecordLinking},
  location = {Dallas, Texas},
  numpages = {4},
  pages = {533--536},
  publisher = {ACM},
  series = {SIGSPATIAL '14},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {GeoBench: A Geospatial Integration Tool for Building a Spatial Entity Matching Benchmark},
  url = {http://doi.acm.org/10.1145/2666310.2666362},
  year = 2014
}

@inproceedings{thakkar2007qualitydriven,
  abstract = {Accurate and efficient integration of geospatial data is an important problem with applications in areas such as emergency response and urban planning. Some of the key challenges in supporting large-scale geospatial data integration are automatically computing the quality of the data provided by a large number of geospatial sources and dynamically providing high quality answers to the user queries based on a quality criteria supplied by the user. We describe a framework called the Quality-driven Geospatial Mediator (QGM) that supports efficient and accurate integration of geospatial data from a large number of sources. The key contributions of our framework are: (1) the ability to automatically estimate the quality of data provided by a source by using the information from another source of known quality, (2) representing the quality of data provided by the sources in a declarative data integration framework, and (3) a query answering technique that exploits the quality information to provide high quality geospatial data in response to user queries. Our experimental evaluation using over 1200 real-world sources shows that QGM can accurately estimate the quality of geospatial sources. Moreover, QGM provides better quality data in response to the user queries compared to the traditional data integration systems and does so with lower response time.},
  acmid = {1341034},
  added-at = {2016-07-23T22:48:25.000+0200},
  address = {New York, NY, USA},
  articleno = {16},
  author = {Thakkar, Snehal and Knoblock, Craig A. and Ambite, Jose Luis},
  biburl = {https://www.bibsonomy.org/bibtex/2658b5dfadf4501c9e096534d7c49af33/vngudivada},
  booktitle = {Proceedings of the 15th Annual ACM International Symposium on Advances in Geographic Information Systems},
  doi = {10.1145/1341012.1341034},
  interhash = {ee4a2dd623e53ef06bdf8f0e70cfc608},
  intrahash = {658b5dfadf4501c9e096534d7c49af33},
  isbn = {978-1-59593-914-2},
  keywords = {DataQuality GeospatialData},
  location = {Seattle, Washington},
  numpages = {8},
  pages = {16:1--16:8},
  publisher = {ACM},
  series = {GIS '07},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Quality-driven Geospatial Data Integration},
  url = {http://doi.acm.org/10.1145/1341012.1341034},
  year = 2007
}

@article{madnick2009overview,
  abstract = {Awareness of data and information quality issues has grown rapidly in light of the critical role played by the quality of information in our data-intensive, knowledge-based economy. Research in the past two decades has produced a large body of data quality knowledge and has expanded our ability to solve many data and information quality problems. In this article, we present an overview of the evolution and current landscape of data and information quality research. We introduce a framework to characterize the research along two dimensions: topics and methods. Representative papers are cited for purposes of illustrating the issues addressed and the methods used. We also identify and discuss challenges to be addressed in future research.},
  acmid = {1516680},
  added-at = {2016-07-23T22:34:24.000+0200},
  address = {New York, NY, USA},
  articleno = {2},
  author = {Madnick, Stuart E. and Wang, Richard Y. and Lee, Yang W. and Zhu, Hongwei},
  biburl = {https://www.bibsonomy.org/bibtex/2ed7ca49de07437f89e90b40b588d1b16/vngudivada},
  doi = {10.1145/1515693.1516680},
  interhash = {55f39ea7aad5e37c6e902274c5bc68a7},
  intrahash = {ed7ca49de07437f89e90b40b588d1b16},
  issn = {1936-1955},
  issue_date = {June 2009},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality Framework},
  month = jun,
  number = 1,
  numpages = {22},
  pages = {2:1--2:22},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Overview and Framework for Data and Information Quality Research},
  url = {http://doi.acm.org/10.1145/1515693.1516680},
  volume = 1,
  year = 2009
}

@book{gudivada2016cognitive,
  abstract = {Cognitive Computing: Theory and Applications, written by internationally renowned experts, focuses on cognitive computing and its theory and applications, including the use of cognitive computing to manage renewable energy, the environment, and other scarce resources, machine learning models and algorithms, biometrics, Kernel Based Models for transductive learning, neural networks, graph analytics in cyber security, neural networks, data driven speech recognition, and analytical platforms to study the brain-computer interface.},
  added-at = {2016-07-16T03:44:32.000+0200},
  author = {Gudivada, Venkat and Raghavan, Vijay and Govindaraju, Venu and Rao, C.R.},
  biburl = {https://www.bibsonomy.org/bibtex/2c951684b6442918b31e699ea9bfb2060/vngudivada},
  interhash = {5acb2f91a236b5f213c95f4ede626b70},
  intrahash = {c951684b6442918b31e699ea9bfb2060},
  isbn = {0444637443 9780444637444},
  keywords = {CognitiveComputing},
  publisher = {Elsevier Science Ltd},
  refid = {946605577},
  series = {Handbook of Statistics},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Cognitive computing: theory and applications},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780444637444},
  volume = 35,
  year = 2016
}

@book{mahadevan2008representation,
  abstract = {Representations are at the heart of artificial intelligence (AI). This book is devoted to the problem of representation discovery: how can an intelligent system construct representations from its experience? Representation discovery re-parameterizes the state space - prior to the application of information retrieval, machine learning, or optimization techniques - facilitating later inference processes by constructing new task-specific bases adapted to the state space geometry. This book presents a general approach to representation discovery using the framework of harmonic analysis, in particular Fourier and wavelet analysis. Biometric compression methods, the compact disc, the computerized axial tomography (CAT) scanner in medicine, JPEG compression, and spectral analysis of time-series data are among the many applications of classical Fourier and wavelet analysis. A central goal of this book is to show that these analytical tools can be generalized from their usual setting in (infinite-dimensional) Euclidean spaces to discrete (finite-dimensional) spaces typically studied in many subfields of AI. Generalizing harmonic analysis to discrete spaces poses many challenges: a discrete representation of the space must be adaptively acquired; basis functions are not pre-defined, but rather must be constructed. Algorithms for efficiently computing and representing bases require dealing with the curse of dimensionality. However, the benefits can outweigh the costs, since the extracted basis functions outperform parametric bases as they often reflect the irregular shape of a particular state space. Case studies from computer graphics, information retrieval, machine learning, and state space planning are used to illustrate the benefits of the proposed framework, and the challenges that remain to be addressed. Representation discovery is an actively developing field, and the author hopes this book will encourage other researchers to explore this exciting area of research.},
  added-at = {2016-07-15T04:58:38.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Mahadevan, Sridhar},
  biburl = {https://www.bibsonomy.org/bibtex/224a2cb76fd5df4c924f715ab6faf98f1/vngudivada},
  interhash = {1376ecc152d6ab0cb9fce6cd87cabe29},
  intrahash = {24a2cb76fd5df4c924f715ab6faf98f1},
  isbn = {9781598296600 1598296604},
  keywords = {HarmonicAnalysis RepresentationDiscovery SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {235585411},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Representation discovery using harmonic analysis},
  url = {http://site.ebrary.com/id/10515578},
  year = 2008
}

@inproceedings{sheppard2011quality,
  abstract = {Citizen science is becoming more valuable as a potential source of environmental data. Involving citizens in data collection has the added educational benefits of increased scientific awareness and local ownership of environmental concerns. However, a common concern among domain experts is the presumed lower quality of data submitted by volunteers. In this paper, we explore data quality assurance practices in River Watch, a community-based monitoring program in the Red River basin. We investigate how the participants in River Watch understand and prioritize data quality concerns. We found that data quality in River Watch is primarily maintained through universal adherence to standard operating procedures, but there remain areas where technological intervention may help. We also found that rigorous data quality assurance practices appear to enhance rather than hinder the educational goals of the program. We draw implications for the design of quality assurance mechanisms for River Watch and other citizen science projects.},
  acmid = {2038565},
  added-at = {2016-07-23T22:43:52.000+0200},
  address = {New York, NY, USA},
  author = {Sheppard, S. Andrew and Terveen, Loren},
  biburl = {https://www.bibsonomy.org/bibtex/29fbf406b0237ec19dcd9a3e63462ce3a/vngudivada},
  booktitle = {Proceedings of the 7th International Symposium on Wikis and Open Collaboration},
  doi = {10.1145/2038558.2038565},
  interhash = {78133d310066cf6950f98573e089e8f3},
  intrahash = {9fbf406b0237ec19dcd9a3e63462ce3a},
  isbn = {978-1-4503-0909-7},
  keywords = {CitizenScience DataQuality},
  location = {Mountain View, California},
  numpages = {10},
  pages = {29--38},
  publisher = {ACM},
  series = {WikiSym '11},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Quality is a Verb: The Operationalization of Data Quality in a Citizen Science Community},
  url = {http://doi.acm.org/10.1145/2038558.2038565},
  year = 2011
}

@inproceedings{na2001practical,
  abstract = {One of the most important objectives of data engineering is to deliver high quality data to users. In this paper, we (1) discuss the definition, problems and improvements methods of data qwuality, and (2) propose a modeling methodology for improving the multimedia data quality. And then, we introduce the method for improving the quality of semantic data using the proposed methodology. In this example, we propose to measure and remove the data ambiguities for improving the data quality.},
  acmid = {500228},
  added-at = {2016-07-23T18:31:12.000+0200},
  address = {New York, NY, USA},
  author = {Na, Kwan-Sang and Baik, Doo-Kwon and Kim, Pan-Koo},
  biburl = {https://www.bibsonomy.org/bibtex/29a23da620edb39ca2a92b80d0c5a2ac7/vngudivada},
  booktitle = {Proceedings of the Ninth ACM International Conference on Multimedia},
  doi = {10.1145/500141.500228},
  interhash = {e69cf58b5cdf4bd766999776c7365fd7},
  intrahash = {9a23da620edb39ca2a92b80d0c5a2ac7},
  isbn = {1-58113-394-4},
  keywords = {DataQuality MultimediaData},
  location = {Ottawa, Canada},
  numpages = {3},
  pages = {516--518},
  publisher = {ACM},
  series = {MULTIMEDIA '01},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {A Practical Approach for Modeling the Quality of Multimedia Data},
  url = {http://doi.acm.org/10.1145/500141.500228},
  year = 2001
}

@article{manning2015computational,
  added-at = {2016-07-21T22:59:18.000+0200},
  author = {Manning, Christopher D.},
  biburl = {https://www.bibsonomy.org/bibtex/229f52346cb6d8f281b062cdaadbc4961/vngudivada},
  doi = {10.1162/coli_a_00239},
  interhash = {3071cafc83edc420498826a820ca75f0},
  intrahash = {29f52346cb6d8f281b062cdaadbc4961},
  journal = {Computational Linguistics},
  keywords = {ComputationalLinguistics DeepLearning},
  month = dec,
  number = 4,
  pages = {701--707},
  publisher = {{MIT} Press - Journals},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Computational Linguistics and Deep Learning},
  url = {http://dx.doi.org/10.1162/COLI_a_00239},
  volume = 41,
  year = 2015
}

@inproceedings{galarus2014quality,
  abstract = {Quality control for near-real-time spatial-temporal data is often presented from the perspective of the original owner and provider of the data, and focuses on general techniques for outlier detection or uses domain-specific knowledge and rules to assess quality. The impact of quality control on the data aggregator and redistributor is neglected. The focus of this paper is to define and demonstrate quality control measures for real-time, spatial-temporal data from the perspective of the aggregator to provide tools for assessment and optimization of system operation and data redistribution. We define simple measures that account for temporal completeness and spatial coverage. The measures and methods developed are tested on real-world data and applications.},
  acmid = {2666426},
  added-at = {2016-07-23T22:41:44.000+0200},
  address = {New York, NY, USA},
  author = {Galarus, Douglas E. and Angryk, Rafal A.},
  biburl = {https://www.bibsonomy.org/bibtex/20739e307d4bf5e530f313822f250fb63/vngudivada},
  booktitle = {Proceedings of the 22Nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  doi = {10.1145/2666310.2666426},
  interhash = {1cd127347f381a1bf53ae01ab536c342},
  intrahash = {0739e307d4bf5e530f313822f250fb63},
  isbn = {978-1-4503-3131-9},
  keywords = {DataQuality SpatioTemporalData},
  location = {Dallas, Texas},
  numpages = {4},
  pages = {389--392},
  publisher = {ACM},
  series = {SIGSPATIAL '14},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Quality Control from the Perspective of a Near-real-time, Spatial-temporal Data Aggregator and (Re)Distributor},
  url = {http://doi.acm.org/10.1145/2666310.2666426},
  year = 2014
}

@inproceedings{petkov2012oriented,
  abstract = {Service oriented composition is a prospective approach, which enables flexible and loose composition of applications. Data, on the other hand, is an integral part of service. Despite the huge number of studies that have been done on service oriented environments, very little has been investigated about the data quality. In this paper we examine some problems in service-oriented architecture and in particular problem concerned with data quality. In addition, issues associated with data incorporation and how data consistency is realized through data modelling concepts will be investigated.},
  acmid = {2383302},
  added-at = {2016-07-23T20:31:15.000+0200},
  address = {New York, NY, USA},
  author = {Petkov, Plamen and Helfert, Markus},
  biburl = {https://www.bibsonomy.org/bibtex/2538ca241986fc101010a60a8afd8a5d1/vngudivada},
  booktitle = {Proceedings of the 13th International Conference on Computer Systems and Technologies},
  doi = {10.1145/2383276.2383302},
  interhash = {e4de0559fd04d7b12df71d59eded3a43},
  intrahash = {538ca241986fc101010a60a8afd8a5d1},
  isbn = {978-1-4503-1193-9},
  keywords = {DataQuality ServiceArchitecture},
  location = {Ruse, Bulgaria},
  numpages = {8},
  pages = {163--170},
  publisher = {ACM},
  series = {CompSysTech '12},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data Oriented Challenges of Service Architectures a Data Quality Perspective},
  url = {http://doi.acm.org/10.1145/2383276.2383302},
  year = 2012
}

@article{cure2012improving,
  abstract = {Many health care systems and services exploit drug related information stored in databases. The poor data quality of these databases, e.g. inaccuracy of drug contraindications, can lead to catastrophic consequences for the health condition of patients. Hence it is important to ensure their quality in terms of data completeness and soundness. In the database domain, standard Functional Dependencies (FDs) and INclusion Dependencies (INDs), have been proposed to prevent the insertion of incorrect data. But they are generally not expressive enough to represent a domain-specific set of constraints. To this end, conditional dependencies, i.e. standard dependencies extended with tableau patterns containing constant values, have been introduced and several methods have been proposed for their discovery and representation. The quality of drug databases can be considerably improved by their usage. Moreover, pharmacology information is inherently hierarchical and many standards propose graph structures to represent them, e.g. the Anatomical Therapeutic Chemical classification (ATC) or OpenGalen’s terminology. In this article, we emphasize that the technologies of the Semantic Web are adapted to represent these hierarchical structures, i.e. in RDFS and OWL. We also present a solution for representing conditional dependencies using a query language defined for these graph oriented structures, namely SPARQL. The benefits of this approach are interoperability with applications and ontologies of the Semantic Web as well as a reasoning-based query execution solution to clean underlying databases.},
  acmid = {2378019},
  added-at = {2016-07-23T21:40:22.000+0200},
  address = {New York, NY, USA},
  articleno = {3},
  author = {Cur{\'e}, Olivier},
  biburl = {https://www.bibsonomy.org/bibtex/235a48ad2c38bfcd7c915876e8e818b4c/vngudivada},
  doi = {10.1145/2378016.2378019},
  interhash = {bbd3e3d8ffa9aaa5424d224f98973caf},
  intrahash = {35a48ad2c38bfcd7c915876e8e818b4c},
  issn = {1936-1955},
  issue_date = {October 2012},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality DrugDatabase},
  month = oct,
  number = 1,
  numpages = {21},
  pages = {3:1--3:21},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Improving the Data Quality of Drug Databases Using Conditional Dependencies and Ontologies},
  url = {http://doi.acm.org/10.1145/2378016.2378019},
  volume = 4,
  year = 2012
}

@techreport{loshin2011busting,
  abstract = {Even though great strides have been made in data quality improvement over the past decades, many myths and misconceptions are perpetuated through popular articles and presentations. Often, these simplified views can be confusing or conflicting, and those blindly accepting the statements may find their attempts at making discrete progress toward improvement may slow and stall. The goal of this paper is to highlight some common ``myths'' about data quality management, explain why these are myths, and to guide the reader to make better choices when deciding to pursue a data quality management strategy.

 Adding critical insight into different aspects of data quality management and putting some common beliefs into perspective will help you put together a more thoughtful plan for a data quality management program that can lead to measurable improvements in the quality and usability of organizational data. There are no substitutes for good data management disciplines, and this paper will advise the practitioner as to what the critical data issues are in the organization and how to leverage the right tools and technologies to address those issues in the most efficient way.

 Defining and deploying well-­‐defined processes within a culture of data governance will simplify technology acquisition and reduce time to value for implementing a data quality program. Our intent is to provide a balanced view of the best practices for data quality improvement by examining some common statements that can help differentiate what you heard, why it may be a myth, and some considerations to planning your approach to data quality improvement.},
  added-at = {2016-07-23T18:43:19.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2c1091ab44b724540e21ac742242b131d/vngudivada},
  editor = {Loshin, David},
  institution = {Knowledge Integrity, Inc.},
  interhash = {c59fb7fa9935d61f1070a5460a549798},
  intrahash = {c1091ab44b724540e21ac742242b131d},
  keywords = {DataQuality DataQualityManagement},
  month = nov,
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Busting 7 Myths about Data Quality Management},
  year = 2011
}

@techreport{atkinson2016going,
  added-at = {2016-07-22T13:13:51.000+0200},
  author = {Atkinson, Robert D.},
  biburl = {https://www.bibsonomy.org/bibtex/2068ca9f5895d8e355d26eb24552787a5/vngudivada},
  institution = {Information Technology & Innovation Foundation},
  interhash = {0b3ea6bed6ba35b9b99126852d1a1db9},
  intrahash = {068ca9f5895d8e355d26eb24552787a5},
  keywords = {ArtificialGeneralIntelligence},
  month = jun,
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {It is Going to Kill Us and Other Myths About the Future of Artificial Intelligence},
  year = 2016
}

@inproceedings{phannachitta2015consistency,
  abstract = {Data quality is an essential aspect in any empirical study, because the validity of models and/or analysis results derived from an empirical data is inherently influenced by its quality. In this empirical study, we focus on data consistency as a critical factor influencing the accuracy of prediction models in software engineering. We propose a software metric called Cases Inconsistency Level (CIL) for analyzing conflicts within software engineering data sets by leveraging probability statistics on project cases and counting the number of conflicting pairs. The result demonstrated that CIL is able to be used as a metric to identify either consistent data sets or inconsistent data sets, which are valuable for building robust prediction models. In addition to measuring the level of consistency, CIL is proved to be applicable to predict whether or not an effort model built from data set can achieve higher accuracy, an important indicator for empirical experiments in software engineering.},
  acmid = {2745820},
  added-at = {2016-07-23T18:48:24.000+0200},
  address = {New York, NY, USA},
  articleno = {19},
  author = {Phannachitta, Passakorn and Monden, Akito and Keung, Jacky and Matsumoto, Kenichi},
  biburl = {https://www.bibsonomy.org/bibtex/282a10acf0f99ece3895720b6e3d729c0/vngudivada},
  booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
  doi = {10.1145/2745802.2745820},
  interhash = {f8e51431dd0399ecc353716dbabea133},
  intrahash = {82a10acf0f99ece3895720b6e3d729c0},
  isbn = {978-1-4503-3350-4},
  keywords = {DataQuality SoftwareEngineeringData},
  location = {Nanjing, China},
  numpages = {10},
  pages = {19:1--19:10},
  publisher = {ACM},
  series = {EASE '15},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Case Consistency: A Necessary Data Quality Property for Software Engineering Data Sets},
  url = {http://doi.acm.org/10.1145/2745802.2745820},
  year = 2015
}

@inproceedings{shirai2014initial,
  abstract = {To meet critical business challenges, software development teams need data to effectively manage product quality, cost, and schedule. The Team Software ProcessSM (TSPSM) provides a framework that teams use to collect software process data in real time, using a defined disciplined process. This data holds promise for use in software engineering research. We combined data from 109 industrial projects into a database to support performance benchmarking and model development. But is the data of sufficient quality to draw conclusions? We applied various tests and techniques to identify data anomalies that affect the quality of the data in several dimensions. In this paper, we report some initial results of our analysis, describing the amount and the rates of identified anomalies and suspect data, including incorrectness, inconsistency, and credibility. To illustrate the types of data available for analysis, we provide three examples. The preliminary results of this empirical study suggest that some aspects of the data quality are good and the data are generally credible, but size data are often missing.},
  acmid = {2600841},
  added-at = {2016-07-23T21:45:27.000+0200},
  address = {New York, NY, USA},
  author = {Shirai, Yasutaka and Nichols, William and Kasunic, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/2c8986cc0e23fd73b084fa2ce7cfd581e/vngudivada},
  booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
  doi = {10.1145/2600821.2600841},
  interhash = {1c895ee3d8114a9ee575b3ff02e9dbb1},
  intrahash = {c8986cc0e23fd73b084fa2ce7cfd581e},
  isbn = {978-1-4503-2754-1},
  keywords = {DataQuality EngineeringData},
  location = {Nanjing, China},
  numpages = {5},
  pages = {25--29},
  publisher = {ACM},
  series = {ICSSP 2014},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Initial Evaluation of Data Quality in a TSP Software Engineering Project Data Repository},
  url = {http://doi.acm.org/10.1145/2600821.2600841},
  year = 2014
}

@book{diez2015openintro,
  added-at = {2016-07-18T04:20:30.000+0200},
  author = {Diez, David M. and Barr, Christopher D. and Çetinkaya-Rundel, Mine},
  biburl = {https://www.bibsonomy.org/bibtex/2a4e881a3ed87390f08b5e2d42b4479ba/vngudivada},
  interhash = {97a9747391e5f2a29bcaddd254e7c220},
  intrahash = {a4e881a3ed87390f08b5e2d42b4479ba},
  isbn = {9781943450039 194345003X},
  keywords = {Book Statistics},
  publisher = {https://www.openintro.org/},
  refid = {920524548},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {OpenIntro statistics},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9781943450039},
  year = 2015
}

@misc{amir2016corelet,
  abstract = {Marching along the DARPA SyNAPSE roadmap, IBM unveils a trilogy of innovations towards the TrueNorth cognitive computing system inspired by the brain's functionand efficiency. The sequential programming paradigm of thevon Neumann architecture is wholly unsuited for TrueNorth. Therefore, as our main contribution, we develop a new programming paradigm that permits construction of complex cognitive algorithms and applications while being efficient for TrueNorth and effective for programmer productivity. The programming paradigm consists of (a) an abstraction for a TrueNorth program, named Corelet, for representing a network of neurosynaptic cores that encapsulates all details except external inputs andoutputs; (b) an object-oriented Corelet Language for creating, composing, and decomposing corelets; (c) a Corelet Library that acts as an ever-growing repository of reusable corelets from which programmers compose new corelets; and (d) an end-to-end Corelet Laboratory that is a programming environment whichintegrates with the TrueNorth architectural simulator, Compass, to support all aspects of the programming cycle from design,through development, debugging, and up to deployment. Thenew paradigm seamlessly scales from a handful of synapses andneurons to networks of neurosynaptic cores of progressively increasing size and complexity. The utility of the new programming paradigm is underscored by the fact that we have designed and implemented more than 100 algorithms as corelets for TrueNorth in a very short time span.},
  added-at = {2016-07-18T02:54:51.000+0200},
  author = {Amir, Arnon and Datta, Pallab and Risk, William P. and Cassidy, Andrew S. and Kusnitz, Jeffrey A. and Esser, Steve K. and Andreopoulos, Alexander and Wong, Theodore M. and Flickner, Myron and Alvarez-Icaza, Rodrigo and McQuinn, Emmett and Shaw, Ben and Pass, Norm and Modha, Dharmendra S.},
  biburl = {https://www.bibsonomy.org/bibtex/297b870aad1164cdf285dd36390ab13fc/vngudivada},
  interhash = {d55ee6c286a611290f6f22e2b248cd38},
  intrahash = {97b870aad1164cdf285dd36390ab13fc},
  keywords = {CognitiveComputing CoreletLanguage NeurosynapticCore TrueNorth},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {A Corelet Language for Composing Networks of Neurosynaptic Cores},
  year = 2016
}

@inproceedings{sadiq2011years,
  abstract = {Data Quality is a cross-disciplinary and often domain specific problem due to the importance of fitness for use in the definition of data quality metrics. It has been the target of research and development for over 4 decades by business analysts, solution architects, database experts and statisticians to name a few. However, the changing landscape of data quality challenges indicate the need for holistic solutions. As a first step towards bridging any gaps between the various research communities, we undertook a comprehensive literature study of data quality research published in the last two decades. In this study we considered a broad range of Information System (IS) and Computer Science (CS) publication (conference and journal) outlets. The main aims of the study were to understand the current landscape of data quality research, to create better awareness of (lack of) synergies between various research communities, and, subsequently, to direct attention towards holistic solutions. In this paper, we present a summary of the findings from the study, that include a taxonomy of data quality problems, identification of the top themes, outlets and main trends in data quality research, as well as a detailed thematic analysis that outlines the overlaps and distinctions between the focus of IS and CS publications.},
  acmid = {2460415},
  added-at = {2016-07-23T17:41:18.000+0200},
  address = {Darlinghurst, Australia, Australia},
  author = {Sadiq, Shazia and Yeganeh, Naiem Khodabandehloo and Indulska, Marta},
  biburl = {https://www.bibsonomy.org/bibtex/2322c0bf8188b7cae119f22ed86c45afa/vngudivada},
  booktitle = {Proceedings of the Twenty-Second Australasian Database Conference - Volume 115},
  interhash = {a28a7027b416d2376ac1ec3da2f84792},
  intrahash = {322c0bf8188b7cae119f22ed86c45afa},
  isbn = {978-1-920682-95-8},
  keywords = {DataQuality},
  location = {Perth, Australia},
  numpages = {10},
  pages = {153--162},
  publisher = {Australian Computer Society, Inc.},
  series = {ADC '11},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {20 Years of Data Quality Research: Themes, Trends and Synergies},
  url = {http://dl.acm.org/citation.cfm?id=2460396.2460415},
  year = 2011
}

@article{lauria2011combining,
  abstract = {This article analyzes the data quality issues that emerge when training a shrinkage-based classifier with noisy data. A statistical text analysis technique based on a shrinkage-based variation of multinomial naive Bayes is applied to a set of free-text discharge diagnoses occurring in a number of hospitalizations. All of these diagnoses were previously coded according to the Spanish Edition of ICD9-CM. We deal with the issue of analyzing the predictive power and robustness of the statistical machine learning algorithm proposed for ICD-9-CM classification. We explore the effect of training the models using both clean and noisy data. In particular our work investigates the extent to which errors in free-text diagnoses propagate to the classification model. A measure of predictive accuracy is calculated for the text classification algorithm under analysis. Subsequently, the quality of the sample data is incrementally deteriorated by simulating errors in the text and/or codes. The predictive accuracy is recomputed for each of the noisy samples for comparison purposes. Our research shows that the shrinkage-based classifier is a valid alternative to automate ICD9-CM coding even under circumstances in which the quality of the training data is in question.},
  acmid = {2063506},
  added-at = {2016-07-23T20:23:10.000+0200},
  address = {New York, NY, USA},
  articleno = {13},
  author = {Laur\'{\i}a, Eitel J. M. and March, Alan D.},
  biburl = {https://www.bibsonomy.org/bibtex/28f7afc535e645bfec0ce6b4b854583af/vngudivada},
  doi = {10.1145/2063504.2063506},
  interhash = {c454f9bf7bb2b2dd369de22a8535fc1e},
  intrahash = {8f7afc535e645bfec0ce6b4b854583af},
  issn = {1936-1955},
  issue_date = {December 2011},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality Healthcare},
  month = dec,
  number = 3,
  numpages = {22},
  pages = {13:1--13:22},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Combining Bayesian Text Classification and Shrinkage to Automate Healthcare Coding: A Data Quality Analysis},
  url = {http://doi.acm.org/10.1145/2063504.2063506},
  volume = 2,
  year = 2011
}

@article{knight2005developing,
  abstract = {The rapid growth of the Internet as an environment for information exchange and the lack of enforceable standards regarding the information it contains has lead to numerous information quality problems. A major issue is the inability of Search Engine technology to wade through the vast expanse of questionable content and return "quality" results to a user's query. This paper attempts to address some of the issues involved in determining what quality is, as it pertains to information retrieval on the Internet. The IQIP model is presented as an approach to managing the choice and implementation of quality related algorithms of an Internet crawling Search Engine.},
  added-at = {2016-07-23T20:54:09.000+0200},
  author = {Knight, Shirlee-ann and Burn, Janice},
  biburl = {https://www.bibsonomy.org/bibtex/2f97c7ff8c954b01bc7aaa72cf9f5300e/vngudivada},
  interhash = {7d90a9eb8d2231018730a1ae410aea16},
  intrahash = {f97c7ff8c954b01bc7aaa72cf9f5300e},
  keywords = {DataQuality WebData},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Developing a Framework for Assessing Information Quality on the World Wide Web },
  year = 2005
}

@article{batini2009methodologies,
  abstract = {The literature provides a wide range of techniques to assess and improve the quality of data. Due to the diversity and complexity of these techniques, research has recently focused on defining methodologies that help the selection, customization, and application of data quality assessment and improvement techniques. The goal of this article is to provide a systematic and comparative description of such methodologies. Methodologies are compared along several dimensions, including the methodological phases and steps, the strategies and techniques, the data quality dimensions, the types of data, and, finally, the types of information systems addressed by each methodology. The article concludes with a summary description of each methodology.},
  acmid = {1541883},
  added-at = {2016-07-23T22:28:21.000+0200},
  address = {New York, NY, USA},
  articleno = {16},
  author = {Batini, Carlo and Cappiello, Cinzia and Francalanci, Chiara and Maurino, Andrea},
  biburl = {https://www.bibsonomy.org/bibtex/29a704add4eed6ca0a2c10f4b507f3aba/vngudivada},
  doi = {10.1145/1541880.1541883},
  interhash = {853fb622eb2b921a2b9c2db31053c096},
  intrahash = {9a704add4eed6ca0a2c10f4b507f3aba},
  issn = {0360-0300},
  issue_date = {July 2009},
  journal = {ACM Comput. Surv.},
  keywords = {DataQuality DataQualityAssessment},
  month = jul,
  number = 3,
  numpages = {52},
  pages = {16:1--16:52},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Methodologies for Data Quality Assessment and Improvement},
  url = {http://doi.acm.org/10.1145/1541880.1541883},
  volume = 41,
  year = 2009
}

@article{wang1996beyond,
  abstract = { Abstract:Poor data quality (DQ) can have substantial social and economic impacts. Although firms are improving data quality with practical approaches and tools, their improvement efforts tend to focus narrowly on accuracy. We believe that data consumers have a much broader data quality conceptualization than IS professionals realize. The purpose of this paper is to develop a framework that captures the aspects of data quality that are important to data consumers.A two-stage survey and a two-phase sorting study were conducted to develop a hierarchical framework for organizing data quality dimensions. This framework captures dimensions of data quality that are important to data consumers. Intrinsic DQ denotes that data have quality in their own right. Contextual DQ highlights the requirement that data quality must be considered within the context ofthe task at hand. Representational DQ and accessibility DQ emphasize the importance of the role of systems. These findings are consistent with our understanding that high-quality data should be intrinsically good, contextually appropriate for the task, clearly represented, and accessible to the data consumer.Our framework has been used effectively in industry and government. Using this framework, IS managers were able to better understand and meet their data consumers’ data quality needs. The salient feature of this research study is that quality attributes of data are collected from data consumers instead of being defined theoretically or based on researchers’ experience. Although exploratory, this research provides a basis for future studies that measure data quality along the dimensions of this framework. },
  added-at = {2016-07-23T18:36:25.000+0200},
  author = {Wang, Richard Y. and Strong, Diane M.},
  biburl = {https://www.bibsonomy.org/bibtex/2a7bfb110f3e2216d961cb0d908a31527/vngudivada},
  doi = {10.1080/07421222.1996.11518099},
  eprint = {http://dx.doi.org/10.1080/07421222.1996.11518099},
  interhash = {3ec26cd111d26e2dc1b8d0bb0669bd91},
  intrahash = {a7bfb110f3e2216d961cb0d908a31527},
  journal = {Journal of Management Information Systems},
  keywords = {DataQuality},
  number = 4,
  pages = {5-33},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Beyond Accuracy: What Data Quality Means to Data Consumers},
  url = {http://dx.doi.org/10.1080/07421222.1996.11518099},
  volume = 12,
  year = 1996
}

@book{grossi2014judgment,
  abstract = {Judgment aggregation is a mathematical theory of collective decision-making. It concerns the methods whereby individual opinions about logically interconnected issues of interest can, or cannot, be aggregated into one collective stance. Aggregation problems have traditionally been of interest for disciplines like economics and the political sciences, as well as philosophy, where judgment aggregation itself originates from, but have recently captured the attention of disciplines like computer science, artificial intelligence and multi-agent systems. Judgment aggregation has emerged in the last decade as a unifying paradigm for the formalization and understanding of aggregation problems. Still, no comprehensive presentation of the theory is available to date. This Synthesis Lecture aims at filling this gap presenting the key motivations, results, abstractions and techniques underpinning it.},
  added-at = {2016-07-15T04:17:46.000+0200},
  author = {Grossi, Davide and Pigozzi, Gabriella},
  biburl = {https://www.bibsonomy.org/bibtex/24a0a7e7dd529483ec4561b4c9052183b/vngudivada},
  interhash = {10f09fe588a08c56dca909efaeda535f},
  intrahash = {4a0a7e7dd529483ec4561b4c9052183b},
  isbn = {9781627050883 1627050884},
  keywords = {JudgmentAggregation SynthesisLecture},
  refid = {877885347},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Judgment aggregation: a primer},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1663904},
  year = 2014
}

@book{lopez2013casebased,
  abstract = {Case-based reasoning is a methodology with a long tradition in artificial intelligence that brings together reasoning and machine learning techniques to solve problems based on past experiences or cases. Given a problem to be solved, reasoning involves the use of methods to retrieve similar past cases in order to reuse their solution for the problem at hand. Once the problem has been solved, learning methods can be applied to improve the knowledge based on past experiences. In spite of being a broad methodology applied in industry and services, case-based reasoning has often been forgotten in both artificial intelligence and machine learning books. The aim of this book is to present a concise introduction to case-based reasoning providing the essential building blocks for the design of case-based reasoning systems, as well as to bring together the main research lines in this field to encourage students to solve current CBR challenges.},
  added-at = {2016-07-15T04:23:11.000+0200},
  address = {San Rafael},
  author = {López, Beatriz},
  biburl = {https://www.bibsonomy.org/bibtex/207d4e3a0b70605f32490b980528f4c8f/vngudivada},
  interhash = {7a72b8418d5d9f5534d8d7da8583e389},
  intrahash = {07d4e3a0b70605f32490b980528f4c8f},
  isbn = {1627050078 9781627050074},
  keywords = {CaseBasedReasoning SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {881112340},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Case-Based Reasoning},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9781627050074},
  year = 2013
}

@inproceedings{chalamalla2014descriptive,
  abstract = {Data cleaning techniques usually rely on some quality rules to identify violating tuples, and then fix these violations using some repair algorithms. Oftentimes, the rules, which are related to the business logic, can only be defined on some target report generated by transformations over multiple data sources. This creates a situation where the violations detected in the report are decoupled in space and time from the actual source of errors. In addition, applying the repair on the report would need to be repeated whenever the data sources change. Finally, even if repairing the report is possible and affordable, this would be of little help towards identifying and analyzing the actual sources of errors for future prevention of violations at the target. In this paper, we propose a system to address this decoupling. The system takes quality rules defined over the output of a transformation and computes explanations of the errors seen on the output. This is performed both at the target level to describe these errors and at the source level to prescribe actions to solve them. We present scalable techniques to detect, propagate, and explain errors. We also study the effectiveness and efficiency of our techniques using the TPC-H Benchmark for different scenarios and classes of quality rules.},
  acmid = {2610520},
  added-at = {2016-07-23T18:52:27.000+0200},
  address = {New York, NY},
  author = {Chalamalla, Anup and Ilyas, Ihab F. and Ouzzani, Mourad and Papotti, Paolo},
  biburl = {https://www.bibsonomy.org/bibtex/29acfd254d3beef71d67d843841de5fde/vngudivada},
  booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
  doi = {10.1145/2588555.2610520},
  interhash = {09fb773b7b08780edb359e4429428e8a},
  intrahash = {9acfd254d3beef71d67d843841de5fde},
  isbn = {978-1-4503-2376-5},
  keywords = {DataCleaning DataQuality},
  location = {Snowbird, Utah, USA},
  numpages = {12},
  pages = {445 -- 456},
  publisher = {ACM},
  series = {SIGMOD '14},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Descriptive and Prescriptive Data Cleaning},
  url = {http://doi.acm.org/10.1145/2588555.2610520},
  year = 2014
}

@misc{naumann2009guest,
  added-at = {2016-07-23T21:36:16.000+0200},
  author = {Naumann, Felix and Raschid, Louiqa},
  biburl = {https://www.bibsonomy.org/bibtex/2e0b3eb0e8f3d7989209df15dcda875bc/vngudivada},
  interhash = {35565f31e7ea6447036db78a996f09b9},
  intrahash = {e0b3eb0e8f3d7989209df15dcda875bc},
  keywords = {DBMS DataQuality},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Guest Editorial for the Special Issue on Data Quality in Databases},
  year = 2009
}

@inproceedings{helfert2010certifying,
  abstract = {Many researchers and practitioners have been attracted to improve data quality due to its monumental importance as a key success factor. Mathematical and statistical models have been deployed to information systems to introduce constrain and transaction based mechanisms to prevent data quality related problems. Entire management of the process and roles involved in data generation has also been scrutinized. Vast amount of knowledge base has been progressed in this area; however, most of the approaches are limited from practical perspective. System development process incorporating quality modelling is rarely integrated. Quality related meta data is absent from most information system. Neither process mapping nor data modelling provides sufficient provision to measure quality or certification of data in the information systems. Furthermore, ongoing monitoring of data for quality conformance through a separate process is expensive and time consuming. Recognising this limitation and aiming to provide a practical-orient comprehensive approach, we propose a process centric quality focused system design incorporating data product quality, conformance monitoring and certification. In this paper we focus on the self certification of data quality based on our earlier work on the process centric framework for ongoing data quality monitoring.},
  acmid = {1839397},
  added-at = {2016-07-23T18:50:22.000+0200},
  address = {New York, NY, USA},
  author = {Helfert, Markus and Hossain, Fakir},
  biburl = {https://www.bibsonomy.org/bibtex/29c63953ef05a2be360184a7d305f0c1c/vngudivada},
  booktitle = {Proceedings of the 11th International Conference on Computer Systems and Technologies and Workshop for PhD Students in Computing on International Conference on Computer Systems and Technologies},
  doi = {10.1145/1839379.1839397},
  interhash = {64c87b03fccb67e95dd1bffec0a136cc},
  intrahash = {9c63953ef05a2be360184a7d305f0c1c},
  isbn = {978-1-4503-0243-2},
  keywords = {DataQuality DataQualityConformance},
  location = {Sofia, Bulgaria},
  numpages = {6},
  pages = {95--100},
  publisher = {ACM},
  series = {CompSysTech '10},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Certifying Data Quality Conformance},
  url = {http://doi.acm.org/10.1145/1839379.1839397},
  year = 2010
}

@inproceedings{lindberg2013generating,
  added-at = {2016-07-16T02:20:22.000+0200},
  author = {Lindberg, David and Popowich, Fred and Nesbit, John and Winne, Phil},
  biburl = {https://www.bibsonomy.org/bibtex/2a3f8bfda6b987db8a196805e4ae02cc0/vngudivada},
  booktitle = {Proceedings of the 14th European Workshop on Natural Language Generation},
  interhash = {5d78f47c14da3a77a759b0b8ad7a58cc},
  intrahash = {a3f8bfda6b987db8a196805e4ae02cc0},
  keywords = {QuestionGeneration},
  pages = {105-114},
  publisher = {Association for Computational Linguistics},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Generating Natural Language Questions to Support Learning On-Line},
  year = 2013
}

@inproceedings{farinha2009using,
  abstract = {Currently available data quality tools provide development environments that significantly decrease the effort in dealing with common data problems, such as those related with attribute domain validation, syntax checking, or value matching against a reference master data repository. On the contrary, more complex and specific data quality functionalities, whose requirements usually derive from application domain business rules, have to be developed from scratch, usually leading to high costs of development and maintenance. This paper introduces the concept of inheritance in a metadata-driven approach to simplified data quality rule management. The approach is based on the belief that even complex data quality rules very often adhere to recurring patterns that can be encoded and encapsulated as reusable, abstract templates. The approach is supported by a metamodel developed on top of OMG's Common Warehouse Metamodel, herein extended with the ability to derive new rule patterns from existing ones, through inheritance. The inheritance metamodel is presented in UML and its application is illustrated with a running example.},
  acmid = {1651417},
  added-at = {2016-07-23T20:56:40.000+0200},
  address = {New York, NY, USA},
  author = {Farinha, Jos{\'e} and Trigueiros, Maria Jos{\'e} and Belo, Orlando},
  biburl = {https://www.bibsonomy.org/bibtex/2f78a361b478390539753019d28a23509/vngudivada},
  booktitle = {Proceedings of the First International Workshop on Model Driven Service Engineering and Data Quality and Security},
  doi = {10.1145/1651415.1651417},
  interhash = {01ff22688165f728a1b3a536e01e0505},
  intrahash = {f78a361b478390539753019d28a23509},
  isbn = {978-1-60558-816-2},
  keywords = {DataQuality DataQualityAssessment Metadata},
  location = {Hong Kong, China},
  numpages = {8},
  pages = {1--8},
  publisher = {ACM},
  series = {MoSE+DQS '09},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Using Inheritance in a Metadata Based Approach to Data Quality Assessment},
  url = {http://doi.acm.org/10.1145/1651415.1651417},
  year = 2009
}

@inproceedings{bosu2013quality,
  abstract = {Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to September 2012. A total of 221 relevant studies met our inclusion criteria and were characterized in terms of their consideration and treatment of data quality. Results: We obtained useful insights as to how the ESE community considers these three elements of data quality. Only 23 of these 221 studies reported on all three elements of data quality considered in this paper. Conclusion: The reporting of data collection procedures is not documented consistently in ESE studies. It will be useful if data collection challenges are reported in order to improve our understanding of why there are problems with software engineering data sets and the models developed from them. More generally, data quality should be given far greater attention by the community. The improvement of data sets through enhanced data collection, pre-processing and quality assessment should lead to more reliable prediction models, thus improving the practice of software engineering.},
  acmid = {2461024},
  added-at = {2016-07-23T20:58:24.000+0200},
  address = {New York, NY, USA},
  author = {Bosu, Michael Franklin and MacDonell, Stephen G.},
  biburl = {https://www.bibsonomy.org/bibtex/26ecfd001b03126b7ac3a990af58818be/vngudivada},
  booktitle = {Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering},
  doi = {10.1145/2460999.2461024},
  interhash = {ea38025fc13223c38015f041dcfcd230},
  intrahash = {6ecfd001b03126b7ac3a990af58818be},
  isbn = {978-1-4503-1848-8},
  keywords = {DataQuality SoftwareEngineeringData},
  location = {Porto de Galinhas, Brazil},
  numpages = {6},
  pages = {171--176},
  publisher = {ACM},
  series = {EASE '13},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Data Quality in Empirical Software Engineering: A Targeted Review},
  url = {http://doi.acm.org/10.1145/2460999.2461024},
  year = 2013
}

@inproceedings{naim2010monitoring,
  abstract = {Data quality is an important component of modern scientific discovery. Many scientific discovery processes consume data from a diverse array of resources such as streaming sensor networks, web services, and databases. The validity of a scientific computation's results is highly dependent on the quality of these input data. Scientific workflow systems are being increasingly used to automate scientific computations by facilitating experiment design, data capture, integration, processing, and analysis. These workflows may execute in grid or cloud environments, and if the data produced during workflow execution is deemed unusable or low in quality, execution should stop to prevent wasting these valuable resources. We propose an approach in the Kepler scientific workflow system for monitoring data quality and demonstrate its use for oceanography and bioinformatics domains.},
  acmid = {1851558},
  added-at = {2016-07-23T22:30:34.000+0200},
  address = {New York, NY, USA},
  author = {Na'im, Aisa and Crawl, Daniel and Indrawan, Maria and Altintas, Ilkay and Sun, Shulei},
  biburl = {https://www.bibsonomy.org/bibtex/2097a39b965bd606cb8ab668f512092f6/vngudivada},
  booktitle = {Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
  doi = {10.1145/1851476.1851558},
  interhash = {953ee7ff6ab92ed4b9746863a35f1b12},
  intrahash = {097a39b965bd606cb8ab668f512092f6},
  isbn = {978-1-60558-942-8},
  keywords = {DataQuality DataQualityMonitoring ScientificData},
  location = {Chicago, Illinois},
  numpages = {5},
  pages = {560--564},
  publisher = {ACM},
  series = {HPDC '10},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Monitoring Data Quality in Kepler},
  url = {http://doi.acm.org/10.1145/1851476.1851558},
  year = 2010
}

@inproceedings{fan2008dependencies,
  abstract = {Dependency theory is almost as old as relational databases themselves, and has traditionally been used to improve the quality of schema, among other things. Recently there has been renewed interest in dependencies for improving the quality of data. The increasing demand for data quality technology has also motivated revisions of classical dependencies, to capture more inconsistencies in real-life data, and to match, repair and query the inconsistent data. This paper aims to provide an overview of recent advances in revising classical dependencies for improving data quality.},
  acmid = {1376940},
  added-at = {2016-07-23T21:24:35.000+0200},
  address = {New York, NY, USA},
  author = {Fan, Wenfei},
  biburl = {https://www.bibsonomy.org/bibtex/24962c20b1f21e187f48c7a18f35053c7/vngudivada},
  booktitle = {Proceedings of the Twenty-seventh ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
  doi = {10.1145/1376916.1376940},
  interhash = {28898788e44544bc1cfe23c33ca4f50f},
  intrahash = {4962c20b1f21e187f48c7a18f35053c7},
  isbn = {978-1-60558-152-1},
  keywords = {DataQuality},
  location = {Vancouver, Canada},
  numpages = {12},
  pages = {159--170},
  publisher = {ACM},
  series = {PODS '08},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Dependencies Revisited for Improving Data Quality},
  url = {http://doi.acm.org/10.1145/1376916.1376940},
  year = 2008
}

@book{mausam2012planning,
  abstract = {Markov Decision Processes (MDPs) are widely popular in Artificial Intelligence for modeling sequential decision-making scenarios with probabilistic dynamics. They are the framework of choice when designing an intelligent agent that needs to act for long periods of time in an environment where its actions could have uncertain outcomes. MDPs are actively researched in two related subareas of AI, probabilistic planning and reinforcement learning. Probabilistic planning assumes known models for the agent's goals and domain dynamics, and focuses on determining how the agent should behave to achieve its objectives. On the other hand, reinforcement learning additionally learns these models based on the feedback the agent gets from the environment. This book provides a concise introduction to the use of MDPs for solving probabilistic planning problems, with an emphasis on the algorithmic perspective. It covers the whole spectrum of the field, from the basics to state-of-the-art optimal and approximation algorithms. We first describe the theoretical foundations of MDPs and the fundamental solution techniques for them. We then discuss modern optimal algorithms based on heuristic search and the use of structured representations. A major focus of the book is on the numerous approximation schemes for MDPs that have been developed in the AI literature. These include determinization-based approaches, sampling techniques, heuristic functions, dimensionality reduction, and hierarchical representations. Finally, we briefly introduce several extensions of the standard MDP classes that model and solve even more complex planning problems.},
  added-at = {2016-07-15T04:49:49.000+0200},
  author = {Mausam and Kolobov, Andrey},
  biburl = {https://www.bibsonomy.org/bibtex/2888fb5a37f18cc513417a161757b6023/vngudivada},
  doi = {10.2200/S00426ED1V01Y201206AIM017},
  eprint = {http://dx.doi.org/10.2200/S00426ED1V01Y201206AIM017},
  interhash = {c59b24bf5809d97caa7f214cf575f73a},
  intrahash = {888fb5a37f18cc513417a161757b6023},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  keywords = {AI MarkovDecisionProcess SynthesisLecture},
  number = 1,
  pages = {1-210},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Planning with Markov Decision Processes: An AI Perspective},
  url = {http://dx.doi.org/10.2200/S00426ED1V01Y201206AIM017},
  volume = 6,
  year = 2012
}

@inproceedings{chu2015katara,
  abstract = {Classical approaches to clean data have relied on using integrity constraints, statistics, or machine learning. These approaches are known to be limited in the cleaning accuracy, which can usually be improved by consulting master data and involving experts to resolve ambiguity. The advent of knowledge bases KBs both general-purpose and within enterprises, and crowdsourcing marketplaces are providing yet more opportunities to achieve higher accuracy at a larger scale. We propose KATARA, a knowledge base and crowd powered data cleaning system that, given a table, a KB, and a crowd, interprets table semantics to align it with the KB, identifies correct and incorrect data, and generates top-k possible repairs for incorrect data. Experiments show that KATARA can be applied to various datasets and KBs, and can efficiently annotate data and suggest possible repairs.},
  acmid = {2749431},
  added-at = {2016-07-23T20:21:47.000+0200},
  address = {New York, NY, USA},
  author = {Chu, Xu and Morcos, John and Ilyas, Ihab F. and Ouzzani, Mourad and Papotti, Paolo and Tang, Nan and Ye, Yin},
  biburl = {https://www.bibsonomy.org/bibtex/271bb89647a43b0f72949052bca7e947b/vngudivada},
  booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  doi = {10.1145/2723372.2749431},
  interhash = {b17217b803602797a11015647d1280dd},
  intrahash = {71bb89647a43b0f72949052bca7e947b},
  isbn = {978-1-4503-2758-9},
  keywords = {CrowdSourcing DataCleaning DataQuality},
  location = {Melbourne, Victoria, Australia},
  numpages = {15},
  pages = {1247--1261},
  publisher = {ACM},
  series = {SIGMOD '15},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {KATARA: A Data Cleaning System Powered by Knowledge Bases and Crowdsourcing},
  url = {http://doi.acm.org/10.1145/2723372.2749431},
  year = 2015
}

@book{stone2007intelligent,
  abstract = {Robotics technology has recently advanced to the point of being widely accessible for relatively low-budget research, as well as for graduate, undergraduate, and even secondary and primary school education. This lecture provides an example of how to productively use a cutting-edge advanced robotics platform for education and research by providing a detailed case study with the Sony AIBO robot, a vision-based legged robot. The case study used for this lecture is the UT Austin Villa RoboCup Four-Legged Team. This lecture describes both the development process and the technical details of its end result. The main contributions of this lecture are (i) a roadmap for new classes and research groups interested in intelligent autonomous robotics who are starting from scratch with a new robot, and (ii) documentation of the algorithms behind our own approach on the AIBOs with the goal of making them accessible for use on other vision-based and/or legged robot platforms.},
  added-at = {2016-07-15T04:47:02.000+0200},
  address = {[San Rafael, Calif.]},
  author = {Stone, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/23318c059940b2c89b07c11fb98349d38/vngudivada},
  interhash = {c8917a03dfc476fa0d6503dd9f5358b4},
  intrahash = {3318c059940b2c89b07c11fb98349d38},
  isbn = {1598291270 9781598291278},
  keywords = {AutonomousRobot Soccer SynthesisLecture},
  publisher = {Morgan & Claypool Publishers},
  refid = {156719348},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Intelligent autonomous robotics: a robot soccer case study},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=881374},
  year = 2007
}

@book{bazzan2014introduction,
  abstract = {Urban mobility is not only one of the pillars of modern economic systems, but also a key issue in the quest for equality of opportunity, once it can improve access to other services. Currently, however, there are a number of negative issues related to traffic, especially in mega-cities, such as economical issues (cost of opportunity caused by delays), environmental (externalities related to emissions of pollutants), and social (traffic accidents). Solutions to these issues are more and more closely tied to information and communication technology. Indeed, a search in the technical literature (using the keyword "urban traffic" to filter out articles on data network traffic) retrieved the following number of articles (as of December 3, 2013): 9,443 (ACM Digital Library), 26,054 (Scopus), and 1,730,000 (Google Scholar). Moreover, articles listed in the ACM query relate to conferences as diverse as MobiCom, CHI, PADS, and AAMAS. This means that there is a big and diverse community of computer scientists and computer engineers who tackle research that is connected to the development of intelligent traffic and transportation systems. It is also possible to see that this community is growing, and that research projects are getting more and more interdisciplinary. To foster the cooperation among the involved communities, this book aims at giving a broad introduction into the basic but relevant concepts related to transportation systems, targeting researchers and practitioners from computer science and information technology. In addition, the second part of the book gives a panorama of some of the most exciting and newest technologies, originating in computer science and computer engineering, that are now being employed in projects related to car-to-car communication, interconnected vehicles, car navigation, platooning, crowd sensing and sensor networks, among others. This material will also be of interest to engineers and researchers from the traffic and transportation community.},
  added-at = {2016-07-15T04:25:42.000+0200},
  author = {Bazzan, Ana L. C. and Klügl, Franziska},
  biburl = {https://www.bibsonomy.org/bibtex/23fb902267d98393bc49cff3b63449699/vngudivada},
  interhash = {8ea808e4cd3d1fed06919f7785501047},
  intrahash = {3fb902267d98393bc49cff3b63449699},
  isbn = {9781627052085 1627052089 1627052070 9781627052078},
  keywords = {IntelligentTransportationSystem SynthesisLecture},
  refid = {867203780},
  timestamp = {2019-03-25T17:13:22.000+0100},
  title = {Introduction to intelligent systems in traffic and transportation},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1585294},
  year = 2014
}

@misc{capgemini2016quality,
  added-at = {2016-07-24T04:43:35.000+0200},
  author = {CapGemini},
  biburl = {https://www.bibsonomy.org/bibtex/2adeaa3ceccd96877c1ad1624d5b61cd8/vngudivada},
  interhash = {7a14b49a5b9d92d7657a7047448f5204},
  intrahash = {adeaa3ceccd96877c1ad1624d5b61cd8},
  keywords = {DataQuality},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data Quality Assurance Platform for Financial Institutions},
  year = 2016
}

@techreport{talend2016pillars,
  abstract = {Big data represents a significant paradigm shift in enterprise technology. Big data radically changes the nature of the data management profession as it introduces new concerns about the volume, velocity and variety of corporate data. This change means that we must also adjust technologies and strategies when fueling the corporation with valuable and actionable information. Big data enables companies to gain insight into new business opportunities (and threats) and stands to transform much of what the modern enterprise is today.},
  added-at = {2016-07-25T00:05:25.000+0200},
  author = {talend},
  biburl = {https://www.bibsonomy.org/bibtex/2f6b72cdf7d880ce56b1692d82c4df3df/vngudivada},
  interhash = {4c1d274e1f6424776ea1ae49a9c57b16},
  intrahash = {f6b72cdf7d880ce56b1692d82c4df3df},
  keywords = {BigData DataQuality},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Four Key Pillars To A Big Data Management Solution},
  year = 2016
}

@techreport{karr2005quality,
  abstract = {We present the old-but–new problem of data quality from a statistical perspective, in part with the goal of attracting more statisticians, especially academics, to become engaged in research on a rich set of exciting challenges. The data quality landscape is described, and its research foundations in computer science, total quality management and statistics are reviewed. Two case studies based on an EDA approach to data quality are used to motivate a set of research challenges for statistics that span theory, methodology and software tools.},
  added-at = {2016-07-24T04:39:12.000+0200},
  author = {Karr, Alan F. and Sanil, Ashish P. and Banks, David L.},
  biburl = {https://www.bibsonomy.org/bibtex/288cbcb2e7abfb5f037997f308dec9ec1/vngudivada},
  institution = {National Institute of Statistical Sciences},
  interhash = {51cd4dbe2fa66d16579e0c727159e154},
  intrahash = {88cbcb2e7abfb5f037997f308dec9ec1},
  keywords = {DataQuality Statistics},
  month = mar,
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data Quality: A Statistical Perspective},
  url = {/brokenurl#www.niss.org},
  year = 2005
}

@article{zaveri2014quality,
  abstract = {The standardization and adoption of Semantic Web technologies has resulted in an unprecedented volume of data being published as Linked Data (LD). However, the "publish first, refine later" philosophy leads to various quality problems arising in the underlying data such as incompleteness, inconsistency and semantic ambiguities. In this article, we describe the current state of Data Quality in the Web of Data along with details of the three papers accepted for the International Journal on Semantic Web and Information Systems' (IJSWIS) Special Issue on Web Data Quality. Additionally, we identify new challenges that are specific to the Web of Data and provide insights into the current progress and future directions for each of those challenges.},
  added-at = {2016-07-24T05:29:43.000+0200},
  address = {Hershey, PA, USA},
  author = {Zaveri, Amrapali and Maurino, Andrea and Equille, Laure-Berti},
  biburl = {https://www.bibsonomy.org/bibtex/28630b9ed1f8c0e1386dd4136eaabf818/vngudivada},
  interhash = {ae343be92651662fd6a2f5b108fc09e0},
  intrahash = {8630b9ed1f8c0e1386dd4136eaabf818},
  issn = {1552-6283},
  issue_date = {April 2014},
  journal = {Int. J. Semant. Web Inf. Syst.},
  keywords = {DataQuality WebData},
  month = apr,
  number = 2,
  pages = {1--6},
  publisher = {IGI Global},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Web Data Quality: Current State and New Challenges},
  volume = 10,
  year = 2014
}

@article{stahl2013translating,
  abstract = {Translating Euclid reports on an effort to transform geometry for students from a stylus-and-clay-tablet corpus of historical theorems to a stimulating computer-supported collaborative-learning inquiry experience.

The origin of geometry was a turning point in the pre-history of informatics, literacy, and rational thought. Yet, this triumph of human intellect became ossified through historic layers of systematization, beginning with Euclid’s organization of the Elements of geometry. Often taught by memorization of procedures, theorems, and proofs, geometry in schooling rarely conveys its underlying intellectual excitement. The recent development of dynamic-geometry software offers an opportunity to translate the study of geometry into a contemporary vernacular. However, this involves transformations along multiple dimensions of the conceptual and practical context of learning.

Translating Euclid steps through the multiple challenges involved in redesigning geometry education to take advantage of computer support. Networked computers portend an interactive approach to exploring dynamic geometry as well as broadened prospects for collaboration. The proposed conception of geometry emphasizes the central role of the construction of dependencies as a design activity, integrating human creation and mathematical discovery to form a human-centered approach to mathematics.

This book chronicles an iterative effort to adapt technology, theory, pedagogy and practice to support this vision of collaborative dynamic geometry and to evolve the approach through on-going cycles of trial with students and refinement of resources. It thereby provides a case study of a design-based research effort in computer-supported collaborative learning from a human-centered informatics perspective.

Table of Contents: Acknowledgments / Background / Vision: The Cognitive Potential of Collaborative Dynamic Geometry / History: The Origin of Geometry / Philosophy: The Obfuscation of Geometry / Mathematics: Demythologizing Geometry / Technology: Deconstructing Geometry / Collaboration: Group Geometry / Research: Analyzing Geometry / Theory: Resources for Geometry / Pedagogy: Designing Geometry / Practice: Doing Geometry / Design-Based Research: Human-Centered Geometry / Author Indes / Bibliography / Author's Biography},
  added-at = {2016-07-26T01:14:15.000+0200},
  author = {Stahl, Gerry},
  biburl = {https://www.bibsonomy.org/bibtex/2ec80b849cee9ad89a5f3849176b4a58f/vngudivada},
  doi = {10.2200/S00492ED1V01Y201303HCI017},
  eprint = {http://dx.doi.org/10.2200/S00492ED1V01Y201303HCI017},
  interhash = {566639e455b996ba22f2608a4c73f1cf},
  intrahash = {ec80b849cee9ad89a5f3849176b4a58f},
  journal = {Synthesis Lectures on Human-Centered Informatics},
  keywords = {Mathematics SynthesisLecture},
  number = 1,
  pages = {1-235},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Translating Euclid: Designing a Human-Centered Mathematics},
  url = {http://dx.doi.org/10.2200/S00492ED1V01Y201303HCI017},
  volume = 5,
  year = 2013
}

@inproceedings{booth2013economics,
  abstract = {The aim of this paper is to present a requirement for assessing the quality of data and the development of efficient methods of valuing and exchanging data among Web Observatories. Using economic and business theory a range of concepts are explored which include a brief review of existing business structures related to the exchange of goods, data or otherwise. The paper calls for a wider discussion by the Web Observatory community to begin to define relevant criteria by which data can be assessed and improved over time. The economic incentives are addressed as part of a price by proxy framework we introduce, which is supported by the need to strive for clear pricing signals and the reduction of information asymmetries. What is presented here is a way of establishing and improving data quality with a view to valuing data exchanges that does not require the presence of money in the transaction, yet it remains tied to revenue generation models as they exist online.},
  acmid = {2488167},
  added-at = {2016-07-24T01:07:07.000+0200},
  address = {New York, NY, USA},
  author = {Booth, Paul and Gaskell, Paul and Hughes, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/2dda04f57ee8e66ac634bd55a0d19e11c/vngudivada},
  booktitle = {Proceedings of the 22Nd International Conference on World Wide Web},
  doi = {10.1145/2487788.2488167},
  interhash = {5c1c5d827c732c7c9768184f129bb277},
  intrahash = {dda04f57ee8e66ac634bd55a0d19e11c},
  isbn = {978-1-4503-2038-2},
  keywords = {DataQuality Economics},
  location = {Rio de Janeiro, Brazil},
  numpages = {8},
  pages = {1309--1316},
  publisher = {ACM},
  series = {WWW '13 Companion},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {The Economics of Data: Quality, Value \&\#38; Exchange in Web Observatories},
  url = {http://doi.acm.org/10.1145/2487788.2488167},
  year = 2013
}

@article{blake2011effects,
  abstract = {Data quality remains a persistent problem in practice and a challenge for research. In this study we focus on the four dimensions of data quality noted as the most important to information consumers, namely accuracy, completeness, consistency, and timeliness. These dimensions are of particular concern for operational systems, and most importantly for data warehouses, which are often used as the primary data source for analyses such as classification, a general type of data mining. However, the definitions and conceptual models of these dimensions have not been collectively considered with respect to data mining in general or classification in particular. Nor have they been considered for problem complexity. Conversely, these four dimensions of data quality have only been indirectly addressed by data mining research. Using definitions and constructs of data quality dimensions, our research evaluates the effects of both data quality and problem complexity on generated data and tests the results in a real-world case. Six different classification outcomes selected from the spectrum of classification algorithms show that data quality and problem complexity have significant main and interaction effects. From the findings of significant effects, the economics of higher data quality are evaluated for a frequent application of classification and illustrated by the real-world case.},
  acmid = {1891881},
  added-at = {2016-07-24T01:09:12.000+0200},
  address = {New York, NY, USA},
  articleno = {8},
  author = {Blake, Roger and Mangiameli, Paul},
  biburl = {https://www.bibsonomy.org/bibtex/2bf053f14eb8e8f6284f585111d627a57/vngudivada},
  doi = {10.1145/1891879.1891881},
  interhash = {8b31ecf98f8af47bdab6f5f52d1b570b},
  intrahash = {bf053f14eb8e8f6284f585111d627a57},
  issn = {1936-1955},
  issue_date = {February 2011},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality ProblemComplexity},
  month = feb,
  number = 2,
  numpages = {28},
  pages = {8:1--8:28},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {The Effects and Interactions of Data Quality and Problem Complexity on Classification},
  url = {http://doi.acm.org/10.1145/1891879.1891881},
  volume = 2,
  year = 2011
}

@article{jagadish2014technical,
  abstract = {Exploring the inherent technical challenges in realizing the potential of Big Data.},
  acmid = {2611567},
  added-at = {2016-07-24T04:12:13.000+0200},
  address = {New York, NY, USA},
  author = {Jagadish, H. V. and Gehrke, Johannes and Labrinidis, Alexandros and Papakonstantinou, Yannis and Patel, Jignesh M. and Ramakrishnan, Raghu and Shahabi, Cyrus},
  biburl = {https://www.bibsonomy.org/bibtex/225fc3412cc0147486ab69e552d2ba108/vngudivada},
  doi = {10.1145/2611567},
  interhash = {dfcf8bc7a908aa0a7f3016bbd20cf772},
  intrahash = {25fc3412cc0147486ab69e552d2ba108},
  issn = {0001-0782},
  issue_date = {July 2014},
  journal = {Commun. ACM},
  keywords = {BigData DataQuality},
  month = jul,
  number = 7,
  numpages = {9},
  pages = {86--94},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Big Data and Its Technical Challenges},
  url = {http://doi.acm.org/10.1145/2611567},
  volume = 57,
  year = 2014
}

@article{keeton2010research,
  abstract = {Information quality (IQ) is a measure of how fit information is for a purpose. Sometimes called Quality of Information (QoI) by analogy with Quality of Service (QoS), it quantifies whether the correct information is being used to make a decision or take an action. Not understanding when information is of adequate quality can lead to bad decisions and catastrophic effects, including system outages, increased costs, lost revenue -- and worse. Quantifying information quality can help improve decision making, but the ultimate goal should be to select or construct information producers that have the appropriate balance between information quality and the cost of providing it. In this paper, we provide a brief introduction to the field, argue the case for applying information quality metrics in the systems domain, and propose a research agenda to explore this space.},
  acmid = {1710121},
  added-at = {2016-07-24T00:44:46.000+0200},
  address = {New York, NY, USA},
  author = {Keeton, Kimberly and Mehra, Pankaj and Wilkes, John},
  biburl = {https://www.bibsonomy.org/bibtex/21024c735e80c812cde9c3120644794c1/vngudivada},
  doi = {10.1145/1710115.1710121},
  interhash = {7f80648890d5d0c850b62a5d8eca2a95},
  intrahash = {1024c735e80c812cde9c3120644794c1},
  issn = {0163-5999},
  issue_date = {December 2009},
  journal = {SIGMETRICS Perform. Eval. Rev.},
  keywords = {DataQuality InformationQuality},
  month = jan,
  number = 3,
  numpages = {6},
  pages = {26--31},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Do You Know Your IQ?: A Research Agenda for Information Quality in Systems},
  url = {http://doi.acm.org/10.1145/1710115.1710121},
  volume = 37,
  year = 2010
}

@article{redman1998impact,
  abstract = {Poor data quality has far-reaching effects and consequences. The article aims to increase the awareness by providing a summary of impacts of poor data quality on a typical enterprise. These impacts include customer dissatisfaction, increased operational cost, less effective decision-making and a reduced ability to make and execute strategy. More subtly perhaps, poor data quality hurts employee morale, breeds organizational mistrust, and makes it more difficult to align the enterprise. Creating awareness of a problem and its impact is a critical first step towards resolution of the problem. The needed awareness of the poor data quality, while growing, has not yet been achieved in many enterprises. After all, the typical executive is already besieged by too many problems, low customer satisfaction, high costs, a data warehouse project that is late, and so forth. Creating awareness of issues of the accuracy level and impacts within the enterprise is the first obstacle that practitioners must overcome when implementing data quality programs.},
  acmid = {269025},
  added-at = {2016-07-24T04:05:42.000+0200},
  address = {New York, NY, USA},
  author = {Redman, Thomas C.},
  biburl = {https://www.bibsonomy.org/bibtex/28b890738484333a7c7dbd1220a4e9347/vngudivada},
  doi = {10.1145/269012.269025},
  interhash = {99468062e8fefe0199eba26ed3689941},
  intrahash = {8b890738484333a7c7dbd1220a4e9347},
  issn = {0001-0782},
  issue_date = {Feb. 1998},
  journal = {Commun. ACM},
  keywords = {DataQuality Economics},
  month = feb,
  number = 2,
  numpages = {4},
  pages = {79--82},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {The Impact of Poor Data Quality on the Typical Enterprise},
  url = {http://doi.acm.org/10.1145/269012.269025},
  volume = 41,
  year = 1998
}

@misc{gudipati2013testing,
  added-at = {2016-07-24T05:24:45.000+0200},
  author = {Gudipati, Mahesh},
  biburl = {https://www.bibsonomy.org/bibtex/24d1cdeb12f3bbea8b27b1e808302800d/vngudivada},
  interhash = {8ae081399121ad9f4247a58ffad33828},
  intrahash = {4d1cdeb12f3bbea8b27b1e808302800d},
  keywords = {DataQuality SoftwareTesting},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Big Data: Testing Approach to Overcome Quality Challenges},
  year = 2013
}

@article{gurevych2016linked,
  abstract = {This book conveys the fundamentals of Linked Lexical Knowledge Bases (LLKB) and sheds light on their different aspects from various perspectives, focusing on their construction and use in natural language processing (NLP). It characterizes a wide range of both expert-based and collaboratively constructed lexical knowledge bases. Only basic familiarity with NLP is required and this book has been written for both students and researchers in NLP and related fields who are interested in knowledge-based approaches to language analysis and their applications.

Lexical Knowledge Bases (LKBs) are indispensable in many areas of natural language processing, as they encode human knowledge of language in machine readable form, and as such, they are required as a reference when machines attempt to interpret natural language in accordance with human perception. In recent years, numerous research efforts have led to the insight that to make the best use of available knowledge, the orchestrated exploitation of different LKBs is necessary. This allows us to not only extend the range of covered words and senses, but also gives us the opportunity to obtain a richer knowledge representation when a particular meaning of a word is covered in more than one resource. Examples where such an orchestrated usage of LKBs proved beneficial include word sense disambiguation, semantic role labeling, semantic parsing, and text classification.

This book presents different kinds of automatic, manual, and collaborative linkings between LKBs. A special chapter is devoted to the linking algorithms employing text-based, graph-based, and joint modeling methods. Following this, it presents a set of higher-level NLP tasks and algorithms, effectively utilizing the knowledge in LLKBs. Among them, you will find advanced methods, e.g., distant supervision, or continuous vector space models of knowledge bases (KB), that have become widely used at the time of this book's writing. Finally, multilingual applications of LLKB's, such as cross-lingual semantic relatedness and computer-aided translation are discussed, as well as tools and interfaces for exploring LLKBs, followed by conclusions and future research directions.

Table of Contents: Foreword / Preface / Acknowledgments / Lexical Knowledge Bases / Linked Lexical Knowledge Bases / Linking Algorithms / Fundamental Disambiguation Methods / Advanced Disambiguation Methods / Multilingual Applications / Interfaces and Tools / Conclusion and Outlook / Acronyms / Bibliography / Authors' Biographies},
  added-at = {2016-07-26T00:50:51.000+0200},
  author = {Gurevych, Iryna and Eckle-Kohler, Judith and Matuschek, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2c1158cae061994cc42973f95b59ee42f/vngudivada},
  doi = {10.2200/S00717ED1V01Y201605HLT034},
  eprint = {http://dx.doi.org/10.2200/S00717ED1V01Y201605HLT034},
  interhash = {fddd783b45b5bb84709dbbf5bc24fa16},
  intrahash = {c1158cae061994cc42973f95b59ee42f},
  journal = {Synthesis Lectures on Human Language Technologies},
  keywords = {KnowledgeBase NLP},
  number = 3,
  pages = {1-146},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Linked Lexical Knowledge Bases: Foundations and Applications},
  url = {http://dx.doi.org/10.2200/S00717ED1V01Y201605HLT034},
  volume = 9,
  year = 2016
}

@misc{wu2016visual,
  abstract = {Visual Question Answering (VQA) is a challenging task that has received
increasing attention from both the computer vision and the natural language
processing communities. Given an image and a question in natural language, it
requires reasoning over visual elements of the image and general knowledge to
infer the correct answer. In the first part of this survey, we examine the
state of the art by comparing modern approaches to the problem. We classify
methods by their mechanism to connect the visual and textual modalities. In
particular, we examine the common approach of combining convolutional and
recurrent neural networks to map images and questions to a common feature
space. We also discuss memory-augmented and modular architectures that
interface with structured knowledge bases. In the second part of this survey,
we review the datasets available for training and evaluating VQA systems. The
various datatsets contain questions at different levels of complexity, which
require different capabilities and types of reasoning. We examine in depth the
question/answer pairs from the Visual Genome project, and evaluate the
relevance of the structured annotations of images with scene graphs for VQA.
Finally, we discuss promising future directions for the field, in particular
the connection to structured knowledge bases and the use of natural language
processing models.},
  added-at = {2016-07-26T05:50:59.000+0200},
  author = {Wu, Qi and Teney, Damien and Wang, Peng and Shen, Chunhua and Dick, Anthony and Hengel, Anton van den},
  biburl = {https://www.bibsonomy.org/bibtex/212449f88a7a521f36ff9e16703dcf483/vngudivada},
  description = {Visual Question Answering: A Survey of Methods and Datasets},
  interhash = {124f32f0821b256cf6a33301930f2016},
  intrahash = {12449f88a7a521f36ff9e16703dcf483},
  keywords = {CBIR NLP QASystem Survey VisualQuestionAnswering},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Visual Question Answering: A Survey of Methods and Datasets},
  url = {http://arxiv.org/abs/1607.05910},
  year = 2016
}

@article{shasha2008statistics,
  abstract = {Statistics is the activity of inferring results about a population given a sample. Historically, statistics books assume an underlying distribution to the data (typically, the normal distribution) and derive results under that assumption. Unfortunately, in real life, one cannot normally be sure of the underlying distribution. For that reason, this book presents a distribution-independent approach to statistics based on a simple computational counting idea called resampling.

This book explains the basic concepts of resampling, then systematically presents the standard statistical measures along with programs (in the language Python) to calculate them using resampling, and finally illustrates the use of the measures and programs in a case study. The text uses junior high school algebra and many examples to explain the concepts. The ideal reader has mastered at least elementary mathematics, likes to think procedurally, and is comfortable with computers.

Table of Contents: The Basic Idea / Bias Corrected Confidence Intervals / Pragmatic Considerations When Using Resampling / Terminology / The Essential Stats / Case Study: New Mexico's 2004 Presidential Ballots / References},
  added-at = {2016-07-26T01:27:04.000+0200},
  author = {Shasha, Dennis and Wilson, Manda},
  biburl = {https://www.bibsonomy.org/bibtex/2d0155dac94527b135421dfe12b8cf206/vngudivada},
  doi = {10.2200/S00142ED1V01Y200807MAS001},
  eprint = {http://dx.doi.org/10.2200/S00142ED1V01Y200807MAS001},
  interhash = {c7199ece8952f010d6258b4a00c7939f},
  intrahash = {d0155dac94527b135421dfe12b8cf206},
  journal = {Synthesis Lectures on Mathematics and Statistics},
  keywords = {Statistics SynthesisLecture},
  number = 1,
  pages = {1-82},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Statistics is Easy!},
  url = {http://dx.doi.org/10.2200/S00142ED1V01Y200807MAS001},
  volume = 1,
  year = 2008
}

@misc{informatica2013metadata,
  added-at = {2016-07-24T05:16:49.000+0200},
  author = {Informatica},
  biburl = {https://www.bibsonomy.org/bibtex/254ed000d152fb608e80959cb1348d016/vngudivada},
  interhash = {b09c8f480a8b6e4f2ecbdd5dd8240fa0},
  intrahash = {54ed000d152fb608e80959cb1348d016},
  keywords = {DataGovernance Metadata},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Metadata Management for Holistic Data Governance},
  year = 2013
}

@techreport{cognizant2014making,
  abstract = {Many organizations have contemplated, or are already on the journey to, making data quality a way of life. What many of them missed, however, are the necessary operational contexts of data quality and its interplay with a well-honed governance program that allows the functions which comprise data quality as a way of life.

This white paper explores the necessary operational activities that make data quality a way of life. It also examines why no data, including what is normally deemed unstructured data and big data, can be excluded from such a program and the necessary governance interplays that are required to make data quality efforts less of a chore and more normal operating procedure.},
  added-at = {2016-07-24T05:10:18.000+0200},
  author = {Cognizant},
  biburl = {https://www.bibsonomy.org/bibtex/202f1705593b467e69d0044c887764ebc/vngudivada},
  institution = {Cognizant},
  interhash = {98833f075ce98a24598807c9941ca820},
  intrahash = {02f1705593b467e69d0044c887764ebc},
  keywords = {DataQuality},
  month = nov,
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Making Data Quality a Way of Life},
  year = 2014
}

@article{morton2014support,
  abstract = {We present a vision of next-generation visual analytics services. We argue that these services should have three related capabilities: support visual and interactive data exploration as they do today, but also suggest relevant data to enrich visualizations, and facilitate the integration and cleaning of that data. Most importantly, they should provide all these capabilities seamlessly in the context of an uninterrupted data analysis cycle. We present the challenges and opportunities in building next-generation visual analytics services.},
  acmid = {2732282},
  added-at = {2016-07-24T01:03:25.000+0200},
  author = {Morton, Kristi and Balazinska, Magdalena and Grossman, Dan and Mackinlay, Jock},
  biburl = {https://www.bibsonomy.org/bibtex/26b7e406f39bcfb52531dcc0e79fbafa8/vngudivada},
  doi = {10.14778/2732279.2732282},
  interhash = {a7a6f7d3df824d99031604a15ea1fc52},
  intrahash = {6b7e406f39bcfb52531dcc0e79fbafa8},
  issn = {2150-8097},
  issue_date = {February 2014},
  journal = {Proc. VLDB Endow.},
  keywords = {DataAnalysis DataQuality},
  month = feb,
  number = 6,
  numpages = {4},
  pages = {453--456},
  publisher = {VLDB Endowment},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Support the Data Enthusiast: Challenges for Next-generation Data-analysis Systems},
  url = {http://dx.doi.org/10.14778/2732279.2732282},
  volume = 7,
  year = 2014
}

@inproceedings{volkovs2014continuous,
  abstract = {In declarative data cleaning, data semantics are encoded as constraints and errors arise when the data violates the constraints. Various forms of statistical and logical inference can be used to reason about and repair inconsistencies (errors) in data. Recently, unified approaches that repair both errors in data and errors in semantics (the constraints) have been proposed. However, both data-only approaches and unified approaches are by and large static in that they apply cleaning to a single snapshot of the data and constraints. We introduce a continuous data cleaning framework that can be applied to dynamic data and constraint environments. Our approach permits both the data and its semantics to evolve and suggests repairs based on the accumulated evidence to date. Importantly, our approach uses not only the data and constraints as evidence, but also considers the past repairs chosen and applied by a user (user repair preferences). We introduce a repair classifier that predicts the type of repair needed to resolve an inconsistency, and that learns from past user repair preferences to recommend more accurate repairs in the future. Our evaluation shows that our techniques achieve high prediction accuracy and generate high quality repairs. Of independent interest, our work makes use of a set of data statistics that are shown to be sensitive to predicting particular repair types.},
  added-at = {2016-07-24T00:14:24.000+0200},
  author = {Volkovs, M. and Chiang, F. and Szlichta, J. and Miller, R. J.},
  biburl = {https://www.bibsonomy.org/bibtex/2580a8186e688025fcb49fe11c0068131/vngudivada},
  booktitle = {2014 IEEE 30th International Conference on Data Engineering},
  doi = {10.1109/ICDE.2014.6816655},
  interhash = {c956c25a0cb8d03b3fe67a5fda4a7e73},
  intrahash = {580a8186e688025fcb49fe11c0068131},
  issn = {1063-6382},
  keywords = {DataCleaning DataQuality},
  month = {March},
  pages = {244-255},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Continuous data cleaning},
  year = 2014
}

@techreport{unece2014suggested,
  added-at = {2016-07-24T04:01:06.000+0200},
  author = {UNECE},
  biburl = {https://www.bibsonomy.org/bibtex/2031356e988d33e8de731da1ce9e2023d/vngudivada},
  interhash = {3393c56bea3a66efccd35b6e77c817eb},
  intrahash = {031356e988d33e8de731da1ce9e2023d},
  keywords = {DataQuality Framework},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {A Suggested Framework for the Quality of Big Data},
  year = 2014
}

@article{patane2016heterogeneous,
  abstract = {New data acquisition techniques are emerging and are providing fast and efficient means for multidimensional spatial data collection. Airborne LIDAR surveys, SAR satellites, stereo-photogrammetry and mobile mapping systems are increasingly used for the digital reconstruction of the environment. All these systems provide extremely high volumes of raw data, often enriched with other sensor data (e.g., beam intensity). Improving methods to process and visually analyze this massive amount of geospatial and user-generated data is crucial to increase the efficiency of organizations and to better manage societal challenges.

Within this context, this book proposes an up-to-date view of computational methods and tools for spatio-temporal data fusion, multivariate surface generation, and feature extraction, along with their main applications for surface approximation and rainfall analysis. The book is intended to attract interest from different fields, such as computer vision, computer graphics, geomatics, and remote sensing, working on the common goal of processing 3D data. To this end, it presents and compares methods that process and analyze the massive amount of geospatial data in order to support better management of societal challenges through more timely and better decision making, independent of a specific data modeling paradigm (e.g., 2D vector data, regular grids or 3D point clouds).

We also show how current research is developing from the traditional layered approach, adopted by most GIS softwares, to intelligent methods for integrating existing data sets that might contain important information on a geographical area and environmental phenomenon. These services combine traditional map-oriented visualization with fully 3D visual decision support methods and exploit semantics-oriented information (e.g., a-priori knowledge, annotations, segmentations) when processing, merging, and integrating big pre-existing data sets.

Table of Contents: List of Figures / List of Tables / Preface / Acknowledgments / Spatio-temporal Data Fusion / Spatial and Environmental Data Approximation / Feature Extraction / Applications to Surface Approximation and Rainfall Analysis / Conclusions / Bibliography / Authors' Biographies},
  added-at = {2016-07-26T01:00:55.000+0200},
  author = {Patanè, Giuseppe and Spagnuolo, Michela},
  biburl = {https://www.bibsonomy.org/bibtex/2dd9f83395b4ffe03eda438579ede1a7f/vngudivada},
  doi = {10.2200/S00711ED1V01Y201603VCP024},
  eprint = {http://dx.doi.org/10.2200/S00711ED1V01Y201603VCP024},
  interhash = {e2b3707e05ced08c324a91cd8eae1f35},
  intrahash = {dd9f83395b4ffe03eda438579ede1a7f},
  journal = {Synthesis Lectures on Visual Computing},
  keywords = {GIS HeterogeneousData SpatialData},
  number = 2,
  pages = {1-155},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Heterogeneous Spatial Data: Fusion, Modeling, and Analysis for GIS Applications},
  url = {http://dx.doi.org/10.2200/S00711ED1V01Y201603VCP024},
  volume = 8,
  year = 2016
}

@inproceedings{li2013truth,
  abstract = {The amount of useful information available on the Web has been growing at a dramatic pace in recent years and people rely more and more on the Web to fulfill their information needs. In this paper, we study truthfulness of Deep Web data in two domains where we believed data are fairly clean and data quality is important to people's lives: {\em Stock} and {\em Flight}. To our surprise, we observed a large amount of inconsistency on data from different sources and also some sources with quite low accuracy. We further applied on these two data sets state-of-the-art {\em data fusion} methods that aim at resolving conflicts and finding the truth, analyzed their strengths and limitations, and suggested promising research directions. We wish our study can increase awareness of the seriousness of conflicting data on the Web and in turn inspire more research in our community to tackle this problem.},
  acmid = {2448943},
  added-at = {2016-07-24T01:19:46.000+0200},
  author = {Li, Xian and Dong, Xin Luna and Lyons, Kenneth and Meng, Weiyi and Srivastava, Divesh},
  biburl = {https://www.bibsonomy.org/bibtex/239f9acf35d0aa6c59f3eae93a69ec6d4/vngudivada},
  booktitle = {Proceedings of the 39th international conference on Very Large Data Bases},
  interhash = {b7a0f727326f76fdeaf9618e525f0d5a},
  intrahash = {39f9acf35d0aa6c59f3eae93a69ec6d4},
  keywords = {DataQuality DeepWeb},
  location = {Trento, Italy},
  numpages = {12},
  pages = {97--108},
  publisher = {VLDB Endowment},
  series = {PVLDB'13},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Truth finding on the deep web: is the problem solved?},
  url = {http://dl.acm.org/citation.cfm?id=2448936.2448943},
  year = 2013
}

@misc{bertiequille2016measuring,
  added-at = {2016-07-24T05:13:23.000+0200},
  author = {Berti-Equille, Laure},
  biburl = {https://www.bibsonomy.org/bibtex/28a9ff52d0f6c27839a426f4f5f42317f/vngudivada},
  interhash = {c47028328c94b7c170b439bcc95f2f76},
  intrahash = {8a9ff52d0f6c27839a426f4f5f42317f},
  keywords = {DataQuality DataQualityAssessment},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Measuring and Modeling Data Quality},
  year = 2016
}

@article{glowalla2014system,
  abstract = {
Purpose -- The purpose of this paper is to facilitate understanding of enterprise resource planning (ERP) system and data quality interdependency by presenting ERP systems’ use within data quality management.

Design/methodology/approach -- The authors apply task technology fit (TTF) in an explorative study, conducting semi-structured expert interviews with participants in information technology strategic decision making. The authors analyzed the interviews with iterative descriptive and subsequent interpretive coding.

Findings -- Although considered sustainable, continuously increasing regulations challenge ERP systems. However, compliance with regulations may serve as a bridge for organizations to engage in data analysis. Organizations are embedded into evolving task environments with the need to continuously adapt their systems or the organization and the need for contextual understanding of data quality.

Research limitations/implications -- With ERP systems being used for administrative functions, future research might draw on extant ERP systems research from the manufacturing sector. However, for insurance-specific tasks, ERP systems and their data need to be considered in a sector-specific context with the need for further research.

Practical implications -- ERP systems are considered sustainable. High initial fit is desirable, but the sector’s relevance for ERP system vendors might be more important for sustainability. Ensuring TTF will be an increasing challenge with increasing task non-routineness.

Originality/value -- Applying TTF provides guidance for fit research, while the qualitative approach accounts for a deeper understanding, especially when exploring data quality issues since deficiencies might have several root causes. The authors show that ERP systems have an impact on data quality beyond its typically examined functionality
},
  added-at = {2016-07-24T00:04:07.000+0200},
  author = {Glowalla, Paul and Sunyaev, Ali},
  biburl = {https://www.bibsonomy.org/bibtex/24ffdea3b3c90e889a098840d4e101a33/vngudivada},
  doi = {10.1108/jeim-08-2013-0062},
  interhash = {1b4678cf904ff377ee447789e940774d},
  intrahash = {4ffdea3b3c90e889a098840d4e101a33},
  journal = {Journal of Ent Info Management},
  keywords = {DataQuality ERP},
  month = sep,
  number = 5,
  pages = {668--686},
  publisher = {Emerald},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {{ERP} system fit {\textendash} an explorative task and data quality perspective},
  url = {http://dx.doi.org/10.1108/JEIM-08-2013-0062},
  volume = 27,
  year = 2014
}

@inproceedings{almeida2012ontologies,
  abstract = {The emergence of new business models, namely, the establishment of partnerships between organizations, the chance that companies have of adding existing data on the web, especially in the semantic web, to their information, led to the emphasis on some problems existing in databases, particularly related to data quality. Poor data can result in loss of competitiveness of the organizations holding these data, and may even lead to their disappearance, since many of their decision-making processes are based on these data. For this reason, data cleaning is essential. Current approaches to solve these problems are closely linked to database schemas and specific domains. In order that data cleaning can be used in different repositories, it is necessary for computer systems to understand these data, i.e., an associated semantic is needed. The solution presented in this paper includes the use of ontologies: (i) for the specification of data cleaning operations and, (ii) as a way of solving the semantic heterogeneity problems of data stored in different sources. With data cleaning operations defined at a conceptual level and existing mappings between domain ontologies and an ontology that results from a database, they may be instantiated and proposed to the expert/specialist to be executed over that database, thus enabling their interoperability.},
  added-at = {2016-07-24T00:19:45.000+0200},
  author = {Almeida, R. and Oliveira, P. and Braga, L. and Barroso, J.},
  biburl = {https://www.bibsonomy.org/bibtex/2247f0384c252267942306e37e0b8d216/vngudivada},
  booktitle = {Semantic Computing (ICSC), 2012 IEEE Sixth International Conference on},
  doi = {10.1109/ICSC.2012.19},
  interhash = {508ba1b7b18ee33b2549de6ad9b40846},
  intrahash = {247f0384c252267942306e37e0b8d216},
  keywords = {DataCleaning DataQuality Ontology},
  month = {Sept},
  pages = {238-241},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Ontologies for Reusing Data Cleaning Knowledge},
  url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6337110},
  year = 2012
}

@techreport{geiger2016quality,
  abstract = {SIX HUNDRED BILLION DOLLARS ANNUALLY – Got your attention? That is what poor data quality costs American businesses, according to the Data Warehousing Institute. Poor data is also the leading cause of many IT project failures. So, given that this is such a serious problem, why aren’t more companies addressing it more aggressively? This session discusses these topics as well as those detailing how companies can improve their data quality using the proven architectural blueprint, the Corporate Information Factory, as a basis for mapping your data quality processes. This session will explain the importance of data quality management, quality expectations and techniques for setting them. Finally, the program ends with practical advice for getting started on your data quality management program.},
  added-at = {2016-07-24T04:56:12.000+0200},
  author = {Geiger, Jonathan G.},
  biburl = {https://www.bibsonomy.org/bibtex/27325f70a52e0adffc675289f9b77d975/vngudivada},
  institution = {Intelligent Solutions, Inc},
  interhash = {ec144561ee91f85c471518cdf488f8a7},
  intrahash = {7325f70a52e0adffc675289f9b77d975},
  keywords = {DataQuality DataQualityManagement},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data Quality Management: The Most Critical Initiative You Can Implement},
  year = 2016
}

@inproceedings{klein2002users,
  abstract = {Although it is generally believed that information quality problems are not uncommon on the World Wide Web, little is known about the conditions in which users find these problems and the strategies they employ for dealing with them. Furthermore, very little theory is available to guide research on user detection of information quality problems on the Internet. This study involves an analysis of questionnaires in which over 300 users were asked about incidents in which they found various kinds of information quality problems while performing tasks using the World Wide Web. The objective of the research is the development of a theoretical model of factors affecting user detection of information quality problems on the World Wide Web. Preliminary results based on 132 questionnaires are discussed in this paper.},
  added-at = {2016-07-24T01:35:38.000+0200},
  author = {Klein, Barbara D.},
  biburl = {https://www.bibsonomy.org/bibtex/2dad7b4201cf0fd92044ef74d58cfdc83/vngudivada},
  booktitle = { Eighth Americas Conference on Information Systems},
  interhash = {49098c23a64bbfd0a77341ae4fc413a5},
  intrahash = {dad7b4201cf0fd92044ef74d58cfdc83},
  keywords = {DataQuality WebData},
  pages = {1101-1103},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {When Do Users Detect Information Quality Problems on the World Wide Web?},
  year = 2002
}

@book{baesens2014analytics,
  abstract = {The guide to targeting and leveraging business opportunities using big data & analytics By leveraging big data & analytics, businesses create the potential to better understand, manage, and strategically exploiting the complex dynamics of customer behavior. Analytics in a Big Data World reveals how to tap into the powerful tool of data analytics to create a strategic advantage and identify new business opportunities. Designed to be an accessible resource, this essential book does not include exhaustive coverage of all analytical techniques, instead focusing on analytics techniques that really...},
  added-at = {2016-07-24T05:03:28.000+0200},
  address = {New York, NY},
  author = {Baesens, Bart},
  biburl = {https://www.bibsonomy.org/bibtex/24f74db774261ea9d8c73a9147fb072cd/vngudivada},
  interhash = {be27624720688334d649a458bf4985d6},
  intrahash = {4f74db774261ea9d8c73a9147fb072cd},
  keywords = {BigData Book DataAnalytics DataQuality},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Analytics in a big data world: the essential guide to data science and its applications},
  year = 2014
}

@article{watts2012essentials,
  abstract = {The Second Edition of this popular book on practical mathematics for engineers includes new and expanded chapters on perturbation methods and theory. This is a book about linear partial differential equations that are common in engineering and the physical sciences. It will be useful to graduate students and advanced undergraduates in all engineering fields as well as students of physics, chemistry, geophysics and other physical sciences and professional engineers who wish to learn about how advanced mathematics can be used in their professions. The reader will learn about applications to heat transfer, fluid flow and mechanical vibrations. The book is written in such a way that solution methods and application to physical problems are emphasized. There are many examples presented in detail and fully explained in their relation to the real world. References to suggested further reading are included. The topics that are covered include classical separation of variables and orthogonal functions, Laplace transforms, complex variables and Sturm-Liouville transforms. This second edition includes two new and revised chapters on perturbation methods, and singular perturbation theory of differential equations.

Table of Contents: Partial Differential Equations in Engineering / The Fourier Method: Separation of Variables / Orthogonal Sets of Functions / Series Solutions of Ordinary Differential Equations / Solutions Using Fourier Series and Integrals / Integral Transforms: The Laplace Transform / Complex Variables and the Laplace Inversion Integral / Solutions with Laplace Transforms / Sturm-Liouville Transforms / Introduction to Perturbation Methods / Singular Perturbation Theory of Differential Equations / Appendix A: The Roots of Certain Transcendental Equations},
  added-at = {2016-07-26T01:16:18.000+0200},
  author = {Watts, Robert G.},
  biburl = {https://www.bibsonomy.org/bibtex/21a4366ac204535b1f8a0cd7afe9e61e0/vngudivada},
  doi = {10.2200/S00377ED1V01Y201202MAS012},
  eprint = {http://dx.doi.org/10.2200/S00377ED1V01Y201202MAS012},
  interhash = {d47adfd7a62eee275b8d44d39f5ee114},
  intrahash = {1a4366ac204535b1f8a0cd7afe9e61e0},
  journal = {Synthesis Lectures on Mathematics and Statistics},
  keywords = {AppliedMathematics SynthesisLecture},
  number = 1,
  pages = {1-189},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Essentials of Applied Mathematics for Engineers and Scientists, Second Edition},
  url = {http://dx.doi.org/10.2200/S00377ED1V01Y201202MAS012},
  volume = 5,
  year = 2012
}

@article{cattell2011scalable,
  abstract = {In this paper, we examine a number of SQL and socalled "NoSQL" data stores designed to scale simple OLTP-style application loads over many servers. Originally motivated by Web 2.0 applications, these systems are designed to scale to thousands or millions of users doing updates as well as reads, in contrast to traditional DBMSs and data warehouses. We contrast the new systems on their data model, consistency mechanisms, storage mechanisms, durability guarantees, availability, query support, and other dimensions. These systems typically sacrifice some of these dimensions, e.g. database-wide transaction consistency, in order to achieve others, e.g. higher availability and scalability.},
  acmid = {1978919},
  added-at = {2016-07-24T19:58:25.000+0200},
  address = {New York, NY, USA},
  author = {Cattell, Rick},
  biburl = {https://www.bibsonomy.org/bibtex/20f07b7d6dbec358c637b4ff01165e307/vngudivada},
  doi = {10.1145/1978915.1978919},
  interhash = {83277b102ac87c6219989f4f0351e92f},
  intrahash = {0f07b7d6dbec358c637b4ff01165e307},
  issn = {0163-5808},
  issue_date = {December 2010},
  journal = {SIGMOD Rec.},
  keywords = {Performance SQL Scalability},
  month = may,
  number = 4,
  numpages = {16},
  pages = {12--27},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Scalable SQL and NoSQL Data Stores},
  url = {http://doi.acm.org/10.1145/1978915.1978919},
  volume = 39,
  year = 2011
}

@book{davis2012ethics,
  abstract = {"What are your organization's policies for generating and using huge datasets full of personal information? This book examines ethical questions raised by the big data phenomenon, and explains why enterprises need to reconsider business decisions concerning privacy and identity. Authors Kord Davis and Doug Patterson provide methods and techniques to help your business engage in a transparent and productive ethical inquiry into your current data practices. Both individuals and organizations have legitimate interests in understanding how data is handled. Your use of data can directly affect brand quality and revenue--as Target, Apple, Netflix, and dozens of other companies have discovered. With this book, you'll learn how to align your actions with explicit company values and preserve the trust of customers, partners, and stakeholders."--Back cover.},
  added-at = {2016-07-24T05:05:04.000+0200},
  address = {Sebastopol, CA},
  author = {Davis, Kord and Patterson, Doug},
  biburl = {https://www.bibsonomy.org/bibtex/2d8c5e38a664ca05049f97d8ac04dc891/vngudivada},
  interhash = {78ba7cd5a4a484b671bbdc99054bb61a},
  intrahash = {d8c5e38a664ca05049f97d8ac04dc891},
  isbn = {9781449311797 1449311792},
  keywords = {BigData Book Ethics},
  publisher = {O'Reilly},
  refid = {768168813},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Ethics of big data},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9781449311797},
  year = 2012
}

@misc{getoor2012entity,
  added-at = {2016-07-24T03:56:32.000+0200},
  author = {Getoor, Lise and Machanavajjhala, Ashwin},
  biburl = {https://www.bibsonomy.org/bibtex/2611ba466d59d9358a7d41083eea25388/vngudivada},
  interhash = {865effc7f004c47e6be37c1008dbd25d},
  intrahash = {611ba466d59d9358a7d41083eea25388},
  keywords = {DataQuality EntityResolution Tutorial},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Entity Resolution: Tutorial},
  year = 2012
}

@misc{obrien2015accounting,
  abstract = {Organisations are facing ever more diverse challenges in managing their enterprise systems as emerging technologies bring both added complexities as well as opportunities to the way they conduct their business. Underpinning this ever-increasing volatility is the importance of having quality data to provide information to make those important enterprise-wide decisions. Numerous studies suggest that many organisations are not paying enough attention to their data and that a major cause of this is their failure to measure its quality and value and/or evaluate the costs of having poor data. This study proposes an integrated framework that organisations can adopt as part of their financial and management control processes to provide a mechanism for quantifying data problems, costing potential solutions and monitoring the on-going costs and benefits, to assist them in improving and then sustaining the quality of their data.},
  added-at = {2016-07-24T04:10:47.000+0200},
  author = {O'Brien, Tony},
  biburl = {https://www.bibsonomy.org/bibtex/241f0790aa5338a91b2220adb8e98547e/vngudivada},
  interhash = {658cb0981352d04dfe85beb435a0ff4d},
  intrahash = {41f0790aa5338a91b2220adb8e98547e},
  keywords = {DataQuality},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Accounting for data quality in enterprise systems},
  year = 2015
}

@techreport{redman2011practitioners,
  added-at = {2016-07-24T04:19:18.000+0200},
  author = {Redman, Thomas C.},
  biburl = {https://www.bibsonomy.org/bibtex/26f35490994624546485f69742900e2ab/vngudivada},
  institution = {University of South Australia},
  interhash = {71602b15acae492c2a67d629f4837a5a},
  intrahash = {6f35490994624546485f69742900e2ab},
  keywords = {BigData DataQuality},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {A Practitioner's View of the Really Big Data Quality (Research) Issues},
  url = {/brokenurl#www.dataqualitysolutions.com},
  year = 2011
}

@article{rahm2000cleaning,
  abstract = {We classify data quality problems that are addressed by data cleaning and provide an overview of the main solution approaches. Data cleaning is especially required when integrating heterogeneous data sources and should be addressed together with schema-related data transformations. In data warehouses, data cleaning is a major part of the so-called ETL process. We also discuss current tool support for data cleaning.},
  added-at = {2016-07-24T00:27:16.000+0200},
  author = {Rahm, Erhard and Do, Hong H.},
  biburl = {https://www.bibsonomy.org/bibtex/25cb1661908665a7e8793aa3b741cb3c8/vngudivada},
  citeulike-article-id = {2641551},
  interhash = {a1030341079f4e8f402c90c6022106f8},
  intrahash = {5cb1661908665a7e8793aa3b741cb3c8},
  journal = {IEEE Data Eng. Bull.},
  keywords = {DataCleaning DataQuality},
  number = 4,
  pages = {3--13},
  posted-at = {2008-04-08 13:53:30},
  priority = {4},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data Cleaning: Problems and Current Approaches},
  url = {http://dblp.uni-trier.de/rec/bibtex/journals/debu/RahmD00},
  volume = 23,
  year = 2000
}

@article{brizan2006survey,
  abstract = {A great deal of research is focused on formation of a data warehouse. This is an important area of research as it could save many computation cycles and thus allow accurate information provided to the right people at the right time. Two considerations when forming a data warehouse are data cleansing (including entity resolution) and with schema integration (including record linkage). Uncleansed and fragmented data requires time to decipher and may lead to increased costs for an organization, so data cleansing and schema integration can save a great many (human) computation cycles and can lead to higher organizational efficiency. In this study we survey the literature for the methodologies proposed or developed for entity resolution and record linkage. This survey provides a foundation for solving many problems in data warehousing. For instance, little or no research has been directed at the problem of maintenance of cleansed and linked relations.},
  added-at = {2016-07-24T03:59:32.000+0200},
  author = {Brizan, David G. and Tansel, Abdullah U.},
  biburl = {https://www.bibsonomy.org/bibtex/20fab80e934fb965d413dadaffc886de3/vngudivada},
  interhash = {6f34166240b9210956038f7c9c88d21d},
  intrahash = {0fab80e934fb965d413dadaffc886de3},
  journal = {Communications of the IIMA},
  keywords = {EntityResolution RecordLinking Survey},
  number = 3,
  pages = {41--50},
  priority = {2},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {{A Survey of Entity Resolution and Record Linkage Methodologies}},
  url = {http://www.iima.org/CIIMA/8\%20CIIMA\%206-3\%2041-50\%20\%20Brizan.pdf},
  volume = 6,
  year = 2006
}

@misc{experian2016state,
  abstract = {Data-driven organizations know that information is the foundation of their intelligence and success. However, most are not putting enough emphasis on data quality. This is translating to inefficient marketing, business intelligence and loyalty efforts.

This data quality white paper provides insight into the current state of data usage, management methods and accuracy levels.},
  added-at = {2016-07-24T05:26:38.000+0200},
  author = {Experian},
  biburl = {https://www.bibsonomy.org/bibtex/2bcd760a12dbe562f1a002948d903deed/vngudivada},
  interhash = {8d96a8f315c43975aa5acf93da1f8985},
  intrahash = {bcd760a12dbe562f1a002948d903deed},
  keywords = {DataQuality},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {The state of data quality},
  year = 2016
}

@article{cai2015challenges,
  abstract = {High-quality data are the precondition for analyzing and using big data and for guaranteeing the value of the data. Currently, comprehensive analysis and research of quality standards and quality assessment methods for big data are lacking. First, this paper summarizes reviews of data quality research. Second, this paper analyzes the data characteristics of the big data environment, presents quality challenges faced by big data, and formulates a hierarchical data quality framework from the perspective of data users. This framework consists of big data quality dimensions, quality characteristics, and quality indexes. Finally, on the basis of this framework, this paper constructs a dynamic assessment process for data quality. This process has good expansibility and adaptability and can meet the needs of big data quality assessment. The research results enrich the theoretical scope of big data and lay a solid foundation for the future by establishing an assessment model and studying evaluation algorithms.},
  added-at = {2016-07-24T04:21:01.000+0200},
  author = {Cai, Li and Zhu, Yangyong},
  biburl = {https://www.bibsonomy.org/bibtex/280474fc2fe517ad4133d6ced2799ec68/vngudivada},
  doi = {10.5334/dsj-2015-002},
  interhash = {5fa860942425dc3b213b7f9106ecb024},
  intrahash = {80474fc2fe517ad4133d6ced2799ec68},
  journal = {{CODATA}},
  keywords = {DataQuality DataQualityAssessment},
  month = may,
  number = 0,
  pages = 2,
  publisher = {Ubiquity Press, Ltd.},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {The Challenges of Data Quality and Data Quality Assessment in the Big Data Era},
  url = {http://dx.doi.org/10.5334/dsj-2015-002},
  volume = 14,
  year = 2015
}

@misc{josko2014quality,
  abstract = {Data Quality Assessment outcomes are essential to improve data quality and are required condition to support analytical processes. There are several successful approaches to automate this support to syntactic data defects. In contrast, the dependence of semantic data defects on data context knowledge implies on human supervision. The visualization systems belong to a class of supervised tools that can turn data defects into visual items. However, there is no design support for this purpose. Hence, this paper presents a framework to assist the design of these systems fitting the visual quality assessment of se-mantic data defects. Such an approach is based on data defects structure, data characteristics and user-centered tasks.},
  added-at = {2016-07-24T04:41:39.000+0200},
  author = {Josko, Joao},
  biburl = {https://www.bibsonomy.org/bibtex/23c7d6dcc015dad45970b9f11f44882e5/vngudivada},
  interhash = {e4c5b61154522465903428d59d4622ba},
  intrahash = {3c7d6dcc015dad45970b9f11f44882e5},
  keywords = {DataQuality Visualization},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data quality assessment of very large database through visualization system},
  year = 2014
}

@article{tobias2011matrices,
  abstract = {This book is intended as an undergraduate text introducing matrix methods as they relate to engineering problems. It begins with the fundamentals of mathematics of matrices and determinants. Matrix inversion is discussed, with an introduction of the well known reduction methods. Equation sets are viewed as vector transformations, and the conditions of their solvability are explored. Orthogonal matrices are introduced with examples showing application to many problems requiring three dimensional thinking. The angular velocity matrix is shown to emerge from the differentiation of the 3-D orthogonal matrix, leading to the discussion of particle and rigid body dynamics.

The book continues with the eigenvalue problem and its application to multi-variable vibrations. Because the eigenvalue problem requires some operations with polynomials, a separate discussion of these is given in an appendix. The example of the vibrating string is given with a comparison of the matrix analysis to the continuous solution.

Table of Contents: Matrix Fundamentals / Determinants / Matrix Inversion / Linear Simultaneous Equation Sets / Orthogonal Transforms / Matrix Eigenvalue Analysis / Matrix Analysis of Vibrating Systems},
  added-at = {2016-07-26T01:19:49.000+0200},
  author = {Tobias, Marvin J.},
  biburl = {https://www.bibsonomy.org/bibtex/294822f5235257b9b4e81e5cfb45c64e6/vngudivada},
  doi = {10.2200/S00352ED1V01Y201105MAS010},
  eprint = {http://dx.doi.org/10.2200/S00352ED1V01Y201105MAS010},
  interhash = {2603eab6289352809a01758309d61380},
  intrahash = {94822f5235257b9b4e81e5cfb45c64e6},
  journal = {Synthesis Lectures on Mathematics and Statistics},
  keywords = {AppliedMathematics Matrices SynthesisLecture},
  number = 2,
  pages = {1-282},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Matrices in Engineering Problems},
  url = {http://dx.doi.org/10.2200/S00352ED1V01Y201105MAS010},
  volume = 4,
  year = 2011
}

@article{cappiello2003timerelated,
  abstract = {Modern organizations offer services through multiple channels, such as branches, ATMs, telephones, and Internet sites, and are supported by multifunctional software architectures. Different functional modules share data, which are typically stored in multiple local databases. Functional modules are usually not integrated across channels, as channels are implemented at different times within independent software projects and are subject to varying requirements of availability and performance. This lack of channel and functional integration raises data quality problems that can impact the quality of the products and services of an organization. In particular, in complex systems in which data are managed in multiple databases, timeliness is critical. This paper focuses on time-related factors of data quality and provides a model that can help companies to evaluate data currency, accuracy, and completeness in software architectures with different degrees of integration across channels and functionalities. The model is validated through simulation based on empirical data on financial information systems. Results indicate how architectural choices on the degree of data integration have a varying impact on currency, accuracy, and completeness depending on the type of financial institution and on customer profiles.},
  acmid = {1277665},
  added-at = {2016-07-24T01:12:31.000+0200},
  address = {Armonk, NY, USA},
  author = {Cappiello, Cinzia and Francalanci, Chiara and Pernici, Barbara},
  biburl = {https://www.bibsonomy.org/bibtex/28b697567df9ba1e6a0564d60c400ed7e/vngudivada},
  interhash = {a588c50a462c3f30d547df554f1da1d6},
  intrahash = {8b697567df9ba1e6a0564d60c400ed7e},
  issue_date = {Number 3/Winter 2003},
  journal = {J. Manage. Inf. Syst.},
  keywords = {DataQuality},
  month = dec,
  number = 3,
  pages = {71--92},
  publisher = {M. E. Sharpe, Inc.},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Time-Related Factors of Data Quality in Multichannel Information Systems},
  url = {http://dl.acm.org/citation.cfm?id=1277661.1277665},
  volume = 20,
  year = 2003
}

@inproceedings{bertix00c9quille2011discovery,
  abstract = {Quantitative Data Cleaning (QDC) is the use of statistical and other analytical techniques to detect, quantify, and correct data quality problems (or glitches). Current QDC approaches focus on addressing each category of data glitch individually. However, in real-world data, different types of data glitches co-occur in complex patterns. These patterns and interactions between glitches offer valuable clues for developing effective domain-specific quantitative cleaning strategies. In this paper, we address the shortcomings of the extant QDC methods by proposing a novel framework, the DEC (Detect-Explore-Clean) framework. It is a comprehensive approach for the definition, detection and cleaning of complex, multi-type data glitches. We exploit the distributions and interactions of different types of glitches to develop data-driven cleaning strategies that may offer significant advantages over blind strategies. The DEC framework is a statistically rigorous methodology for evaluating and scoring glitches and selecting the quantitative cleaning strategies that result in cleaned data sets that are statistically proximal to user specifications. We demonstrate the efficacy and scalability of the DEC framework on very large real-world and synthetic data sets.},
  added-at = {2016-07-24T00:18:08.000+0200},
  author = {Berti &#x00C9;quille, L. and Dasu, T. and Srivastava, D.},
  biburl = {https://www.bibsonomy.org/bibtex/2dd30fb1b014952eba4cded2d792b4ab9/vngudivada},
  booktitle = {2011 IEEE 27th International Conference on Data Engineering},
  doi = {10.1109/ICDE.2011.5767864},
  interhash = {3009039ae7aabf1d50ef1cfd1566bc42},
  intrahash = {dd30fb1b014952eba4cded2d792b4ab9},
  issn = {1063-6382},
  keywords = {DataCleaning DataQuality},
  month = {April},
  pages = {733-744},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Discovery of complex glitch patterns: A novel approach to Quantitative Data Cleaning},
  url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5767864},
  year = 2011
}

@inproceedings{saha2014quality,
  abstract = {In our Big Data era, data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Recent studies have shown that poor quality data is prevalent in large databases and on the Web. Since poor quality data can have serious consequences on the results of data analyses, the importance of veracity, the fourth `V' of big data is increasingly being recognized. In this tutorial, we highlight the substantial challenges that the first three `V's, volume, velocity and variety, bring to dealing with veracity in big data. Due to the sheer volume and velocity of data, one needs to understand and (possibly) repair erroneous data in a scalable and timely manner. With the variety of data, often from a diversity of sources, data quality rules cannot be specified a priori; one needs to let the &#x201C;data to speak for itself&#x201D; in order to discover the semantics of the data. This tutorial presents recent results that are relevant to big data quality management, focusing on the two major dimensions of (i) discovering quality issues from the data itself, and (ii) trading-off accuracy vs efficiency, and identifies a range of open problems for the community.},
  added-at = {2016-07-24T05:08:00.000+0200},
  author = {Saha, B. and Srivastava, D.},
  biburl = {https://www.bibsonomy.org/bibtex/224a98fd79c8b72c21f3683a43cee5a14/vngudivada},
  booktitle = {2014 IEEE 30th International Conference on Data Engineering},
  doi = {10.1109/ICDE.2014.6816764},
  interhash = {893e1044676c0e761d34e041e5da55ba},
  intrahash = {24a98fd79c8b72c21f3683a43cee5a14},
  issn = {1063-6382},
  keywords = {BigData DataQuality Tutorial},
  month = mar,
  pages = {1294-1297},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data quality: The other face of Big Data},
  year = 2014
}

@article{cohen2016bayesian,
  abstract = {Natural language processing (NLP) went through a profound transformation in the mid-1980s when it shifted to make heavy use of corpora and data-driven techniques to analyze language. Since then, the use of statistical techniques in NLP has evolved in several ways. One such example of evolution took place in the late 1990s or early 2000s, when full-fledged Bayesian machinery was introduced to NLP. This Bayesian approach to NLP has come to accommodate for various shortcomings in the frequentist approach and to enrich it, especially in the unsupervised setting, where statistical learning is done without target prediction examples.

We cover the methods and algorithms that are needed to fluently read Bayesian learning papers in NLP and to do research in the area. These methods and algorithms are partially borrowed from both machine learning and statistics and are partially developed "in-house" in NLP. We cover inference techniques such as Markov chain Monte Carlo sampling and variational inference, Bayesian estimation, and nonparametric modeling. We also cover fundamental concepts in Bayesian statistics such as prior distributions, conjugacy, and generative modeling. Finally, we cover some of the fundamental modeling techniques in NLP, such as grammar modeling and their use with Bayesian analysis.

Table of Contents: Preface / Acknowledgments / Preliminaries / Introduction / Priors / Bayesian Estimation / Sampling Methods / Variational Inference / Nonparametric Priors / Bayesian Grammar Models / Closing Remarks / Bibliography / Author's Biography / Index},
  added-at = {2016-07-26T00:48:28.000+0200},
  author = {Cohen, Shay},
  biburl = {https://www.bibsonomy.org/bibtex/2bec69ab914d5575a1f4c334d3778ec71/vngudivada},
  doi = {10.2200/S00719ED1V01Y201605HLT035},
  interhash = {f2d08f5faa7788ed670441ce279a5a95},
  intrahash = {bec69ab914d5575a1f4c334d3778ec71},
  journal = {Synthesis Lectures on Human Language Technologies},
  keywords = {BayesianModel NLP SynthesisLecture},
  number = 2,
  pages = {1 -- 274},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Bayesian Analysis in Natural Language Processing},
  volume = 9,
  year = 2016
}

@article{even2007utilitydriven,
  abstract = {Data consumers assess quality within specific business contexts or decision tasks. The same data resource may have an acceptable level of quality for some contexts but this quality may be unacceptable for other contexts. However, existing data quality metrics are mostly derived impartially, disconnected from the specific contextual characteristics. This study argues for the need to revise data quality metrics and measurement techniques to incorporate and better reflect contextual assessment. It contributes to that end by developing new metrics for assessing data quality along commonly used dimensions - completeness, validity, accuracy, and currency. The metrics are driven by data utility, a conceptual measure of the business value that is associated with the data within a specific usage context. The suggested data quality measurement framework uses utility as a scaling factor for calculating quality measurements at different levels of data hierarchy. Examples are used to demonstrate the use of utility-driven assessment in real-world data management scenarios and the broader implications for data management are discussed},
  acmid = {1240623},
  added-at = {2016-07-24T01:28:48.000+0200},
  address = {New York, NY, USA},
  author = {Even, Adir and Shankaranarayanan, G.},
  biburl = {https://www.bibsonomy.org/bibtex/250e1b313481515c5ffe411143dff7804/vngudivada},
  doi = {10.1145/1240616.1240623},
  interhash = {3a10949c5475a43d0d687925e9e37d67},
  intrahash = {50e1b313481515c5ffe411143dff7804},
  issn = {0095-0033},
  issue_date = {May 2007},
  journal = {SIGMIS Database},
  keywords = {DataQuality DataQualityAssessment},
  month = may,
  number = 2,
  numpages = {19},
  pages = {75--93},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Utility-driven Assessment of Data Quality},
  url = {http://doi.acm.org/10.1145/1240616.1240623},
  volume = 38,
  year = 2007
}

@article{barnard2016computational,
  abstract = {"This is clearly the most comprehensive and thoughtful compendium of knowledge on language/vision integration out there, and I'm sure it will be a valuable resources to many researchers and instructors." - Sven Dickinson, Series Editor (University of Toronto)

Modeling data from visual and linguistic modalities together creates opportunities for better understanding of both, and supports many useful applications. Examples of dual visual-linguistic data includes images with keywords, video with narrative, and figures in documents. We consider two key task-driven themes: translating from one modality to another (e.g., inferring annotations for images) and understanding the data using all modalities, where one modality can help disambiguate information in another. The multiple modalities can either be essentially semantically redundant (e.g., keywords provided by a person looking at the image), or largely complementary (e.g., meta data such as the camera used). Redundancy and complementarity are two endpoints of a scale, and we observe that good performance on translation requires some redundancy, and that joint inference is most useful where some information is complementary.

Computational methods discussed are broadly organized into ones for simple keywords, ones going beyond keywords toward natural language, and ones considering sequential aspects of natural language. Methods for keywords are further organized based on localization of semantics, going from words about the scene taken as whole, to words that apply to specific parts of the scene, to relationships between parts. Methods going beyond keywords are organized by the linguistic roles that are learned, exploited, or generated. These include proper nouns, adjectives, spatial and comparative prepositions, and verbs. More recent developments in dealing with sequential structure include automated captioning of scenes and video, alignment of video and text, and automated answering of questions about scenes depicted in images.

Table of Contents: Acknowledgments / Figure Credits / Introduction / The Semantics of Images and Associated Text / Sources of Data for Linking Visual and Linguistic Information / Extracting and Representing Visual Information / Text and Speech Processing / Modeling Images and Keywords / Beyond Simple Nouns / Sequential Structure / Bibliography / Author's Biography},
  added-at = {2016-07-26T01:03:51.000+0200},
  author = {Barnard, Kobus},
  biburl = {https://www.bibsonomy.org/bibtex/20a40c7104f1f987e01268079b7d37ee2/vngudivada},
  doi = {10.2200/S00705ED1V01Y201602COV007},
  eprint = {http://dx.doi.org/10.2200/S00705ED1V01Y201602COV007},
  interhash = {98565f5d6d73c75f5f6a325836fb8b10},
  intrahash = {0a40c7104f1f987e01268079b7d37ee2},
  journal = {Synthesis Lectures on Computer Vision},
  keywords = {ComputerVision NLP},
  number = 1,
  pages = {1-227},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Computational Methods for Integrating Vision and Language},
  url = {http://dx.doi.org/10.2200/S00705ED1V01Y201602COV007},
  volume = 6,
  year = 2016
}

@inproceedings{moussouni2007database,
  abstract = {In human health and life sciences, researchers extensively collaborate with each other, sharing genomic, biomedical and experimental results. This necessitates dynamically integrating different databases into a single repository or a warehouse. The data integrated in these warehouses are extracted from various heterogeneous sources, having different degrees of quality and trust. Most of the time, they are neither rigorously chosen nor carefully controlled for data quality. Data preparation and data quality metadata are recommended but still insufficiently exploited for ensuring quality and validating the results of information retrieval or data mining techniques. In a previous work, we built a data warehouse called GEDAW (Gene Expression Data Warehouse) that stores various information: data on genes expressed in the liver during iron overload and liver diseases, relevant information from public databanks (mostly in XML), DNA-chips home experiments and also medical records. Based on our past experience, this paper reports briefly on the lessons learned from biomedical data integration and data quality issues, and the solutions we propose to the numerous problems of schema evolution of both data sources and warehousing system. In this context, we present QDex, a Quality driven bio-Data Exploration tool, which provides a functional and modular architecture for database profiling and exploration, enabling users to set up query workflows and take advantage of data quality profiling metadata before the complex processes of data integration in the warehouse. An illustration with QDex Tool is shown afterwards.},
  added-at = {2016-07-24T04:47:16.000+0200},
  author = {Moussouni, Fouzia and Berti{-}Equille, Laure and Roz{\'{e}}, G. and Lor{\'{e}}al, Olivier and Gu{\'{e}}rin, Emilie},
  biburl = {https://www.bibsonomy.org/bibtex/229f4abc0606c44e13e0c8baf4c3646b7/vngudivada},
  booktitle = {Web Information Systems Engineering - {WISE} 2007 Workshops},
  interhash = {a30028d826c4f1b0e0d215a54124be08},
  intrahash = {29f4abc0606c44e13e0c8baf4c3646b7},
  keywords = {DataQuality ETL},
  pages = {5--16},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {QDex: {A} Database Profiler for Generic Bio-data Exploration and Quality Aware Integration},
  year = 2007
}

@article{krantz2011integral,
  abstract = {This book treats all of the most commonly used theories of the integral. After motivating the idea of integral, we devote a full chapter to the Riemann integral and the next to the Lebesgue integral. Another chapter compares and contrasts the two theories. The concluding chapter offers brief introductions to the Henstock integral, the Daniell integral, the Stieltjes integral, and other commonly used integrals. The purpose of this book is to provide a quick but accurate (and detailed) introduction to all aspects of modern integration theory. It should be accessible to any student who has had calculus and some exposure to upper division mathematics.

Table of Contents: Introduction / The Riemann Integral / The Lebesgue Integral / Comparison of the Riemann and Lebesgue Integrals / Other Theories of the Integral},
  added-at = {2016-07-26T01:28:31.000+0200},
  author = {Krantz, Steven G.},
  biburl = {https://www.bibsonomy.org/bibtex/24c1a3ec8e5ba8e74442f5b9c7867be74/vngudivada},
  doi = {10.2200/S00327ED1V01Y201101MAS009},
  eprint = {http://dx.doi.org/10.2200/S00327ED1V01Y201101MAS009},
  interhash = {5de7483f63f87cdc5b51044ab2fbb880},
  intrahash = {4c1a3ec8e5ba8e74442f5b9c7867be74},
  journal = {Synthesis Lectures on Mathematics and Statistics},
  keywords = {Mathematics},
  number = 1,
  pages = {1-105},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {The Integral: A Crux for Analysis},
  url = {http://dx.doi.org/10.2200/S00327ED1V01Y201101MAS009},
  volume = 4,
  year = 2011
}

@article{monge2000matching,
  abstract = {Detecting database records that are approximate duplicates, but not exact duplicates, is an important task. Databases may contain duplicate records concerning the same real-world entity because of data entry errors, unstandardized abbreviations, or differences in the detailed schemas of records from multi- ple databases - such as what happens in data warehousing where records from multiple data sources are integrated into a single source of information - among other reasons. In this paper we review a system to detect approximate duplicate records in a database and provide properties that a pair-wise record matching algorithm must have in order to have a successful duplicate detection system. Many of the current technological improvements has lead to an explosion in the growth of data available in digital form. The most significant of these being the popularity of the Internet and specifically the world wide web. There are other more traditional sources of data however that has also contributed to this exponential growth. The commercial success of relational databases in the early 1980's has lead to the efficient storage and retrieval of data. Hardware improvements have also contributed significantly now that external storage has faster access times and continue to increase in density. Such technological changes allow for easy and widespread distribution and publishing of data. The availability of these data sources increases not only the amount of data, but also the variety of and quality in which such data appears. These factors create a number of problems. The work presented here concentrates on one such problem: the detection of multiple representations of a single entity.},
  added-at = {2016-07-24T00:33:00.000+0200},
  author = {Monge, Alvaro E.},
  biburl = {https://www.bibsonomy.org/bibtex/2dfb72112f1ab692d2440dc4307024557/vngudivada},
  interhash = {1c6abef9029f359a6fd766a98bf4c50c},
  intrahash = {dfb72112f1ab692d2440dc4307024557},
  journal = {Bulletin of the Technical Committee on Data Engineering},
  keywords = {DuplicateDetection MatchingAlgorithm},
  pages = {14 - 20},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Matching Algorithms within a Duplicate Detection System},
  volume = 23,
  year = 2000
}

@misc{blanco2016characterizing,
  abstract = {An increasing number of web sites offer structured information about recognizable concepts, relevant to many application domains, such as finance, sport, commercial products. However, web data is inherently imprecise and uncertain, and conflicting values can be provided by different web sources. Characterizing the uncertainty of web data represents an important issue and several models have been recently proposed in the literature. The paper illustrates state-of-the-art Bayesan models to evaluate the quality of data extracted from the Web and reports the results of an extensive application of the models on real life web data. Our experimental results show that for some applications even simple approaches can provide effective results, while sophisticated solutions are needed to obtain a more precise characterization of the uncertainty.},
  added-at = {2016-07-24T04:23:07.000+0200},
  author = {Blanco, Lorenzo},
  biburl = {https://www.bibsonomy.org/bibtex/25d25e51a36af097361397ddb5f77b694/vngudivada},
  interhash = {26698592551aed39bb8f8453d9e5eb7d},
  intrahash = {5d25e51a36af097361397ddb5f77b694},
  keywords = {DataQuality WebData},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Characterizing the Uncertainty of Web Data: Models and Experiences},
  year = 2016
}

@misc{crain2013indatabase,
  abstract = {Data quality and high performance have joined forces. Today is an era of big data, extremely large data warehouses and potential security issues for moving data. Traditional data quality is performed with an ETL-like operation of extracting, processing, and publishing back to the source. Performance, or a potential for security issues associated with moving data, requires a call for a new approach. This paper explains how SAS® data quality functions can be invoked in-database, eliminating the need to move data and thus delivering data quality that meets the need for near real-time performance for today’s business. Graphic results comparing performance metrics of traditional data quality operations against in-database data quality will be presented along with details illustrating how these results scale with database resources.},
  added-at = {2016-07-24T04:53:02.000+0200},
  author = {Crain, Charlotte and Frost, Mike and Gidley, Scott},
  biburl = {https://www.bibsonomy.org/bibtex/2fe5fb500fe0d413f22d5175fcb45f004/vngudivada},
  interhash = {da190b1c7b6ecae40f9dce85b859009e},
  intrahash = {fe5fb500fe0d413f22d5175fcb45f004},
  keywords = {DataQuality SAS},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {In-Database Data Quality - Performance for Big Data},
  year = 2013
}

@inproceedings{furber2011towards,
  abstract = {Reliable decision-making and reliable information based on Semantic Web data requires methodologies and techniques for managing the quality of the published data. To make things more complicated, the judgment of what is "good" data will often depend on the task at hand or the subjective requirements of data owners or data consumers. Some data quality requirements can be modeled using data quality rules, i.e. executable definitions that allow the identification and measurement of data quality problems. In this paper, we provide a conceptual model that allows the representation of such rules and other quality-related knowledge using the Resource Description Framework (RDF) and the Web Ontology Language (OWL). Based on our model, it is possible to monitor and assess the quality of data sources and to automate data cleansing tasks. The use of a generic conceptual model based on Semantic Web formalisms supports the definition of reusable, broadly applicable SPARQL queries and portable applications for data quality management (DQM). Furthermore, the explicit representation of rules in RDF/OWL facilitates rule management tasks, e.g. for analyzing consistency among the rules, and allows to collaborate and create a shared understanding.},
  acmid = {1966903},
  added-at = {2016-07-24T01:15:41.000+0200},
  address = {New York, NY, USA},
  author = {F\"{u}rber, Christian and Hepp, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/290fe3d9aa2ddac353a0964a8a2c2b6cb/vngudivada},
  booktitle = {Proceedings of the 1st International Workshop on Linked Web Data Management},
  doi = {10.1145/1966901.1966903},
  interhash = {ba0a0e2e1ed2c542645a3dae717e0f73},
  intrahash = {90fe3d9aa2ddac353a0964a8a2c2b6cb},
  isbn = {978-1-4503-0608-9},
  keywords = {DataQuality DataQualityManagement},
  location = {Uppsala, Sweden},
  numpages = {8},
  pages = {1--8},
  publisher = {ACM},
  series = {LWDM '11},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Towards a Vocabulary for Data Quality Management in Semantic Web Architectures},
  url = {http://doi.acm.org/10.1145/1966901.1966903},
  year = 2011
}

@inproceedings{mockus2014engineering,
  abstract = {Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles.},
  acmid = {2593889},
  added-at = {2016-07-24T04:57:42.000+0200},
  address = {New York, NY, USA},
  author = {Mockus, Audris},
  biburl = {https://www.bibsonomy.org/bibtex/2ef95e61b751b6407822f84d3de19dad7/vngudivada},
  booktitle = {Proceedings of the on Future of Software Engineering},
  doi = {10.1145/2593882.2593889},
  interhash = {ede41d7a71223911efd7aa12cb6f52aa},
  intrahash = {ef95e61b751b6407822f84d3de19dad7},
  isbn = {978-1-4503-2865-4},
  keywords = {BigData DataQuality},
  location = {Hyderabad, India},
  numpages = {15},
  pages = {85--99},
  publisher = {ACM},
  series = {FOSE 2014},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Engineering Big Data Solutions},
  url = {http://doi.acm.org/10.1145/2593882.2593889},
  year = 2014
}

@article{batini2015quality,
  abstract = {This article investigates the evolution of data quality issues from traditional structured data managed in relational databases to Big Data. In particular, the paper examines the nature of the relationship between Data Quality and several research coordinates that are relevant in Big Data, such as the variety of data types, data sources and application domains, focusing on maps, semi-structured texts, linked open data, sensor & sensor networks and official statistics. Consequently a set of structural characteristics is identified and a systematization of the a posteriori correlation between them and quality dimensions is provided. Finally, Big Data quality issues are considered in a conceptual framework suitable to map the evolution of the quality paradigm according to three core coordinates that are significant in the context of the Big Data phenomenon: the data type considered, the source of data, and the application domain. Thus, the framework allows ascertaining the relevant changes in data quality emerging with the Big Data phenomenon, through an integrative and theoretical literature review.},
  acmid = {2845129},
  added-at = {2016-07-25T14:06:10.000+0200},
  address = {Hershey, PA, USA},
  author = {Batini, Carlo and Rula, Anisa and Scannapieco, Monica and Viscusi, Gianluigi},
  biburl = {https://www.bibsonomy.org/bibtex/2458ed6cd36ec9891b364faf30a32f00d/vngudivada},
  interhash = {073e2bc3a727ec88b50faf9c93eb8b6f},
  intrahash = {458ed6cd36ec9891b364faf30a32f00d},
  issue_date = {January 2015},
  journal = {J. Database Manage.},
  keywords = {BigDataQuality DataQuality},
  month = jan,
  number = 1,
  pages = {60--82},
  publisher = {IGI Global},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {From Data Quality to Big Data Quality},
  volume = 26,
  year = 2015
}

@techreport{becker2013quality,
  abstract = {A set of four case studies related to data quality in the context of the management and use of Big Data are being performed and reported separately; these will also be compiled into a summary overview report. The report herein documents one of those four cases studies. The purpose of this document is to present information about the various data quality issues related to the design, implementation and operation of a specific data initiative, the U.S. Army’s Medical Command (MEDCOM) Medical Operational Data System (MODS) project. While MODS is not currently a Big Data initiative, potential future Big Data requirements under consideration (in the areas of geospatial data, document and records data, and textual data) could easily move MODS into the realm of Big Data. Each of these areas has its own data quality issues that must be considered. By better understanding the data quality issues in these Big Data areas of growth, we hope to explore specific differences in the nature and type of Big Data quality problems from what is typically experienced in traditionally sized data sets. This understanding should facilitate the acquisition of the MODS data warehouse though improvements in the requirements and downstream design efforts. It should also enable the crafting of better strategies and tools for profiling, measurement, assessment, and action processing of Big Data Quality problems.},
  added-at = {2016-07-24T04:15:54.000+0200},
  author = {Becker, Dave and King, Trish Dunn and McMullen, Bill and Lalis, Lisa Deifer and Bloom, David and Obaidi, Ali and Fickett, Donna},
  biburl = {https://www.bibsonomy.org/bibtex/2dd18148a3a3d0b683b11b1f186d344ca/vngudivada},
  institution = {The MITRE Corporation},
  interhash = {69533862235d19976354ed2afa58cfb1},
  intrahash = {dd18148a3a3d0b683b11b1f186d344ca},
  keywords = {BigData CaseStudy DataQuality},
  month = sep,
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Big Data Quality Case Study Preliminary Findings},
  year = 2013
}

@techreport{eckerson2016quality,
  abstract = {As we enter the 21st century, we are at the dawn of the Information Age. Data and information are now as vital to an organization’s well being and future success as oxygen is to humans. And without a fresh supply of clean, unpolluted data, companies will struggle to survive. The Data Warehousing Institute estimates that data quality problems cost U.S. businesses more than $600 billion a year. Yet, most executives are oblivious to the data quality lacerations that are slowly bleeding their companies to death. More injurious than the unnecessary printing, postage, and staffing costs is the slow but steady erosion of an organization’s credibility among customers and suppliers, as well as its inability to make sound decisions based on accurate information. The problem with data is that its quality quickly degenerates over time. Experts say 2 percent of records in a customer file become obsolete in one month because customers die, divorce, marry, and move. In addition, data entry errors, systems migrations, and changes to source systems, among other things, generate bucket loads of errors. More perniciously, as organizations fragment into different divisions and units, interpretations of data elements mutate to meet the local business needs. A data element that one individual finds valuable may be nonsense to an individual in a different group. Fortunately, new strategic initiatives, such as CRM, business intelligence, and supply chain management are sounding a wake-up call to top executives. Many are learning the hard way that data quality problems can sabotage the best laid strategies and expose errors to a much broader, and critical, external audience. The Goal Is Achievable. The good news is that achieving high quality data is not beyond the means of any company. The keys are to treat data as a strategic corporate resource; develop a program for managing data quality with a commitment from the top; and hire, train, or outsource experienced data quality professionals to oversee and carry out the program. Then, it is critical for organizations to sustain a commitment to managing data quality over time and adjust monitoring and cleansing processes to changes in the business and underlying systems. Commercial data quality tools and service bureaus automate the process of auditing, cleaning, and monitoring data quality. They can play a significant role in data quality efforts and be well worth the investment. Most commercial tools are now moving beyond auditing and scrubbing name and address data to tackle other data types. They are also beginning to step up to the challenge of validating company-specific business rules, and augmenting addresses with geospatial and demographic data, among other things. Data is a vital resource. Companies that invest proportionally to manage this resource will stand a stronger chance of succeeding in today’s competitive global economy than those that squander this critical resource by neglecting to ensure adequate levels of quality},
  added-at = {2016-07-24T04:27:25.000+0200},
  author = {Eckerson, Wayne W.},
  biburl = {https://www.bibsonomy.org/bibtex/2fc3b1856bf3e102d0d662e4bc8a7b120/vngudivada},
  institution = {THE DATA WAREHOUSING INSTITUTE},
  interhash = {65721a9179b388c3b9fe6d99f2ec8d3b},
  intrahash = {fc3b1856bf3e102d0d662e4bc8a7b120},
  keywords = {DataQuality Economics},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data Quality and the Bottom Line: Achieving Business Success through a Commitment to High Quality Data},
  url = {/brokenurl#www.dw-institute.com},
  year = 2016
}

@article{klein2009representing,
  abstract = {Sensors in smart-item environments capture data about product conditions and usage to support business decisions as well as production automation processes. A challenging issue in this application area is the restricted quality of sensor data due to limited sensor precision and sensor failures. Moreover, data stream processing to meet resource constraints in streaming environments introduces additional noise and decreases the data quality. In order to avoid wrong business decisions due to dirty data, quality characteristics have to be captured, processed, and provided to the respective business task. However, the issue of how to efficiently provide applications with information about data quality is still an open research problem. In this article, we address this problem by presenting a flexible model for the propagation and processing of data quality. The comprehensive analysis of common data stream processing operators and their impact on data quality allows a fruitful data evaluation and diminishes incorrect business decisions. Further, we propose the data quality model control to adapt the data quality granularity to the data stream interestingness.},
  acmid = {1577845},
  added-at = {2016-07-24T00:41:12.000+0200},
  address = {New York, NY, USA},
  articleno = {10},
  author = {Klein, A. and Lehner, W.},
  biburl = {https://www.bibsonomy.org/bibtex/21bde2012e1a946035ae7d27add362f5b/vngudivada},
  doi = {10.1145/1577840.1577845},
  interhash = {70a9aa6ef9dab047bcd5585d85762443},
  intrahash = {1bde2012e1a946035ae7d27add362f5b},
  issn = {1936-1955},
  issue_date = {September 2009},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality SensorData StreamingData},
  month = sep,
  number = 2,
  numpages = {28},
  pages = {10:1--10:28},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Representing Data Quality in Sensor Data Streaming Environments},
  url = {http://doi.acm.org/10.1145/1577840.1577845},
  volume = 1,
  year = 2009
}

@misc{gft2016component,
  added-at = {2016-07-24T04:49:43.000+0200},
  author = {GFT},
  biburl = {https://www.bibsonomy.org/bibtex/241965101a2ffce44951d43f000a43e5c/vngudivada},
  interhash = {be05b3325ac81caa30166b1861e284f5},
  intrahash = {41965101a2ffce44951d43f000a43e5c},
  keywords = {DataQuality},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Component Driven Methodology: At the heart of GFT's Data Quality Practice},
  year = 2016
}

@book{batini2006quality,
  abstract = {Presents a comprehensive introduction to the array of issues related to data quality. Beginning with a description of the parameters of data quality, this text describes techniques and methodologies from core data quality research as well as from related fields like data mining, probability theory, statistical data analysis, and machine learning.},
  added-at = {2016-07-25T14:19:48.000+0200},
  address = {Berlin; New York},
  author = {Batini, Carlo and Scannapieca, Monica},
  biburl = {https://www.bibsonomy.org/bibtex/2201d0728ba4c140b7c9a93726f474cb0/vngudivada},
  interhash = {80256fe02933f051db3927c88da432b3},
  intrahash = {201d0728ba4c140b7c9a93726f474cb0},
  isbn = {3540331727 9783540331728 3540331735 9783540331735},
  keywords = {Book DataQuality},
  publisher = {Springer},
  refid = {77530791},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data quality : concepts, methodologies and techniques},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=323344},
  year = 2006
}

@inproceedings{huner2009towards,
  abstract = {High-quality corporate data is a prerequisite for world-wide business process harmonization, global spend analysis, integrated service management, and compliance with regulatory and legal requirements. Corporate Data Quality Management (CDQM) describes the quality oriented organization and control of a company's key data assets such as material, customer, and vendor data. With regard to the aforementioned business drivers, companies demand an instrument to assess the progress and performance of their CDQM initiative. This paper proposes a reference model for CDQM maturity assessment. The model is intended to be used for supporting the build process of CDQM. A case study shows how the model has been successfully implemented in a real-world scenario.},
  acmid = {1529334},
  added-at = {2016-07-24T01:14:25.000+0200},
  address = {New York, NY, USA},
  author = {H\"{u}ner, Kai M. and Ofner, Martin and Otto, Boris},
  biburl = {https://www.bibsonomy.org/bibtex/2eea8d29515932accb46d2886694c5471/vngudivada},
  booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
  doi = {10.1145/1529282.1529334},
  interhash = {dc9e7461cbf4a30c3090b63b122e275f},
  intrahash = {eea8d29515932accb46d2886694c5471},
  isbn = {978-1-60558-166-8},
  keywords = {DataQuality DataQualityManagement},
  location = {Honolulu, Hawaii},
  numpages = {8},
  pages = {231--238},
  publisher = {ACM},
  series = {SAC '09},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Towards a Maturity Model for Corporate Data Quality Management},
  url = {http://doi.acm.org/10.1145/1529282.1529334},
  year = 2009
}

@book{christen2012matching,
  abstract = {
Data matching (also known as record or data linkage, entity resolution, object identification, or field matching) is the task of identifying, matching and merging records that correspond to the same entities from several databases or even within one database. Based on research in various domains including applied statistics, health informatics, data mining, machine learning, artificial intelligence, database management, and digital libraries, significant advances have been achieved over the last decade in all aspects of the data matching process, especially on how to improve the accuracy of data matching, and its scalability to large databases.

Peter Christen's book is divided into three parts: Part I, ``Overview'', introduces the subject by presenting several sample applications and their special challenges, as well as a general overview of a generic data matching process. Part II, ``Steps of the Data Matching Process'', then details its main steps like pre-processing, indexing, field and record comparison, classification, and quality evaluation. Lastly, part III, ``Further Topics'', deals with specific aspects like privacy, real-time matching, or matching unstructured data. Finally, it briefly describes the main features of many research and open source systems available today.

By providing the reader with a broad range of data matching concepts and techniques and touching on all aspects of the data matching process, this book helps researchers as well as students specializing in data quality or data matching aspects to familiarize themselves with recent research advances and to identify open research challenges in the area of data matching. To this end, each chapter of the book includes a final section that provides pointers to further background and research material. Practitioners will better understand the current state of the art in data matching as well as the internal workings and limitations of current systems. Especially, they will learn that it is often not feasible to simply implement an existing off-the-shelf data matching system without substantial adaption and customization. Such practical considerations are discussed for each of the major steps in the data matching process.},
  added-at = {2016-07-24T01:00:31.000+0200},
  address = {Berlin; New York},
  author = {Christen, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/27d5efaa756ade45a39638eabeb05dcb9/vngudivada},
  interhash = {b4830645e96075017831185ddf6f27fa},
  intrahash = {7d5efaa756ade45a39638eabeb05dcb9},
  isbn = {9783642311642 3642311644},
  keywords = {Book DataMatching DuplicateDetection EntityMatching RecordLinking},
  publisher = {Springer},
  refid = {799876026},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data matching: concepts and techniques for record linkage, entity resolution, and duplicate detection},
  url = {http://www.books24x7.com/marc.asp?bookid=50706},
  year = 2012
}

@techreport{sas2016understanding,
  abstract = {It’s easy to get caught up in the idea of big data. You’ve got massive amounts of data streaming into your business – and it has lots of potential. How can you apply analytics to reveal key business insights? What can you learn about your customers and products? How soon can you get started?

But wait. There are things you need to do first – reliable data management practices you need to put in place before fully taking advantage of big data. This paper explains why data quality and data governance are so important to large-scale analytics. It will help you learn how to balance governance with usability so you can come up with a strategic plan for managing big data, and it includes a checklist of things to look for when evaluating data management tools for big data.},
  added-at = {2016-07-24T01:21:26.000+0200},
  author = {SAS},
  biburl = {https://www.bibsonomy.org/bibtex/217cd6135884a51634ecddc63346d08d2/vngudivada},
  interhash = {8973a5f3ad45651ea62566cd63788a2f},
  intrahash = {17cd6135884a51634ecddc63346d08d2},
  keywords = {BigData DataQuality},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Understanding Big Data Quality for Maximum Information Usability},
  year = 2016
}

@techreport{informatica2014quality,
  added-at = {2016-07-24T04:51:20.000+0200},
  author = {Informatica},
  biburl = {https://www.bibsonomy.org/bibtex/213fd3d1e8263bc42258f4c962f328991/vngudivada},
  interhash = {cd70ab30e8e20f7ed2e9014e1b9613af},
  intrahash = {13fd3d1e8263bc42258f4c962f328991},
  keywords = {DataQuality WhitePaper},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data Quality Management: Beyond the Basics},
  year = 2014
}

@inproceedings{wang2014sampleandclean,
  abstract = {In emerging Big Data scenarios, obtaining timely, high-quality answers to aggregate queries is difficult due to the challenges of processing and cleaning large, dirty data sets. To increase the speed of query processing, there has been a resurgence of interest in sampling-based approximate query processing (SAQP). In its usual formulation, however, SAQP does not address data cleaning at all, and in fact, exacerbates answer quality problems by introducing sampling error. In this paper, we explore an intriguing opportunity. That is, we explore the use of sampling to actually improve answer quality. We introduce the Sample-and-Clean framework, which applies data cleaning to a relatively small subset of the data and uses the results of the cleaning process to lessen the impact of dirty data on aggregate query answers. We derive confidence intervals as a function of sample size and show how our approach addresses error bias. We evaluate the Sample-and-Clean framework using data from three sources: the TPC-H benchmark with synthetic noise, a subset of the Microsoft academic citation index and a sensor data set. Our results are consistent with the theoretical confidence intervals and suggest that the Sample-and-Clean framework can produce significant improvements in accuracy compared to query processing without data cleaning and speed compared to data cleaning without sampling.},
  acmid = {2610505},
  added-at = {2016-07-24T00:51:37.000+0200},
  address = {New York, NY, USA},
  author = {Wang, Jiannan and Krishnan, Sanjay and Franklin, Michael J. and Goldberg, Ken and Kraska, Tim and Milo, Tova},
  biburl = {https://www.bibsonomy.org/bibtex/2bc265e2bc93e6ded519f2b27d72a7f69/vngudivada},
  booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
  doi = {10.1145/2588555.2610505},
  interhash = {305939cf7851df6011194c7dd227d4be},
  intrahash = {bc265e2bc93e6ded519f2b27d72a7f69},
  isbn = {978-1-4503-2376-5},
  keywords = {DataCleaning DataQuality},
  location = {Snowbird, Utah, USA},
  numpages = {12},
  pages = {469--480},
  publisher = {ACM},
  series = {SIGMOD '14},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {A Sample-and-clean Framework for Fast and Accurate Query Processing on Dirty Data},
  url = {http://doi.acm.org/10.1145/2588555.2610505},
  year = 2014
}

@misc{oracle2014control,
  added-at = {2016-07-24T04:29:39.000+0200},
  author = {Oracle},
  biburl = {https://www.bibsonomy.org/bibtex/20f3274319da86ab3789292c578cf6876/vngudivada},
  interhash = {0622a410f3eae6884e993114299a63bc},
  intrahash = {0f3274319da86ab3789292c578cf6876},
  keywords = {DataQuality},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Take Control of Data Governance and Data Quality},
  year = 2014
}

@article{xu2015important,
  abstract = {The accounting information system (AIS) is one of the most critical systems in any organization. Data quality plays a critical role in a data-intensive, knowledge-based economy. The objective of this study is to identify the most important factors for accounting information quality and their impact on AIS data quality outcomes. The article includes an extensive literature review and summarizes studies in quality management, data quality, accounting information systems, and enterprise planning in helping to identify a set of critical success factors for data quality. The study uses empirical data to answer the research question and test the research hypothesis. Study results show that the top three most important factors that affect accounting information systems’ data quality are top management commitment, the nature of the accounting information systems (such as the suitability of the systems), and input controls. The article further uses regression analysis to test the effect of those factors on AIS data quality, finding that there is a significant positive relationship between the perceived performance of the three most important factors and perceived AIS data quality outcomes.},
  acmid = {2700833},
  added-at = {2016-07-24T01:31:02.000+0200},
  address = {New York, NY, USA},
  articleno = {14},
  author = {Xu, Hongjiang},
  biburl = {https://www.bibsonomy.org/bibtex/23ab676705e1c135c4be014066b25064f/vngudivada},
  doi = {10.1145/2700833},
  interhash = {60bf4a81b1324d99ff2abd8f6185cff4},
  intrahash = {3ab676705e1c135c4be014066b25064f},
  issn = {1936-1955},
  issue_date = {February 2015},
  journal = {J. Data and Information Quality},
  keywords = {DataQuality},
  month = mar,
  number = 4,
  numpages = {22},
  pages = {14:1--14:22},
  publisher = {ACM},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {What Are the Most Important Factors for Accounting Information Quality and Their Impact on AIS Data Quality Outcomes?},
  url = {http://doi.acm.org/10.1145/2700833},
  volume = 5,
  year = 2015
}

@inproceedings{mimno2011bayesian,
  abstract = {Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.},
  added-at = {2016-07-26T00:42:05.000+0200},
  address = {Stroudsburg, PA, USA},
  author = {Mimno, David and Blei, David},
  biburl = {https://www.bibsonomy.org/bibtex/2916f75fdaaf4146113b5849e906f8cad/vngudivada},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  interhash = {34be32037be00e86c151f972b9f64825},
  intrahash = {916f75fdaaf4146113b5849e906f8cad},
  keywords = {BayesianModel TopicModel},
  pages = {227--237},
  publisher = {Association for Computational Linguistics},
  series = {EMNLP '11},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Bayesian Checking for Topic Models},
  year = 2011
}

@book{talburt2011entity,
  abstract = {Customers and products are the heart of any business, and corporations collect more data about them every year. However, just because you have data doesn't mean you can use it effectively. If not properly integrated, data can actually encourage false conclusions that result in bad decisions and lost opportunities. Entity Resolution (ER) is a powerful tool for transforming data into accurate, value-added information. Using entity resolution methods and techniques, you can identify equivalent records from multiple sources corresponding to the same real-world person, place, or thing. This emerging area of data management is clearly explained throughout the book. It teaches you the process of locating and linking information about the same entity - eliminating duplications - and making crucial business decisions based on the results. This book is an authoritative, vendor-independent technical reference for researchers, graduate students and practitioners, including architects, technical analysts, and solution developers. In short, Entity Resolution and Information Quality gives you the applied level know-how you need to aggregate data from disparate sources and form accurate customer and product profiles that support effective marketing and sales. It is an invaluable guide for succeeding in today's info-centric environment. First authoritative reference explaining entity resolution and how to use it effectively Provides practical system design advice to help you get a competitive advantage Includes a companion site with synthetic customer data for applicatory exercises, and access to a Java-based Entity Resolution program.},
  added-at = {2016-07-24T03:53:25.000+0200},
  address = {San Francisco, Calif.; Oxford},
  author = {Talburt, John R.},
  biburl = {https://www.bibsonomy.org/bibtex/23fd49eb92167311a665e41eac93935a9/vngudivada},
  interhash = {cdf4c72a6f6961c13f763654c32451eb},
  intrahash = {3fd49eb92167311a665e41eac93935a9},
  isbn = {9780123819727 0123819725 9780123819734 0123819733 1282955020 9781282955028},
  keywords = {Book DataQuality EntityResolution InformationQuality},
  publisher = {Morgan Kaufmann ; Elsevier Science [distributor]},
  refid = {694441104},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Entity resolution and information quality},
  url = {http://www.123library.org/book_details/?id=37259},
  year = 2011
}

@inproceedings{lima2006research,
  abstract = {Information Quality (IQ), though not a recent study field, faces a problem in organizing its theoretical body. Furthermore, lack of a defined, robust field of knowledge for research in IQ is detected. The purpose of this article is to analyze the state-of-the-art in the scientific production within the area, focusing on thematic and methodological aspects. One-hundred and seventy one articles were catalogued within five major events. In order to organize the themes collected in IQ, the structure supplied by Conceptual Maps for knowledge organization was used; as a result of this approach, three central views for research into IQ are proposed: Organizational, Behavioral, and Operational.},
  added-at = {2016-07-24T00:49:40.000+0200},
  author = {Lima, Luís Francisco Ramos and Maçada, Antonio Carlos Gastaud and Vargas, Lilia Maria},
  biburl = {https://www.bibsonomy.org/bibtex/2b6c833aac54ec80cd3b004a8254adaac/vngudivada},
  booktitle = {ICIQ},
  crossref = {conf/iq/2006},
  editor = {Talburt, John R. and Pierce, Elizabeth M. and Wu, Ningning and Campbell, Traci},
  ee = {http://mitiq.mit.edu/iciq/iqdownload.aspx?ICIQYear=2006&File=Research+into+Information+Quality-+A+Study+of+the+State-of-the+Art+in+IQ+and+its+Consolidation.pdf},
  interhash = {809967a3d6f2e0133a418008317bc9ac},
  intrahash = {b6c833aac54ec80cd3b004a8254adaac},
  keywords = {DataQuality InformationQuality},
  pages = {146-158},
  publisher = {MIT},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Research into Information Quality: A Study of the State of the Art in IQ and Its Consolidation},
  url = {http://dblp.uni-trier.de/db/conf/iq/iq2006.html#LimaMV06},
  year = 2006
}

@article{carlo2011quality,
  abstract = {We present a Heterogenous Data Quality Methodology (HDQM) for Data Quality (DQ) assessment and improvement that considers all types of data managed in an organization, namely structured data represented in databases, semistructured data usually represented in XML, and unstructured data represented in documents. We also define a meta-model in order to describe the relevant knowledge managed in the methodology. The different types of data are translated in a common conceptual representation. We consider two dimensions widely analyzed in the specialist literature and used in practice: Accuracy and Currency. The methodology provides stakeholders involved in DQ management with a complete set of phases for data quality assessment and improvement. A non trivial case study from the business domain is used to illustrate and validate the methodology.},
  added-at = {2016-07-25T14:09:23.000+0200},
  author = {Carlo, Batini and Daniele, Barone and Federico, Cabitza and Simone, Grega},
  biburl = {https://www.bibsonomy.org/bibtex/2b66944a808b25926abedca2dbb5272e5/vngudivada},
  doi = {10.5121/ijdms.2011.3105},
  interhash = {6913173762d096ed90ff5141f34a2971},
  intrahash = {b66944a808b25926abedca2dbb5272e5},
  journal = {{IJDMS}},
  keywords = {DataQuality HeterogeneousData},
  month = feb,
  number = 1,
  pages = {60--79},
  publisher = {Academy and Industry Research Collaboration Center ({AIRCC})},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {A Data Quality Methodology for Heterogeneous Data},
  url = {http://dx.doi.org/10.5121/ijdms.2011.3105},
  volume = 3,
  year = 2011
}

@inproceedings{mecca2012transformation,
  abstract = {Mapping and translating data across different representations is a crucial problem in information systems. Many formalisms and tools are currently used for this purpose, to the point that developers typically face a difficult question: "what is the right tool for my translation task?" In this paper, we introduce several techniques that contribute to answer this question. Among these, a fairly general definition of a data transformation system, a new and very efficient similarity measure to evaluate the outputs produced by such a system, and a metric to estimate user efforts. Based on these techniques, we are able to compare a wide range of systems on many translation tasks, to gain interesting insights about their effectiveness, and, ultimately, about their "intelligence".},
  acmid = {2396872},
  added-at = {2016-07-24T00:46:43.000+0200},
  address = {New York, NY, USA},
  author = {Mecca, Giansalvatore and Papotti, Paolo and Raunich, Salvatore and Santoro, Donatello},
  biburl = {https://www.bibsonomy.org/bibtex/2125aefbc3791f64597dd4a42f1f2cda9/vngudivada},
  booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
  doi = {10.1145/2396761.2396872},
  interhash = {1b3ad8e7c334db26f631f1b57988e654},
  intrahash = {125aefbc3791f64597dd4a42f1f2cda9},
  isbn = {978-1-4503-1156-4},
  keywords = {DataQuality ETL InformationQuality},
  location = {Maui, Hawaii, USA},
  numpages = {10},
  pages = {872--881},
  publisher = {ACM},
  series = {CIKM '12},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {What is the IQ of Your Data Transformation System?},
  url = {http://doi.acm.org/10.1145/2396761.2396872},
  year = 2012
}

@inproceedings{kontokostas2014testdriven,
  abstract = {Linked Open Data (LOD) comprises an unprecedented volume of structured data on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowdsourced or extracted data of often relatively low quality. We present a methodology for test-driven quality assessment of Linked Data, which is inspired by test-driven software development. We argue that vocabularies, ontologies and knowledge bases should be accompanied by a number of test cases, which help to ensure a basic level of quality. We present a methodology for assessing the quality of linked data resources, based on a formalization of bad smells and data quality problems. Our formalization employs SPARQL query templates, which are instantiated into concrete quality test case queries. Based on an extensive survey, we compile a comprehensive library of data quality test case patterns. We perform automatic test case instantiation based on schema constraints or semi-automatically enriched schemata and allow the user to generate specific test case instantiations that are applicable to a schema or dataset. We provide an extensive evaluation of five LOD datasets, manual test case instantiation for five schemas and automatic test case instantiations for all available schemata registered with Linked Open Vocabularies (LOV). One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics.},
  acmid = {2568002},
  added-at = {2016-07-24T01:05:07.000+0200},
  address = {New York, NY, USA},
  author = {Kontokostas, Dimitris and Westphal, Patrick and Auer, S\"{o}ren and Hellmann, Sebastian and Lehmann, Jens and Cornelissen, Roland and Zaveri, Amrapali},
  biburl = {https://www.bibsonomy.org/bibtex/2055b0803da55ef8abb539d2a622d53e8/vngudivada},
  booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
  doi = {10.1145/2566486.2568002},
  interhash = {66a6d782062b615d9b4fa141ceb2473a},
  intrahash = {055b0803da55ef8abb539d2a622d53e8},
  isbn = {978-1-4503-2744-2},
  keywords = {DataQuality LinkedData},
  location = {Seoul, Korea},
  numpages = {12},
  pages = {747--758},
  publisher = {ACM},
  series = {WWW '14},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Test-driven Evaluation of Linked Data Quality},
  url = {http://doi.acm.org/10.1145/2566486.2568002},
  year = 2014
}

@misc{alliance2012consumer,
  added-at = {2016-07-24T04:14:22.000+0200},
  author = {Alliance, Open Data Center},
  biburl = {https://www.bibsonomy.org/bibtex/2d474b15ff0b09bc0e64cd01e6a27458e/vngudivada},
  interhash = {ab2f21d4b44b937d15686314e1c051ca},
  intrahash = {d474b15ff0b09bc0e64cd01e6a27458e},
  keywords = {BigData},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Big Data Consumer Guide},
  year = 2012
}

@article{vassiliadis2000arktos,
  added-at = {2016-07-24T00:38:36.000+0200},
  author = {Vassiliadis, Panos and Vagena, Zografoula and Skiadopoulos, Spiros and Karayannidis, Nikos},
  biburl = {https://www.bibsonomy.org/bibtex/27fc3378a21ce207fe3c331a8b934ef43/vngudivada},
  description = {dblp},
  ee = {http://sites.computer.org/debull/A00DEC-CD.pdf},
  interhash = {eeaaf265be511453c0d6403bb7d3150c},
  intrahash = {7fc3378a21ce207fe3c331a8b934ef43},
  journal = {IEEE Data Eng. Bull.},
  keywords = {DataCleaning DataQuality ETL},
  number = 4,
  pages = {42-47},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {ARKTOS: A Tool For Data Cleaning and Transformation in Data Warehouse Environments.},
  url = {http://dblp.uni-trier.de/db/journals/debu/debu23.html#VassiliadisVSK00},
  volume = 23,
  year = 2000
}

@inproceedings{aguilera2015yesquel,
  abstract = {Based on a brief history of the storage systems for Web applications, we motivate the need for a new storage system. We then describe the architecture of such a system, called Yesquel. Yesquel supports the sql query language and offers performance similar to nosql storage systems.},
  acmid = {2684504},
  added-at = {2016-07-24T20:00:49.000+0200},
  address = {New York, NY, USA},
  articleno = {40},
  author = {Aguilera, Marcos K. and Leners, Joshua B. and Kotla, Ramakrishna and Walfish, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/250f56852c7713a79bf28631b82eb34f9/vngudivada},
  booktitle = {Proceedings of the 2015 International Conference on Distributed Computing and Networking},
  doi = {10.1145/2684464.2684504},
  interhash = {b7e91fa3cf490eff41bd02cf2018e2e0},
  intrahash = {50f56852c7713a79bf28631b82eb34f9},
  isbn = {978-1-4503-2928-6},
  keywords = {Performance SQL Scalability},
  location = {Goa, India},
  numpages = {4},
  pages = {40:1--40:4},
  publisher = {ACM},
  series = {ICDCN '15},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Yesquel: Scalable SQL Storage for Web Applications},
  url = {http://doi.acm.org/10.1145/2684464.2684504},
  year = 2015
}

@techreport{ag2016master,
  abstract = {The supply chain is a complex set of operations involving many different organizations, suppliers and parts. It includes manufacturing, logistics and fulfillment as well as retailers and consumers of the products. There are many pain points across the supply chain, some of which are specific to the supply chain, while others span across numerous or all industries. Some of these pain points can be related to incorrect or incomplete data used by the business processes. Enterprise information touches so many aspects of the supply chain it becomes imperative that the information used in your business processes and day-to-day operations is of the highest quality possible. The cornerstone of an efficient supply chain begins with complete and accurate product, customer and reference data. The best way to get high-quality data for your supply chain is by implementing a Master Data Management (MDM) platform. An MDM solution can help your business:

• Ensure proper storage in the warehouse
• Lower transportation costs and emissions
• Reduce last-minute, expensive shipping due to incorrect product information
• Reduce product ``road miles,'' potential spoilage and returns to the warehouse from deliveries that don’t match orders
• Reduce exceptions
• Facilitate the order-to-cash process
• Ensure customer satisfaction and avoid customer retention issues or negative publicity surrounding the product, brand or company

MDM's function is to aggregate product, reference, customer and potentially other data from disparate systems. It also performs data-quality processes to merge records and remove duplications, validate the data against business rules and then approve the data through workflows. Trusted data then is available for applications, Service-Oriented Architecture (SOA) services, business processes, and data warehouse and business intelligence systems. Read this white paper for more details on how a MDM strategy and solution can help you resolve many of the pain points in your supply chain and improve business performance as well.},
  added-at = {2016-07-24T00:10:02.000+0200},
  author = {AG},
  biburl = {https://www.bibsonomy.org/bibtex/2f716efc8f5cbc3d216fcc99c6b2fb2a6/vngudivada},
  interhash = {d66afe7584a07a4fb443f96453b9ecd1},
  intrahash = {f716efc8f5cbc3d216fcc99c6b2fb2a6},
  keywords = {DataIntegration DataQuality MDM SupplyChain},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {How master data management relieves the top 10 pain points in supply chains
},
  year = 2016
}

@misc{hellerstein2008quantitative,
  abstract = {Data collection has become a ubiquitous function of large organizations – not only for record keeping, but to support a variety of data analysis tasks that are critical to the organizational mission. Data analysis typically drives decision-making processes and efficiency optimizations, and in an increasing number of settings is the raison d’etre of entire agencies or firms.},
  added-at = {2016-07-24T05:20:24.000+0200},
  author = {Hellerstein, Joseph M.},
  biburl = {https://www.bibsonomy.org/bibtex/2118329ae4f984ec0cc7839688e3f49d8/vngudivada},
  interhash = {04737afef8d424fda7eb959885541ba5},
  intrahash = {118329ae4f984ec0cc7839688e3f49d8},
  keywords = {DataCleaning DataQuality},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Quantitative Data Cleaning for Large Databases},
  year = 2008
}

@article{simon2008introduction,
  abstract = {The text is designed for use in a forty-lecture introductory course covering linear algebra, multivariable differential calculus, and an introduction to real analysis.

The core material of the book is arranged to allow for the main introductory material on linear algebra, including basic vector space theory in Euclidean space and the initial theory of matrices and linear systems, to be covered in the first ten or eleven lectures, followed by a similar number of lectures on basic multivariable analysis, including first theorems on differentiable functions on domains in Euclidean space and a brief introduction to submanifolds. The book then concludes with further essential linear algebra, including the theory of determinants, eigenvalues, and the spectral theorem for real symmetric matrices, and further multivariable analysis, including the contraction mapping principle and the inverse and implicit function theorems. There is also an appendix which provides a nine-lecture introduction to real analysis. There are various ways in which the additional material in the appendix could be integrated into a course--for example in the Stanford Mathematics honors program, run as a four-lecture per week program in the Autumn Quarter each year, the first six lectures of the nine-lecture appendix are presented at the rate of one lecture per week in weeks two through seven of the quarter, with the remaining three lectures per week during those weeks being devoted to the main chapters of the text.

It is hoped that the text would be suitable for a quarter or semester course for students who have scored well in the BC Calculus advanced placement examination (or equivalent), particularly those who are considering a possible major in mathematics.

The author has attempted to make the presentation rigorous and complete, with the clarity and simplicity needed to make it accessible to an appropriately large group of students.

Table of Contents: Linear Algebra / Analysis in R / More Linear Algebra / More Analysis in R / Appendix: Introductory Lectures on Real Analysis},
  added-at = {2016-07-26T01:21:39.000+0200},
  author = {Simon, Leon},
  biburl = {https://www.bibsonomy.org/bibtex/2a72bd6e19c15983b88b252228c249097/vngudivada},
  doi = {10.2200/S00147ED1V01Y200808MAS003},
  eprint = {http://dx.doi.org/10.2200/S00147ED1V01Y200808MAS003},
  interhash = {a3e9e69eaf6d905597e8906f5b3aac83},
  intrahash = {a72bd6e19c15983b88b252228c249097},
  journal = {Synthesis Lectures on Mathematics and Statistics},
  keywords = {MultivariableMathematics SynthesisLecture},
  number = 1,
  pages = {1-132},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {An Introduction to Multivariable Mathematics},
  url = {http://dx.doi.org/10.2200/S00147ED1V01Y200808MAS003},
  volume = 1,
  year = 2008
}

@inproceedings{held2012towards,
  abstract = {In order to enable proper system and integration testing, it is often necessary to have huge test data inventories, reflecting the heterogeneous live system. Although the maintenance of large data stores can be guided by advice obtained from data quality evaluations, this technique can be only partly applied to test data inventories. Assessing test data quality is difficult, as the well-known data quality dimensions are not applicable in an easy fashion. For example, an otherwise good value of 100% for correctness would not allow to store erroneous test data items. The need for data quality dimensions dedicated to assessing test data quality can't be satisfied by well-known data quality dimensions. In this paper, we present our thesis approach to identify and validate new quality dimensions applicable for test data quality and develop quantification methods. We propose proximity to reality and degree of coverage as two new test data quality dimension and sketch quantification approach to measures, specifically suited for test data.},
  acmid = {2320830},
  added-at = {2016-07-24T01:17:34.000+0200},
  address = {New York, NY, USA},
  author = {Held, Johannes and Lenz, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/26db4628e9ca4f3c0c80870ccf0f243b5/vngudivada},
  booktitle = {Proceedings of the 2012 Joint EDBT/ICDT Workshops},
  doi = {10.1145/2320765.2320830},
  interhash = {7fd9285d4676f2fe239ba9e5553e9233},
  intrahash = {6db4628e9ca4f3c0c80870ccf0f243b5},
  isbn = {978-1-4503-1143-4},
  keywords = {DataQuality TestData},
  location = {Berlin, Germany},
  numpages = {6},
  pages = {233--238},
  publisher = {ACM},
  series = {EDBT-ICDT '12},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Towards Measuring Test Data Quality},
  url = {http://doi.acm.org/10.1145/2320765.2320830},
  year = 2012
}

@misc{gartner2011measuring,
  abstract = {Improved data quality is a primary source of value for many IT-enabled business initiatives. By applying the Gartner Business Value Model, organizations can link data quality improvement to metrics that matter.},
  added-at = {2016-07-24T05:14:29.000+0200},
  author = {Gartner},
  biburl = {https://www.bibsonomy.org/bibtex/217d7475467e5c733f3119701430170d8/vngudivada},
  interhash = {0f24445a38d66350af23fa14122e0440},
  intrahash = {17d7475467e5c733f3119701430170d8},
  keywords = {DataQuality DataQualityAssessment},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Measuring the Business Value of Data Quality},
  year = 2011
}

@book{wheelan2014naked,
  abstract = {Demystifies the study of statistics by stripping away the technical details to examine the underlying intuition essential for understanding statistical concepts.},
  added-at = {2016-07-29T04:02:26.000+0200},
  author = {Wheelan, Charles J.},
  biburl = {https://www.bibsonomy.org/bibtex/225f46cf5b8b0f7e109bbdb33982d04b8/vngudivada},
  interhash = {3ce0d9734b2bbf74b1c55fab4411c780},
  intrahash = {25f46cf5b8b0f7e109bbdb33982d04b8},
  isbn = {039334777X 9780393347777},
  keywords = {Book ParExcellence Statistics},
  note = {Great book. Get a copy.},
  refid = {846889447},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Naked statistics: stripping the dread from the data},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780393347777},
  year = 2014
}

@misc{yu2010exploratory,
  abstract = {Today there are quite a few widespread misconceptions of exploratory data analysis (EDA). One of these misperceptions is that EDA is said to be opposed to statistical modeling. Actually, the essence of EDA is not about putting aside all modeling and preconceptions; rather, researchers are urged not to start the analysis with a strong preconception only, and thus modeling is still legitimate in EDA. In addition, the nature of EDA has been changing due to the emergence of new methods and convergence between EDA and other methodologies, such as data mining and resampling. Therefore, conventional conceptual frameworks of EDA might no longer be capable of coping with this trend. In this article, EDA is introduced in the context of data mining and resampling with an emphasis on three goals: cluster detection, variable selection, and pattern recognition. TwoStep clustering, classification trees, and neural networks, which are powerful techniques to accomplish the preceding goals, respectively, are illustrated with concrete examples.},
  added-at = {2016-07-29T00:50:55.000+0200},
  author = {Yu, Chong Ho},
  biburl = {https://www.bibsonomy.org/bibtex/22c0aaa8eeb3a2e49193d0b519c7ab499/vngudivada},
  interhash = {9e0f6dc96b7da3c70887dbf159209b5c},
  intrahash = {2c0aaa8eeb3a2e49193d0b519c7ab499},
  keywords = {DataMining Resampling Statistics StatisticsExploratoryDataAnalysis},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Exploratory Data Analysis in the Context of Data Mining and Resampling},
  year = 2010
}

@book{urdan2010statistics,
  abstract = {This introductory textbook provides an inexpensive, brief overview of statistics to help readers gain a better understanding of how statistics work and how to interpret them correctly. Each chapter describes a different statistical technique, ranging from basic concepts like central tendency and describing distributions to more advanced concepts such as t tests, regression, repeated measures ANOVA, and factor analysis. Each chapter begins with a short description of the statistic and when it should be used. This is followed by a more in-depth explanation of how the statistic works. Finally, each chapter ends with an example of the statistic in use, and a sample of how the results of analyses using the statistic might be written up for publication. A glossary of statistical terms and symbols is also included. Using the author’s own data and examples from published research and the popular media, the book is a straightforward and accessible guide to statistics.

New features in the fourth edition include:

sets of work problems in each chapter with detailed solutions and additional problems online to help students test their understanding of the material,
new "Worked Examples" to walk students through how to calculate and interpret the statistics featured in each chapter,
new examples from the author’s own data and from published research and the popular media to help students see how statistics are applied and written about in professional publications,
many more examples, tables, and charts to help students visualize key concepts, clarify concepts, and demonstrate how the statistics are used in the real world.
a more logical flow, with correlation directly preceding regression, and a combined glossary appearing at the end of the book,
a Quick Guide to Statistics, Formulas, and Degrees of Freedom at the start of the book, plainly outlining each statistic and when students should use them,
greater emphasis on (and description of) effect size and confidence interval reporting, reflecting their growing importance in research across the social science disciplines
an expanded website at www.routledge.com/cw/urdan with PowerPoint presentations, chapter summaries, a new test bank, interactive problems and detailed solutions to the text’s work problems, SPSS datasets for practice, links to useful tools and resources, and videos showing how to calculate statistics, how to calculate and interpret the appendices, and how to understand some of the more confusing tables of output produced by SPSS.
Statistics in Plain English, Fourth Edition is an ideal guide for statistics, research methods, and/or for courses that use statistics taught at the undergraduate or graduate level, or as a reference tool for anyone interested in refreshing their memory about key statistical concepts. The research examples are from psychology, education, and other social and behavioral sciences.},
  added-at = {2016-07-29T03:59:09.000+0200},
  address = {New York, NY},
  author = {Urdan, Timothy C.},
  biburl = {https://www.bibsonomy.org/bibtex/20d749a3b8411a504776b1f2d09206645/vngudivada},
  edition = {Fourth},
  interhash = {d6879aa3f97b5da1b3feeb0c53f3a5ba},
  intrahash = {0d749a3b8411a504776b1f2d09206645},
  isbn = {978-1138838345},
  keywords = {Book ParExcellence Statistics},
  note = {Great book. Get a copy.},
  publisher = {Routledge},
  refid = {436030486},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Statistics in plain English},
  year = 2010
}

@book{clark2010handbook,
  abstract = {This comprehensive reference work provides an overview of the concepts, methodologies, and applications in computational linguistics and natural language processing (NLP). Features contributions by the top researchers in the field, reflecting the work that is driving the discipline forward. Includes an introduction to the major theoretical issues in these fields, as well as the central engineering applications that the work has produced. Presents the major developments in an accessible way, explaining the close connection between scientific understanding of the computational properties of natural language and the creation of effective language technologies. Serves as an invaluable state-of-the-art reference source for computational linguists and software engineers developing NLP applications in industrial research and development labs of software companies.},
  added-at = {2016-07-27T21:40:25.000+0200},
  address = {Chichester, West Sussex; Malden, MA},
  author = {Clark, Alexander and Fox, Chris and Lappin, Shalom},
  biburl = {https://www.bibsonomy.org/bibtex/24e4386131cdae6c78f360d9bf9d1396b/vngudivada},
  interhash = {59a873154269a24f306e2036fc9cf691},
  intrahash = {4e4386131cdae6c78f360d9bf9d1396b},
  isbn = {9781405155816 1405155817 9781118347188 1118347188},
  keywords = {Book ComputationalLinguistics Handbook NLP},
  publisher = {Wiley-Blackwell},
  refid = {500823419},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {The handbook of computational linguistics and natural language processing},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9781118347188},
  year = 2010
}

@book{knaflic2015storytelling,
  abstract = {Sometimes it is easy to generate charts and graphs look fine to you, but don't clearly communicate the information that they represent. Knaflic's own knack for clarity will help you sharpen your message when using data in your business presentations.},
  added-at = {2016-07-29T04:29:46.000+0200},
  author = {Knaflic, Cole Nussbaumer},
  biburl = {https://www.bibsonomy.org/bibtex/2e99fc239ee460b8c589efc05fc7f78f2/vngudivada},
  interhash = {81fe61523f9eb55ad717f864265e4362},
  intrahash = {e99fc239ee460b8c589efc05fc7f78f2},
  isbn = {9781119002253 1119002257},
  keywords = {Book DataVisualization Statistics Visualization},
  note = {Get a copy of this book.},
  refid = {909318525},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Storytelling with data : a data visualization guide for business professionals},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9781119002253},
  year = 2015
}

@techreport{oberhofer2015beyond,
  abstract = {Social Master Data Management (Social MDM) is the new revolution in business data processing that marries customer and product centricity with big data to radically improve customer experiences and product strategy. Traditional master data management (MDM)—the set of processes, practices, and technologies for creating a single view of common core business objects shared across multiple business processes and multiple systems (such as customer, product, vendor, and location )—is widely used by enterprises to improve the marketing, operational, and support processes for their customers. However, the focus of traditional MDM is structured data—and today, valuable information about customers and products is locked inside of vast amounts of unstructured, transactional, and social data such as tweets, blogs, Facebook, email, call center transcripts, call data records, and so on. There has been an explosion in technology like Hadoop and BigInsights to extract that information, but often those efforts have limited reach because they are not tied into the existing insight about customers and products contained in MDM systems.

In Beyond Big Data: Using Social MDM to Drive Deep Customer Insight , we explain how the union of social, mobile, location, and master data:

• Creates a richer relationship with existing customers
• Improves how you find and target new customers with the right products
• Delivers deeper understanding of how your customers think and feel about your products
• Brings the immediacy of mobile technology to create new ways to engage with customers},
  added-at = {2016-07-28T01:41:44.000+0200},
  author = {Oberhofer, Martin and Hechler, Eberhard and Milman, Ivan and Schumacher, Scott and Wolfson, Dan},
  biburl = {https://www.bibsonomy.org/bibtex/248e0366cafd7dcf19a898b73e2727e4c/vngudivada},
  institution = {IBM},
  interhash = {32ce365a77c676b7251db55e265fbc1f},
  intrahash = {48e0366cafd7dcf19a898b73e2727e4c},
  keywords = {MDM SocialMDM SocialMedia},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Beyond Big Data: Using Social MDM to Drive Deep Customer Insight},
  year = 2015
}

@book{foreman2014smart,
  abstract = {Data ScienceData SmartYou'll even learn what a dead squirrel has to do with optimization modeling, which you no doubt are dying to know.},
  added-at = {2016-07-29T04:28:11.000+0200},
  author = {Foreman, John W.},
  biburl = {https://www.bibsonomy.org/bibtex/2fc7e60a8f32e915ed585daf1ec6bfa64/vngudivada},
  interhash = {5cb607b30b6138c4a2d2b9599ab6d908},
  intrahash = {fc7e60a8f32e915ed585daf1ec6bfa64},
  isbn = {9781118661482 1118661486 9781118839867 1118839862},
  keywords = {Book DataScience Statistics},
  note = {Get a copy of this book.},
  refid = {862611724},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Data smart: using data science to transform information into insight},
  url = {http://www.123library.org/book_details/?id=114933},
  year = 2014
}

@book{hastie2009elements,
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics.},
  added-at = {2016-07-27T21:05:00.000+0200},
  address = {New York, NY},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
  biburl = {https://www.bibsonomy.org/bibtex/21f7f828974187ca26c58906d209b997a/vngudivada},
  interhash = {52d1772f39be836e3b298d37b8c0cfa1},
  intrahash = {1f7f828974187ca26c58906d209b997a},
  isbn = {978-0387848570},
  keywords = {Book DataMining MachineLearning StatisticalLearning},
  publisher = {Springer},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {The elements of statistical learning: data mining, inference, and prediction},
  year = 2009
}

@book{indurkhya2010handbook,
  abstract = {The Handbook of Natural Language Processing, Second Edition presents practical tools and techniques for implementing natural language processing in computer systems. Along with removing outdated material, this edition updates every chapter and expands the content to include emerging areas, such as sentiment analysis.},
  added-at = {2016-07-27T21:32:44.000+0200},
  address = {Boca Raton, FL},
  author = {Indurkhya, Nitin and Damerau, Frederick J.},
  biburl = {https://www.bibsonomy.org/bibtex/2905e3de8d3662fcd2544769d0a3ff15c/vngudivada},
  interhash = {91ae391464c62a716d050220737280c9},
  intrahash = {905e3de8d3662fcd2544769d0a3ff15c},
  keywords = {Book Handbook NLP},
  publisher = {Chapman \& Hall/CRC},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Handbook of natural language processing},
  year = 2010
}

@book{sag2003syntactic,
  abstract = {This second edition of Syntactic Theory: A Formal Introduction expands and improves upon a truly unique introductory syntax textbook. Like the first edition, its focus is on the development of precisely formulated grammars whose empirical predictions can be directly tested. There is also considerable emphasis on the prediction and evaluation of grammatical hypotheses, as well as on integrating syntactic hypotheses with matters of semantic analysis.

The book covers the core areas of English syntax from the last quarter century, including complementation, control, "raising constructions," passives, the auxiliary system, and the analysis of long distance dependency constructions. Syntactic Theory's step-by-step introduction to a consistent grammar in these core areas is complemented by extensive problem sets drawing from a variety of languages.

The book's theoretical perspective is presented in the context of current models of language processing, and the practical value of the constraint-based, lexicalist grammatical architecture proposed has already been demonstrated in computer language processing applications. This thoroughly reworked second edition includes revised and extended problem sets, updated analyses, additional examples, and more detailed exposition throughout.},
  added-at = {2016-07-27T21:11:20.000+0200},
  address = {Stanford, Calif.},
  author = {Sag, Ivan A. and Wasow, Thomas and Bender, Emily M.},
  biburl = {https://www.bibsonomy.org/bibtex/2431d76cc1ee433bc0b1762d7ae6f77c6/vngudivada},
  interhash = {a8ec49e003bb098295b56988100ca631},
  intrahash = {431d76cc1ee433bc0b1762d7ae6f77c6},
  isbn = {1575863995 9781575863993 1575864002 9781575864006},
  keywords = {sys:relevantfor:ecu-cc-research Book NLP SyntacticTheory},
  publisher = {Center for the Study of Language and Information},
  refid = {52272688},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Syntactic theory: a formal introduction},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9781575864006},
  year = 2003
}

@book{olive2011handbook,
  abstract = {This comprehensive handbook, written by leading experts in the field, details the groundbreaking research conducted under the breakthrough GALE program--The Global Autonomous Language Exploitation within the Defense Advanced Research Projects Agency (DARPA), while placing it in the context of previous research in the fields of natural language and signal processing, artificial intelligence and machine translation. The most fundamental contrast between GALE and its predecessor programs was its holistic integration of previously separate or sequential processes. In earlier language research programs, each of the individual processes was performed separately and sequentially: speech recognition, language recognition, transcription, translation, and content summarization. The GALE program employed a distinctly new approach by executing these processes simultaneously. Speech and language recognition algorithms now aid translation and transcription processes and vice versa. This combination of previously distinct processes has produced significant research and performance breakthroughs and has fundamentally changed the natural language processing and machine translation fields. This comprehensive handbook provides an exhaustive exploration into these latest technologies in natural language, speech and signal processing, and machine translation, providing researchers, practitioners and students with an authoritative reference on the topic.},
  added-at = {2016-07-27T21:35:36.000+0200},
  address = {New York},
  author = {Olive, Joseph P. and Christianson, Caitlin and McCary, John},
  biburl = {https://www.bibsonomy.org/bibtex/202e65e1dcbcaee7c8437e77692b1322a/vngudivada},
  interhash = {b32c2449f6d28c527aae4af2e9e9d677},
  intrahash = {02e65e1dcbcaee7c8437e77692b1322a},
  isbn = {9781441977137 1441977139},
  keywords = {Book Handbook NLP},
  publisher = {Springer},
  refid = {719361712},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Handbook of natural language processing and machine translation : DARPA global autonomous language exploitation},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=690848},
  year = 2011
}

@book{jurafsky2009speech,
  abstract = {An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology - at all levels and with all modern technologies - this book takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. Builds each chapter around one or more worked examples demonstrating the main idea of the chapter, usingthe examples to illustrate the relative strengths and weaknesses of various approaches. Adds coverage of statistical sequence labeling, information extraction, question answering and summarization, advanced topics in speech recognition, speech synthesis. Revises coverage of language modeling, formal grammars, statistical parsing, machine translation, and dialog processing. A useful reference for professionals in any of the areas of speech and language processing. -- Book Description from Website.},
  added-at = {2016-07-27T20:53:01.000+0200},
  address = {Upper Saddle River, N.J.},
  author = {Jurafsky, Dan and Martin, James H.},
  biburl = {https://www.bibsonomy.org/bibtex/2c92ab42eb04bd5a1d2ab55893edd3463/vngudivada},
  edition = {Second},
  interhash = {5f4a309a36c3da5e3becbf0ac5d88413},
  intrahash = {c92ab42eb04bd5a1d2ab55893edd3463},
  keywords = {Book NLP SpeechProcessing},
  publisher = {Pearson Prentice Hall},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
  year = 2009
}

@book{ceri2013information,
  abstract = {With the proliferation of huge amounts of (heterogeneous) data on the Web, the importance of information retrieval (IR) has grown considerably over the last few years. Big players in the computer industry, such as Google, Microsoft and Yahoo!, are the primary contributors of technology for fast access to Web-based information; and searching capabilities are now integrated into most information systems, ranging from business management software and customer relationship systems to social networks and mobile phone applications. Ceri and his co-authors aim at taking their readers from the foundations of modern information retrieval to the most advanced challenges of Web IR. To this end, their book is divided into three parts. The first part addresses the principles of IR and provides a systematic and compact description of basic information retrieval techniques (including binary, vector space and probabilistic models as well as natural language search processing) before focusing on its application to the Web. Part two addresses the foundational aspects of Web IR by discussing the general architecture of search engines (with a focus on the crawling and indexing processes), describing link analysis methods (specifically Page Rank and HITS), addressing recommendation and diversification, and finally presenting advertising in search (the main source of revenues for search engines). The third and final part describes advanced aspects of Web search, each chapter providing a self-contained, up-to-date survey on current Web research directions. Topics in this part include meta-search and multi-domain search, semantic search, search in the context of multimedia data, and crowd search.},
  added-at = {2016-07-27T21:00:01.000+0200},
  author = {Ceri, Stefano},
  biburl = {https://www.bibsonomy.org/bibtex/2307a88ef1d82eebf347d4fd52318ec43/vngudivada},
  interhash = {3c239dde07e22fbfb0e951ecdba2f89b},
  intrahash = {307a88ef1d82eebf347d4fd52318ec43},
  isbn = {9783642393143 3642393144},
  keywords = {Book IR WebIR},
  refid = {857892865},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Web information retrieval},
  url = {http://www.books24x7.com/marc.asp?bookid=77020},
  year = 2013
}

@incollection{rabiner1990readings,
  abstract = {After more than two decades of research activity, speech recognition has begun to live up to its promise as a practical technology and interest in the field is growing dramatically. Readings in Speech Recognition provides a collection of seminal papers that have influenced or redirected the field and that illustrate the central insights that have emerged over the years.

The editors provide an introduction to the field, its concerns and research problems. Subsequent chapters are devoted to the main schools of thought and design philosophies that have motivated different approaches to speech recognition system design. Each chapter includes an introduction to the papers that highlights the major insights or needs that have motivated an approach to a problem and describes the commonalities and differences of that approach to others in the book.},
  acmid = {108253},
  added-at = {2016-07-28T16:58:05.000+0200},
  address = {San Francisco, CA, USA},
  author = {Rabiner, Lawrence R.},
  biburl = {https://www.bibsonomy.org/bibtex/2d27ac6902bd6eb4999a258de1c7766a0/vngudivada},
  chapter = {A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition},
  editor = {Waibel, Alex and Lee, Kai-Fu},
  interhash = {e2968b0dcea8ca056456b98232b99db2},
  intrahash = {d27ac6902bd6eb4999a258de1c7766a0},
  isbn = {1-55860-124-4},
  keywords = {HMM ParExcellence SpeechRecognition Tutorial},
  note = {For errata, see http://alumni.media.mit.edu/~rahimi/rabiner/rabiner-errata/},
  numpages = {30},
  pages = {267--296},
  publisher = {Morgan Kaufmann Publishers Inc.},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Readings in Speech Recognition},
  url = {http://dl.acm.org/citation.cfm?id=108235.108253},
  year = 1990
}

@book{nist2002exploratory,
  added-at = {2016-07-29T00:45:38.000+0200},
  address = {Gaithersburg, MD},
  author = {NIST},
  biburl = {https://www.bibsonomy.org/bibtex/2c3e9e792cb17dd881e373be643b59594/vngudivada},
  interhash = {f9eb2cbcebf80c4bc28ae5248f23787d},
  intrahash = {c3e9e792cb17dd881e373be643b59594},
  keywords = {Book EDA ExploratoryDataAnalysis NIST},
  publisher = {National Institute of Standards and Technology},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Exploratory Data Analysis},
  year = 2002
}

@misc{derthick2005perspectives,
  abstract = {Web search engines have gained tremendous audiences for information retrieval from unstructured documents. The number of structured and semi-structured documents available on the web is also huge, and collections of these are more amenable to data mining. Yet there has been no similar explosion of interest in this kind of exploration. Finding patterns in databases of political contributions, pollution and environmental data, or hospital and school performance would surely interest many citizens. The Perspectives Browser is intended to support this kind of exploration for users with little or no training in statistics or programming. Given an "advanced search" type query, it visualizes dependencies on the query of up to 30 variables. In preliminary studies, participants found interesting three-variable dependencies in an art collection. We concentrate on image databases because the content can be concisely summarized, but the dependency visualization applies to any hierarchically organized nominal or ordinal variables.},
  added-at = {2016-07-29T00:56:49.000+0200},
  author = {Derthick, Mark and Zimmerman, John},
  biburl = {https://www.bibsonomy.org/bibtex/28d208533fd29007729422e4c046d1596/vngudivada},
  interhash = {c23a1b80fa967b2e3ee7e5d3f5e05105},
  intrahash = {8d208533fd29007729422e4c046d1596},
  keywords = {ExploratoryDataAnalysis PerspectivesBrowser Statistics},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {The Perspectives Browser: Exploratory Data Analysis for Everyone},
  year = 2005
}

@article{behrens1997principles,
  abstract = {Exploratory data analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns to foster hypothesis development and refinement. These tools and attitudes complement the use of significance and hypothesis tests used in confirmatory data analysis (CDA). Although EDA complements rather than replaces CDA, use of CDA without EDA is seldom warranted. Even when well-specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data. This article introduces the central heuristics and computational tools of EDA and contrasts it with CDA and exploratory statistics in general. EDA techniques are illustrated using previously published psychological data. Changes in statistical training and practice are recommended to incorporate these tools.},
  added-at = {2016-07-29T00:20:30.000+0200},
  author = {Behrens, J. T.},
  biburl = {https://www.bibsonomy.org/bibtex/272d190bd06c5a8cacf7ebb7d0392e50f/vngudivada},
  interhash = {a78611f9175eb7d9fb84572319fdf02a},
  intrahash = {72d190bd06c5a8cacf7ebb7d0392e50f},
  journal = {Psychological Methods},
  keywords = {EDA ExploratoryDataAnalysis Statistics},
  pages = {131--160},
  priority = {2},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Principles and procedures of exploratory data analysis},
  volume = 2,
  year = 1997
}

@book{given2008encyclopedia,
  abstract = {"Qualitative research is designed to explore the human elements of a given topic, while specific qualitative methods examine how individuals see and experience the world. Qualitative approaches are typically used to explore new phenomena and to capture individuals' thoughts, feelings, or interpretations of meaning and process. Such methods are central to research conducted in education, nursing, sociology, anthropology, information studies, and other disciplines in the humanities, social sciences, and health sciences. Qualitative research projects are informed by a wide range of methodologies and theoretical frameworks." "The SAGE Encyclopedia of Qualitative Research Methods presents current and complete information as well as ready-to-use techniques, facts, and examples from the field of qualitative research in a very accessible style. In taking an interdisciplinary approach, these two volumes target a broad audience and fill a gap in the existing reference literature for a general guide to the core concepts that inform qualitative research practices. The entries cover every major facet of qualitative methods, including access to research participants, data coding, research ethics, the role of theory in qualitative research, and much more - all without overwhelming the informed reader."--Back cover.},
  added-at = {2016-07-29T00:29:12.000+0200},
  address = {Los Angeles, Calif.},
  author = {Given, Lisa M.},
  biburl = {https://www.bibsonomy.org/bibtex/2618b7ee940352761cb6ee9aa7e988a96/vngudivada},
  interhash = {cf39e6cfa461da58cb159e1f5a93661f},
  intrahash = {618b7ee940352761cb6ee9aa7e988a96},
  isbn = {9781412941631 1412941636},
  keywords = {Book Encyclopedia ResearchMethod Statistics},
  publisher = {Sage Publications},
  refid = {185031301},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {The Sage encyclopedia of qualitative research methods},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9781412941631},
  year = 2008
}

@misc{freeman2016exploratory,
  abstract = {The ModelMap package (Freeman, 2009) for R (R Development Core Team, 2008) now includes tools for exploratory data analysis. The model.explore function offers several graphical tools for exploring predictor correlation, and relationships between the available training data set and the raster files of predictor data over which the map will be made. It will identify regions of the map where the predictor lies outside the range of the training data, and show the distribution of training data over the range of each predictor.},
  added-at = {2016-07-29T00:54:49.000+0200},
  author = {Freeman, Elizabeth A. and Frescino, Tracey S. and Moisen, Gretchen G.},
  biburl = {https://www.bibsonomy.org/bibtex/2ace0fc18cf34e97ae1159551ea38f7a5/vngudivada},
  interhash = {c67cf34ebd94b5fd25be1fe5e8d21c79},
  intrahash = {ace0fc18cf34e97ae1159551ea38f7a5},
  keywords = {ExploratoryDataAnalysis ModelMap Statistics},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {Exploratory Data Analysis with ModelMap},
  year = 2016
}

@misc{goldberg2015primer,
  abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
  added-at = {2016-07-28T01:44:28.000+0200},
  author = {Goldberg, Yoav},
  biburl = {https://www.bibsonomy.org/bibtex/29aa674995bbb6c7ef0ff91827a3a6a38/vngudivada},
  interhash = {0d4411af22df74aa8795081804889c29},
  intrahash = {9aa674995bbb6c7ef0ff91827a3a6a38},
  keywords = {NLP NeuralNetwork},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {A Primer on Neural Network Models for Natural Language Processing},
  year = 2015
}

@misc{sun2013survey,
  abstract = {Visual analytics employs interactive visualizations to integrate users’ knowledge and inference capability into numerical/algorithmic data analysis processes. It is an active research field that has applications in many sectors, such as security, finance, and business. The growing popularity of visual analytics in recent years creates the need for a broad survey that reviews and assesses the recent developments in the field. This report reviews and classifies recent work into a set of application categories including space and time, multivariate, text, graph and network, and other applications. More importantly, this report presents analytics space, inspired by design space, which relates each application category to the key steps in visual analytics, including visual mapping, model-based analysis, and user interactions. We explore and discuss the analytics space to add the current understanding and better understand research trends in the field.},
  added-at = {2016-07-29T00:41:35.000+0200},
  author = {Sun, Guo-Dao},
  biburl = {https://www.bibsonomy.org/bibtex/24f96c1ea22dd57297364e91d61bc924f/vngudivada},
  interhash = {ef888e968fc0b6d202c340ef4eeab879},
  intrahash = {4f96c1ea22dd57297364e91d61bc924f},
  keywords = {Survey VisualAnalytics},
  timestamp = {2019-03-25T17:13:02.000+0100},
  title = {A Survey of Visual Analytics Techniques and Applications: State-of-the-Art Research and Future Challenges},
  year = 2013
}

@inproceedings{liu2011visual,
  abstract = {With the increasing availability of metropolitan transportation data, such as those from vehicle GPSs (Global Positioning Systems) and road-side sensors, it becomes viable for authorities, operators, as well as individuals to analyze the data for a better understanding of the transportation system and possibly improved utilization and planning of the system. We report our experience in building the VAST (Visual Analytics for Smart Transportation) system. Our key observation is that metropolitan transportation data are inherently visual as they are spatio-temporal around road networks. Therefore, we visualize traffic data together with digital maps and support analytical queries through this interactive visual interface. As a case study, we demonstrate VAST on real-world taxi GPS and meter data sets from 15, 000 taxis running two months in a Chinese city of over 10 million population. We discuss the technical challenges in data cleaning, storage, visualization, and query processing, and offer our first-hand lessons learned from developing the system.},
  acmid = {2094053},
  added-at = {2016-07-31T15:05:53.000+0200},
  address = {New York, NY, USA},
  author = {Liu, Siyuan and Liu, Ce and Luo, Qiong and Ni, Lionel M. and Qu, Huamin},
  biburl = {https://www.bibsonomy.org/bibtex/2b3621fc38dd923076749b2c1ef76057f/vngudivada},
  booktitle = {Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  doi = {10.1145/2093973.2094053},
  interhash = {5ad15b7f85132664497fe43234d3be49},
  intrahash = {b3621fc38dd923076749b2c1ef76057f},
  isbn = {978-1-4503-1031-4},
  keywords = {IntelligentTransportationSystem VisualAnalytics},
  location = {Chicago, Illinois},
  numpages = {4},
  pages = {477--480},
  publisher = {ACM},
  series = {GIS '11},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {A Visual Analytics System for Metropolitan Transportation},
  url = {http://doi.acm.org/10.1145/2093973.2094053},
  year = 2011
}

@book{mitchell2003concepts,
  abstract = {For undergraduate and beginning graduate students, this textbook explains and examines the central concepts used in modern programming languages, such as functions, types, memory management, and control. The book is unique in its comprehensive presentation and comparison of major object-oriented programming languages. Separate chapters examine the history of objects, Simula and Smalltalk, and the prominent languages C++ and Java. The author presents foundational topics, such as lambda calculus and denotational semantics, in an easy-to-read, informal style, focusing on the main insights provided by these theories. Advanced topics include concurrency, concurrent object-oriented programming, program components, and inter-language interoperability. A chapter on logic programming illustrates the importance of specialized programming methods for certain kinds of problems. This book will give the reader a better understanding of the issues and tradeoffs that arise in programming language design, and a better appreciation of the advantages and pitfalls of the programming languages they use.},
  added-at = {2016-08-06T14:15:01.000+0200},
  address = {New York, NY},
  author = {Mitchell, John C.},
  biburl = {https://www.bibsonomy.org/bibtex/22568cd8cec25b0d2157454c813bdc300/vngudivada},
  description = {Concepts in Programming Languages elucidates the central concepts used in modern programming languages, such as functions, types, memory management, and control. The book is unique in its comprehensive presentation and comparison of major object-oriented programming languages. Separate chapters examine the history of objects, Simula and Smalltalk, and the prominent languages C++ and Java. The author presents foundational topics, such as lambda calculus and denotational semantics, in an easy-to-read, informal style, focusing on the main insights provided by these theories. Advanced topics include concurrency, concurrent object-oriented programming, program components, and inter-language interoperability. A chapter on logic programming illustrates the importance of specialized programming methods for certain kinds of problems.},
  interhash = {68a6626010538fcecefcec3fd0acb0a0},
  intrahash = {2568cd8cec25b0d2157454c813bdc300},
  isbn = {0511040911 9780511040917 0521780985 9780521780988 9780511804175 0511804172 051103492X 9780511034923 9780511045912 0511045913},
  keywords = {Book ProgrammingLanguage},
  publisher = {Cambridge University Press},
  refid = {56408594},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Concepts in programming languages},
  year = 2003
}

@inproceedings{yin2015understanding,
  abstract = {Accessibility is an important element in urban transportation planning. Accessibility measures combine mobility and land use measures to provide a more complete picture of the transportation-land use nexus than either of these measures alone. By providing insights into the varying degrees to which different areas of a region are connected to opportunities by the transportation system, accessibility analysis helps urban planners to understand the relationship between transportation and land use, and provides reference for them to improve the equality of the residents. Calculating accurate accessibility values and visualizing them in an efficient way is a complex and challenging process. In this paper, we present a web-based system that visualizes multimodal accessibility to multiple land uses of Chicago metropolitan area, as the first step of an effort to build an integrated platform for accessibility analysis tasks. We also discuss some use cases of this system, and show its effectiveness by providing experts feedback of this prototype.},
  acmid = {2835036},
  added-at = {2016-07-31T15:03:55.000+0200},
  address = {New York, NY, USA},
  author = {Yin, Shi and Li, Moyin and Tilahun, Nebiyou and Forbes, Angus and Johnson, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/24f1a47cd86190f997d8542564ccf1bda/vngudivada},
  booktitle = {Proceedings of the 1st International ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
  doi = {10.1145/2835022.2835036},
  interhash = {3ead0a3e2e829e0b79207ca517ca278a},
  intrahash = {4f1a47cd86190f997d8542564ccf1bda},
  isbn = {978-1-4503-3973-5},
  keywords = {IntelligentTransportationSystem VisualAnalytics},
  location = {Bellevue, WA, USA},
  numpages = {8},
  pages = {77--84},
  publisher = {ACM},
  series = {UrbanGIS'15},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Understanding Transportation Accessibility of Metropolitan Chicago Through Interactive Visualization},
  url = {http://doi.acm.org/10.1145/2835022.2835036},
  year = 2015
}

@electronic{wang2009intelligent,
  abstract = {"This book tackles those data sets and covers a variety of issues in relation to intelligent data analysis so that patterns from frequent or rare events in spatial or temporal spaces can be revealed. This book brings together current research, results, problems, and applications from both theoretical and practical approaches"--Provided by publisher.},
  added-at = {2016-08-03T22:04:43.000+0200},
  address = {Hershey, PA},
  author = {Wang, Hsiao-Fan},
  biburl = {https://www.bibsonomy.org/bibtex/2fce2e9db10211a6edcb71c45d7889a1c/vngudivada},
  interhash = {d721aea78d47a17c80798be256e21a51},
  intrahash = {fce2e9db10211a6edcb71c45d7889a1c},
  isbn = {9781599049830 159904983X},
  keywords = {Book DataAnalysis},
  publisher = {Information Science Reference},
  refid = {236090898},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Intelligent data analysis developing new methodologies through pattern discovery and recovery},
  url = {http://site.ebrary.com/id/10245848},
  year = 2009
}

@book{arenas2014foundations,
  abstract = {The problem of exchanging data between different databases with different schemas is an area of immense importance. Consequently data exchange has been one of the most active research topics in databases over the past decade. Foundational questions related to data exchange largely revolve around three key problems: how to build target solutions; how to answer queries over target solutions; and how to manipulate schema mappings themselves? The last question is also known under the name 'metadata management', since mappings represent metadata, rather than data in the database. In this book the authors summarize the key developments of a decade of research. Part I introduces the problem of data exchange via examples, both relational and XML; Part II deals with exchanging relational data; Part III focuses on exchanging XML data; and Part IV covers metadata management. --Back cover.},
  added-at = {2016-08-06T14:05:03.000+0200},
  address = {New York, NY},
  author = {Arenas, Marcelo},
  biburl = {https://www.bibsonomy.org/bibtex/27fd1ee6af31529309d27c62ac0ca92fc/vngudivada},
  interhash = {bd147292699658ce3af2e43f2b6d9423},
  intrahash = {7fd1ee6af31529309d27c62ac0ca92fc},
  isbn = {9781139060158 1139060155 1107016169 9781107016163 9781107779396 1107779391},
  keywords = {Book DataExchange DataIntegration ETL},
  publisher = {Cambridge University Press},
  refid = {878059653},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Foundations of Data Exchange},
  year = 2014
}

@book{plattner2012inmemory,
  abstract = {In the last fifty years the world has been completely transformed through the use of IT. We have now reached a new inflection point. This book presents, for the first time, how in-memory data management is changing the way businesses are run. Today, enterprise data is split into separate databases for performance reasons. Multi-core CPUs, large main memories, cloud computing and powerful mobile devices are serving as the foundation for the transition of enterprises away from this restrictive model. This book provides the technical foundation for processing combined transactional and analytical operations in the same database. In the year since we published the first edition of this book, the performance gains enabled by the use of in-memory technology in enterprise applications has truly marked an inflection point in the market. The new content in this second edition focuses on the development of these in-memory enterprise applications, showing how they leverage the capabilities of in-memory technology. The book is intended for university students, IT-professionals and IT-managers, but also for senior management who wish to create new business processes.},
  added-at = {2016-08-05T13:41:15.000+0200},
  address = {Berlin},
  author = {Plattner, Hasso and Zeier, Alexander},
  biburl = {https://www.bibsonomy.org/bibtex/28be33435b608efade2c77c81b92984c7/vngudivada},
  interhash = {0efce272616150c932fbd804984332a9},
  intrahash = {8be33435b608efade2c77c81b92984c7},
  isbn = {3642295746 9783642295744 9783642295751 3642295754},
  keywords = {Book InMemoryDBMS},
  publisher = {Springer Berlin},
  refid = {812194509},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {In-Memory Data Management Technology and Applications},
  year = 2012
}

@book{danneman2014social,
  abstract = {The growth of social media over the last decade has revolutionized the way individuals interact and industries conduct business. Individuals produce data at an unprecedented rate by interacting, sharing, and consuming content through social media. However, analyzing this ever-growing pile of data is quite tricky and, if done erroneously, could lead to wrong inferences. By using this essential guide, you will gain hands-on experience with generating insights from social media data. This book provides detailed instructions on how to obtain, process, and analyze a variety of socially-generated data while providing a theoretical background to help you accurately interpret your findings. You will be shown R code and examples of data that can be used as a springboard as you get the chance to undertake your own analyses of business, social, or political data. The book begins by introducing you to the topic of social media data, including its sources and properties. It then explains the basics of R programming in a straightforward, unassuming way. Thereafter, you will be made aware of the inferential dangers associated with social media data and how to avoid them, before describing and implementing a suite of social media mining techniques. Social Media Mining in R provides a light theoretical background, comprehensive instruction, and state-of-the-art techniques, and by reading this book, you will be well equipped to embark on your own analyses of social media data. Whether you are an undergraduate who wishes to get hands-on experience working with social data from the Web, a practitioner wishing to expand your competencies and learn unsupervised sentiment analysis, or you are simply interested in social data analysis, this book will prove to be an essential asset. No previous experience with R or statistics is required, though having knowledge of both will enrich your experience.},
  added-at = {2016-08-06T13:54:40.000+0200},
  author = {Danneman, Nathan and Heimann, Richard and Wood, Monseé G.},
  biburl = {https://www.bibsonomy.org/bibtex/2ec5127c9f3773564ddd358cfe2c7f18f/vngudivada},
  interhash = {123eceec710fcae18e651d0de4be3253},
  intrahash = {ec5127c9f3773564ddd358cfe2c7f18f},
  keywords = {Book DataMining R SocialMedia},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Social media mining with R: deploy cutting-edge sentiment analysis techniques to real-world social media data using R},
  year = 2014
}

@book{hey2015computing,
  abstract = {"Computers now impact almost every aspect of our lives, from our social interactions to the safety and performance of our cars. How did this happen in such a short time? And this is just the beginning. In this book, Tony Hey and Gyuri Pápay lead us on a journey from the early days of computers in the 1930s to the cutting-edge research of the present day that will shape computing in the coming decades. Along the way, they explain the ideas behind hardware, software, algorithms, Moore's Law, the birth of the personal computer, the Internet and the Web, the Turing Test, Jeopardy's Watson, World of Warcraft, spyware, Google, Facebook, and quantum computing. This book also introduces the fascinating cast of dreamers and inventors who brought these great technological developments into every corner of the modern world. This exciting and accessible introduction will open up the universe of computing to anyone who has ever wondered where his or her smartphone came from"--},
  added-at = {2016-08-06T14:02:45.000+0200},
  author = {Hey, Anthony J. G. and Pápay, Gyuri},
  biburl = {https://www.bibsonomy.org/bibtex/2832899f9363b3acf4f9fd5de934e939b/vngudivada},
  interhash = {4b6d94544c62e75e99dfead421ae6d2c},
  intrahash = {832899f9363b3acf4f9fd5de934e939b},
  isbn = {9780521766456 0521766451 9780521150187 0521150183},
  keywords = {Book ComputingHistory ComputingResearch},
  refid = {885313543},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {The computing universe  a journey through a revolution},
  year = 2015
}

@book{salsburg2001tasting,
  abstract = {An insightful, revealing history of the magical mathematics that transformed our world.

    At a summer tea party in Cambridge, England, a guest states that tea poured into milk tastes different from milk poured into tea. Her notion is shouted down by the scientific minds of the group. But one man, Ronald Fisher, proposes to scientifically test the hypothesis. There is no better person to conduct such an experiment, for Fisher is a pioneer in the field of statistics.

    The Lady Tasting Tea spotlights not only Fisher's theories but also the revolutionary ideas of dozens of men and women which affect our modern everyday lives. Writing with verve and wit, David Salsburg traces breakthroughs ranging from the rise and fall of Karl Pearson's theories to the methods of quality control that rebuilt postwar Japan's economy, including a pivotal early study on the capacity of a small beer cask at the Guinness brewing factory. Brimming with intriguing tidbits and colorful characters, The Lady Tasting Tea salutes the spirit of those who dared to look at the world in a new way.

    The seemingly unlikely title refers to an actual gathering on a summer afternoon in Cambridge, England in the late 1920s. One of the women in the group insisted that tea tastes different depending on whether it is poured into milk or if the milk is poured into the tea. How might one test whether this proposition is true? It turns out that there were some mathematical luminaries present at that tea party, and what unfolded as a result of their thoughts about this problem of testing is what we now call statistics.

    The use of statistics today is ubiquitous. It starts in the morning with the weather report that forecasts the chances of rain for the day, and goes on to political polling, or the results of pharmaceutical research regarding some health issue, or economic probabilities that this or that might happen. We are bombarded with statistics, and in many cases use them, well or poorly, to inform decision making. In general, even if we never studied the subject in school, we think we have some feeling for what they mean. After reading Salsburg's book I now have a clearer understanding of how mathematical probabilities are related to the real world and to specific individuals. He does not include any statistical formulas, but rather describes how this way of thinking has replaced the world of phenomena, and his clear thinking is well worth the read.

    Salsburg describes mathematician Ronald A. Fisher starting to work with statistics early on, collecting data on the increasing weight of his newborn son. This added an element of time to the data that made the seemingly independent data of real life relational. He went on to develop parameters to deal with this that were oversimplified to the point of being untrue. In an exciting chapter on Andrei Kolmogorov's contributions to dealing with this element, ``time,"" Salsburg suggests that if Kolmogorov had not died prematurely, his work would have revolutionized science as we know it.

    The book is full of real-life examples of questions that concern us, and shows how individual personalities, described in careful chronological order, added their contributions to the general understanding of concrete questions. Salsburg has a gift for characterizing the personalities and their quirks, and tells a fast-moving, humorous tale of the human quest to quantify uncertainty. There is a fascinating thread woven through the book concerning the limitations and inadequacies, both practical and philosophical, of looking at the world statistically. This thread is mixed with true admiration and appreciation for the genuine value of the many contributions that were made to this way of looking at the world. At the end of the book, Salsburg hints that there is a need for ``geniuses" of a new kind, similar to Kolmogorov, who, in the near future, might start a whole new paradigm.

    This is the fun side of statistics. David Salsburg's popular account of some of the great statisticians is a great read, full of anecdotes and unusual personal information. It starts around the turn of the last century with Francis Galton, Karl Pearson, W. S. Gossett ('Student' of the t-test) and, of course, R. A. Fisher, and continues with Egon Pearson, Jerzy Neyman and Florence Nightingale David.

    Thus far, the action is all at University College London and Cambridge, but the book then spreads out worldwide. There are accounts of the life and work of many twentieth-century statisticians, from A. N. Kolmogorov's work on the axioms of probability, through I. J. Good's work on cryptography at Bletchley Park and subsequently on bayesian methods, to the many achievements of John Tukey, not least the fast Fourier transform. Along the way there are descriptions of the great (and still unresolved) wrangles between statisticians about inverse probability and the foundations of inductive inference.

    The title of the book refers to a famous chapter in Fisher's The Design of Experiments (1935). In this chapter Fisher illustrates the principles of experimental design by discussing the hypothetical problem of how to test the claim that it is possible to tell whether the tea is put into the cup before or after the milk. The first riveting fact I discovered from this book is that the problem is not entirely hypothetical. It is based on an actual incident at a tea party in Cambridge in the late 1920s (the author knows someone who was there).

    The book is crammed with such personal anecdotes, both amusing and sometimes (thanks to Hitler and Stalin) harrowing. For example, I discovered that I.J. Good, who worked for much of his life in the United States, is the son of a Polish immigrant to London's East End who owned a well-known antique jewellery shop (Cameo Corner) near the British Museum (I bought a ring there). Salsburg does not, however, mention Florence David's predilection for cigars of churchillian proportions. I have often wondered what impression they must have made when she eventually moved to California; these days she would probably be arrested.

    Salsburg's account is entirely non-mathematical, and he makes a creditable attempt at verbal descriptions of some of the great work. Nevertheless, the difficulty of conveying the ideas in words was brought home to me by the modest amount that I felt I understood in those areas I did not know about already, such as martingales (I trust his mathematical definition is more accurate than his nautical definition of this word).

    When it comes to the science in the book -- as opposed to history and anecdote -- Salsburg's views will not gain universal agreement. One of his recurring themes is that the old, deterministic clockwork Universe was swept aside by the ``statistical revolution ... The `things' of science are not the observables but the mathematical distribution functions that describe the probabilities associated with observations."

    My problem here is that Salsburg makes little distinction between the unavoidable variability that results from the random behavior of atoms, and avoidable variability due to errors of observation. At the level of individual molecules, atoms and subatomic particles, random behavior is part of the physics of the system. In subatomic physics and in my own field of single-ion channels, it is quite true that what we observe are distributions. But nevertheless we can, and do, regard the (true) means of such distributions as deterministic constants. We can measure the duration of random lifetimes with high accuracy relative to the real variability of the system.

    But these are specialist areas of research. In most areas, the variability is seldom of this unavoidable single-molecule sort. Most people would think it entirely reasonable that when you measure, for example, the equilibrium constant for a well-defined reaction, there is a true value. Of course there will be experimental errors in measuring this value, but the problem is essentially deterministic.

    Crucial though statistical ideas are in many areas, I cannot help thinking that Salsburg exaggerates when he talks of statistics as ``probably the single most important tool of biological science". What about the genome, the electron microscope and the patch clamp? If you read the great papers in my own area, for example those of Hodgkin and Huxley, of Bernard Katz, or of Neher and Sakmann, you will find very little statistics (at least of the sort that refers to measurement errors), and there was no need for any. In other areas, such as clinical trials and psychology, the use of statistics is critically important. However, one is reminded of the dangers of excessive enthusiasm for numbers when the author unblushingly tells us that ``psychology developed techniques of measuring intelligence".

    Most practical scientists who are dealing with large numbers of molecules that behave in an essentially deterministic way remain largely untouched by the ``statistical revolution". The lady tasting tea is famous among statisticians, but none of my colleagues recognized the allusion, and very few are interested in the basis of scientific inference. This may show a lack of intellectual curiosity -- or may merely reflect the amount of time they have to spend on bureaucratic activities imposed by governments and universities -- but most of the time it does not hinder their science very much. It would be nice to think that the book would be read by many biologists, and indeed by the general public, but I suspect the main audience will be statisticians -- amateur and professional.

    Fisher's lady tasting tea was intended to illustrate the crucial importance of randomization in experimental design. This lesson has been learned well in areas such as the design of clinical trials, but is still largely ignored in laboratory sciences. Perhaps Salsburg does have a point. Studies on transgenic animals are a mainstay of research in the post-genomic era, and they can give useful results. But it is impossible to randomize gene-knockout experiments. And nobody seems to worry that this makes it impossible to do a valid significance test on the results of their experiments. Perhaps molecular biologists should read about Fisher's lady.},
  added-at = {2016-07-30T17:59:07.000+0200},
  address = {New York, NY},
  author = {Salsburg, David},
  biburl = {https://www.bibsonomy.org/bibtex/24ec3226435f95b2232f98d55bbb72d7d/vngudivada},
  interhash = {5924d709dbcd1406cb313abccd8ee9fa},
  intrahash = {4ec3226435f95b2232f98d55bbb72d7d},
  keywords = {Book Statistics},
  publisher = {W.H. Freeman},
  refid = {45129162},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {The lady tasting tea: how statistics revolutionized science in the twentieth century},
  year = 2001
}

@book{murach2014murachs,
  abstract = {Section 1: Introduction to servlets and JSPsBegin with the concepts and terms that you need for Java web programming, then learn how to use the Tomcat web server on your own PC and take a crash course in HTML.Section 2: The essence of web programmingLearn how to develop JSPs and servlets and how to get the best from both by using the Model 2 architecture. Then, learn how to use sessions, cookies, JavaBeans, and custom JSP tags.Section 3: Essential database skillsLearn how to use JDBC for database operations, and learn the specifics for working with MySQL, a popular open-source database that's often used with Java web applications.Section 4: Advanced servlet and JSP skillsMaster the special skills that you need for some applications: JavaMail, SSL, security, HTTP, XML, and EJBs.Section 5: The Music Store web siteStudy a complete e-commerce web site that uses all of the skills presented in the book, and use the source code on the CD as the starting point for your own web applications.},
  added-at = {2016-08-05T13:19:22.000+0200},
  author = {Murach, Joel and Urban, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2ec6c8e76511eceeda5ee7a2bec6185bb/vngudivada},
  interhash = {b1bee407676cd0759c3fb63d4f6eed6c},
  intrahash = {ec6c8e76511eceeda5ee7a2bec6185bb},
  isbn = {9781890774783 1890774782},
  keywords = {Book JSP Murach Servlet},
  refid = {889840261},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Murach's Java servlets and JSP : training \& reference},
  year = 2014
}

@book{patterson2014computer,
  abstract = {"Presents the fundamentals of hardware technologies, assembly language, computer arithmetic, pipelining, memory hierarchies and I/O"--Provided by publisher.},
  added-at = {2016-07-30T17:40:23.000+0200},
  author = {Patterson, David A. and Hennessy, John L.},
  biburl = {https://www.bibsonomy.org/bibtex/2cb5bab899bc4de230fe6e3a66b52c5f6/vngudivada},
  interhash = {4fa1375996008fef85e9b1d1b1a158fb},
  intrahash = {cb5bab899bc4de230fe6e3a66b52c5f6},
  isbn = {9780124077263},
  keywords = {Book ComputerDesign ComputerOrganization ParExcellence TextyAward},
  refid = {859555917},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Computer organization and design: the hardware/software interface},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780124077263},
  year = 2014
}

@book{carter2014actionable,
  abstract = {Building an analysis ecosystem for a smarter approach to intelligence
Keith Carter's Actionable Intelligence: A Guide to Delivering Business Results with Big Data Fast! is the comprehensive guide to achieving the dream that business intelligence practitioners have been chasing since the concept itself came into being. Written by an IT visionary with extensive global supply chain experience and insight, this book describes what happens when team members have accurate, reliable, usable, and timely information at their fingertips. With a focus on leveraging big data, the book provides expert guidance on developing an analytical ecosystem to effectively manage, use the internal and external information to deliver business results.

This book is written by an author who's been in the trenches for people who are in the trenches. It's for practitioners in the real world, who know delivering results is easier said than done – fraught with failure, and difficult politics. A landscape where reason and passion are needed to make a real difference.

This book lays out the appropriate way to establish a culture of fact-based decision making, innovation, forward looking measurements, and appropriate high-speed governance. Readers will enable their organization to:

Answer strategic questions faster
Reduce data acquisition time and increase analysis time to improve outcomes
Shift the focus to positive results rather than past failures
Expand opportunities by more effectively and thoughtfully leveraging information
Big data makes big promises, but it cannot deliver without the right recipe of people, processes and technology in place. It's about choosing the right people, giving them the right tools, and taking a thoughtful—rather than formulaic--approach. Actionable Intelligence provides expert guidance toward envisioning, budgeting, implementing, and delivering real benefits.},
  added-at = {2016-07-31T15:10:23.000+0200},
  author = {Carter, Keith B.},
  biburl = {https://www.bibsonomy.org/bibtex/23cef37b123b81c2b8562c7a477bb8632/vngudivada},
  interhash = {6694f06f7679d8af65a8988687f51561},
  intrahash = {3cef37b123b81c2b8562c7a477bb8632},
  isbn = {9781118920602 1118920600 9781118920657 1118920651 1118915232 9781118915233},
  keywords = {Book DataAnalytics},
  refid = {883391878},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Actionable intelligence: a guide to delivering business results with big data fast!},
  url = {http://swbplus.bsz-bw.de/bsz414079906cov.htm},
  year = 2014
}

@book{ridge2015guerrilla,
  abstract = {Project data itself arrives piecemeal, is added to, replaced, contains undiscovered flaws and comes from a variety of sources. Despite these disruptions, a data science team must get off the ground fast and begin demonstrating value with traceable, tested work products. This book covers: guerrilla analytics principles: simple rules of thumb for maintaining data provenance across the entire analytics life cycle from data extraction, through analysis to reporting; reproducible, traceable analytics; practice tips and war stories based on real-world project challenges encountered in consulting, pre-sales and research; preparing for battle: how to set up a team's analytics environment in terms of tooling, skill sets, workflows and conventions; data gymnastics: over a dozen analytics patterns that a team will encounter again and again in projects. --},
  added-at = {2016-07-30T17:46:34.000+0200},
  address = {Amsterdam; Boston},
  author = {Ridge, Enda},
  biburl = {https://www.bibsonomy.org/bibtex/2ad040ff1d74e6cf80c185c6d8f22a98e/vngudivada},
  interhash = {4b0dbd69b2053db955a91323ea7671ff},
  intrahash = {ad040ff1d74e6cf80c185c6d8f22a98e},
  isbn = {9780128005033 0128005033 9781322158488 1322158487 0128002182 9780128002186},
  keywords = {Book DataAnalytics},
  publisher = {Morgan Kaufmann, an imprint of Elsevier},
  refid = {892922276},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Guerrilla analytics: techniques for managing data and analytics teams},
  url = {http://www.books24x7.com/marc.asp?bookid=73409},
  year = 2015
}

@book{hepp2008ontology,
  abstract = {Managing ontologies and annotated data throughout their life-cycles is at the core of semantic systems of all kinds. Ontology Management, an edited volume by senior researchers in the field, provides an up-to-date, concise and easy-to-read reference on this topic. This volume describes relevant tasks, practical and theoretical challenges, limitations and methodologies, plus available software tools. The editors discuss integrating the conceptual and technical dimensions with a business view on using ontologies, by stressing the cost dimension of ontology engineering and by providing guidance on how up-to-date tooling helps to build, maintain, and use ontologies. Also included is a one-stop reference on all aspects of managing ontological data and best practices on ontology management for a number of application domains. Ontology Management is designed as a reference or secondary text for researchers and advanced-level students interested in the Semantic Web, Semantic Web Services (SWS)and Web Services, information systems, data and knowledge engineering, ontologies, or other aspects of semantic systems. Practitioners in industry will find this work invaluable as well. Foreword by Professor Dieter Fensel, DERI, University of Innsbruck.},
  added-at = {2016-08-05T13:42:49.000+0200},
  address = {New York, NY},
  author = {Hepp, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2d2b01f0aa4cd049214fa4af79bbf7f8a/vngudivada},
  interhash = {1948523d6b0a7d76f38d5e9c4d892aa7},
  intrahash = {d2b01f0aa4cd049214fa4af79bbf7f8a},
  isbn = {9780387698991 038769899X 9780387699004 0387699007},
  keywords = {Book Ontology},
  publisher = {Springer},
  refid = {234546923},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Ontology management: semantic web, semantic web services, and business applications},
  year = 2008
}

@book{han2012mining,
  abstract = {Mining of Data with Complex Structures explores nature of data with complex structure including sequences, trees and graphs. Readers will find a detailed description of the state-of-the-art of sequence mining, tree mining and graph mining, and more.},
  added-at = {2016-07-30T17:33:42.000+0200},
  author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  biburl = {https://www.bibsonomy.org/bibtex/27824e9e8d914ee961c71b6fdb6931957/vngudivada},
  edition = {Third},
  interhash = {5c5f35ebd4f5e4c121e3736a585ddb95},
  intrahash = {7824e9e8d914ee961c71b6fdb6931957},
  isbn = {9789380931913 9380931913},
  keywords = {Book DataMining},
  note = {The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge.  Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges.  Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of your data},
  publisher = {Elsevier},
  refid = {818864884},
  series = {The Morgan Kaufmann Series in Data Management Systems},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Data mining: concepts and techniques},
  year = 2012
}

@book{celko2006celkos,
  abstract = {Before SQL programmers could begin working with OLTP (On-Line Transaction Processing) systems, they had to unlearn procedural, record-oriented programming before moving on to SQLs declarative, set-oriented programming. This book covers the next step in your growth. OLAP (On-Line Analytical Processing), Data Warehousing and Analytics involve seeing data in the aggregate and over time, not as single transactions. Once more it is time to unlearn what you were previously taught. This book is not an in-depth look at particular subjects, but an overview of many subjects that will give the working RDBMS programmers a map of the terra incognita they will faceif they want to grow. * Expert advice from a noted SQL authority and award-winning columnist, who has given ten years of service to the ANSI SQL standards committee and many more years of dependable help to readers of online forums. * First book that teaches what SQL programmers need in order to successfully make the transition from transactional systems (OLTP) into the world of data warehouse data and OLAP. * Offers real-world insights and lots of practical examples. * Covers the OLAP extensions in SQL-99; ETL tools, OLAP features supported in DBMSs, other query tools, simple reports, and statistical software.},
  added-at = {2016-07-29T04:40:16.000+0200},
  address = {San Francisco, Calif.; Oxford},
  author = {Celko, Joe},
  biburl = {https://www.bibsonomy.org/bibtex/2723384ba831d7935cf6616b71869e4cf/vngudivada},
  interhash = {d98bfe05105da390a03a2232dfdd9928},
  intrahash = {723384ba831d7935cf6616b71869e4cf},
  isbn = {9780123695123 0123695120 9780080495934 0080495931},
  keywords = {Book DataAnalytics SQL},
  publisher = {Morgan Kaufmann ; Elsevier Science [distributor]},
  refid = {162130476},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Joe Celko's analytics and OLAP in SQL},
  url = {http://www.123library.org/book_details/?id=36677},
  year = 2006
}

@book{perkins2014python,
  abstract = {This book will show you the essential techniques of text and language processing. Starting with tokenization, stemming, and the WordNet dictionary, you'll progress to part-of-speech tagging, phrase chunking, and named entity recognition. You'll learn how various text corpora are organized, as well as how to create your own custom corpus. Then, you'll move onto text classification with a focus on sentiment analysis. And because NLP can be computationally expensive on large bodies of text, you'll try a few methods for distributed text processing. Finally, you'll be introduced to a number of other small but complementary Python libraries for text analysis, cleaning, and parsing.},
  added-at = {2016-08-05T13:25:32.000+0200},
  address = {Birmingham, UK},
  author = {Perkins, Jacob},
  biburl = {https://www.bibsonomy.org/bibtex/2ee22b85e7d50987fa9e284ccde650e00/vngudivada},
  interhash = {9ad8e013f671a69e18a2fab23ae41b1b},
  intrahash = {ee22b85e7d50987fa9e284ccde650e00},
  isbn = {9781782167860 1782167862 1782167854 9781782167853},
  keywords = {Book NLTK Python3 TextProcessing},
  publisher = {Packt Pub.},
  refid = {891786402},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Python 3 text processing with NLTK 3 cookbook over 80 practical recipes on natural language processing techniques using Python's NLTK 3.0},
  url = {http://proquest.safaribooksonline.com/?fpi=9781782167853},
  year = 2014
}

@book{stevens2015python,
  abstract = {Do you have a biological question that could be readily answered by computational techniques, but little experience in programming? Do you want to learn more about the core techniques used in computational biology and bioinformatics? Written in an accessible style, this guide provides a foundation for both newcomers to computer programming and those interested in learning more about computational biology. The chapters guide the reader through: a complete beginners' course to programming in Python, with an introduction to computing jargon; descriptions of core bioinformatics methods with working Python examples; scientific computing techniques, including image analysis, statistics and machine learning. This book also functions as a language reference written in straightforward English, covering the most common Python language elements and a glossary of computing and biological terms. This title will teach undergraduates, postgraduates and professionals working in the life sciences how to program with Python, a powerful, flexible and easy-to-use language.},
  added-at = {2016-08-06T13:56:52.000+0200},
  author = {Stevens, Tim and Boucher, Wayne},
  biburl = {https://www.bibsonomy.org/bibtex/20e93b34f5b21913ba78a986b305154b4/vngudivada},
  interhash = {853a227f05610f96bf0e79b75e31b6cb},
  intrahash = {0e93b34f5b21913ba78a986b305154b4},
  isbn = {9780521895835 0521895839 9780521720090 0521720095},
  keywords = {Biology Book Python},
  refid = {906759353},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Python programming for biology : bioinformatics, and beyond},
  year = 2015
}

@book{han2006mining,
  abstract = {Highly anticipated second edition of the definitive reference on data mining by the top authority.},
  added-at = {2016-07-30T17:27:32.000+0200},
  address = {Amsterdam; Boston; San Francisco, CA},
  author = {Han, Jiawei and Kamber, Micheline},
  biburl = {https://www.bibsonomy.org/bibtex/243f10aa633fdd58acc4e97f00b8a7863/vngudivada},
  interhash = {9ad2758c79cf179ea2a2d0a8fb783095},
  intrahash = {43f10aa633fdd58acc4e97f00b8a7863},
  isbn = {9780080475585 0080475582},
  keywords = {Book DataMining},
  publisher = {Elsevier ; Morgan Kaufmann},
  refid = {143252170},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Data mining: concepts and techniques},
  url = {http://www.books24x7.com/marc.asp?bookid=37225},
  year = 2006
}

@article{modha2011cognitive,
  abstract = {Unite neuroscience, supercomputing, and nanotechnology to discover, demonstrate, and deliver the brain's core algorithms.},
  acmid = {1978559},
  added-at = {2016-07-31T14:59:06.000+0200},
  address = {New York, NY, USA},
  author = {Modha, Dharmendra S. and Ananthanarayanan, Rajagopal and Esser, Steven K. and Ndirango, Anthony and Sherbondy, Anthony J. and Singh, Raghavendra},
  biburl = {https://www.bibsonomy.org/bibtex/2f0fa959008b8b8006d2d9f96f9fb0a0e/vngudivada},
  doi = {10.1145/1978542.1978559},
  interhash = {f8d6ca6fe80f221fd6ae2fe19e61a1b9},
  intrahash = {f0fa959008b8b8006d2d9f96f9fb0a0e},
  issn = {0001-0782},
  issue_date = {August 2011},
  journal = {Commun. ACM},
  keywords = {CognitiveComputing},
  month = aug,
  number = 8,
  numpages = {10},
  pages = {62--71},
  publisher = {ACM},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Cognitive Computing},
  url = {http://doi.acm.org/10.1145/1978542.1978559},
  volume = 54,
  year = 2011
}

@inproceedings{franceschini2016watson,
  abstract = {Watson Concept Insights (WCI) is a service that was recently made publicly available by IBM. WCI provides an information retrieval framework that is designed to facilitate search and exploration of text documents, and is particularly effective on sparse data sets. Its methodology consists of first defining a dictionary of concepts which are interconnected in a concept graph and then modeling a document by predicting its relevance to any given concept in the concept graph using the concepts that are directly mentioned in the document itself. This technique in effect increases the document recall for any given query, even for very sparse data sets, exposing the user to a variety of connections between their query and a data set of interest.},
  added-at = {2016-07-31T14:54:15.000+0200},
  address = {Republic and Canton of Geneva, Switzerland},
  author = {Franceschini, Michele M. and Soares, Livio B. and Lastras Monta\ {n}o, Luis A.},
  biburl = {https://www.bibsonomy.org/bibtex/26a68f0628f846c6b6e354663078e3930/vngudivada},
  booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
  interhash = {c95f687199ee90c92857338348438cd1},
  intrahash = {6a68f0628f846c6b6e354663078e3930},
  keywords = {ConceptExtraction FeatureExtraction IBMWatson},
  pages = {179--182},
  publisher = {International World Wide Web Conferences Steering Committee},
  series = {WWW '16 Companion},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Watson Concept Insights: A Conceptual Association Framework},
  year = 2016
}

@book{mlodinow2009drunkards,
  abstract = {An irreverent look at how randomness influences our lives, and how our successes and failures are far more dependent on chance events than we recognize.},
  added-at = {2016-07-30T18:06:40.000+0200},
  address = {New York},
  author = {Mlodinow, Leonard},
  biburl = {https://www.bibsonomy.org/bibtex/26bb1729d10c8b9f5c0043b775dcd7337/vngudivada},
  interhash = {86fa8cdf425914b73b3c1b0cecda806b},
  intrahash = {6bb1729d10c8b9f5c0043b775dcd7337},
  isbn = {9780307275172 0307275175},
  keywords = {Book Randomness Statistics},
  publisher = {Pantheon Books},
  refid = {259266137},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {The Drunkard's walk: how randomness rules our lives},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780307275172},
  year = 2009
}

@book{khosrowpour2015encyclopedia,
  abstract = {"This 10-volume compilation of authoritative, research-based articles contributed by thousands of researchers and experts from all over the world emphasized modern issues and the presentation of potential opportunities, prospective solutions, and future directions in the field of information science and technology"--Provided by publisher.},
  added-at = {2016-08-05T13:28:55.000+0200},
  author = {Khosrow-Pour, Mehdi},
  biburl = {https://www.bibsonomy.org/bibtex/25d91e89301b02fc70936370594f858f8/vngudivada},
  edition = {Second},
  interhash = {4f13fad4e3f766ebae876d3bbbaf8283},
  intrahash = {5d91e89301b02fc70936370594f858f8},
  isbn = {9781466658899 1466658894},
  keywords = {Book Encyclopedia},
  refid = {891329667},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Encyclopedia of information science and technology},
  year = 2015
}

@book{nettleton2014commercial,
  abstract = {Whether you are brand new to data mining or working on your tenth predictive analytics project, Commercial Data Mining will be there for you as an accessible reference outlining the entire process and related themes. In this book, you'll learn that your organization does not need a huge volume of data or a Fortune 500 budget to generate business using existing information assets. Expert author David Nettleton guides you through the process from beginning to end and covers everything from business objectives to data sources, and selection to analysis and predictive modeling. Commercial Data Mining includes case studies and practical examples from Nettleton's more than 20 years of commercial experience. Real-world cases covering customer loyalty, cross-selling, and audience prediction in industries including insurance, banking, and media illustrate the concepts and techniques explained throughout the book. Illustrates cost-benefit evaluation of potential projects Includes vendor-agnostic advice on what to look for in off-the-shelf solutions as well as tips on building your own data mining tools Approachable reference can be read from cover to cover by readers of all experience levelsIncludes practical examples and case studies as well as actionable business insights from author's own experience.},
  added-at = {2016-07-30T17:49:55.000+0200},
  address = {Boston, MA},
  author = {Nettleton, David},
  biburl = {https://www.bibsonomy.org/bibtex/2db1871a5ee8a03559c827fbb1f2121f3/vngudivada},
  interhash = {1154dfd91b5ca4e85510bf2cd1cb3231},
  intrahash = {db1871a5ee8a03559c827fbb1f2121f3},
  isbn = {9780124166585 012416658X},
  keywords = {Book DataAnalytics DataMining PredictiveAnalytics},
  publisher = {Morgan Kaufmann},
  refid = {870333399},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Commercial data mining: processing, analysis and modeling for predictive analytics projects},
  url = {http://www.sciencedirect.com/science/book/9780124166028},
  year = 2014
}

@book{melton2006querying,
  abstract = {One-stop shopping: what software developers and architects need to know to master XML querying and retrieval.},
  added-at = {2016-08-05T13:11:18.000+0200},
  address = {San Francisco, Calif.},
  author = {Melton, Jim and Buxton, Stephen},
  biburl = {https://www.bibsonomy.org/bibtex/2df14f84552293deb9333e28b96aa58ab/vngudivada},
  interhash = {e9a6cda9ceabf90d2d1ff85c37dcaaac},
  intrahash = {df14f84552293deb9333e28b96aa58ab},
  isbn = {9780080540160 0080540163 1558607110 9781558607118},
  keywords = {Book XML},
  publisher = {Morgan Kaufmann},
  refid = {234198154},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Querying XML XQuery, XPath, and SQL/XML in context},
  year = 2006
}

@book{huth2004logic,
  abstract = {"Recent years have seen the development of powerful tools for verifying hardware and software systems, as companies worldwide realise the need for improved means of validating their products. There is increasing demand for training in basic methods in formal reasoning so that students can gain proficiency in logic-based verification methods. At the same time, the shift towards internet-based distributed computing creates the need for individuals who are able to reason about sophisticated autonomous agent-oriented software acting on large networks." "The second edition of this successful textbook addresses both those requirements, by continuing to provide an introduction to formal reasoning that is both relevant to the needs of modern computer science and rigorous enough for practical application. The presentation is clear and simple, with core material being described early in the book, and further technicalities introduced only where they are needed by the applications. A key feature is the full exposition of model-checking, and the new edition supports the most up-to--date versions of the tools NuSMV and Alloy. Improvements to the first edition have been made throughout, with extra and expanded sections on linear-time temporal logic model checking, SAT solvers, second-order logic, the Alloy specification tool, and programming by contract. The coverage of model-checking has been substantially updated. Further exercises have been added."--Jacket.},
  added-at = {2016-08-06T13:59:33.000+0200},
  address = {Cambridge [U.K.]; New York},
  author = {Huth, Michael and Ryan, Mark},
  biburl = {https://www.bibsonomy.org/bibtex/2012167e8834a5c2d2aeaf83694841b87/vngudivada},
  edition = {Second},
  interhash = {aa67e9133236648e9355fac3520ce171},
  intrahash = {012167e8834a5c2d2aeaf83694841b87},
  isbn = {052154310X 9780521543101 9781139636131 1139636138},
  keywords = {Book FormalMethods Logic Modeling Reasoning},
  note = {The second edition of this successful textbook continues to provide a clear introduction to formal reasoning relevant to the needs of modern computer science and sufficiently exacting for practical applications. Improvements have been made throughout with many new and expanded text sections. The coverage of model-checking has been substantially updated and additional exercises are included. Internet support includes worked solutions for teacher exercises and model solutions to some student exercises.},
  publisher = {Cambridge University Press},
  refid = {54960031},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Logic in computer science : modelling and reasoning about systems},
  year = 2004
}

@book{zafarani2014social,
  abstract = {The growth of social media over the last decade has revolutionized the way individuals interact and industries conduct business. Individuals produce data at an unprecedented rate by interacting, sharing, and consuming content through social media. Understanding and processing this new type of data to glean actionable patterns presents challenges and opportunities for interdisciplinary research, novel algorithms, and tool development. Social Media Mining integrates social media, social network analysis, and data mining to provide a convenient and coherent platform for students, practitioners, researchers, and project managers to understand the basics and potentials of social media mining. It introduces the unique problems arising from social media data and presents fundamental concepts, emerging issues, and effective algorithms for network analysis and data mining. Suitable for use in advanced undergraduate and beginning graduate courses as well as professional short courses, the text contains exercises of different degrees of difficulty that improve understanding and help apply concepts, principles, and methods in various scenarios of social media mining.},
  added-at = {2016-08-06T13:52:53.000+0200},
  author = {Zafarani, Reza and Abbasi, Mohammad Ali and Liu, Huan},
  biburl = {https://www.bibsonomy.org/bibtex/2e91688286e5b74c862a90af6758d9800/vngudivada},
  interhash = {8af8492e5e19a638d8145bb10e82bedf},
  intrahash = {e91688286e5b74c862a90af6758d9800},
  isbn = {9781107018853 1107018854},
  keywords = {Book DataMining SocialMedia},
  refid = {857356656},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Social media mining : an introduction},
  year = 2014
}

@book{leopold2013natural,
  abstract = {Natural language is one of the most important means of human communication. It enables us to express our will, to exchange thoughts, and to document our knowledge in written sources. Owing to its substantial role in many facets of human life, technology for automatically analyzing and processing natural language has recently become increasingly important. In fact, natural language processing tools have paved the way for entirely new business opportunities. The goal of this book is to facilitate the automatic analysis of natural language in process models and to employ this analysis for assisting process model stakeholders. Therefore, a technique is defined that automatically recognizes and annotates process model element labels. In addition, this technique is leveraged to support organizations in effectively utilizing their process models in various ways. The book is organized into seven chapters. It starts with an overview of business process management and linguistics and continues with conceptual contributions on parsing and annotating process model elements, with the detection and correction of process model guideline violations, with the generation of natural language from process models, and finally ends with the derivation of service candidates from process models.},
  added-at = {2016-08-05T13:39:16.000+0200},
  author = {Leopold, Henrik},
  biburl = {https://www.bibsonomy.org/bibtex/24bc2a3f2eed26de75cc0f430e7259e26/vngudivada},
  interhash = {5f85e3b930581767ff9a1964318768a1},
  intrahash = {4bc2a3f2eed26de75cc0f430e7259e26},
  isbn = {9783319041759 3319041754 3319041746 9783319041742},
  keywords = {Book BusinessProcess NLP},
  refid = {868638658},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Natural language in business process models : theoretical foundations, techniques, and applications},
  year = 2013
}

@book{henderson2009encyclopedia,
  abstract = {Synopsis:  The encyclopedia features about 600 cross-referenced entries - including more than 175 new to this volume - that reflect the new ways of using information technology and the important social issues that arise from such use.  To articulate the main themes of the book and to aid the reader, the introduction features a complete list of the entries grouped into specific categories, including business and e-commerce applications, computer industry, databases, future computing, Internet and World Wide Web, programming languages, and software development and engineering.  It is an indispensable resource that will meet the specific demands of students, interested lay people, and computer professionals who need accurate and straightforward information.},
  added-at = {2016-08-05T13:32:59.000+0200},
  address = {New York, NY},
  author = {Henderson, Harry},
  biburl = {https://www.bibsonomy.org/bibtex/2edb2ab8338dcf4b3b6319ddc4258a536/vngudivada},
  interhash = {b21b1e84d01bfb539aa487ad1c9c1dfd},
  intrahash = {edb2ab8338dcf4b3b6319ddc4258a536},
  isbn = {9780816063826 0816063826},
  keywords = {Book Encyclopedia},
  publisher = {Facts on File},
  refid = {233591925},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Encyclopedia of computer science and technology},
  year = 2009
}

@book{turbak2008design,
  abstract = {"This comprehensive text uses a simple and concise framework to teach key ideas in programming language design and implementation. The book's unique approach is based on a family of syntactically simple pedagogical languages that allow students to explore programming language concepts systematically. It takes as its premise and starting point the idea that when language behaviors become incredibly complex, the description of the behaviors must be incredibly simple." "The book is suitable as a text for an introductory graduate or advanced undergraduate programming languages course; it can also serve as a reference for researchers and practitioners."--Jacket.},
  added-at = {2016-08-06T14:13:12.000+0200},
  address = {Cambridge, Mass.},
  author = {Turbak, Franklyn Albin and Gifford, David K. and Sheldon, Mark A.},
  biburl = {https://www.bibsonomy.org/bibtex/2b8b27efaee242a146e53aa9531269285/vngudivada},
  description = {Hundreds of programming languages are in use today--scripting languages for Internet commerce, user interface programming tools, spreadsheet macros, page format specification languages, and many others. Designing a programming language is a metaprogramming activity that bears certain similarities to programming in a regular language, with clarity and simplicity even more important than in ordinary programming. This comprehensive text uses a simple and concise framework to teach key ideas in programming language design and implementation. The book's unique approach is based on a family of syntactically simple pedagogical languages that allow students to explore programming language concepts systematically. It takes as premise and starting point the idea that when language behaviors become incredibly complex, the description of the behaviors must be incredibly simple. The book presents a set of tools (a mathematical metalanguage, abstract syntax, operational and denotational semantics) and uses it to explore a comprehensive set of programming language design dimensions, including dynamic semantics (naming, state, control, data), static semantics (types, type reconstruction, polymporphism, effects), and pragmatics (compilation, garbage collection). The many examples and exercises offer students opportunities to apply the foundational ideas explained in the text. Specialized topics and code that implements many of the algorithms and compilation methods in the book can be found on the book's Web site, along with such additional material as a section on concurrency and proofs of the theorems in the text. The book is suitable as a text for an introductory graduate or advanced undergraduate programming languages course; it can also serve as a reference for researchers and practitioners.},
  interhash = {317472c8091c92e468b5659615c8a31b},
  intrahash = {b8b27efaee242a146e53aa9531269285},
  isbn = {9780262285216 0262285215 9781435665392 1435665392 9780262303156 0262303159},
  keywords = {Book Design ProgrammingLanguage},
  publisher = {MIT Press},
  refid = {252079407},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Design concepts in programming languages},
  year = 2008
}

@inproceedings{satish2014navigating,
  abstract = {Graph algorithms are becoming increasingly important for analyzing large datasets in many fields. Real-world graph data follows a pattern of sparsity, that is not uniform but highly skewed towards a few items. Implementing graph traversal, statistics and machine learning algorithms on such data in a scalable manner is quite challenging. As a result, several graph analytics frameworks (GraphLab, CombBLAS, Giraph, SociaLite and Galois among others) have been developed, each offering a solution with different programming models and targeted at different users. Unfortunately, the "Ninja performance gap" between optimized code and most of these frameworks is very large (2-30X for most frameworks and up to 560X for Giraph) for common graph algorithms, and moreover varies widely with algorithms. This makes the end-users' choice of graph framework dependent not only on ease of use but also on performance. In this work, we offer a quantitative roadmap for improving the performance of all these frameworks and bridging the "ninja gap". We first present hand-optimized baselines that get performance close to hardware limits and higher than any published performance figure for these graph algorithms. We characterize the performance of both this native implementation as well as popular graph frameworks on a variety of algorithms. This study helps end-users delineate bottlenecks arising from the algorithms themselves vs. programming model abstractions vs. the framework implementations. Further, by analyzing the system-level behavior of these frameworks, we obtain bottlenecks that are agnostic to specific algorithms. We recommend changes to alleviate these bottlenecks (and implement some of them) and reduce the performance gap with respect to native code. These changes will enable end-users to choose frameworks based mostly on ease of use.},
  acmid = {2610518},
  added-at = {2016-07-30T16:43:25.000+0200},
  address = {New York, NY, USA},
  author = {Satish, Nadathur and Sundaram, Narayanan and Patwary, Md. Mostofa Ali and Seo, Jiwon and Park, Jongsoo and Hassaan, M. Amber and Sengupta, Shubho and Yin, Zhaoming and Dubey, Pradeep},
  biburl = {https://www.bibsonomy.org/bibtex/2912cbc7e4750a29f900138faa9a300c1/vngudivada},
  booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
  doi = {10.1145/2588555.2610518},
  interhash = {53499c1116d95e251debd54baeed448c},
  intrahash = {912cbc7e4750a29f900138faa9a300c1},
  isbn = {978-1-4503-2376-5},
  keywords = {GraphAlgorithms GraphAnalytics GraphDatasets},
  location = {Snowbird, Utah, USA},
  numpages = {12},
  pages = {979--990},
  publisher = {ACM},
  series = {SIGMOD '14},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Navigating the Maze of Graph Analytics Frameworks Using Massive Graph Datasets},
  url = {http://doi.acm.org/10.1145/2588555.2610518},
  year = 2014
}

@book{bishop2006pattern,
  abstract = {The field of pattern recognition has undergone substantial development over the years. This book reflects these developments while providing a grounding in the basic concepts of pattern recognition and machine learning. It is aimed at advanced undergraduates or first year PhD students, as well as researchers and practitioners.},
  added-at = {2016-08-04T13:45:58.000+0200},
  address = {New York, NY},
  author = {Bishop, Christopher M.},
  biburl = {https://www.bibsonomy.org/bibtex/2b4c50dba3304df0728e303a885f80567/vngudivada},
  interhash = {2c106f24cf8e31f168166791080bfc89},
  intrahash = {b4c50dba3304df0728e303a885f80567},
  isbn = {978-0387310732},
  keywords = {Book MachineLearning PatternRecognition},
  publisher = {Springer},
  refid = {71008143},
  series = {Information Science and Statistics},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Pattern recognition and machine learning},
  year = 2006
}

@book{harrington2010clearly,
  abstract = {Although the core of the SQL language remains relatively unchanged, the most recent release of the SQL standard (SQL:2008) includes two sets of extensions: 1) support for object-relational databases and 2) support for XML. The second edition of this book included some material about the object-relational extensions. However, that set of commands has been greatly extended and the new edition takes that into account. The XML extensions are entirely new to the third edition. A chapter on basic relational concepts will be also added to broaden the audience and make the book more complete in its own right. All of the chapters will be revised to be sure they are up to date. Chapter 10 ("Views, Temporary Tables, and Indexes") from the second edition will be expanded to include common table expression (CTEs). The material in the existing Chapter 14 ("Unimplemented SQL-92 Features 251") will be disbursed through the chapters where the particular type of operation is discussed. While the content throughout will be updated where necessary, the existing organizational structure through chapter 14 will remain largely intact because this coverage represents the stable portion of the SQL language. There will be two NEW chapters at the beginning of the book. Chapter 1 ("The Relational Data Model") covers the relational data model conceptual material; Chapter 2 ("Manipulating Relations with Relational Algebra") covers the relational algebra material that will be pulled from the retrieval chapters. Part 5 ("Non-relational SQL Extensions") will now include the non-relational SQL extensions in three chapters. Chapter 17 (NEW) will cover XML. Chapter 18 will contain the object-relational conceptual material from the original chapter 15 and chapter 19 (NEW) will cover SQL object-relational support. (Some of chapter 19 will come from the second edition, however, a great deal of object-relational support has been added to the current standard and therefore needs to be added.) The other NEW chapter is Chapter 14, which covers triggers and stored procedures. This material was missing from previous editions. Many readers should find it a useful addition. Demonstrates how to formulate SQL queries and how queries are processed to maximize performance of the database management system Explains use of SQL to enter, modify or delete data to maintain database structural elements Covers in great detail new SQL application for XML to meet the growing XML usage in development of online content.},
  added-at = {2016-08-05T13:23:15.000+0200},
  address = {Burlington, MA},
  author = {Harrington, Jan L.},
  biburl = {https://www.bibsonomy.org/bibtex/29c5e16ecfc862c19dfa52e19029b23f3/vngudivada},
  interhash = {ba46091867a81c52987da1831bd17398},
  intrahash = {9c5e16ecfc862c19dfa52e19029b23f3},
  isbn = {9780123756985 0123756987 9780080517582 0080517587},
  keywords = {Book SQL},
  publisher = {Morgan Kaufmann},
  refid = {613958927},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {SQL clearly explained},
  year = 2010
}

@book{silver2012signal,
  abstract = {The author has built an innovative system for predicting baseball performance, predicted the 2008 election within a hair's breadth, and has become a national sensation as a blogger. Drawing on his own groundbreaking work, he examines the world of prediction. Human beings have to make plans and strategize for the future. As the pace of our lives becomes faster and faster, we have to do so more often and more quickly. But are our predictions any good? Is there hope for improvement? In this book the author examines the world of prediction, investigating how we can distinguish a true signal from a universe of noisy, ever-increasing data. Many predictions fail, often at great cost to society, because most of us have a poor understanding of probability and uncertainty. We are wired to detect a signal, and we mistake more confident predictions for more accurate ones. But overconfidence is often the reason for failure. If our appreciation of uncertainty improves, our predictions can get better too. This is the prediction paradox: the more humility we have about our ability to make predictions, and the more we are willing to learn from our mistakes, the more we can turn information into knowledge and data into foresight. The author examines both successes and failures to determine what more accurate forecasters have in common. In keeping with his own aim to seek truth from data, he visits innovative forecasters in a range of areas, from hurricanes to baseball, from the poker table to the stock market, from Capitol Hill to the NBA. Even when their innovations are modest, we can learn from their methods. How can we train ourselves to think probabilistically, as they do? How can the insights of an eighteenth-century Englishman unlock the twenty-first-century challenges of global warming and terrorism? How can being smarter about the future help us make better decisions in the present?},
  added-at = {2016-07-29T04:34:00.000+0200},
  address = {New York},
  author = {Silver, Nate},
  biburl = {https://www.bibsonomy.org/bibtex/2ef8a396db5a272cadf857d733bfd4245/vngudivada},
  interhash = {d653bcf3c6cabda83820e94fb4bbeca1},
  intrahash = {ef8a396db5a272cadf857d733bfd4245},
  isbn = {9781594204111 159420411X 9780143124009 0143124005 9780143125082 0143125087},
  keywords = {Book PredictiveAnalytics},
  publisher = {Penguin Press},
  refid = {780480483},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {The signal and the noise: why so many predictions fail--but some don't},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780143125082},
  year = 2012
}

@book{gorton2013dataintensive,
  abstract = {"A reference describing the general principles of the emerging field of data-intensive computing, along with methods for designing, managing and analyzing the big data sets of today"-- "A Challenge for the 21st Century Introduction In a world of rapid technological change such as the one we inhabit, it's occasionally instructive to contemplate how much things have changed in the last few years. For many, remembering life without the ability to view the World Wide Web (WWW) through the windows of a browser will be difficult (if not impossible for less 'mature' readers). And is it only seven years since YouTube, a Web site that is ingrained in so many facets of modern life, first came to life? How did we all really survive without FaceBook all those (actually, about 5) years ago? Various estimates put the amount of data stored by consumers and businesses around the world in 2010 in the vicinity of 13 exabytes, with a growth rate of 20--25% per annum. That's a lot of data. No wonder IBM is pursuing building a 120 petabyte storage array . There's obviously going to be a market for such devices in the future. As data volumes of all types, from video and photos to text documents and binary files for science, continue to grow in number and resolution, it's clear we have genuinely entered the realm of data intensive, or big data, computing."--},
  added-at = {2016-08-06T14:09:44.000+0200},
  author = {Gorton, Ian and Gracio, Deborah K.},
  biburl = {https://www.bibsonomy.org/bibtex/2532ecbed60c0a91265e16c67168b1c3c/vngudivada},
  description = {The world is awash with digital data from social networks, blogs, business, science, and engineering. Data-intensive computing facilitates understanding of complex problems that must process massive amounts of data. Through the development of new classes of software, algorithms, and hardware, data-intensive applications can provide timely and meaningful analytical results in response to exponentially growing data complexity and associated analysis requirements. This emerging area brings many challenges that are different from traditional high-performance computing. This reference for computing professionals and researchers describes the dimensions of the field, the key challenges, the state of the art, and the characteristics of likely approaches that future data-intensive problems will require. Chapters cover general principles and methods for designing such systems and for managing and analyzing the big data sets of today that live in the cloud, and describe example applications in bioinformatics and cybersecurity that illustrate these principles in practice.},
  interhash = {237a4bb59e1ca0755df4e7f0ddc7ba25},
  intrahash = {532ecbed60c0a91265e16c67168b1c3c},
  isbn = {9780521191951 0521191955},
  keywords = {Book DataScience},
  refid = {794640218},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Data-intensive computing : architectures, algorithms, and applications},
  year = 2013
}

@book{murach2011murachs,
  abstract = {This is the 4th edition of Murach's classic Java book that's trained thousands of developers in the last 10 years. Now fully updated to take advantage of the NetBeans IDE, this book helps any programmer learn Java faster and better than ever before. It's the one Java book that presents object-oriented features like inheritance, interfaces, and polymorphism in a way that's both understandable and useful in the real world. It moves at the professional pace that's expected on the job. It is full of practical coding examples that enhance training and that work as time-saving models for new applications. And it's all done in the distinctive Murach style that has been training professional programmers for more than 37 years.},
  added-at = {2016-08-05T13:16:33.000+0200},
  address = {Fresno, CA},
  author = {Murach, Joel},
  biburl = {https://www.bibsonomy.org/bibtex/2cd2b3c128d95faff999590750890557f/vngudivada},
  edition = {Fourth},
  interhash = {cb71be2d5c88723c372d1b5976500942},
  intrahash = {cd2b3c128d95faff999590750890557f},
  isbn = {9781890774653 1890774650},
  keywords = {Book Java Murach},
  publisher = {Murach},
  refid = {773041675},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Murach's java programming},
  year = 2011
}

@book{borland2014pentaho,
  abstract = {Bo Borland is the vice president of field technical sales at Pentaho, a leading Big Data analytics software provider. He has a passion for building teams and helping companies improve performance with data analytics. Prior to joining Pentaho, Bo worked as a management consultant at Deloitte and as a solution architect at Cognos, an IBM company. He founded a successful analytics consulting company, Management Signals, which merged with Pentaho in 2012. His 14 years' experience in professional analytics includes roles in management, sales, consulting, and sales engineering. Pentaho Corporation is a leading Big Data analytics company headquartered in Orlando, FL, with offices in San Francisco, London, and Portugal. Pentaho tightly couples data integration with full business analytics capabilities into a single, integrated software platform. The company's subscriptionbased software offers SMEs and global enterprises the ability to reduce the time taken to design, develop, and deploy Big Data analytics solutions.},
  added-at = {2016-07-30T17:21:50.000+0200},
  author = {Borland, Bo},
  biburl = {https://www.bibsonomy.org/bibtex/2afa22c64732749106092550eb7b8f2a3/vngudivada},
  interhash = {e4d2943564d1bbf5c48cc9e95172c4ed},
  intrahash = {afa22c64732749106092550eb7b8f2a3},
  isbn = {9781782168362 1782168362 1782168354 9781782168355},
  keywords = {Book DataAnalytics MongoDB},
  refid = {880969511},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Pentaho Analytics for MongoDB: combine Pentaho Analytics and MongoDB to create powerful analysis and reporting solutions},
  url = {http://site.ebrary.com/id/10842107},
  year = 2014
}

@book{patil2013pentaho,
  abstract = {The book is a practical guide, full of step-by-step examples that are easy to follow and implement. This book is for developers, system administrators, and business intelligence professionals looking to learn how to get more out of their data through Pentaho. In order to best engage with the examples, some knowledge of Java will be required.},
  added-at = {2016-07-30T17:43:24.000+0200},
  address = {Birmingham, UK},
  author = {Patil, Manoj R.},
  biburl = {https://www.bibsonomy.org/bibtex/2550cfafefc49c4c5366d36ed728dc1e0/vngudivada},
  interhash = {0c737529addacca732100f098f3631f0},
  intrahash = {550cfafefc49c4c5366d36ed728dc1e0},
  isbn = {9781783282166 1783282169 1306157692 9781306157698},
  keywords = {BigDataAnalytics Book DataAnalytics Pentaho},
  publisher = {Packt Publishing},
  refid = {864382389},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Pentaho for Big Data Analytics},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1561447},
  year = 2013
}

@book{clocksin2003programming,
  abstract = {Originally published in 1981, this was the first textbook on programming in the Prolog language and is still the definitive introductory text on Prolog. Though many Prolog textbooks have been published since, this one has withstood the test of time because of its comprehensiveness, tutorial approach, and emphasis on general programming applications. Prolog has continued to attract a great deal of interest in the computer science community, and has turned out to be a basis for an important new generation of programming languages and systems for Artificial Intelligence. Since the previous edition of Programming in Prolog, the language has been standardised by the International Organization for Standardization (ISO) and this book has been updated accordingly. The authors have also introduced some new material, clarified some explanations, corrected a number of minor errors, and removed appendices about Prolog systems that are now obsolete.},
  added-at = {2016-08-06T14:42:56.000+0200},
  address = {Berlin; New York},
  author = {Clocksin, W. F. and Mellish, C. S.},
  biburl = {https://www.bibsonomy.org/bibtex/2b121bad6d0be3adb4dee5ec7115f0d6a/vngudivada},
  edition = {Fifth},
  interhash = {c634da1a1269623500f0223aa4237b47},
  intrahash = {b121bad6d0be3adb4dee5ec7115f0d6a},
  isbn = {3540006788 9783540006787 0387006788 9780387006789},
  keywords = {Book LogicProgramming Prolog},
  publisher = {Springer-Verlag},
  refid = {51752491},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Programming in Prolog},
  year = 2003
}

@book{hennessy2012computer,
  abstract = {Computer Architecture: A Quantitative Approach explores the ways that software and technology in the cloud are accessed by digital media, such as cell phones, computers, tablets, and other mobile devices. The book became a part of Intel's 2012 recommended reading list for developers, and it covers the revolution of mobile computing. The text also highlights the two most important factors in architecture today: parallelism and memory hierarchy. The six chapters that this book is composed of follow a consistent framework: explanation of the ideas in each chapter; a "crosscutting issues" section, which presents how the concepts covered in one chapter connect with those given in other chapters; a "putting it all together" section that links these concepts by discussing how they are applied in real machine; and detailed examples of misunderstandings and architectural traps commonly encountered by developers and architects. The first chapter of the book includes formulas for energy, static and dynamic power, integrated circuit costs, reliability, and availability. Chapter 2 discusses memory hierarchy and includes discussions about virtual machines, SRAM and DRAM technologies, and new material on Flash memory. The third chapter covers the exploitation of instruction-level parallelism in high-performance processors, superscalar execution, dynamic scheduling and multithreading, followed by an introduction to vector architectures in the fourth chapter. Chapters 5 and 6 describe multicore processors and warehouse-scale computers (WSCs), respectively. This book is an important reference for computer architects, programmers, application developers, compiler and system software developers, computer system designers and application developers. Fully updated fifth edition covers the twin shifts to mobile and cloud computing, with new material, exercises, and case studies.--Publisher website.},
  added-at = {2016-08-06T17:32:55.000+0200},
  address = {Waltham, MA},
  author = {Hennessy, John L. and Patterson, David A. and Asanovic, Krste},
  biburl = {https://www.bibsonomy.org/bibtex/2b96c62e250447b189258db368a1e960f/vngudivada},
  description = {The computing world today is in the middle of a revolution: mobile clients and cloud computing have emerged as the dominant paradigms driving programming and hardware innovation today. The Fifth Edition of Computer Architecture focuses on this dramatic shift, exploring the ways in which software and technology in the cloud are accessed by cell phones, tablets, laptops, and other mobile computing devices. Each chapter includes two real-world examples, one mobile and one datacenter, to illustrate this revolutionary change. Updated to cover the mobile computing revolution. Emphasizes the two most important topics in architecture today: memory hierarchy and parallelism in all its forms. Develops common themes throughout each chapter: power, performance, cost, dependability, protection, programming models, and emerging trends ("What's Next") Includes three review appendices in the printed text. Additional reference appendices are available online. Includes updated Case Studies and completely new exercises.},
  interhash = {35d838f794194cace6cef33ed64d2eb1},
  intrahash = {b96c62e250447b189258db368a1e960f},
  isbn = {9780123838728 012383872X},
  keywords = {Book ComputerArchitecture},
  publisher = {Morgan Kaufmann},
  refid = {755102367},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Computer architecture: a quantitative approach},
  year = 2012
}

@book{marr2015using,
  abstract = {Forward by Henrik von Scheel. Convert the promise of big data into real world results There is so much buzz around big data. We all need to know what it is and how it works - that much is obvious. But is a basic understanding of the theory enough to hold your own in strategy meetings? Probably. But what will set you apart from the rest is actually knowing how to USE big data to get solid, real-world business results - and putting that in place to improve performance. Big Data will give you a clear understanding, blueprint, and step-by-step approach to building your own big data strategy. This is a well-needed practical introduction to actually putting the topic into practice. Illustrated with numerous real-world examples from a cross section of companies and organisations, Big Data will take you through the five steps of the SMART model: Start with Strategy, Measure Metrics and Data, Apply Analytics, Report Results, Transform. Discusses how companies need to clearly define what it is they need to know - Outlines how companies can collect relevant data and measure the metrics that will help them answer their most important business questions Addresses how the results of big data analytics can be visualised and communicated to ensure key decisions-makers understand them - Discusses how companies need to clearly define what it is they need to know - Includes many high-profile case studies from the author's work with some of the world's best known brands},
  added-at = {2016-08-06T20:11:05.000+0200},
  author = {Marr, Bernard},
  biburl = {https://www.bibsonomy.org/bibtex/2abfdc5497cbfe1e8432d997fe47c69c4/vngudivada},
  interhash = {2be1713306865eebabc01ae437648439},
  intrahash = {abfdc5497cbfe1e8432d997fe47c69c4},
  keywords = {BigData BigDataAnalytics Book},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Big data: using smart big data, analytics and metrics to make better decisions and improve performance},
  year = 2015
}

@book{reynolds2008schaums,
  abstract = {
Learn the essentials of computer science Schaum's Outline of Principles of Computer Science provides a concise overview of the theoretical foundation of computer science. It also includes focused review of object-oriented programming using Java.},
  added-at = {2016-08-06T17:53:52.000+0200},
  address = {New York, NY},
  author = {Reynolds, Carl and Tymann, Paul T.},
  biburl = {https://www.bibsonomy.org/bibtex/2b23c6f6294adc89a27a289e139602869/vngudivada},
  interhash = {9560d66b1c81ed79e7a813402a39511b},
  intrahash = {b23c6f6294adc89a27a289e139602869},
  isbn = {9780071510370 0071510370},
  keywords = {Book SchaumsOutline},
  publisher = {McGraw-Hill Education},
  refid = {213386456},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Schaum's outline of principles of computer science},
  year = 2008
}

@electronic{zhang2009artificial,
  abstract = {"This book is the first book to provide opportunities for millions working in economics, accounting, finance and other business areas education on HONNs, the ease of their usage, and directions on how to obtain more accurate application results. It provides significant, informative advancements in the subject and introduces the HONN group models and adaptive HONNs"--Provided by publisher.},
  added-at = {2016-08-06T14:34:22.000+0200},
  address = {Hershey},
  author = {Zhang, Ming and Global, IGI},
  biburl = {https://www.bibsonomy.org/bibtex/23a4d3084d47dca55c721f33aedb72c30/vngudivada},
  description = {Artificial Higher Order Neural Networks (HONNs) significantly change the research methodology that is used in economics and business areas for nonlinear data simulation and prediction. With the important advances in HONNs, it becomes imperative to remain knowledgeable about its benefits and improvements. Artificial Higher Order Neural Networks for Economics and Business is the first book to provide practical education and applications for the millions of professionals working in economics, accounting, finance and other business areas on HONNs and the ease of their usage to obtain more accurate application results. This source provides significant, informative advancements in the subject and introduces the concepts of HONN group models and adaptive HONNs.},
  interhash = {6407e1aefdf4968e45e8f2c9172c4d25},
  intrahash = {3a4d3084d47dca55c721f33aedb72c30},
  isbn = {9781599048987 1599048981},
  keywords = {ANN Book},
  publisher = {Information Science Reference},
  refid = {550481838},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Artificial higher order neural networks for economics and business},
  year = 2009
}

@book{lee2015structures,
  abstract = {This clearly structured and easy to read textbook explains the concepts and techniques required to write programs that can handle large amounts of data efficiently. Project-oriented and classroom-tested, the book presents a number of important algorithms supported by motivating examples that bring meaning to the problems faced by computer programmers. The idea of computational complexity is also introduced, demonstrating what can and cannot be computed efficiently so that the programmer can make informed judgements about the algorithms they use. The text assumes some basic experience in computer programming and familiarity in an object-oriented language, but not necessarily with Python. Topics and features: Includes both introductory and advanced data structures and algorithms topics, with suggested chapter sequences for those respective courses provided in the preface Provides learning goals, review questions and programming exercises in each chapter, as well as numerous illustrative examples Offers downloadable programs and supplementary files at an associated website, with instructor materials available from the author Presents a primer on Python for those coming from a different language background Reviews the use of hashing in sets and maps, along with an examination of binary search trees and tree traversals, and material on depth first search of graphs Discusses topics suitable for an advanced course, such as membership structures, heaps, balanced binary search trees, B-trees and heuristic search Students of computer science will find this clear and concise textbook to be invaluable for undergraduate courses on data structures and algorithms, at both introductory and advanced levels. The book is also suitable as a refresher guide for computer programmers starting new jobs working with Python.},
  added-at = {2016-08-06T23:16:05.000+0200},
  author = {Lee, Kent D. and Hubbard, Steve},
  biburl = {https://www.bibsonomy.org/bibtex/23ea38f284dc60e117428dc48970e8bfd/vngudivada},
  interhash = {5b9153085a11a8b1b934b969776eb9f8},
  intrahash = {3ea38f284dc60e117428dc48970e8bfd},
  keywords = {Book DataStructures Python},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Data structures and algorithms with Python},
  year = 2015
}

@book{wang2008encyclopedia,
  abstract = {The Encyclopedia of Data Warehousing and Mining provides a comprehensive, critical and descriptive examination of concepts, issues, trends, and challenges in this rapidly expanding field of data warehousing and mining (DWM). This encyclopedia consists of more than 350 contributors from 32 countries, 1,800 terms and definitions, and more than 4,400 references. This authoritative publication offers in-depth coverage of evolutions, theories, methodologies, functionalities, and applications of DWM in such interdisciplinary industries as healthcare informatics, artificial intelligence, financial modeling, and applied statistics, making it a single source of knowledge and latest discoveries in the field of DWM.},
  added-at = {2016-08-06T23:07:28.000+0200},
  address = {Hershey, PA},
  author = {Wang, John},
  biburl = {https://www.bibsonomy.org/bibtex/2a2dbde63f2808dd2105375ffd03e500e/vngudivada},
  interhash = {8684d59d77e4cb3d842ee4f9cf95de97},
  intrahash = {a2dbde63f2808dd2105375ffd03e500e},
  keywords = {Book DataMining DataWarehouse Encyclopedia},
  publisher = {Information Science Reference},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Encyclopedia of data warehousing and mining},
  year = 2008
}

@book{sikos2015mastering,
  abstract = {A major limitation of conventional web sites is their unorganized and isolated contents, which is created mainly for human consumption. This limitation can be addressed by organizing and publishing data, using powerful formats that add structure and meaning to the content of web pages and link related data to one another. Computers can "understand" such data better, which can be useful for task automation. The web sites that provide semantics (meaning) to software agents form the Semantic Web, the Artificial Intelligence extension of the World Wide Web.},
  added-at = {2016-08-06T19:57:58.000+0200},
  author = {Sikos, Leslie F.},
  biburl = {https://www.bibsonomy.org/bibtex/206e51855ca72abed834966a6f1f40c60/vngudivada},
  interhash = {d484f482a6de511cffc77faa69db2a10},
  intrahash = {06e51855ca72abed834966a6f1f40c60},
  isbn = {9781484210499 1484210492},
  keywords = {LinkedData OpenData SemanticWeb},
  refid = {913783645},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Mastering structured data on the Semantic Web : from HTML5 microdata to linked open data},
  year = 2015
}

@book{kale2014guide,
  abstract = {Guide to Cloud Computing for Business and Technology Managers: From Distributed Computing to Cloudware Applications unravels the mystery of cloud computing and explains how it can transform the operating contexts of business enterprises. It provides a clear understanding of what cloud computing really means, what it can do, and when it is practical to use. Addressing the primary management and operation concerns of cloudware, including performance, measurement, monitoring, and security, this pragmatic book: Introduces the enterprise applications integration (EAI) solutions that were a first step toward enabling an integrated enterprise Details service-oriented architecture (SOA) and related technologies that paved the road for cloudware applications Covers delivery models like IaaS, PaaS, and SaaS, and deployment models like public, private, and hybrid clouds Describes Amazon, Google, and Microsoft cloudware solutions and services, as well as those of several other players Demonstrates how cloud computing can reduce costs, achieve business flexibility, and sharpen strategic focus Unlike customary discussions of cloud computing, Guide to Cloud Computing for Business and Technology Managers: From Distributed Computing to Cloudware Applications emphasizes the key differentiator - that cloud computing is able to treat enterprise-level services not merely as discrete stand-alone services, but as Internet-locatable, composable, and repackageable building blocks for generating dynamic real-world enterprise business processes.},
  added-at = {2016-08-07T00:03:07.000+0200},
  author = {Kale, Vivek},
  biburl = {https://www.bibsonomy.org/bibtex/219784def1c3c902ccdebdb94caa6bba6/vngudivada},
  interhash = {00b54e5b0a30980f0f1e2d04a45ae1b0},
  intrahash = {19784def1c3c902ccdebdb94caa6bba6},
  keywords = {Book CloudComputing},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Guide to Cloud Computing for Business and Technology Managers: From Distributed Computing to Cloudware Applications},
  year = 2014
}

@book{urbano2014spatial,
  abstract = {This book guides animal ecologists, biologists and wildlife and data managers through a step-by-step procedure to build their own advanced software platforms to manage and process wildlife tracking data. This unique, problem-solving-oriented guide focuses on how to extract the most from GPS animal tracking data, while preventing error propagation and optimizing analysis performance. Based on the open source PostgreSQL/PostGIS spatial database, the software platform will allow researchers and managers to integrate and harmonize GPS tracking data together with animal characteristics, environmental data sets, including remote sensing image time series, and other bio-logged data, such as acceleration data. Moreover, the book shows how the powerful R statistical environment can be integrated into the software platform, either connecting the database with R, or embedding the same tools in the database through the PostgreSQL extension Pl/R. The client/server architecture allows users to remotely connect a number of software applications that can be used as a database front end, including GIS software and WebGIS. Each chapter offers a real-world data management and processing problem that is discussed in its biological context; solutions are proposed and exemplified through ad hoc SQL code, progressively exploring the potential of spatial database functions applied to the respective wildlife tracking case. Finally, wildlife tracking management issues are discussed in the increasingly widespread framework of collaborative science and data sharing. GPS animal telemetry data from a real study, freely available online, are used to demonstrate the proposed examples. This book is also suitable for undergraduate and graduate students, if accompanied by the basics of databases.},
  added-at = {2016-08-06T23:52:51.000+0200},
  address = {New York, NY},
  author = {Urbano, Ferdinando},
  biburl = {https://www.bibsonomy.org/bibtex/277aed5f2ac7382ef6172954fab846e74/vngudivada},
  interhash = {de726f077d890421b9aa6c9edbfbaab1},
  intrahash = {77aed5f2ac7382ef6172954fab846e74},
  keywords = {Book SpatialData Wildlife},
  publisher = {Springer},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Spatial Database for GPS Wildlife Tracking Data: A Practical Guide to Creating a Data Management System with PostgreSQL},
  year = 2014
}

@book{ahlemeyerstubbe2014practical,
  abstract = {"Data mining is well on its way to becoming a recognized discipline in the overlapping areas of IT, statistics, machine learning, and AI. Practical Data Mining for Business presents a user-friendly approach to data mining methods, covering the typical uses to which it is applied. The methodology is complemented by case studies to create a versatile reference book, allowing readers to look for specific methods as well as for specific applications. The book is formatted to allow statisticians, computer scientists, and economists to cross-reference from a particular application or method to sectors of interest."--},
  added-at = {2016-08-06T23:49:48.000+0200},
  author = {Ahlemeyer-Stubbe, Andrea},
  biburl = {https://www.bibsonomy.org/bibtex/2434fa1f2a5b3a2849a32e2c314bef1f7/vngudivada},
  interhash = {5e6a438cf35eba67a255547f2f70c72d},
  intrahash = {434fa1f2a5b3a2849a32e2c314bef1f7},
  keywords = {Book DataMining PracticalGuide},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {A practical guide to data mining for business and industry},
  year = 2014
}

@book{miller2012programming,
  abstract = {"Formal systems that describe computations over syntactic structures occur frequently in computer science. Logic programming provides a natural framework for encoding and animating such systems. However, these systems often embody variable binding, a notion that must be treated carefully at a computational level. This book aims to show that a programming language based on a simply typed version of higher-order logic provides an elegant, declarative means for providing such a treatment. Three broad topics are covered in pursuit of this goal. First, a proof-theoretic framework that supports a general view of logic programming is identified. Second, an actual language called [Lambda]Prolog is developed by applying this view to higher-order logic. Finally, a methodology for programming with specifications is exposed by showing how several computations over formal objects such as logical formulas, functional programs, and [lambda]-terms and [pi]-calculus expressions can be encoded in [Lambda]Prolog"--},
  added-at = {2016-08-06T14:27:57.000+0200},
  author = {Miller, Dale and Nadathur, Gopalan},
  biburl = {https://www.bibsonomy.org/bibtex/2d3637a287a50fbbf829990ffc76e5023/vngudivada},
  interhash = {b8fbe4434bdfd1c8d2ccab55c1a09d2e},
  intrahash = {d3637a287a50fbbf829990ffc76e5023},
  isbn = {9780521879408 052187940X},
  keywords = {Book HigherOrderLogic Logic},
  refid = {774491609},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Programming with higher-order logic},
  year = 2012
}

@article{wu2007algorithms,
  abstract = {Features

    Presents the most influential algorithms identified by a top-tier conference in the field, the 2006 IEEE International Conference on Data Mining (ICDM)

    Provides algorithm descriptions, available software, illustrative examples and applications, advanced topics, and exercises in each chapter

    Explores classification, clustering, statistical learning, association analysis, and link mining

    Promotes data mining to wider real-world applications

    Summary

    Identifying some of the most influential algorithms that are widely used in the data mining community, The Top Ten Algorithms in Data Mining provides a description of each algorithm, discusses its impact, and reviews current and future research. Thoroughly evaluated by independent reviewers, each chapter focuses on a particular algorithm and is written by either the original authors of the algorithm or world-class researchers who have extensively studied the respective algorithm.

    The book concentrates on the following important algorithms: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART. Examples illustrate how each algorithm works and highlight its overall performance in a real-world application. The text covers key topics—including classification, clustering, statistical learning, association analysis, and link mining—in data mining research and development as well as in data mining, machine learning, and artificial intelligence courses.

    By naming the leading algorithms in this field, this book encourages the use of data mining techniques in a broader realm of real-world applications. It should inspire more data mining researchers to further explore the impact and novel research issues of these algorithms.},
  added-at = {2016-08-06T18:54:49.000+0200},
  address = {New York, NY},
  author = {Wu, Xindong and Kumar, Vipin and Ross Quinlan, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
  biburl = {https://www.bibsonomy.org/bibtex/2cec0557d53d024ec23247a17c7710219/vngudivada},
  interhash = {8abee778b794d606f5bd5b422559251a},
  intrahash = {cec0557d53d024ec23247a17c7710219},
  issue_date = {December 2007},
  journal = {Knowl. Inf. Syst.},
  keywords = {Algorithms DataMining MachineLearning},
  month = dec,
  number = 1,
  pages = {1--37},
  publisher = {Springer-Verlag},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Top 10 Algorithms in Data Mining},
  volume = 14,
  year = 2007
}

@book{rabunal2006artificial,
  abstract = {"This book offers an outlook of the most recent works at the field of the Artificial Neural Networks (ANN), including theoretical developments and applications of systems using intelligent characteristics for adaptability"--Provided by publisher.},
  added-at = {2016-08-06T14:31:26.000+0200},
  address = {Hershey PA},
  author = {Rabunal, Juan Ramon and Dorrado, Julian},
  biburl = {https://www.bibsonomy.org/bibtex/2f55c2164603ed9ad9f3fe724cd4483a1/vngudivada},
  interhash = {21ef154917ee97b246db26ceea5383a3},
  intrahash = {f55c2164603ed9ad9f3fe724cd4483a1},
  isbn = {1591409047 9781591409045 1591409020 9781591409021 1591409039 9781591409038},
  keywords = {ANN Book},
  publisher = {Idea Group Pub.},
  refid = {62279984},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Artificial neural networks in real-life applications},
  url = {http://www.books24x7.com/marc.asp?bookid=12295},
  year = 2006
}

@book{hart2013linked,
  abstract = {Geographic Information has an important role to play in linking and combining datasets through shared location, but the potential is still far from fully realized because the data is not well organized and the technology to aid this process has not been available. Developments in the Semantic Web and Linked Data, however, are making it possible to integrate data based on Geographic Information in a way that is more accessible to users. Drawing on the industry experience of a geographer and a computer scientist, Linked Data: A Geographic Perspective is a practical guide to implementing Geograph.},
  added-at = {2016-08-06T23:58:01.000+0200},
  address = {Boca Raton},
  author = {Hart, Glen and Dolbear, Catherine},
  biburl = {https://www.bibsonomy.org/bibtex/26b10766fa6c40424cf7d4c65fb560a00/vngudivada},
  interhash = {ec222a39fe94e48ad9030648a7ed173e},
  intrahash = {6b10766fa6c40424cf7d4c65fb560a00},
  keywords = {Book GIS LinkedData},
  publisher = {Taylor & Francis},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Linked data a geographic perspective},
  year = 2013
}

@book{akerkar2014computing,
  abstract = {"To tackle the challenges of Big Data, novel approaches and tools have emerged. Moreover, the technology required for big-data computing is developing at a satisfactory rate due to market forces and technological evolution. This book presents a mix of theory and industry cases that discuss the technical and practical issues related to Big Data in intelligent information management. It emphasizes the adoption and diffusion of Big Data tools and technologies in real practical applications. In addition, the book balances between academic and industry contributions"--},
  added-at = {2016-08-07T00:14:40.000+0200},
  author = {Akerkar, Rajendra},
  biburl = {https://www.bibsonomy.org/bibtex/2d7574c78c8cf0d049230017eb5bfc488/vngudivada},
  description = {Due to market forces and technological evolution, Big Data computing is developing at an increasing rate. A wide variety of novel approaches and tools have emerged to tackle the challenges of Big Data, creating both more opportunities and more challenges for students and professionals in the field of data computation and analysis. Presenting a mix of industry cases and theory, Big Data Computing discusses the technical and practical issues related to Big Data in intelligent information management. Emphasizing the adoption and diffusion of Big Data tools and technologies in industry, the book introduces a broad range of Big Data concepts, tools, and techniques. It covers a wide range of research, and provides comparisons between state-of-the-art approaches. Comprised of five sections, the book focuses on: What Big Data is and why it is important Semantic technologies Tools and methods Business and economic perspectives Big Data applications across industries},
  interhash = {52f844ad842307276332ea1b5946ed08},
  intrahash = {d7574c78c8cf0d049230017eb5bfc488},
  keywords = {BigData},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Big data computing},
  year = 2014
}

@book{friedman2008essentials,
  abstract = {"This book provides students with a working understanding of the essential concepts of programming languages. Most of these essentials relate to the semantics, or meaning, of program elements, and the text uses interpreters (short programs that directly analyze an abstract representation of the program text) to express the semantics of many essential language elements. The book provides views of programming languages using varying levels of abstraction, maintaining a connection between the high-level and low-level views. The complete Scheme code for all the interpreters and analyzers in the book can be found online through The MIT Press Web site." "For this new edition, each chapter has been revised and new exercises have been added. Additions have been made to the text, including new chapters on modules and continuation-passing style. Essentials of Programming Languages can be used for both graduate and undergraduate courses, and for continuing education courses for programmers."--Jacket.},
  added-at = {2016-08-06T14:19:34.000+0200},
  address = {Cambridge, MA},
  author = {Friedman, Daniel P. and Wand, Mitchell},
  biburl = {https://www.bibsonomy.org/bibtex/29dcd752e1fab6e649db94a63c6027aed/vngudivada},
  edition = {Third},
  interhash = {1d3dc5f362151d9f734f33a7773d3912},
  intrahash = {9dcd752e1fab6e649db94a63c6027aed},
  isbn = {9780262273275 0262273276 9781435647596 1435647599 0262062798 9780262062794},
  keywords = {Book ProgrammingLanguage},
  publisher = {MIT Press},
  refid = {232302838},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Essentials of programming languages},
  year = 2008
}

@book{ledolter2013mining,
  abstract = {Collecting, analyzing, and extracting valuable information from a large amount of data requires easily accessible, robust, computational and analytical tools. Data Mining and Business Analytics with R utilizes the open source software R for the analysis, exploration, and simplification of large high-dimensional data sets. As a result, readers are provided with the needed guidance to model and interpret complicated data and become adept at building powerful models for prediction and classification. Highlighting both underlying concepts and practical computational skills, Data Mining and Business Analytics with R begins with coverage of standard linear regression and the importance of parsimony in statistical modeling. The book includes important topics such as penalty-based variable selection (LASSO); logistic regression; regression and classification trees; clustering; principal components and partial least squares; and the analysis of text and network data. In addition, the book presents: * A thorough discussion and extensive demonstration of the theory behind the most useful data mining tools * Illustrations of how to use the outlined concepts in real-world situations * Readily available additional data sets and related R code allowing readers to apply their own analyses to the discussed materials * Numerous exercises to help readers with computing skills and deepen their understanding of the material. Data Mining and Business Analytics with R is an excellent graduate-level textbook for courses on data mining and business analytics. The book is also a valuable reference for practitioners who collect and analyze data in the fields of finance, operations management, marketing, and the information sciences.},
  added-at = {2016-08-06T23:46:56.000+0200},
  author = {Ledolter, Johannes},
  biburl = {https://www.bibsonomy.org/bibtex/20f20abb30ee83969b2ad12e0d39461a5/vngudivada},
  interhash = {a87909822b758b62b20442ef18c429fb},
  intrahash = {0f20abb30ee83969b2ad12e0d39461a5},
  keywords = {Book BusinessAnalytics DataAnalytics DataMining R},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Data mining and business analytics with R},
  year = 2013
}

@book{berry2005understanding,
  abstract = {"The second edition of Understanding Search Engines: Mathematical Modeling and Text Retrieval follows the basic premise of the first edition by discussing many of the key design issues for building search engines and emphasizing the important role that applied mathematics can play in improving information retrieval. The authors discuss important data structures, algorithms, and software as well as user-centered issues such as interfaces, manual indexing, and document preparation. There is a completely new chapter on link-structure algorithms used in search engines such as Google, and the chapter on user interface has been rewritten to specifically focus on search engine usability. There are new recommendations for further reading and an expanded bibliography, and the index has been updated and streamlined to make it more reader friendly."--Jacket.},
  added-at = {2016-08-06T17:24:39.000+0200},
  address = {Philadelphia, PA},
  author = {Berry, Michael W. and Browne, Murray},
  biburl = {https://www.bibsonomy.org/bibtex/25d634d13769329ad574c82bbc6565fde/vngudivada},
  description = {There is no other information retrieval/search book where the heart is the mathematical foundations. This book is greatly needed to further establish information retrieval as a serious academic, as well as practical and industrial, area." ---Jaime Carbonell, Carnegie Mellon University. Berry and Browne describe most of what you need to know to design your own search engine. Their strength is the description of the solid mathematical underpinnings at a level that is understandable to competent engineering undergraduates, perhaps with a bit of instructor guidance. They discuss the algorithms used by most commercial search engines, so you may find your use of Google and its kind becomes more effective, too. --George Corliss, Marquette University. This book gives a valuable, generally non-technical, insight into how search engines work, how to improve the users' success in Information Retrieval (IR), and an in-depth analysis of a mathematical algorithm for improving a search engine's performance. Written in an informal style, the book is easy to read and is a good introduction on how search engines operate Christopher Dean, Mathematics Today, October 1999. The second edition of Understanding Search Engines: Mathematical Modeling and Text Retrieval follows the basic premise of the first edition by discussing many of the key design issues for building search engines and emphasizing the important role that applied mathematics can play in improving information retrieval. The authors discuss important data structures, algorithms, and software as well as user-centered issues such as interfaces, manual indexing, and document preparation. Readers will find that the second edition includes significant changes that bring the text up to date on current information retrieval methods. For example, the authors have added a completely new chapter on link-structure algorithms used in search engines such as Google, and the chapter on user interface has been rewritten to specifically focus on search engine usability. To reflect updates in the literature on information retrieval, the authors have added new recommendations for further reading and expanded the bibliography. In addition, the index has been updated and streamlined to make it more reader friendly. Instructors will find that the book serves as an excellent companion text for courses in information retrieval, applied linear algebra, and scientific computing. Because of the authors informal, conversational tone, readers with nonmathematical backgrounds also will appreciate the less technical chapters of the text.},
  interhash = {349052c8acf122d7278da764888ea8ea},
  intrahash = {5d634d13769329ad574c82bbc6565fde},
  isbn = {0898715814 9780898715811},
  keywords = {Book SearchEngine},
  publisher = {SIAM, Society for Industrial and Applied Mathematics},
  refid = {57573973},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Understanding search engines: mathematical modeling and text retrieval},
  year = 2005
}

@book{sukthankar2014activity,
  abstract = {Plan recognition, activity recognition, and intent recognition together combine and unify techniques from user modeling, machine vision, intelligent user interfaces, human/computer interaction, autonomous and multi-agent systems, natural language understanding, and machine learning.},
  added-at = {2016-08-06T17:15:06.000+0200},
  address = {Waltham, MA},
  author = {Sukthankar, Gita},
  biburl = {https://www.bibsonomy.org/bibtex/2e5566e590875ddd52b9258fd9e10c74f/vngudivada},
  interhash = {ff6b86b19848ce5c9f89e1fd75078133},
  intrahash = {e5566e590875ddd52b9258fd9e10c74f},
  isbn = {9780124017108 012401710X 0123985323 9780123985323},
  keywords = {AutonomousSystem Book ML NLU UserModeling},
  publisher = {Morgan Kaufmann},
  refid = {875004006},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Plan, activity, and intent recognition theory and pratice},
  url = {/brokenurl#]},
  year = 2014
}

@book{dominus2005higherorder,
  abstract = {Most Perl programmers were originally trained as C and Unix programmers, so the Perl programs that they write bear a strong resemblance to C programs. However, Perl incorporates many features that have their roots in other languages such as Lisp. These advanced features are not well understood and are rarely used by most Perl programmers, but they are very powerful. They can automate tasks in everyday programming that are difficult to solve in any other way. One of the most powerful of these techniques is writing functions that manufacture or modify other functions. For example, instead of writing ten similar functions, a programmer can write a general pattern or framework that can then create the functions as needed according to the pattern. For several years Mark Jason Dominus has worked to apply functional programming techniques to Perl. Now Mark brings these flexible programming methods that he has successfully taught in numerous tutorials and training sessions to a wider audience. * Introduces powerful programming methodsnew to most Perl programmersthat were previously the domain of computer scientists* Gradually builds up confidence by describing techniques of progressive sophistication* Shows how to improve everyday programs and includes numerous engaging code examples to illustrate the methods},
  added-at = {2016-08-06T14:37:10.000+0200},
  address = {Amsterdam; Boston, Mass.},
  author = {Dominus, Mark Jason},
  biburl = {https://www.bibsonomy.org/bibtex/25d3fa9e4c8992a251cefe92082dc3023/vngudivada},
  interhash = {f3a468d963dc2a5172dfec897bf82be2},
  intrahash = {5d3fa9e4c8992a251cefe92082dc3023},
  isbn = {1423708172 9781423708179},
  keywords = {Book FunctionalProgramming Perl},
  publisher = {Morgan Kaufmann Publishers},
  refid = {60692541},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Higher-order Perl : a guide to program transformation},
  year = 2005
}

@book{beaver2004practical,
  abstract = {Following in the footsteps of its bestselling predecessor, The Practical Guide to HIPAA Privacy and Security Compliance, Second Edition is a one-stop, up-to-date resource on Health Insurance Portability and Accountability Act (HIPAA) privacy and security, including details on the HITECH Act, the 2013 Omnibus Rule, and the pending rules. Updated and revised with several new sections, this edition defines what HIPAA is, what it requires, and what you need to do to achieve compliance.

The book provides an easy-to-understand overview of HIPAA privacy and security rules and compliance tasks. Supplying authoritative insights into real-world HIPAA privacy and security issues, it summarizes the analysis, training, and technology needed to properly plan and implement privacy and security policies, training, and an overall program to manage information risks. Instead of focusing on technical jargon, the book spells out what your organization must do to achieve and maintain compliance requirements on an ongoing basis.},
  added-at = {2016-08-06T23:25:07.000+0200},
  address = {Boca Raton},
  author = {Beaver, Kevin and Herold, Rebecca},
  biburl = {https://www.bibsonomy.org/bibtex/221ccc88293c31fbbb40106fb96dbe35f/vngudivada},
  interhash = {bf59b51d2642b49c3ac80b27024efd7c},
  intrahash = {21ccc88293c31fbbb40106fb96dbe35f},
  keywords = {Book HIPPA Healthcare},
  publisher = {Auerbach Publications},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {The practical guide to HIPAA privacy and security compliance},
  year = 2004
}

@book{zaki2013mining,
  abstract = {The fundamental algorithms in data mining and analysis form the basis for the emerging field of data science, which includes automated methods to analyze patterns and models for all kinds of data, with applications ranging from scientific discovery to business intelligence and analytics. This textbook for senior undergraduate and graduate data mining courses provides a broad yet in-depth overview of data mining, integrating related concepts from machine learning and statistics. The main parts of the book include exploratory data analysis, pattern mining, clustering, and classification. The book lays the basic foundations of these tasks, and also covers cutting-edge topics such as kernel methods, high-dimensional data analysis, and complex graphs and networks. With its comprehensive coverage, algorithmic perspective, and wealth of examples, this book offers solid guidance in data mining for students, researchers, and practitioners alike. Key features: Covers both core methods and cutting-edge research Algorithmic approach with open-source implementations Minimal prerequisites: all key mathematical concepts are presented, as is the intuition behind the formulas Short, self-contained chapters with class-tested examples and exercises allow for flexibility in designing a course and for easy reference Supplementary website with lecture slides, videos, project ideas, and more},
  added-at = {2016-08-07T00:18:51.000+0200},
  author = {Zaki, Mohammed J. and Meira, Wagner},
  biburl = {https://www.bibsonomy.org/bibtex/2bb36439b2b99c83aff8b20e4cab575c1/vngudivada},
  interhash = {fdece91d1bbc458c68ce072096196cbd},
  intrahash = {bb36439b2b99c83aff8b20e4cab575c1},
  keywords = {Book DataAnalysis DataMining},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Data mining and analysis : fundamental concepts and algorithms},
  year = 2013
}

@book{celko2004celkos,
  abstract = {Joe Celko's Trees and Hierarchies in SQL is an intermediate to advanced-level practitioner's guide to mastering the two most challenging aspects of developing database applications in SQL. In this book, Celko illustrates several major approaches to representing trees and hierarchies and related topics that should be of interest to the working database programmer. These topics include hierarchical encoding schemes, graphs, IMS, binary trees, and more. This book covers SQL-92 and SQL:1999. Includes graph theory and programming techniques. Running examples throughout the book help illustrate and tie concepts together. Loads of code, available for download from www.mkp.com.},
  added-at = {2016-08-06T14:56:32.000+0200},
  address = {San Francisco, Calif.; Oxford},
  author = {Celko, Joe},
  biburl = {https://www.bibsonomy.org/bibtex/2f90930906aa1d1dfbfe0888a5fea31cc/vngudivada},
  interhash = {d6f3a1d9bdf26fb479cc8e56d0c60a5f},
  intrahash = {f90930906aa1d1dfbfe0888a5fea31cc},
  isbn = {1558609202 9781558609204},
  keywords = {Book Hierarchy SQL Tree},
  publisher = {Morgan Kaufmann ; Elsevier Science},
  refid = {56458097},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Joe Celko's SQL for smarties: trees and hierarchies},
  year = 2004
}

@book{bramer2013logic,
  abstract = {Logic Programming is the name given to a distinctive style of programming, very different from that of conventional programming languages such as C++ and Java. By far the most widely used Logic Programming language is Prolog. Prolog is a good choice for developing complex applications, especially in the field of Artificial Intelligence. Logic Programming with Prolog does not assume that the reader is an experienced programmer or has a background in Mathematics, Logic or Artificial Intelligence. It starts from scratch and aims to arrive at the point where quite powerful programs can be written in the language. It is intended both as a textbook for an introductory course and as a self-study book. On completion readers will know enough to use Prolog in their own research or practical projects. Each chapter has self-assessment exercises so that readers may check their own progress. A glossary of the technical terms used completes the book. This second edition has been revised to be fully compatible with SWI-Prolog, a popular multi-platform public domain implementation of the language. Additional chapters have been added covering the use of Prolog to analyse English sentences and to illustrate how Prolog can be used to implement applications of an 'Artificial Intelligence' kind.},
  added-at = {2016-08-06T14:40:06.000+0200},
  author = {Bramer, M. A.},
  biburl = {https://www.bibsonomy.org/bibtex/202e0342be612062e9f825017e7252e64/vngudivada},
  edition = {Second},
  interhash = {7642d83ed31e1b8a50891a22fbe33ab9},
  intrahash = {02e0342be612062e9f825017e7252e64},
  isbn = {9781447154877 1447154878 144715486X 9781447154860},
  keywords = {Book LogicProgramming Prolog},
  refid = {863234712},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Logic programming with Prolog},
  year = 2013
}

@misc{briggs2008breaking,
  added-at = {2016-08-06T23:34:29.000+0200},
  author = {Briggs, William M.},
  biburl = {https://www.bibsonomy.org/bibtex/27f0cb344f15570ad5c04bb697229b66f/vngudivada},
  interhash = {3978f5612663237a8e94c99809a6a3e8},
  intrahash = {7f0cb344f15570ad5c04bb697229b66f},
  keywords = {Probability Statistics},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Breaking the Law of Averages: Real-Life Probability and Statistics in Plain English},
  year = 2008
}

@book{feinleib2014bootcamp,
  abstract = {Investors and technology gurus have called big data one of the most important trends to come along in decades. Big Data Bootcamp, a short but intensive overview, explains what big data is and how you can use it in your company to become one of tomorrow?s market leaders. Along the way, it explains the very latest technologies, companies, and advancements. Big data holds the keys to delivering better customer service, offering more attractive products and marketing them better, and unlocking innovation. That's why, to remain competitive, every organization should become a big data company. It's also why every manager and technology professional should become knowledgeable about big data and how it is transforming not just their own industries but the global economy. And that knowledge is just what this book delivers. It explains components of big data like Hadoop and NoSQL databases; how big data is compiled, queried, and analyzed; how to create a big data application; and the business sectors ripe for big data-inspired products and services like retail, healthcare, finance, and education. Best of all, your guide is David Feinleib, renowned entrepreneur, venture capitalist, and author of Why Startups Fail. Feinleib?s Big Data Landscape, a market map featured and explained in the book, is an industry benchmark that has been viewed more than 150,000 times and is used as a reference by VMWare, Dell, Intel, the U.S. Government Accountability Office, and many other organizations. Feinleib also explains: Why every businessperson needs to understand the fundamentals of big data or get run over by those who do. How big data differs from traditional database management systems How to create and run a big data project The technical details powering the big data revolution Whether you?re a Fortune 500 executive or the proprietor of a restaurant or web design studio, Big Data Bootcamp will explain how you can take full advantage of new technologies to transform your company and your career.},
  added-at = {2016-08-06T20:06:44.000+0200},
  author = {Feinleib, David},
  biburl = {https://www.bibsonomy.org/bibtex/214a32019e7900362eb61866aa5f162b6/vngudivada},
  interhash = {f78053478f307195f601eae866d5e399},
  intrahash = {14a32019e7900362eb61866aa5f162b6},
  keywords = {BigData},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Big data bootcamp: what managers need to know to profit from the big data revolution},
  year = 2014
}

@book{long2014relevance,
  abstract = {In plain, uncomplicated language, and using detailed examples to explain the key concepts, models, and algorithms in vertical search ranking, Relevance Ranking for Vertical Search Engines teaches readers how to manipulate ranking algorithms to achieve better results in real-world applications. This reference book for professionals covers concepts and theories from the fundamental to the advanced, such as relevance, query intention, location-based relevance ranking, and cross-property ranking. It covers the most recent developments in vertical search ranking applications, such as freshness-based relevance theory for new search applications, location-based relevance theory for local search applications, and cross-property ranking theory for applications involving multiple verticals. Introduces ranking algorithms and teaches readers how to manipulate ranking algorithms for the best resultsCovers concepts and theories from the fundamental to the advancedDiscusses the state of the art: development of theories and practices in vertical search ranking applicationsIncludes detailed examples, case studies and real-world examples.},
  added-at = {2016-08-06T17:23:13.000+0200},
  author = {Long, Bo and Chang, Yi},
  biburl = {https://www.bibsonomy.org/bibtex/2e8d54ecd085dbe971301657d67a56e3d/vngudivada},
  interhash = {549b656279e46f89db47d78a94c15358},
  intrahash = {e8d54ecd085dbe971301657d67a56e3d},
  isbn = {9780124072022 012407202X 9781306415439 1306415438},
  keywords = {Book IR RekevanceRanking SearchEngine},
  refid = {870333355},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Relevance ranking for vertical search engines},
  url = {http://www.books24x7.com/marc.asp?bookid=62206},
  year = 2014
}

@book{zullighoven2005objectoriented,
  abstract = {Successful businesses and organizations are continually looking for ways to improve service and customer satisfaction in order to achieve long-term customer loyalty. In light of these goals, software developers must ask the question: how does customer orientation influence traditional approaches, methods, and principles of software development? In this book, a leading software architect and his team of software engineers describe how the idea of customer orientation in an organization leads to the creation of application-oriented software. This book describes what application-oriented software development is and how it can be conceptually and constructively designed with object-oriented techniques. It goes further to describe how to best fit together the many different methodologies and techniques that have been created for object-orientation (such as frameworks, platforms, components, UML, Unified Process, design patterns, and eXtreme Programming) to design and build software for real projects. This book brings together the best of research, development, and day-to-day project work to the task of building large software systems. *Written by and for developers of large, interactive, and long-lived software systems *Includes patterns of proven analysis, design, and documentation techniques *Shows how to develop an appropriate design approach and concrete software development techniques.},
  added-at = {2016-08-06T17:48:36.000+0200},
  author = {Züllighoven, Heinz and Beeger, Robert F. and Bäumer, Dirk},
  biburl = {https://www.bibsonomy.org/bibtex/2f723adae184653f4f841a1a73b200b47/vngudivada},
  interhash = {4621f6e614a2c0f6ae2ad7dcec96ac65},
  intrahash = {f723adae184653f4f841a1a73b200b47},
  isbn = {9781417549771 1417549777 9781592782307 1592782302 9780080491967 0080491960},
  keywords = {Book Handbook ObjectOrientedSoftware},
  refid = {56833643},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Object-oriented construction handbook: developing application-oriented software with the tools & materials approach},
  year = 2005
}

@book{segaran2009programming,
  abstract = {Provides information on using the programming techniques of the Semantic Web to create, enrich, and simplify Web applications.},
  added-at = {2016-08-06T20:00:03.000+0200},
  address = {Beijing; Sebastopol, CA},
  author = {Segaran, Toby and Evans, Colin and Taylor, Jamie},
  biburl = {https://www.bibsonomy.org/bibtex/20718b7a25b930200c53522b4437b58e0/vngudivada},
  description = {With this book, the promise of the Semantic Web -- in which machines can find, share, and combine data on the Web -- is not just a technical possibility, but a practical reality Programming the Semantic Web demonstrates several ways to implement semantic web applications, using current and emerging standards and technologies. You'll learn how to incorporate existing data sources into semantically aware applications and publish rich semantic data. Each chapter walks you through a single piece of semantic technology and explains how you can use it to solve real problems. Whether you're writing a simple mashup or maintaining a high-performance enterprise solution,Programming the Semantic Web provides a standard, flexible approach for integrating and future-proofing systems and data. This book will help you:Learn how the Semantic Web allows new and unexpected uses of data to emerge Understand how semantic technologies promote data portability with a simple, abstract model for knowledge representation Become familiar with semantic standards, such as the Resource Description Framework (RDF) and the Web Ontology Language (OWL) Make use of semantic programming techniques to both enrich and simplify current web applications},
  interhash = {551e2cf8ddb21fd537469615a3810114},
  intrahash = {0718b7a25b930200c53522b4437b58e0},
  isbn = {9780596153816 0596153813},
  keywords = {OWL RDF SemanticWeb},
  publisher = {O'Reilly},
  refid = {310395882},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Programming the Semantic Web},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=9780596153816},
  year = 2009
}

@book{degyurky2006cognitive,
  abstract = {This book has three major objectives: To propose an ontology for computer software; To provide a methodology for development of large software systems to cost and schedule that is based on the ontology; To offer an alternative vision regarding the development of truly autonomous systems.},
  added-at = {2016-08-06T17:51:56.000+0200},
  address = {Hoboken, NJ},
  author = {De Gyurky, Szabolcs Michael and Tarbell, Mark A. and Sons, John Wiley &},
  biburl = {https://www.bibsonomy.org/bibtex/2285b252319b2845c2f27982abb9121e3/vngudivada},
  description = {A groundbreaking, unifying theory of computer science for low-cost, high-quality softwareThe Cognitive Dynamics of Computer Science represents the culmination of more than thirty years of the author's hands-on experience in software development, which has resulted in a remarkable and sensible philosophy and practice of software development. It provides a groundbreaking ontology of computer science, while describing the processes, methodologies, and constructs needed to build high-quality, large-scale computer software systems on schedule and on budget.Based on his own experience in developing successful, low-cost software projects, the author makes a persuasive argument for developers to understand the philosophical underpinnings of software. He asserts that software in reality is an abstraction of the human thought system. The author draws from the seminal works of the great German philosophers--Kant, Hegel, and Schopenhauer--and recasts their theories of human mind and thought to create a unifying theory of computer science, cognitive dynamics, that opens the door to the next generation of computer science and forms the basic architecture for total autonomy.* Four detailed cases studies effectively demonstrate how philosophy and practice merge to meet the objective of high-quality, low-cost software.* The Autonomous Cognitive System chapter sets forth a model for a completely autonomous computer system, using the human thought system as the model for functional architecture and the human thought process as the model for the functional data process.* Although rooted in philosophy, this book is practical, addressing all the key areas that software professionals need to master in order to remain competitive and minimize costs, such as leadership, management, communication, and organization.This thought-provoking work will change the way students and professionals in computer science and software development conceptualize and perform their work. It provides them with both a philosophy and a set of practical tools to produce high-quality, low-cost software.},
  interhash = {cf449b8c37217aa7b5329d672a1e03db},
  intrahash = {285b252319b2845c2f27982abb9121e3},
  isbn = {0470036443 9780470036440 0470036435 9780470036433 1280551518 9781280551512},
  keywords = {CognitiveDynamics ComputerScience},
  publisher = {John Wiley \& Sons},
  refid = {85821053},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {The cognitive dynamics of computer science: cost-effective large scale software development},
  year = 2006
}

@book{dowek2009principles,
  abstract = {We have known about algorithms for millennia, but we have only been writing computer programs for a few decades. A big difference between the Euclidean or Eratosthenes age and ours is that since the middle of the twentieth century, we express the algorithms we conceive using formal languages: programming languages. Computer scientists are not the only ones who use formal languages. Optometrists, for example, prescribe eyeglasses using very technical expressions, such as OD: -1.25 (-0.50) 180 OS: -1.00 (-0.25) 180, in which the parentheses are essential. Many such formal languages have been created throughout history: musical notation, algebraic notation, etc. In particular, such languages have long been used to control machines, such as looms and cathedral chimes. However, until the appearance of programming languages, those languages were only of limited importance: they were restricted to specialized fields with only a few specialists and written texts of those languages remained relatively scarce. This situation has changed with the appearance of programming languages, which have a wider range of applications than the prescription of eye glasses or the control of a loom,are used by large communities, and have allowed the creation of programs of many hundreds of thousands of lines.},
  added-at = {2016-08-06T14:24:22.000+0200},
  address = {London},
  author = {Dowek, Gilles},
  biburl = {https://www.bibsonomy.org/bibtex/2c17cb87190b1ebb042f38d1825034fd3/vngudivada},
  interhash = {f03aab30621908028f42c648f6a4cc95},
  intrahash = {c17cb87190b1ebb042f38d1825034fd3},
  isbn = {9781848820326 1848820321 1848820313 9781848820319},
  keywords = {Book ProgrammingLanguage},
  publisher = {Springer},
  refid = {405546110},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Principles of programming languages},
  year = 2009
}

@book{celko2008celkos,
  abstract = {Perfectly intelligent programmers often struggle when forced to work with SQL. Why? Joe Celko believes the problem lies with their procedural programming mindset, which keeps them from taking full advantage of the power of declarative languages. The result is overly complex and inefficient code, not to mention lost productivity. This book will change the way you think about the problems you solve with SQL programs. Focusing on three key table-based techniques, Celko reveals their power through detailed examples and clear explanations. As you master these techniques, youll find you are able to conceptualize problems as rooted in sets and solvable through declarative programming. Before long, youll be coding more quickly, writing more efficient code, and applying the full power of SQL Filled with the insights of one of the worlds leading SQL authorities - noted for his knowledge and his ability to teach what he knows. Focuses on auxiliary tables (for computing functions and other values by joins), temporal tables (for temporal queries, historical data, and audit information), and virtual tables (for improved performance). Presents clear guidance for selecting and correctly applying the right table technique.},
  added-at = {2016-08-06T14:51:20.000+0200},
  address = {Boston, MA},
  author = {Celko, Joe},
  biburl = {https://www.bibsonomy.org/bibtex/28f2aec11351d01b85300c102f70ef574/vngudivada},
  interhash = {201e1bf4ae3995e7f6327d830b379a03},
  intrahash = {8f2aec11351d01b85300c102f70ef574},
  isbn = {9780123741370 0123741378 9780080557526 008055752X 128117212X 9781281172129},
  keywords = {SQL Sets},
  publisher = {Elsevier/ Morgan Kaufmann},
  refid = {228148104},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Joe Celko's thinking in sets: auxiliary, temporal, and virtual tables in SQL},
  year = 2008
}

@book{lee2011python,
  abstract = {Computer programming is a skill that can bring great enjoyment from the creativity involved in designing and implementing a solution to a problem. This classroom-tested and easy-to-follow textbook teaches the reader how to program using Python, an accessible language which can be learned incrementally. Through an extensive use of examples and practical exercises, students will learn to recognize and apply abstract patterns in programming, as well as how to inspect the state of a program using a debugger tool.Features:Contains numerous examples and solved practice exercises designed for an interactive classroom environment Highlights several patterns which commonly appear in programs, and presents exercises that reinforce recognition and application of these patterns Introduces the use of a debugger, and includes supporting material that reveals how programs work Presents the Tkinter framework for building graphical user interface applications and event-driven programs Provides helpful additional resources for instructors at the associated website: http://cs.luther.edu/leekent/CS1This hands-on textbook for active learning in the classroom will enable undergraduates in computer science to develop the necessary skills to begin developing their own programs. It employs Python as the introductory language due to the wealth of support available for programmers.Dr. Kent D. Lee is Associate Professor of Computer Science at the Department of Computer Science at Luther College, Decorah, Iowa, USA. He is the author of the successful Springer textbook, Programming Languages: An Active Learning Approach.},
  added-at = {2016-08-06T23:13:06.000+0200},
  address = {New York, NY},
  author = {Lee, Kent D.},
  biburl = {https://www.bibsonomy.org/bibtex/228b19540a20cdeaac3fbb91e905910f5/vngudivada},
  interhash = {4d38ff2992ead829a617a53896a273b9},
  intrahash = {28b19540a20cdeaac3fbb91e905910f5},
  keywords = {Book Python},
  publisher = {Springer},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Python programming fundamentals},
  year = 2011
}

@book{riolo2010genetic,
  abstract = {Genetic Programming Theory and Practice VII presents the results of the annual Genetic Programming Theory and Practice Workshop, contributed by the foremost international researchers and practitioners in the GP arena. Contributions examine the similarities and differences between theoretical and empirical results on real-world problems, and explore the synergy between theory and practice, producing a comprehensive view of the state of the art in GP application. Application areas include chemical process control, circuit design, financial data mining and bio-informatics, to name a few. About this book: Discusses the hurdles encountered when solving large-scale, cutting-edge applications, provides in-depth presentations of the latest and most significant applications of GP and the most recent theoretical results with direct applicability to state-of-the-art problems. Genetic Programming Theory and Practice VII is suitable for researchers, practitioners and students of Genetic Programming, including industry technical staffs, technical consultants and business entrepreneurs.},
  added-at = {2016-08-06T17:40:06.000+0200},
  address = {Boston, MA},
  author = {Riolo, Rick and Workshop on Genetic Programming, Theory and Practice and Workshop on Genetic Programming, Theory and Practice},
  biburl = {https://www.bibsonomy.org/bibtex/2f10e06aad31b714eb173a60704cfd26e/vngudivada},
  interhash = {06ac67e060d8c054115ce5665d846b26},
  intrahash = {f10e06aad31b714eb173a60704cfd26e},
  isbn = {9781441916259 1441916253},
  keywords = {GeneticAlgorithms GeneticProgramming},
  publisher = {Springer Science+Business Media, LLC},
  refid = {845693957},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Genetic Programming Theory and Practice VII},
  year = 2010
}

@book{blake2012semantic,
  abstract = {Over the last decade, a great amount of effort and resources have been invested in the development of Semantic Web Service (SWS) frameworks. Numerous description languages, frameworks, tools, and matchmaking and composition algorithms have been proposed. Nevertheless, when faced with a real-world problem, it is still very hard to decide which of these different approaches to use. In this book, the editors present an overall overview and comparison of the main current evaluation initiatives for SWS. The presentation is divided into four parts, each referring to one of the evaluation initiatives. Part I covers the long-established first two tracks of the Semantic Service Selection (S3) Contest {u2013} the OWL-S matchmaker evaluation and the SAWSDL matchmaker evaluation. Part II introduces the new S3 Jena Geography Dataset (JGD) cross evaluation contest. Part III presents the Semantic Web Service Challenge. Lastly, Part IV reports on the semantic aspects of the Web Service Challenge. The introduction to each part provides an overview of the evaluation initiative and overall results for its latest evaluation workshops. The following chapters in each part, written by the participants, detail their approaches, solutions and lessons learned. This book is aimed at two different types of readers. Researchers on SWS technology receive an overview of existing approaches in SWS with a particular focus on evaluation approaches; potential users of SWS technologies receive a comprehensive summary of the respective strengths and weaknesses of current systems and thus guidance on factors that play a role in evaluation.},
  added-at = {2016-08-06T18:00:38.000+0200},
  address = {Berlin; London},
  author = {Blake, M. Brian},
  biburl = {https://www.bibsonomy.org/bibtex/234c63c9885681775d77d1db99b9fdbee/vngudivada},
  interhash = {92b951461cdf20961502500bba87a0ce},
  intrahash = {34c63c9885681775d77d1db99b9fdbee},
  isbn = {9783642287350 3642287352},
  keywords = {Book SemanticWeb},
  publisher = {Springer},
  refid = {798092101},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Semantic web services: advancement through evaluation},
  year = 2012
}

@book{krishnan2013warehousing,
  abstract = {"In conclusion as you come to the end of this book, the concept of a Data Warehouse and its primary goal of serving the enterprise version of truth, and being the single platform for all the source of information will continue to remain intact and valid for many years to come. As we have discussed across many chapters and in many case studies, the limitations that existed with the infrastructures to create, manage and deploy Data Warehouses have been largely eliminated with the availability of Big Data technologies and infrastructure platforms, making the goal of the single version of truth a feasible reality. Integrating and extending Big Data into the Data Warehouse, and creating a larger decision support platform will benefit businesses for years to come. This book has touched upon governance and information lifecycle management aspects of Big Data in the larger program, however you can reuse all the current program management techniques that you follow for the Data Warehouse for this program and even implement agile approaches to integrating and managing data in the Data Warehouse. Technologies will continue to evolve in this spectrum and there will be more additions of solutions, which can be integrated if you follow the modular integration approaches to building and managing the Data Warehouse. The Appendix sections contain many more case studies and a special section on Healthcare Information Factory based on Big Data approaches. These are more guiding posts to help you align your thoughts and goals to building and integrating Big Data in your Data Warehouse"--},
  added-at = {2016-08-06T23:19:10.000+0200},
  author = {Krishnan, Krish},
  biburl = {https://www.bibsonomy.org/bibtex/21bf747520a89a208d8e20ce14f53218f/vngudivada},
  description = {Data Warehousing in the Age of the Big Data will help you and your organization make the most of unstructured data with your existing data warehouse. As Big Data continues to revolutionize how we use data, it doesn't have to create more confusion. Expert author Krish Krishnan helps you make sense of how Big Data fits into the world of data warehousing in clear and concise detail. The book is presented in three distinct parts. Part 1 discusses Big Data, its technologies and use cases from early adopters. Part 2 addresses data warehousing, its shortcomings, and new architecture options, workloads, and integration techniques for Big Data and the data warehouse. Part 3 deals with data governance, data visualization, information life-cycle management, data scientists, and implementing a Big Data-ready data warehouse. Extensive appendixes include case studies from vendor implementations and a special segment on how we can build a healthcare information factory. Ultimately, this book will help you navigate through the complex layers of Big Data and data warehousing while providing you information on how to effectively think about using all these technologies and the architectures to design the next-generation data warehouse.Learn how to leverage Big Data by effectively integrating it into your data warehouse. Includes real-world examples and use cases that clearly demonstrate Hadoop, NoSQL, HBASE, Hive, and other Big Data technologies Understand how to optimize and tune your current data warehouse infrastructure and integrate newer infrastructure matching data processing workloads and requirements},
  interhash = {2c5c4d461f148e77e27bdf20d36713a4},
  intrahash = {1bf747520a89a208d8e20ce14f53218f},
  keywords = {BigData Book DataWarehouse},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Data warehousing in the age of big data},
  year = 2013
}

@book{colomb2007ontology,
  abstract = {In order for information systems supporting two different organizations to interoperate, there must be an agreement as to what the words mean. There are many such agreements in place, supporting information systems interoperation in many different application areas. Most of these agreements have been created as part of diverse systems development processes, but since the advent of the Semantic Web in the late 1990s, they have been studied as a kind of software artifact in their own right, called an ontology, or description of a shared world. This book brings together developments from philosophy, artificial intelligence and information systems to formulate a collection of functional requirements for ontology development. Once the functional requirements are established, the book looks at several ontology representation languages: RDFS, OWL, Common Logic and Topic Maps, to show how these languages support the functional requirements, what deficiencies there are, and how the languages relate to each other. Besides a collection of running examples used throughout the book, the entire treatment is supported by an extended example of a hypothetical ontology for the Olympic Games presented first as a set of chapter-end exercises and then as a set of solutions which illustrate the various points made in the text in the context of a single coherent development.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences},
  added-at = {2016-08-06T19:00:28.000+0200},
  author = {Colomb, Robert M.},
  biburl = {https://www.bibsonomy.org/bibtex/20948435c341af8e12c3728c2e36e865c/vngudivada},
  interhash = {b31db8418c3b51f17b23d703e405118a},
  intrahash = {0948435c341af8e12c3728c2e36e865c},
  keywords = {Book Ontology SemanticWeb},
  series = {Frontiers in Artificial Intelligence and Applications},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Ontology and the Semantic Web},
  volume = 156,
  year = 2007
}

@book{prajapati2013analytics,
  abstract = {Big Data Analytics with R and Hadoop is a tutorial style book that focuses on all the powerful big data tasks that can be achieved by integrating R and Hadoop. This book is ideal for R developers who are looking for a way to perform big data analytics with Hadoop. This book is also aimed at those who know Hadoop and want to build some intelligent applications over Big data with R packages. It would be helpful if readers have basic knowledge of R.},
  added-at = {2016-08-06T23:43:25.000+0200},
  address = {Birmingham},
  author = {Prajapati, Vignesh},
  biburl = {https://www.bibsonomy.org/bibtex/2aa84983339f3f91fea80493ecd36dff5/vngudivada},
  description = {Set up an integrated infrastructure of R and Hadoop to turn your data analytics into Big Data analytics Overview Write Hadoop MapReduce within R Learn data analytics with R and the Hadoop platform Handle HDFS data within R Understand Hadoop streaming with R Encode and enrich datasets into R In Detail Big data analytics is the process of examining large amounts of data of a variety of types to uncover hidden patterns, unknown correlations, and other useful information. Such information can provide competitive advantages over rival organizations and result in business benefits, such as more effective marketing and increased revenue. New methods of working with big data, such as Hadoop and MapReduce, offer alternatives to traditional data warehousing. Big Data Analytics with R and Hadoop is focused on the techniques of integrating R and Hadoop by various tools such as RHIPE and RHadoop. A powerful data analytics engine can be built, which can process analytics algorithms over a large scale dataset in a scalable manner. This can be implemented through data analytics operations of R, MapReduce, and HDFS of Hadoop. You will start with the installation and configuration of R and Hadoop. Next, you will discover information on various practical data analytics examples with R and Hadoop. Finally, you will learn how to import/export from various data sources to R. Big Data Analytics with R and Hadoop will also give you an easy understanding of the R and Hadoop connectors RHIPE, RHadoop, and Hadoop streaming. What you will learn from this book Integrate R and Hadoop via RHIPE, RHadoop, and Hadoop streaming Develop and run a MapReduce application that runs with R and Hadoop Handle HDFS data from within R using RHIPE and RHadoop Run Hadoop streaming and MapReduce with R Import and export from various data sources to R Approach Big Data Analytics with R and Hadoop is a tutorial style book that focuses on all the powerful big data tasks that can be achieved by integrating R and Hadoop. Who this book is written for This book is ideal for R developers who are looking for a way to perform big data analytics with Hadoop. This book is also aimed at those who know Hadoop and want to build some intelligent applications over Big data with R packages. It would be helpful if readers have basic knowledge of R.},
  interhash = {368fba35e4075f41bc1acdcf943b2cf8},
  intrahash = {aa84983339f3f91fea80493ecd36dff5},
  keywords = {BigDataAnalytics Book Hadoop R},
  publisher = {Packt Publishing},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Big Data analytics with R and Hadoop},
  year = 2013
}

@book{gollapudi2013getting,
  abstract = {Standard tutorial-based approach.""Getting Started with Greenplum for Big Data"" Analytics is great for data scientists and data analysts with a basic knowledge of Data Warehousing and Business Intelligence platforms who are new to Big Data and who are looking to get a good grounding in how to use the Greenplum Platform. It's assumed that you will have some experience with database design and programming as well as be familiar with analytics tools like R and Weka.},
  added-at = {2016-08-07T00:08:25.000+0200},
  author = {Gollapudi, Sunila},
  biburl = {https://www.bibsonomy.org/bibtex/2196155e0e5f145b6c15f1d78605cd9bb/vngudivada},
  description = {
Organizations are leveraging the use of data and analytics to gain a competitive advantage over their opposition. Therefore, organizations are quickly becoming more and more data driven. With the advent of Big Data, existing Data Warehousing and Business Intelligence solutions are becoming obsolete, and a requisite for new agile platforms consisting of all the aspects of Big Data has become inevitable. From loading/integrating data to presenting analytical visualizations and reports, the new Big Data platforms like Greenplum do it all. It is now the mindset of the user that requires a tuning to put the solutions to work. "Getting Started with Greenplum for Big Data Analytics" is a practical, hands-on guide to learning and implementing Big Data Analytics using the Greenplum Integrated Analytics Platform. From processing structured and unstructured data to presenting the results/insights to key business stakeholders, this book explains it all. "Getting Started with Greenplum for Big Data Analytics" discusses the key characteristics of Big Data and its impact on current Data Warehousing platforms. It will take you through the standard Data Science project lifecycle and will lay down the key requirements for an integrated analytics platform. It then explores the various software and appliance components of Greenplum and discusses the relevance of each component at every level in the Data Science lifecycle. You will also learn Big Data architectural patterns and recap some key advanced analytics techniques in detail. The book will also take a look at programming with R and integration with Greenplum for implementing analytics. Additionally, you will explore MADlib and advanced SQL techniques in Greenplum for analytics. This book also elaborates on the physical architecture aspects of Greenplum with guidance on handling high-availability, back-up, and recovery. "Getting Started with Greenplum for Big Data" Analytics is great for data scientists and data analysts with a basic knowledge of Data Warehousing and Business Intelligence platforms who are new to Big Data and who are looking to get a good grounding in how to use the Greenplum Platform. It's assumed that you will have some experience with database design and programming as well as be familiar with analytics tools like R and Weka. You will learn: Load data from multiple data sources using the built-in ELT / ETL, Learn Parallel Processing / MPP / MapReduce techniques, Program with R and MADlib, Understand back-up and recovery implementation in Greenplum, Optimize data processing and querying using optimal distribution and partitioning strategies, Exchange data between the Greenplum Database and Hadoop, Handle high-availability requirements on Greenplum, Integrate ETL, reporting, and visualization tools.},
  interhash = {a1bae7326fd69e98f7e987b541bc35c9},
  intrahash = {196155e0e5f145b6c15f1d78605cd9bb},
  keywords = {BigDataAnalytics Book Greenplum},
  publisher = {Packt Publishing},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Getting Started with Greenplum for Big Data Analytics},
  year = 2013
}

@book{robichau2014healthcare,
  abstract = {This book outlines the new regulatory regime, and provides IT professionals with the processes and protocols, standards, and governance tools they need to maintain a secure and legal environment for data and records. Topics covered include : identity and access management systems; proper application design; physical and environmental safeguards; systemwide and client-based security configurations; safeguards for patient data; training and auditing procedures; governance and policy administration; the basics of HITECH, HIPAA, and other federal laws driving change in healthcare IT; how to identify the core issues of privacy and security in a healthcare environment; the steps required to develop and execute a security project plan; the technologies and tools available to aid in the process of deploying a secure EHR system; the core regulatory issues and practical matters of security in a healthcare environment; the frameworks and methodology that will assist in tackling the issues of privacy and security; and the technologies impacted by matters of privacy and security along with proposals for managing them effectively. This book is for healthcare professionals who work in information technology, information security, health information management, as well as in the more specialized areas of privacy, compliance, and informatics. The book is also useful for consultants working in the field of information security, EMR vendors, and technology professionals seeking to make an entry into the world of healthcare IT.},
  added-at = {2016-08-06T23:21:14.000+0200},
  author = {Robichau, Bernard Peter},
  biburl = {https://www.bibsonomy.org/bibtex/26600d3d3a1a76aa6a159c921d09e0bac/vngudivada},
  interhash = {aecf227c05cc2ee57fb6151fd741d673},
  intrahash = {6600d3d3a1a76aa6a159c921d09e0bac},
  keywords = {Book Healthcare Privacy Security},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Healthcare information privacy and security : regulatory compliance and data security in the age of electronic health records},
  year = 2014
}

@misc{schapire2016machine,
  added-at = {2016-08-06T18:56:54.000+0200},
  author = {Schapire, Rob},
  biburl = {https://www.bibsonomy.org/bibtex/2d37a6f8981ad3f7ebaeb77df2e58dcfb/vngudivada},
  interhash = {3b763aa2c27ba40dd3a1f82fb030b8e6},
  intrahash = {d37a6f8981ad3f7ebaeb77df2e58dcfb},
  keywords = {Classification ML Slides},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Machine Learning Algorithms for Classification},
  year = 2016
}

@book{ghavami2016governance,
  abstract = {Data is the new Gold and Analytics is the machinery to mine, mold and mint it. Data analytics has become core to business and decision making. The rapid increase in data volume, velocity and variety, known as big data, offers both opportunities and challenges. While open source solutions to store big data, like Hadoop and NoSQL offer platforms for exploring value and insight from big data, they were not originally developed with data security and governance in mind. Organizations that are launching big data initiatives face significant challenges for managing this data effectively. In this book, the author has collected best practices from the world’s leading organizations who have successfully implemented big data platforms. He offers the latest techniques and methods for managing big data effectively. The book offers numerous policies, strategies and recipes for managing big data. It addresses many issues that are prevalent with data security, privacy, controls and life cycle management offering modern principles and open source architectures for successful governance of big data. Topics that cover the entire data management life cycle, data quality, data stewardship, regulatory considerations, data council, architectural and operational models are presented for successful management of big data. The book is a must-read for data scientists, data engineers and information technology leaders who are implementing big data platforms in their organizations.},
  added-at = {2016-08-06T20:03:59.000+0200},
  address = {Washington},
  author = {Ghavami, Peter K.},
  biburl = {https://www.bibsonomy.org/bibtex/20b9b77fb1dac3fae2c44f9641be3001b/vngudivada},
  interhash = {490c34e2985dabc1536b21d2518f2192},
  intrahash = {0b9b77fb1dac3fae2c44f9641be3001b},
  keywords = {BigDataAnalytics BigDataGovernance Book},
  publisher = {Peter K. Ghavami},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Big Data Governance: Modern Data Management Principles for Hadoop, NoSQL & Big Data Analytics},
  year = 2016
}

@book{konheim2009hashing,
  abstract = {Written by one of the developers of the technology, Hashing is both a historical document on the development of hashing and an analysis of the applications of hashing in a society increasingly concerned with security. The material in this book is based on courses taught by the author, and key points are reinforced in sample problems and an accompanying instructor s manual. Graduate students and researchers in mathematics, cryptography, and security will benefit from this overview of hashing and the complicated mathematics that it requires.},
  added-at = {2016-08-06T14:49:09.000+0200},
  address = {Oxford},
  author = {Konheim, Alan G.},
  biburl = {https://www.bibsonomy.org/bibtex/2730de9038e7933b23d0d2dfb66ea96d3/vngudivada},
  interhash = {477440e1bfff39b6d4b4b48d9265e0dc},
  intrahash = {730de9038e7933b23d0d2dfb66ea96d3},
  isbn = {9780470344736 0470344733},
  keywords = {Book Hashing},
  publisher = {Wiley-Blackwell},
  refid = {267185960},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Hashing in Computer Science: Fifty Years of Slicing and Dicing},
  year = 2009
}

@book{hofstedt2011multiparadigm,
  abstract = {Programming languages are often classified according to their paradigms, e.g. imperative, functional, logic, constraint-based, object-oriented, or aspect-oriented. A paradigm characterizes the style, concepts, and methods of the language for describing situations and processes and for solving problems, and each paradigm serves best for programming in particular application areas. Real-world problems, however, are often best implemented by a combination of concepts from different paradigms, because they comprise aspects from several realms, and this combination is more comfortably realized using multiparadigm programming languages. This book deals with the theory and practice of multiparadigm constraint programming languages. The author first elaborates on programming paradigms and languages, constraints, and the merging of programming concepts which yields multiparadigm (constraint) programming languages. In the second part the author inspects two concrete approaches on multiparadigm constraint programming {u2013} the concurrent constraint functional language CCFL, which combines the functional and the constraint-based paradigms and allows the description of concurrent processes; and a general framework for multiparadigm constraint programming and its implementation, Meta-S. The book is appropriate for researchers and graduate students in the areas of programming and artificial intelligence. ¡},
  added-at = {2016-08-06T14:17:22.000+0200},
  address = {New York, NY},
  author = {Hofstedt, Petra},
  biburl = {https://www.bibsonomy.org/bibtex/24a2a45e324bb2ceff2ebc7fe4c1773b5/vngudivada},
  interhash = {86bbf68bc6187c79dcb480717d64b3f2},
  intrahash = {4a2a45e324bb2ceff2ebc7fe4c1773b5},
  isbn = {9783642173301 3642173306},
  keywords = {Book ConstraintProgramming ProgrammingLanguage},
  publisher = {Springer},
  refid = {745003895},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Multiparadigm constraint programming languages},
  year = 2011
}

@book{magnani2013modelbased,
  abstract = {This book contains contributions presented during the international conference on Model-Based Reasoning (MBR'012), held on June 21-23 in Sestri Levante, Italy. Interdisciplinary researchers discuss in this volume how scientific cognition and other kinds of cognition make use of models, abduction, and explanatory reasoning in order to produce important or creative changes in theories and concepts. Some of the contributions analyzed the problem of model-based reasoning in technology and stressed the issues of scientific and technological innovation. The book is divided in three main parts: models, mental models, representations; abduction, problem solving and practical reasoning; historical, epistemological and technological issues.},
  added-at = {2016-08-06T17:58:09.000+0200},
  author = {Magnani, Lorenzo},
  biburl = {https://www.bibsonomy.org/bibtex/2c4a37badc76480c07e2d6dbe3641e230/vngudivada},
  interhash = {7785b9380f498c944819e39d029220ac},
  intrahash = {c4a37badc76480c07e2d6dbe3641e230},
  isbn = {9783642374289 364237428X 3642374271 9783642374272},
  keywords = {Book CognitiveScience Reasoning},
  refid = {857906187},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Model-based reasoning in science and technology : theoretical and cognitive issues},
  url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=637771},
  year = 2013
}

@book{celko2013celkos,
  abstract = {Joe Celko's Complete Guide to NoSQL provides a complete overview of non-relational technologies so that you can become more nimble to meet the needs of your organization. As data continues to explode and grow more complex, SQL is becoming less useful for querying data and extracting meaning. In this new world of bigger and faster data, you will need to leverage non-relational technologies to get the most out of the information you have. Learn where, when, and why the benefits of NoSQL outweigh those of SQL with Joe Celko's Complete Guide to NoSQL. This book covers three areas that make today's new data different from the data of the past: velocity, volume and variety. When information is changing faster than you can collect and query it, it simply cannot be treated the same as static data. Celko will help you understand velocity, to equip you with the tools to drink from a fire hose. Old storage and access models do not work for big data. Celko will help you understand volume, as well as different ways to store and access data such as petabytes and exabytes. Not all data can fit into a relational model, including genetic data, semantic data, and data generated by social networks. Celko will help you understand variety, as well as the alternative storage, query, and management frameworks needed by certain kinds of data. Gain a complete understanding of the situations in which SQL has more drawbacks than benefits so that you can better determine when to utilize NoSQL technologies for maximum benefit Recognize the pros and cons of columnar, streaming, and graph databases.},
  added-at = {2016-08-06T14:53:11.000+0200},
  address = {Waltham, MA},
  author = {Celko, Joe},
  biburl = {https://www.bibsonomy.org/bibtex/2440b909fa3d4e88ae7df187ad4ddef6c/vngudivada},
  interhash = {01d817bff44adcee09d4078269d7f870},
  intrahash = {440b909fa3d4e88ae7df187ad4ddef6c},
  isbn = {9780124072206 0124072208 1299981232 9781299981232},
  keywords = {Book NoSQL},
  publisher = {Morgan Kaufmann},
  refid = {860923737},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Joe Celko's complete guide to NoSQL},
  year = 2013
}

@book{tupper2011architecture,
  abstract = {Data is an expensive and expansive asset. Information capture has forced storage capacity from megabytes to terabytes, exabytes and, pretty soon, zetabytes of data. So the need for accessible storage space for this data is great. To make this huge amount of data usable and relevant, it needs to be organized effectively. Database Base Management Systems, such as Oracle, IBM's DB2, and Microsoft SqlServer are used often, but these are being enhanced continuously and auxiliary tools are being developed every week; there needs to be a fundamental starting point for it all. That stating point is Data Architecture, the blueprint for organizing and structuring of information for services, service providers, and the consumers of that data. Data Architecture: From Zen to Reality explains the principles underlying data architecture, how data evolves with organizations, and the challenges organizations face in structuring and managing their data. It also discusses proven methods and technologies to solve the complex issues dealing with data. The book uses a holistic approach to the field of data architecture by covering the various applied areas of data, including data modelling and data model management, data quality, data governance, enterprise information management, database design, data warehousing, and warehouse design. This book is a core resource for anyone emplacing, customizing or aligning data management systems, taking the Zen-like idea of data architecture to an attainable reality. Presents fundamental concepts of enterprise architecture with definitions and real-world applications and scenarios Teaches data managers and planners about the challenges of building a data architecture roadmap, structuring the right team, and building a long term set of solutions Includes the detail needed to illustrate how the fundamental principles are used in current business practice.},
  added-at = {2016-08-06T17:41:54.000+0200},
  address = {Boston, MA},
  author = {Tupper, Charles D.},
  biburl = {https://www.bibsonomy.org/bibtex/26e4061547dca98b43da0bdf29d536d77/vngudivada},
  interhash = {782551f37198d1e8a4bdc3437edcfc18},
  intrahash = {6e4061547dca98b43da0bdf29d536d77},
  isbn = {9780123851260 0123851262 9780123851277 0123851270},
  keywords = {Book DataArchitecture},
  publisher = {Morgan Kaufmann},
  refid = {713324747},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Data architecture: from zen to reality},
  year = 2011
}

@book{peltier2014information,
  abstract = {"Effective security rules and procedures do not exist for their own sake-they are put in place to protect critical assets, thereby supporting overall business objectives. Recognizing security as a business enabler is the first step in building a successful program. Information Security Fundamentals allows future security professionals to gain a solid understanding of the foundations of the field and the entire range of issues that practitioners must address. This book enables students to understand the key elements that comprise a successful information security program and eventually apply these concepts to their own efforts. The book examines the elements of computer security, employee roles and responsibilities, and common threats. It examines the need for management controls, policies and procedures, and risk analysis, and also presents a comprehensive list of tasks and objectives that make up a typical information protection program. The volume discusses organizationwide policies and their documentation, and legal and business requirements. It explains policy format, focusing on global, topic-specific, and application-specific policies. Following a review of asset classification, the book explores access control, the components of physical security, and the foundations and processes of risk analysis and risk management. Information Security Fundamentals concludes by describing business continuity planning, including preventive controls, recovery strategies, and ways to conduct a business impact analysis"--},
  added-at = {2016-08-07T00:11:47.000+0200},
  author = {Peltier, Thomas R.},
  biburl = {https://www.bibsonomy.org/bibtex/262cc23f338c0f1922e3667dab017c5f2/vngudivada},
  description = {
Developing an information security program that adheres to the principle of security as a business enabler must be the first step in an enterprises effort to build an effective security program. Following in the footsteps of its bestselling predecessor, Information Security Fundamentals, Second Edition provides information security professionals with a clear understanding of the fundamentals of security required to address the range of issues they will experience in the field.The book examines the elements of computer security, employee roles and responsibilities, and common threats. It discusses the legal requirements that impact security policies, including Sarbanes-Oxley, HIPAA, and the Gramm-Leach-Bliley Act. Detailing physical security requirements and controls, this updated edition offers a sample physical security policy and includes a complete list of tasks and objectives that make up an effective information protection program. Includes ten new chapters Broadens its coverage of regulations to include FISMA, PCI compliance, and foreign requirements Expands its coverage of compliance and governance issues Adds discussions of ISO 27001, ITIL, COSO, COBIT, and other frameworks Presents new information on mobile security issues Reorganizes the contents around ISO 27002 The book discusses organization-wide policies, their documentation, and legal and business requirements. It explains policy format with a focus on global, topic-specific, and application-specific policies. Following a review of asset classification, it explores access control, the components of physical security, and the foundations and processes of risk analysis and risk management.The text concludes by describing business continuity planning, preventive controls, recovery strategies, and how to conduct a business impact analysis. Each chapter in the book has been written by a different expert to ensure you gain the comprehensive understanding of what it takes to develop an effective information security program.},
  interhash = {60e4939a522e79a07604a7b61e8d7b91},
  intrahash = {62cc23f338c0f1922e3667dab017c5f2},
  keywords = {Book InformationSecurity},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Information security fundamentals},
  year = 2014
}

@book{schmarzo2013understanding,
  abstract = {Leverage big data to add value to your business Social media analytics, web-tracking, and other technologies help companies acquire and handle massive amounts of data to better understand their customers, products, competition, and markets. Armed with the insights from big data, companies can improve customer experience and products, add value, and increase return on investment. The tricky part for busy IT professionals and executives is how to get this done, and that's where this practical book comes in. Big Data: Understanding How Data Powers Big Business is a complete.},
  added-at = {2016-08-06T20:09:10.000+0200},
  address = {Hoboken, NJ},
  author = {Schmarzo, Bill},
  biburl = {https://www.bibsonomy.org/bibtex/23ea05924a3cac8d53990361c82151347/vngudivada},
  interhash = {e1bbb1634efe2982a702773898acfd6e},
  intrahash = {3ea05924a3cac8d53990361c82151347},
  keywords = {BigData Book},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Big Data Understanding How Data Powers Big Business},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1411623},
  year = 2013
}

@book{kay2012managing,
  abstract = {Annotation Offering key insights into managing people working on scientific projects and high-tech enterprises, this practical volume addresses issues unique to these pressurized environments and will be required reading for supervisors and policy makers alike.},
  added-at = {2016-08-06T17:56:42.000+0200},
  author = {Kay, Ronald},
  biburl = {https://www.bibsonomy.org/bibtex/2db7516e1fdb9f1d91c7423e42d167732/vngudivada},
  description = {Addressing the issues unique to managers of creative technical staff, this guide reflects not only Ronald Kays long experience observing and teaching successful management techniques, but also treats the expanding challenges due to increasingly globally-based projects and staff. As before, Kays guide helps readers to prepare themselves, graduate students and others to understand and improve their managerial skills and covers such practical, yet sometimes overlooked, steps such as: individual and team behavior of creative technical staff; managing their own and others RD projects; hiring, evaluating and compensating technical staff; RD proposals and administrative functions; and presentations, meetings and organizational culture. New to this edition are a chapter on the global impact of high-tech enterprises and sections on the roles of foundations and government funding and task-force participation. Also tackled are the basics of starting, financing and staffing venture-capital-funded enterprises. Whats more, this book also serves to increase the awareness and knowledge base of anyone who needs to meet the challenge of managing people with the creative energies that drive technologically-based economic growth.},
  interhash = {b7a2e7ffcb3db5d1aa82fab24365a9e0},
  intrahash = {db7516e1fdb9f1d91c7423e42d167732},
  isbn = {9783642246357 3642246354},
  keywords = {Book Creativity},
  refid = {794925355},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Managing creativity in science and hi-tech},
  year = 2012
}

@book{wood2014linked,
  abstract = {Linked Data is a standards-driven model for representing structured data on the Web that gives developers, publishers, and information architects a consistent, predictable way to publish, merge and consume data. It's been adopted by many well-known institutions, including Google, Facebook, IBM, Oracle, and government agencies, as well projects such as Drupal and WordPress. Linked Data presents the Linked Data model in plain, jargon-free language and offers practical techniques using everyday tools like JavaScript and Python. Step-by-step, it works through examples of increasing complexity while explaining foundational concepts such as HTTP URIs, the Resource Description Framework (RDF), and the SPARQL query language. Readers will learn to use various Linked Data document formats to create powerful Web applications and mashups, and to effectively use emerging Web standards to access, find and query structured data on the Web.},
  added-at = {2016-08-06T23:56:32.000+0200},
  author = {Wood, David and Zaidman, Marsha and Ruth, Luke and Hausenblas, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2c023830dab131a0e4b6777a65f17d53c/vngudivada},
  description = {Summary Linked Data presents the Linked Data model in plain, jargon-free language to Web developers. Avoiding the overly academic terminology of the Semantic Web, this new book presents practical techniques, using everyday tools like JavaScript and Python. About this Book The current Web is mostly a collection of linked documents useful for human consumption. The evolving Web includes data collections that may be identified and linked so that they can be consumed by automated processes. The W3C approach to this is Linked Data and it is already used by Google, Facebook, IBM, Oracle, and government agencies worldwide. Linked Data presents practical techniques for using Linked Data on the Web via familiar tools like JavaScript and Python. You'll work step-by-step through examples of increasing complexity as you explore foundational concepts such as HTTP URIs, the Resource Description Framework (RDF), and the SPARQL query language. Then you'll use various Linked Data document formats to create powerful Web applications and mashups. Written to be immediately useful to Web developers, this book requires no previous exposure to Linked Data or Semantic Web technologies.},
  interhash = {48b95e6cb80020c1d454c0f45daeaede},
  intrahash = {c023830dab131a0e4b6777a65f17d53c},
  keywords = {Book LinkedData},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Linked data: structured data on the Web},
  year = 2014
}

@book{ingersoll2013taming,
  abstract = {Taming Text, winner of the 2013 Jolt Awards for Productivity, is a hands-on, example-driven guide to working with unstructured text in the context of real-world applications. This book explores how to automatically organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization. The book guides you through examples illustrating each of these topics, as well as the foundations upon which they are built.

  There is so much text in our lives, we are practically drowning in it. Fortunately, there are innovative tools and techniques for managing unstructured information that can throw the smart developer a much-needed lifeline. You will find them in this book.

  Taming Text is a practical, example-driven guide to working with text in real applications. This book introduces you to useful techniques like full-text search, proper name recognition, clustering, tagging, information extraction, and summarization. You will explore real use cases as you systematically absorb the foundations upon which they are built. Written in a clear and concise style, this book avoids jargon, explaining the subject in terms you can understand without a background in statistics or natural language processing. Examples are in Java, but the concepts can be applied in any language.

  Written for Java developers, the book requires no prior knowledge of GWT.

  Purchase of the print book comes with an offer of a free PDF, ePub, and Kindle eBook from Manning. Also available is all code from the book.

  Winner of 2013 Jolt Awards: The Best Books—one of five notable books every serious programmer should read.

  What is Inside

  When to use text-taming techniques

  Important open-source libraries like Solr and Mahout

  How to build text-processing applications

  Grant Ingersoll is an engineer, speaker, and trainer, a Lucene committer, and a co-founder of the Mahout machine-learning project. Thomas Morton is the primary developer of OpenNLP and Maximum Entropy. Drew Farris is a technology consultant, software developer, and contributor to Mahout, Lucene, and Solr.

  Table of Contents

  Getting started taming text

  Foundations of taming text

  Searching

  Fuzzy string matching

  Identifying people, places, and things

  Clustering text

  Classification, categorization, and tagging

  Building an example question answering system

  Untamed text: exploring the next frontier},
  added-at = {2016-08-06T20:16:19.000+0200},
  address = {Shelter Island, New York},
  author = {Ingersoll, Grant S. and Morton, Thomas S. and Farris, Andrew L.},
  biburl = {https://www.bibsonomy.org/bibtex/2bd3bcdc81eb6fb4c88b5199b264cd54b/vngudivada},
  interhash = {ffc7863b29466401e3c790c8c2d99599},
  intrahash = {bd3bcdc81eb6fb4c88b5199b264cd54b},
  isbn = {978-1933988382},
  keywords = {Book IR PersonalizedLearning TextMining TextProcessing},
  publisher = {Manning},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Taming text: how to find, organise, and manipulate it},
  year = 2013
}

@book{balasubramanian2014conformal,
  abstract = {"Traditional, low-dimensional, small scale data have been successfully dealt with using conventional software engineering and classical statistical methods, such as discriminant analysis, neural networks, genetic algorithms and others. But the change of scale in data collection and the dimensionality of modern data sets has profound implications on the type of analysis that can be done. Recently several kernel-based machine learning algorithms have been developed for dealing with high-dimensional problems, where a large number of features could cause a combinatorial explosion. These methods are quickly gaining popularity, and it is widely believed that they will help to meet the challenge of analysing very large data sets. Learning machines often perform well in a wide range of applications and have nice theoretical properties without requiring any parametric statistical assumption about the source of data (unlike traditional statistical techniques). However, a typical drawback of many machine learning algorithms is that they usually do not provide any useful measure of confidence in the predicted labels of new, unclassifed examples. Confidence estimation is a well-studied area of both parametric and non-parametric statistics; however, usually only low-dimensional problems are considered"--},
  added-at = {2016-08-06T17:34:22.000+0200},
  address = {Boston, MA},
  author = {Balasubramanian, Vineeth and Ho, Shen-Shyang and Vovk, Vladimir},
  biburl = {https://www.bibsonomy.org/bibtex/262d6ecaafa3215622739122fe83e07e1/vngudivada},
  description = {The conformal predictions framework is a recent development in machine learning that can associate a reliable measure of confidence with a prediction in any real-world pattern recognition application, including risk-sensitive applications such as medical diagnosis, face recognition, and financial risk prediction. Conformal Predictions for Reliable Machine Learning: Theory, Adaptations and Applications captures the basic theory of the framework, demonstrates how to apply it to real-world problems, and presents several adaptations, including active learning, change detection, and anomaly detection. As practitioners and researchers around the world apply and adapt the framework, this edited volume brings together these bodies of work, providing a springboard for further research as well as a handbook for application in real-world problems.Understand the theoretical foundations of this important framework that can provide a reliable measure of confidence with predictions in machine learningBe able to apply this framework to real-world problems in different machine learning settings, including classification, regression, and clusteringLearn effective ways of adapting the framework to newer problem settings, such as active learning, model selection, or change detection},
  interhash = {019b4215a6b34b58e88a83ab1441ee90},
  intrahash = {62d6ecaafa3215622739122fe83e07e1},
  isbn = {9780124017153 0124017150 1306697484 9781306697484},
  keywords = {Book ConformalPrediction ML},
  publisher = {Morgan Kaufmann},
  refid = {878922864},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Conformal prediction for reliable machine learning theory, adaptations, and applications},
  year = 2014
}

@book{pourabbas2014geographical,
  abstract = {"Web services, cloud computing, location based services, NoSQLdatabases, and Semantic Web offer new ways of accessing, analyzing, and elaborating geo-spatial information in both real-world and virtual spaces. This book explores the how-to of the most promising recurrent technologies and trends in GIS, such as Semantic GIS, Web GIS, Mobile GIS, NoSQL Geographic Databases, Cloud GIS, Spatial Data Warehousing-OLAP, and Open GIS. The text discusses and emphasizes the methodological aspects of such technologies and their applications in GIS"--},
  added-at = {2016-08-07T00:05:25.000+0200},
  author = {Pourabbas, Elaheh},
  biburl = {https://www.bibsonomy.org/bibtex/2e8bc9ab252cab92711d585f6d9b16c7f/vngudivada},
  interhash = {67beccc3a1db67329521a4d86495f319},
  intrahash = {e8bc9ab252cab92711d585f6d9b16c7f},
  keywords = {Book GIS},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Geographical information systems : trends and technologies},
  year = 2014
}

@book{harth2014linked,
  abstract = {"With the growing popularity of the Semantic Web, more and more semantic data and data sources become available and accessible for everyone. By establishing semantic links between the data, answers to (complex) queries can be evaluated based on the data on multiple providers instead of just one. This book motivates, introduces, and details techniques for processing heterogeneous structured data on the Web by providing a comprehensive overview for database researchers and practitioners about this new publishing paradigm on the web, and show how the abundance of data published as Linked Data can serve as a fertile ground for database research and experimentation"--},
  added-at = {2016-08-06T23:54:53.000+0200},
  author = {Harth, Andreas and Hose, Katja and Schenkel, Ralf},
  biburl = {https://www.bibsonomy.org/bibtex/2fbf70cb889468a9612cf4cb7afd7cd9d/vngudivada},
  description = {Linked Data Management presents techniques for querying and managing Linked Data that is available on todays Web. The book shows how the abundance of Linked Data can serve as fertile ground for research and commercial applications. The text focuses on aspects of managing large-scale collections of Linked Data. It offers a detailed introduction to Linked Data and related standards, including the main principles distinguishing Linked Data from standard database technology. Chapters also describe how to generate links between datasets and explain the overall architecture of data integration systems based on Linked Data. A large part of the text is devoted to query processing in different setups. After presenting methods to publish relational data as Linked Data and efficient centralized processing, the book explores lookup-based, distributed, and parallel solutions. It then addresses advanced topics, such as reasoning, and discusses work related to read-write Linked Data for system interoperation. Despite the publication of many papers since Tim Berners-Lee developed the Linked Data principles in 2006, the field lacks a comprehensive, unified overview of the state of the art. Suitable for both researchers and practitioners, this book provides a thorough, consolidated account of the new data publishing and data integration paradigm. While the book covers query processing extensively, the Linked Data abstraction furnishes more than a mechanism for collecting, integrating, and querying data from the open Web - the Linked Data technology stack also allows for controlled, sophisticated applications deployed in an enterprise environment.},
  interhash = {3300232020e359c4c7b58c1a28dad3dc},
  intrahash = {fbf70cb889468a9612cf4cb7afd7cd9d},
  keywords = {Book LinkedData},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Linked data management},
  year = 2014
}

@book{wachsmuth2015analysis,
  abstract = {This monograph proposes a comprehensive and fully automatic approach to designing text analysis pipelines for arbitrary information needs that are optimal in terms of run-time efficiency and that robustly mine relevant information from text of any kind. Based on state-of-the-art techniques from machine learning and other areas of artificial intelligence, novel pipeline construction and execution algorithms are developed and implemented in prototypical software. Formal analyses of the algorithms and extensive empirical experiments underline that the proposed approach represents an essential step towards the ad-hoc use of text mining in web search and big data analytics. Both web search and big data analytics aim to fulfill peoplesll needs for information in an adhoc manner. The information sought for is often hidden in large amounts of natural language text. Instead of simply returning links to potentially relevant texts, leading search and analytics engines have started to directly mine relevant information from the texts. To this end, they execute text analysis pipelines that may consist of several complex information-extraction and text-classification stages. Due to practical requirements of efficiency and robustness, however, the use of text mining has so far been limited to anticipated information needs that can be fulfilled with rather simple, manually constructed pipelines.},
  added-at = {2016-08-06T20:13:55.000+0200},
  author = {Wachsmuth, Henning},
  biburl = {https://www.bibsonomy.org/bibtex/2f906d27ba841bd2478f1c6ab0a3c299b/vngudivada},
  interhash = {da41be06bf882e497735432b29b8cd71},
  intrahash = {f906d27ba841bd2478f1c6ab0a3c299b},
  keywords = {Book DataMining TextAnalytics TextMining},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Text analysis pipelines: towards ad-hoc large scale text mining},
  year = 2015
}

@book{macfarland2014introduction,
  abstract = {Through real-world datasets, this book shows the reader how to work with material in biostatistics using the open source software R. These include tools that are critical to dealing with missing data, which is a pressing scientific issue for those engaged in biostatistics. Readers will be equipped to run analyses and make graphical presentations based on the sample dataset and their own data. The hands-on approach will benefit students and ensure the accessibility of this book for readers with a basic understanding of R. Topics include: an introduction to Biostatistics and R, data exploration, descriptive statistics and measures of central tendency, t-Test for independent samples, t-Test for matched pairs, ANOVA, correlation and linear regression, and advice for future work.},
  added-at = {2016-08-06T23:38:04.000+0200},
  author = {MacFarland, Thomas W.},
  biburl = {https://www.bibsonomy.org/bibtex/2224ac0b738120e3096e80fda1c04d9d1/vngudivada},
  interhash = {6deda8d384f78a48023da65230713759},
  intrahash = {224ac0b738120e3096e80fda1c04d9d1},
  keywords = {Biostatistics Book R},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Introduction to data analysis and graphical presentation in biostatistics with R : statistics in the large},
  year = 2014
}

@book{celko2011celkos,
  abstract = {SQL for Smarties was hailed as the first book devoted explicitly to the advanced techniques needed to transform an experienced SQL programmer into an expert. Now, 15 years later and in its fourth edition, this classic reference still reigns supreme as the only book written by a SQL master that teaches programmers and practitioners to become SQL masters themselves! These are not just tips and techniques; also offered are the best solutions to old and new challenges. Joe Celko conveys the way you need to think in order to get the most out of SQL programming efforts for both correctness and performance. New to the fourth edition, Joe features new examples to reflect the ANSI/ISO Standards so anyone can use it. He also updates data element names to meet new ISO-11179 rules and he expands coverage of SSD, parallel processors and how new hardware will change how SQL works, all with the same experience-based teaching style that made the previous editions the classics they are today. KEY FEATURES Expert advice from a noted SQL authority and award-winning columnist who has given ten years service to the ANSI SQL standards committee Teaches scores of advanced techniques that can be used with any product, in any SQL environment, whether it is an SQL 92 or SQL 2008 environment Offers tips for working around deficiencies and gives insight into real-world challenges.},
  added-at = {2016-08-06T14:54:44.000+0200},
  address = {Burlington, MA},
  author = {Celko, Joe},
  biburl = {https://www.bibsonomy.org/bibtex/29a73210a875c63923fce886939456ec0/vngudivada},
  edition = {Fourth},
  interhash = {9bf9db26895bf0d35cbd1ca85904384d},
  intrahash = {9a73210a875c63923fce886939456ec0},
  isbn = {9780123820228 0123820227 9780123820235 0123820235},
  keywords = {Book SQL},
  publisher = {Elsevier},
  refid = {677829483},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Joe Celko's SQL for smarties: advanced sql programming},
  year = 2011
}

@book{wilson1999encyclopedia,
  abstract = {Since the 1970s the cognitive sciences have offered multidisciplinary ways of understanding the mind and cognition. The MIT Encyclopedia of the Cognitive Sciences (MITECS) is a landmark, comprehensive reference work that represents the methodological and theoretical diversity of this changing field.At the core of the encyclopedia are 471 concise entries, from Acquisition and Adaptationism to Wundt and X-bar Theory. Each article, written by a leading researcher in the field, provides an accessible introduction to an important concept in the cognitive sciences, as well as references or further readings. Six extended essays, which collectively serve as a roadmap to the articles, provide overviews of each of six major areas of cognitive science: Philosophy; Psychology; Neurosciences; Computational Intelligence; Linguistics and Language; and Culture, Cognition, and Evolution. For both students and researchers, MITECS will be an indispensable guide to the current state of the cognitive sciences.},
  added-at = {2016-08-06T22:38:24.000+0200},
  address = {Cambridge, Mass.},
  author = {Wilson, Robert A. and Keil, Frank C.},
  biburl = {https://www.bibsonomy.org/bibtex/2dcce907456805f43d3ea8733f91a65cc/vngudivada},
  interhash = {b05769980eaa02bfad67b7fc8d4a3bbe},
  intrahash = {dcce907456805f43d3ea8733f91a65cc},
  keywords = {CognitiveScience Encyclopedia},
  publisher = {MIT Press},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {The MIT encyclopedia of the cognitive sciences},
  year = 1999
}

@book{croft2010search,
  abstract = {"Search Engines: Information Retrieval in Practice introduces the key issues in information retrieval (IR) and shows how they affect the design and implementation of search engines, with mathematical models reinforcing important concepts. This book is ideal for an introductory course on IR at either the undergraduate or master's level or for professionals seeking an authoritative introduction. An extensive set of resources is available to instructors."--Jacket.},
  added-at = {2016-08-06T17:28:41.000+0200},
  address = {Boston, Massachusetts},
  author = {Croft, W. Bruce and Metzler, Donald and Strohman, Trevor},
  biburl = {https://www.bibsonomy.org/bibtex/26dbe8ff9de4f8b16c442247baf8abe73/vngudivada},
  interhash = {bc2cb2c872ddae363967b53064670cd8},
  intrahash = {6dbe8ff9de4f8b16c442247baf8abe73},
  isbn = {9780136072249},
  keywords = {Book IR SearchEngine},
  publisher = {Addison-Wesley},
  refid = {268788295},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Search engines: information retrieval in practice},
  year = 2010
}

@book{pries2015analytics,
  abstract = {Covering prominent software packages, including Hadoop, Oracle Endeca, and SAP HANA, this book demonstrates the utility and promise of these applications. It also demonstrates the need to understand data quality and the ability of statistics to mislead when due rigor is not applied. As the authors are both ASQ-certified Six Sigma Black Belts, they demonstrate how common statistical tools and investigative methodologies can mitigate risks that arise from limitations in the data.},
  added-at = {2016-08-06T23:41:41.000+0200},
  address = {Boca Raton, FL},
  author = {Pries, Kim H. and Dunnigan, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/2c777f510171d9b180d4fa928f9706359/vngudivada},
  description = {With this book, managers and decision makers are given the tools to make more informed decisions about big data purchasing initiatives. Big Data Analytics: A Practical Guide for Managers not only supplies descriptions of common tools, but also surveys the various products and vendors that supply the big data market.Comparing and contrasting the different types of analysis commonly conducted with big data, this accessible reference presents clear-cut explanations of the general workings of big data tools. Instead of spending time on HOW to install specific packages, it focuses on the reasons WHY readers would install a given package.The book provides authoritative guidance on a range of tools, including open source and proprietary systems. It details the strengths and weaknesses of incorporating big data analysis into decision-making and explains how to leverage the strengths while mitigating the weaknesses. Describes the benefits of distributed computing in simple terms Includes substantial vendor/tool material, especially for open source decisions Covers prominent software packages, including Hadoop andOracle Endeca Examines GIS and machine learning applications Considers privacy and surveillance issues The book further explores basic statistical concepts that, when misapplied, can be the source of errors. Time and again, big data is treated as an oracle that discovers results nobody would have imagined. While big data can serve this valuable function, all too often these results are incorrect, yet are still reported unquestioningly. The probability of having erroneous results increases as a larger number of variables are compared unless preventative measures are taken.The approach taken by the authors is to explain these concepts so managers can ask better questions of their analysts and vendors as to the appropriateness of the methods used to arrive at a conclusion. Because the world of science and medicine has been grappling with similar issues in the publication of studies, the authors draw on their efforts and apply them to big data.},
  interhash = {bdfc6d001c14d527dd84962a6fe0cde5},
  intrahash = {c777f510171d9b180d4fa928f9706359},
  keywords = {BigDataAnalytics Book},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:12:45.000+0100},
  title = {Big data analytics: a practical guide for managers},
  year = 2015
}

@book{mcconnell2004complete,
  abstract = {Widely considered one of the best practical guides to programming, Steve McConnell's original Code Complete has been helping developers write better software for more than a decade. Now this book has been fully updated and revised with leading-edge practices - and hundreds of new code samples - illustrating the art and science of software construction. Capturing the body of knowledge available from research, academia, and everyday commercial practice, McConnell synthesizes the most effective techniques and must-know principles into clear, pragmatic guidance. No matter what your experience level, development environment, or project size, this book will inform and stimulate your thinking - and help you build the highest quality code.},
  added-at = {2016-08-07T00:26:46.000+0200},
  address = {Redmond, WA},
  author = {McConnell, Steve},
  biburl = {https://www.bibsonomy.org/bibtex/2702c591fb668fa7cc0f25d465b9f9186/vngudivada},
  edition = {Second},
  interhash = {676d501781e7fca5bd3b16020f02ec0e},
  intrahash = {702c591fb668fa7cc0f25d465b9f9186},
  keywords = {Book Code SoftwareConstruction},
  publisher = {Microsoft Press},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Code complete},
  year = 2004
}

@book{subbarao2012series,
  abstract = {The field of statistics not only affects all areas of scientific activity, but also many other matters such as public policy. It is branching rapidly into so many different subjects that a series of handbooks is the only way of comprehensively presenting the various aspects of statistical methodology, applications, and recent developments. The Handbook of Statistics is a series of self-contained reference books. Each volume is devoted to a particular topic in statistics, with Volume 30 dealing with time series. The series is addressed to the entire community of statisticians and scientists in various disciplines who use statistical methodology in their work. At the same time, special emphasis is placed on applications-oriented techniques, with the applied statistician in mind as the primary audience. * Comprehensively presents the various aspects of statistical methodology* Discusses a wide variety of diverse applications and recent developments* Contributors are internationally renowened experts in their respective areas},
  added-at = {2016-08-07T00:23:41.000+0200},
  address = {Amsterdam; London},
  author = {Subba Rao, T. and Subba Rao, Suhasini and Rao, C. Radhakrishna},
  biburl = {https://www.bibsonomy.org/bibtex/2d63d43ef2969a45973f73d6f64584463/vngudivada},
  interhash = {dea04d953d94df74c33a388e92b4e195},
  intrahash = {d63d43ef2969a45973f73d6f64584463},
  keywords = {Book Handbook TimeSeriesAnalysis},
  publisher = {North Holland},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Time series analysis : methods and applications},
  year = 2012
}

@book{stlaurent2013introducing,
  abstract = {
If you're new to Erlang, its functional style can seem difficult, but with help from this hands-on introduction, you'll scale the learning curve and discover how enjoyable, powerful, and fun this language can be.Author Simon St. Laurent shows you how to write simple Erlang programs by teaching you one basic skill at a time. You'll learn about pattern matching, recursion, message passing, process-oriented programming, and establishing pathways for data rather than telling it where to go. By the end of your journey, you'll understand why Erlang is ideal for concurrency and resilience.Get cozy with Erlang's shell, its command line interface Become familiar with Erlang's basic structures by working with numbers Discover atoms, pattern matching, and guards: the foundations of your program structure Delve into the heart of Erlang processing with recursion, strings, lists, and higher-order functions Create processes, send messages among them, and apply pattern matching to incoming messages Store and manipulate structured data with Erlang Term Storage and the Mnesia database Learn about Open Telecom Platform, Erlang's open source libraries and tools},
  added-at = {2016-08-08T23:36:18.000+0200},
  address = {Sebastopol, CA},
  author = {St. Laurent, Simon},
  biburl = {https://www.bibsonomy.org/bibtex/2d207147d36346d5f8d6080c1d5c06ce5/vngudivada},
  interhash = {afe54e56358f2260a01fb9f0d8816eca},
  intrahash = {d207147d36346d5f8d6080c1d5c06ce5},
  keywords = {Book Erlang},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Introducing Erlang},
  year = 2013
}

@article{vinnik2016analysis,
  abstract = {We present a novel framework for comprehensive exploration of OLAP data by means of user-defined dynamic hierarchical visualizations. The multidimensional data model behind the OLAP architecture is particularly suitable for sophisticated analysis of large data volumes. However, the ultimate benefit of applying OLAP technology depends on the “intelligence” and usability of visual tools available to end-users. The explorative framework of our proposed interface consists of the navigation structure, a selection of hierarchical visualization techniques, and a set of interaction features. The navigation interface allows users to pursue arbitrary disaggregation paths within single data cubes and, more importantly, across multiple cubes. In the course of interaction, the navigation view adapts itself to display the chosen path and the options valid in the current context. Special effort has been invested in handling non-trivial relationships (e.g., mixed granularity) within hierarchical dimensions in a way transparent to the user. We propose a visual structure called Enhanced Decomposition Tree to to be used along with popular “state-of-the-art” hierarchical visualization techniques. Each level of the tree is produced by a disaggregation step, whereas the nodes display the specified subset of measures, either as plain numbers or as an embedded chart. The proposed technique enables a stepwise descent towards the desired level of detail while preserving the history of the interaction. Aesthetic hierarchical layout of the node-link tree ensures clear structural separation between the analyzed values embedded in the nodes and their dimensional characteristics which label the links. Our framework provides an intuitive and powerful interface for exploring complex multidimensional data sets.},
  added-at = {2016-08-07T02:14:17.000+0200},
  author = {Vinnik, Svetlana and Mansmann, Florian},
  biburl = {https://www.bibsonomy.org/bibtex/24959a803567d527f527190737a695ccf/vngudivada},
  interhash = {4088dc205e849dbd4f2c5021ee92f251},
  intrahash = {4959a803567d527f527190737a695ccf},
  keywords = {ExploratoryDataAnalysis OLAP},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {From Analysis to Interactive Exploration: Building Visual Hierarchies from OLAP Cubes},
  year = 2016
}

@book{chisholm2013exploring,
  abstract = {A step-by-step tutorial style using examples so that users of different levels will benefit from the facilities offered by RapidMiner. If you are a computer scientist or an engineer who has real data from which you want to extract value, this book is ideal for you. You will need to have at least a basic awareness of data mining techniques and some exposure to RapidMiner.},
  added-at = {2016-08-17T00:17:06.000+0200},
  author = {Chisholm, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/2681af3a11ec44bce2a3e30bb87cc02d0/vngudivada},
  description = {Explore, understand, and prepare real data using RapidMiner's practical tips and tricks Overview See how to import, parse, and structure your data quickly and effectively Understand the visualization possibilities and be inspired to use these with your own data Structured in a modular way to adhere to standard industry processes In Detail Data is everywhere and the amount is increasing so much that the gap between what people can understand and what is available is widening relentlessly. There is a huge value in data, but much of this value lies untapped. 80% of data mining is about understanding data, exploring it, cleaning it, and structuring it so that it can be mined. RapidMiner is an environment for machine learning, data mining, text mining, predictive analytics, and business analytics. It is used for research, education, training, rapid prototyping, application development, and industrial applications. Exploring Data with RapidMiner is packed with practical examples to help practitioners get to grips with their own data. The chapters within this book are arranged within an overall framework and can additionally be consulted on an ad-hoc basis. It provides simple to intermediate examples showing modeling, visualization, and more using RapidMiner. Exploring Data with RapidMiner is a helpful guide that presents the important steps in a logical order. This book starts with importing data and then lead you through cleaning, handling missing values, visualizing, and extracting additional information, as well as understanding the time constraints that real data places on getting a result. The book uses real examples to help you understand how to set up processes, quickly.. This book will give you a solid understanding of the possibilities that RapidMiner gives for exploring data and you will be inspired to use it for your own work. What you will learn from this book Import real data from files in multiple formats and from databases Extract features from structured and unstructured data Restructure, reduce, and summarize data to help you understand it more easily and process it more quickly Visualize data in new ways to help you understand it Detect outliers and methods to handle them Detect missing data and implement ways to handle it Understand resource constraints and what to do about them Approach A step-by-step tutorial style using examples so that users of different levels will benefit from the facilities offered by RapidMiner. Who this book is written for If you are a computer scientist or an engineer who has real data from which you want to extract value, this book is ideal for you. You will need to have at least a basic awareness of data mining techniques and some exposure to RapidMiner.},
  interhash = {d29ebafb26af81aa563ac650c23eb7ab},
  intrahash = {681af3a11ec44bce2a3e30bb87cc02d0},
  keywords = {Book DataMining RapidMiner Visualization},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Exploring data with RapidMiner: explore, understand, and prepare real data using rapidminer's practical tips and tricks},
  year = 2013
}

@book{vogt2014selecting,
  abstract = {What are the most effective methods to code and analyze data for a particular study? This thoughtful and engaging book reviews the selection criteria for coding and analyzing any set of data--whether qualitative, quantitative, mixed, or visual. The authors systematically explain when to use verbal, numerical, graphic, or combined codes, and when to use qualitative, quantitative, graphic, or mixed-methods modes of analysis. Chapters on each topic are organized so that researchers can read them sequentially or can easily "flip and find" answers to specific questions. Nontechnical discussions of cutting-edge approaches--illustrated with real-world examples--emphasize how to choose (rather than how to implement) the various analyses. The book shows how using the right analysis methods leads to more justifiable conclusions and more persuasive presentations of research results. Useful features for teaching or self-study: *Chapter-opening preview boxes that highlight useful topics addressed. *End-of-chapter summary tables recapping the 'dos and don'ts' and advantages and disadvantages of each analytic technique. *Annotated suggestions for further reading and technical resources on each topic. Subject Areas/Keywords: analyses, coding, combined methods, data analysis, data collection, dissertation, graphical, interpretation, mixed methods, qualitative, quantitative, research analysis, research designs, research methods, social sciences, thesis, visual Audience: Researchers, instructors, and graduate students in a range of disciplines, including psychology, education, social work, sociology, health, and management; administrators and managers who need to make data-driven decisions.},
  added-at = {2016-08-17T00:10:26.000+0200},
  author = {Vogt, W. Paul and Vogt, Elaine R. and Gardner, Dianne C. and Haeffele, Lynne M.},
  biburl = {https://www.bibsonomy.org/bibtex/28299703c98e23a1247c366803baa2358/vngudivada},
  interhash = {93ee33d7650bd33985ad191ac581c77c},
  intrahash = {8299703c98e23a1247c366803baa2358},
  keywords = {Book EDA Visualization},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Selecting the right analyses for your data: quantitative, qualitative, and mixed methods},
  year = 2014
}

@book{wahbi2013algorithms,
  abstract = {DisCSP (Distributed Constraint Satisfaction Problem) is a general framework for solving distributed problems arising in Distributed Artificial Intelligence. A wide variety of problems in artificial intelligence are solved using the constraint satisfaction problem paradigm. However, there are several applications in multi-agent coordination that are of a distributed nature. In this type of application, the knowledge about the problem, that is, variables and constraints, may be logically or geographically distributed among physical distributed agents.},
  added-at = {2016-08-07T00:46:55.000+0200},
  address = {Hoboken, NJ},
  author = {Wahbi, Mohamed},
  biburl = {https://www.bibsonomy.org/bibtex/23151aa8afba49875b3ef1a83ba123179/vngudivada},
  interhash = {9cb7d3efbb7df0e38f5784276eb716fb},
  intrahash = {3151aa8afba49875b3ef1a83ba123179},
  keywords = {Book ConstraintSatisfactionProblem DistributedComputing},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Algorithms and ordering heuristics for distributed constraint satisfaction problems},
  year = 2013
}

@book{nandeshwar2013tableau,
  abstract = {This Cookbook contains step-by-step instructions for Tableau users to create effective graphics. The book is designed in such a way that you can refer to it chapter by chapter; you can look at the list of recipes and read them in no particular order. You'll gain the most from this book if you have basic understanding of various chart types and of their importance. Knowing when to employ a certain graphic will be equally useful. This book will get you up to speed if you just started using Tableau. You'll find this book useful if you spend a lot of time conducting data analysis and creating repor.},
  added-at = {2016-08-17T00:12:01.000+0200},
  author = {Nandeshwar, Ashutosh},
  biburl = {https://www.bibsonomy.org/bibtex/2ee8be7cd00e5bc6142c476d732d213df/vngudivada},
  description = {Over 70 recipes for creating visual stories with your data using Tableau Overview Quickly create impressive and effective graphics which would usually take hours in other tools Lots of illustrations to keep you on track Includes examples that apply to a general audience In Detail You know the feeling when you are asked to change or add a certain data point in your graph at the last minute. Usually, you have to scramble to complete the project and risk accuracy; this is not so with Tableau, however. Tableau is a revolutionary toolkit that lets you simply and effectively create high quality data visualizations. "Tableau Data Visualization Cookbook" will show you the exact steps required to generate simple to complex graphics. Whether they are pie charts or box plots, you can create such graphics with ease and confidence; no more searching for scripts or laborious Excel hacks. This book will help you make the most of Tableau and show you how to finish your projects quicker using this toolkit. In this book youll start with getting your data into Tableau, move onto generating progressively complex graphics, and end with the finishing touches and packaging your work for distribution. This book is filled with practical recipes to help you create filled maps, use custom markers, add slider selectors, and create dashboards. You will learn how to manipulate data in various ways by applying various filters, logic, and calculating various aggregate measures. Then, we will create animated graphs and provide search box and drop-down selectors to users. This book will help you to create stunning graphics in very short amount of time. If you want to effortlessly create beautiful visualizations of data then "Tableau Data Visualization Cookbook" is for you! What you will learn from this book Forecast with trend lines Manipulate and transform data Import data from various sources Create filled maps and use any shape file Share your work easily Create dashboards and scorecards Format and finish the graphic Observe statistical distributions Approach This Cookbook contains step-by-step instructions for Tableau users to create effective graphics. The book is designed in such a way that you can refer to it chapter by chapter; you can look at the list of recipes and read them in no particular order. Who this book is written for Youll gain the most from this book if you have basic understanding of various chart types and of their importance. Knowing when to employ a certain graphic will be equally useful. This book will get you up to speed if you just started using Tableau. Youll find this book useful if you spend a lot of time conducting data analysis and creating reports.},
  interhash = {14235aae2670a034702a1ec351142bb4},
  intrahash = {ee8be7cd00e5bc6142c476d732d213df},
  keywords = {Book Tableau Visualization},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Tableau data visualization cookbook},
  year = 2013
}

@book{martinez2002computational,
  abstract = {Approaching computational statistics through its theoretical aspects can be daunting. Often intimidated or distracted by the theory, researchers and students can lose sight of the actual goals and applications of the subject. What they need are its key concepts, an understanding of its methods, experience with its implementation, and practice with computational software.Focusing on the computational aspects of statistics rather than the theoretical, Computational Statistics Handbook with MATLAB uses a down-to-earth approach that makes statistics accessible to a wide range of users. The authors integrate the use of MATLAB throughout the book, allowing readers to see the actual implementation of algorithms, but also include step-by-step procedures to allow implementation with any suitable software. The book concentrates on the simulation/Monte Carlo point of view, and contains algorithms for exploratory data analysis, modeling, Monte Carlo simulation, pattern recognition, bootstrap, classification, cross-validation methods, probability density estimation, random number generation, and other computational statistics methods.Emphasis on the practical aspects of statistics, details of the latest techniques, and real implementation experience make the Computational Statistics Handbook with MATLAB more than just the first book to use MATLAB to solve computational problems in statistics. It also forms an outstanding, introduction to statistics for anyone in the many disciplines that involve data analysis.},
  added-at = {2016-08-08T23:19:32.000+0200},
  address = {Boca Raton},
  author = {Martinez, Wendy L. and Martinez, Angel R.},
  biburl = {https://www.bibsonomy.org/bibtex/2222104054633a7385710479fe7c1f96c/vngudivada},
  interhash = {4e33284e5d3c2bb3cd641f8a5c68a647},
  intrahash = {222104054633a7385710479fe7c1f96c},
  keywords = {Book Handbook Matlab Statistics},
  publisher = {Chapman & Hall/CRC},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Computational statistics handbook with MATLAB},
  year = 2002
}

@book{jones2014communicating,
  abstract = {Go beyond spreadsheets and tables and design a data presentation that really makes an impact. This practical guide shows you how to use Tableau Software to convert raw data into compelling data visualizations that provide insight or allow viewers to explore the data for themselves.Ideal for analysts, engineers, marketers, journalists, and researchers, this book describes the principles of communicating data and takes you on an in-depth tour of common visualization methods. You'll learn how to craft articulate and creative data visualizations with Tableau Desktop 8.1 and Tableau Public 8.1.Present comparisons of how much and how manyUse blended data sources to create ratios and ratesCreate charts to depict proportions and percentagesVisualize measures of mean, median, and modeLean how to deal with variation and uncertaintyCommunicate multiple quantities in the same viewShow how quantities and events change over timeUse maps to communicate positional dataBuild dashboards to combine several visualizations},
  added-at = {2016-08-17T00:05:41.000+0200},
  address = {Sebastopol, CA},
  author = {Jones, Ben},
  biburl = {https://www.bibsonomy.org/bibtex/23ac4e14e09788204761fd02240455be0/vngudivada},
  interhash = {f5fa6d4a436078d00fdbdaee1c77ffd2},
  intrahash = {3ac4e14e09788204761fd02240455be0},
  keywords = {Book Tableau VisualAnalytics Visualization},
  publisher = {O'Reilly Media},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Communicating data with Tableau},
  year = 2014
}

@book{schilling2012fundamentals,
  abstract = {This second edition text focuses on the fundamentals of digital signal processing with an emphasis on practical applications. In order to motivate students, many of the examples illustrate the processing of speech and music. This theme is also a focus of the course software that features facilities for recording and playing sound on a standard PC. The accompanying website contains a comprehensive MATLAB software package called the Fundamentals of Digital Signal Processing (FDSP) toolbox version 2.0. The FDSP toolbox includes chapter GUI modules, an extensive library of DSP functions, direct access to all of the computational examples, figures, and tables, solutions to selected problems, and onliine help documentation. Using the interactive GUI modules, students can explore, compare, and directly experience the effects of signal processing techniques without any need for programming.},
  added-at = {2016-08-09T23:32:00.000+0200},
  address = {Stamford, CT},
  author = {Schilling, Robert J. and Harris, Sandra L.},
  biburl = {https://www.bibsonomy.org/bibtex/2a0ba024e3d45309496767430983ddf4e/vngudivada},
  interhash = {f5843267d70a2a63832b47334705b1d2},
  intrahash = {a0ba024e3d45309496767430983ddf4e},
  keywords = {Book SignalProcessing},
  publisher = {Cengage Learning},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Fundamentals of digital signal processing using MATLAB},
  year = 2012
}

@inproceedings{essa2012student,
  abstract = {We propose a novel design of a Student Success System (S3), a holistic analytical system for identifying and treating at-risk students. S3 synthesizes several strands of risk analytics: the use of predictive models to identify academically at-risk students, the creation of data visualizations for reaching diagnostic insights, and the application of a case-based approach for managing interventions. Such a system poses numerous design, implementation, and research challenges. In this paper we discuss a core research challenge for designing early warning systems such as S3. We then propose our approach for meeting that challenge. A practical implementation of an student risk early warning system, utilizing predictive models, must meet two design criteria: a) the methodology for generating predictive models must be flexible to allow generalization from one context to another; b) the underlying mechanism of prediction should be easily interpretable by practitioners whose end goal is to design meaningful interventions on behalf of students. Our proposed solution applies an ensemble method for predictive modeling using a strategy of decomposition. Decomposition provides a flexible technique for generating and generalizing predictive models across different contexts. Decomposition into interpretable semantic units, when coupled with data visualizations and case management tools, allows practitioners, such as instructors and advisors, to build a bridge between prediction and intervention.},
  added-at = {2016-08-11T14:41:39.000+0200},
  address = {New York, NY, USA},
  author = {Essa, Alfred and Ayad, Hanan},
  biburl = {https://www.bibsonomy.org/bibtex/2084ea3181411868556ef341953f648b0/vngudivada},
  booktitle = {Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge},
  interhash = {50885b9c4e5a2942e3f058aaec41e38d},
  intrahash = {084ea3181411868556ef341953f648b0},
  keywords = {DiagnosticAnalytics Modeling PredictiveAnalytics RiskAnalytics Visualization},
  pages = {158--161},
  publisher = {ACM},
  series = {LAK '12},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Student Success System: Risk Analytics and Data Visualization Using Ensembles of Predictive Models},
  url = {http://doi.acm.org/10.1145/2330601.2330641},
  year = 2012
}

@inproceedings{takama2009visualization,
  abstract = {This paper proposes the visualization cube for modeling interaction in exploratory analysis of spatiotemporal trend information. Recent growth of the Web has brought us various type of information, among which trend information is one of the important information, because it is crucial for our decision making. Although interactive information visualization system is useful for supporting exploratory data analysis of trend information, the design of such systems tends to depend on the idea of each designer. It is expected that establishing adequate interaction model will contribute to the design of those systems. The visualization cube defines the abstract data structure of spatiotemporal trend information, which consists of 4 axes: spatial, temporal, statistics-value, and type-of-views axes. Interactions for generating views for analytic purpose are defined as the operations on it. Based on the interaction model, the prototype system is developed and used in actual classes of an elementary school, of which the result shows the system has enough usability for 5th-grade elementary school students to perform exploratory data analysis. It is expected the visualization cube contributes to the design of interactive information visualization systems.},
  added-at = {2016-08-11T14:15:31.000+0200},
  author = {Takama, Y. and Yamada, T.},
  biburl = {https://www.bibsonomy.org/bibtex/28feaf936867bf74622e031ac7b9f0691/vngudivada},
  booktitle = {Web Intelligence and Intelligent Agent Technologies, 2009. WI-IAT '09. IEEE/WIC/ACM International Joint Conferences on},
  doi = {10.1109/WI-IAT.2009.216},
  interhash = {fcf236992b9878d33429dcad934bc8cc},
  intrahash = {8feaf936867bf74622e031ac7b9f0691},
  keywords = {EDA SpatioTemporalData Visualization},
  month = {Sept},
  pages = {1-4},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Visualization Cube: Modeling Interaction for Exploratory Data Analysis of Spatiotemporal Trend Information},
  url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5285102},
  volume = 3,
  year = 2009
}

@book{provost2013science,
  abstract = {Provides an introduction to the fundamental principles of data science, walking the reader through the "data-analytic thinking" necessary for extracting useful knowledge and business value from collected data.},
  added-at = {2016-08-07T03:57:53.000+0200},
  address = {Sebastopol, Calif.},
  author = {Provost, Foster and Fawcett, Tom},
  biburl = {https://www.bibsonomy.org/bibtex/2cae851af4ffcb2b1df2c2b4b5084f56a/vngudivada},
  description = {Written by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the "data-analytic thinking" necessary for extracting useful knowledge and business value from the data you collect. This guide also helps you understand the many data-mining techniques in use today.Based on an MBA course Provost has taught at New York University over the past ten years, Data Science for Business provides examples of real-world business problems to illustrate these principles. You'll not only learn how to improve communication between business stakeholders and data scientists, but also how participate intelligently in your company's data science projects. You'll also discover how to think data-analytically, and fully appreciate how data science methods can support business decision-making.Understand how data science fits in your organization-and how you can use it for competitive advantageTreat data as a business asset that requires careful investment if you're to gain real valueApproach business problems data-analytically, using the data-mining process to gather good data in the most appropriate wayLearn general concepts for actually extracting knowledge from dataApply data science principles when interviewing data science job candidates},
  interhash = {c20b2f33eddcc1f3faf5f297e55c87f5},
  intrahash = {cae851af4ffcb2b1df2c2b4b5084f56a},
  keywords = {Book DataScience},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Data science for business: what you need to know about data mining and data-analytic thinking},
  year = 2013
}

@book{adamson2006mastering,
  abstract = {This is the first book to provide in-depth coverage of star schema aggregates used in dimensional modeling-from selection and design, to loading and usage, to specific tasks and deliverables for implementation projects. It covers the principles of aggregate schema design and the pros and cons of various types of commercial solutions for navigating and building aggregates and discusses how to include aggregates in data warehouse development projects that focus on incremental development, iterative builds, and early data loads--Resource description page.},
  added-at = {2016-08-07T01:59:51.000+0200},
  address = {Indianapolis, IN},
  author = {Adamson, Christopher},
  biburl = {https://www.bibsonomy.org/bibtex/23521ae3ab0123a5ddb628c94543cb2f6/vngudivada},
  interhash = {b2040fed2756cccb8e2e37115b0e130c},
  intrahash = {3521ae3ab0123a5ddb628c94543cb2f6},
  keywords = {Book DataWarehouse StarSchema},
  publisher = {Wiley Pub.},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Mastering data warehouse aggregates: solutions for star schema performance},
  year = 2006
}

@inproceedings{drachsler2015competition,
  abstract = {The LAK Data Challenge 2015 continues the research efforts of the previous data competitions in 2013 and 2014 by stimulating research on the evolving fields Learning Analytics (LA) and Educational Data Mining (EDM). Building on a series of activities of the LinkedUp project, the challenge aims to generate new insights and analysis on the LA & EDM disciplines and is supported through the LAK Dataset - a unique corpus of LA & EDM literature, exposed in structured and machine-readable formats.},
  added-at = {2016-08-11T14:52:03.000+0200},
  address = {New York, NY, USA},
  author = {Drachsler, Hendrik and Dietze, Stefan and Herder, Eelco and d'Aquin, Mathieu and Taibi, Davide and Scheffel, Maren},
  biburl = {https://www.bibsonomy.org/bibtex/21239ba827440eb66f9289167fa01c7d5/vngudivada},
  booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
  interhash = {bdfbf75b90303b665e32d0e44f2b6a83},
  intrahash = {1239ba827440eb66f9289167fa01c7d5},
  keywords = {DataCompetition DiagnosticAnalytics LAK},
  pages = {396--397},
  publisher = {ACM},
  series = {LAK '15},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {The 3rd LAK Data Competition},
  url = {http://doi.acm.org/10.1145/2723576.2723641},
  year = 2015
}

@book{hills2016nosql,
  abstract = {The Concept and Object Modeling Notation (COMN) is able to cover the full spectrum of analysis and design. This book will teach you: the simple and familiar graphical notation of COMN with its three basic shapes and four line styles how to think about objects, concepts, types, and classes in the real world, using the ordinary meanings of English words that aren't tangled with confused techno-speak how to express logical data designs that are freer from implementation considerations than is possible in any other notation how to understand key-value, document, columnar, and table-oriented database designs in logical and physical terms how to use COMN to specify physical database implementations in any NoSQL or SQL database with the precision necessary for model-driven development},
  added-at = {2016-08-15T00:46:55.000+0200},
  address = {Basking Ridge, New Jersey},
  author = {Hills, Ted},
  biburl = {https://www.bibsonomy.org/bibtex/25dab42838d566e4711e49ffdee576088/vngudivada},
  interhash = {1f7b76e24481e5857f9826dd6c460fa4},
  intrahash = {5dab42838d566e4711e49ffdee576088},
  keywords = {Book DataModeling NoSQL SQL},
  publisher = {Technics Publications},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {NoSQL and SQL data modeling: bringing together data, semantics, and software},
  year = 2016
}

@book{kimball2013warehouse,
  abstract = {The most authoritative and comprehensive guide to dimensional modeling, from its originators—fully updated

Ralph Kimball introduced the industry to the techniques of dimensional modeling in the first edition of The Data Warehouse Toolkit (1996). Since then, dimensional modeling has become the most widely accepted approach for presenting information in data warehouse and business intelligence (DW/BI) systems. The Data Warehouse Toolkit is recognized as the definitive source for dimensional modeling techniques, patterns, and best practices.

This third edition of the classic reference delivers the most comprehensive library of dimensional modeling techniques ever assembled. Fully updated with fresh insights and best practices, this book provides clear guidelines for designing dimensional models—and does so in a style that serves the needs of those new to data warehousing as well as experienced professionals.

All the techniques in the book are illustrated with real-world case studies based on the authors' actual DW/BI design experiences. In addition, the Kimball Group's "official" list of dimensional modeling techniques is summarized in a single chapter for easy reference, with pointers from each technique to the case studies where the concepts are brought to life.

The third edition of The Data Warehouse Toolkit covers:

Practical design techniques—both basic and advanced—for dimension and fact tables
14 case studies, including retail sales, electronic commerce, customer relationship management, procurement, inventory, order management, accounting, human resources, financial services, healthcare, insurance, education, telecommunications, and transportation
Sample data warehouse bus matrices for 12 case studies
Dimensional modeling pitfalls and mistakes to avoid
Enhanced slowly changing dimension techniques type 0 through 7
Bridge tables for ragged variable depth hierarchies and multivalued attributes
Best practices for Big Data analytics
Guidelines for collaborative, interactive design sessions with business stakeholders
An overview of the Kimball DW/BI project lifecycle methodology
Comprehensive review of extract, transformation, and load (ETL) systems and design considerations
The 34 ETL subsystems and techniques to populate dimension and fact tables},
  added-at = {2016-08-07T05:14:31.000+0200},
  address = {Indianapolis, Ind.},
  author = {Kimball, Ralph and Ross, Margy},
  biburl = {https://www.bibsonomy.org/bibtex/23d78744b58c5ed4f7eaa5e280148267d/vngudivada},
  edition = {Third},
  interhash = {c0e3e1feaae084d658d43b00ff21a2c4},
  intrahash = {3d78744b58c5ed4f7eaa5e280148267d},
  keywords = {Book DataWarehouse DimensionalModeling},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {The data warehouse toolkit: the definitive guide to dimensional modeling},
  year = 2013
}

@book{zobel2014writing,
  abstract = {All researchers need to write or speak about their work, and to have research that is worth presenting. Based on the author's decades of experience as a researcher and advisor, this third edition provides detailed guidance on writing and presentations and a comprehensive introduction to research methods, the how-to of being a successful scientist. Topics include: · Development of ideas into research questions; · How to find, read, evaluate, and referee other research; · Design and evaluation of experiments, and appropriate use of statistics; · Ethics, the principles of science, and examples of science gone wrong. Much of the book is a step-by-step guide to effective communication, with advice on: · Writing style and editing; · Figures, graphs, and tables; · Mathematics and algorithms; · Literature reviews and referees? reports; · Structuring of arguments and results into papers and theses; · Writing of other professional documents; · Presentation of talks and posters. Written in an accessible style, and including handy checklists and exercises, Writing for Computer Science is not only an introduction to the doing and describing of research, but is a valuable reference for working scientists in the computing and mathematical sciences.},
  added-at = {2016-08-09T22:18:41.000+0200},
  author = {Zobel, Justin},
  biburl = {https://www.bibsonomy.org/bibtex/25a4851ad01d86e473f1ed6ba90b64c36/vngudivada},
  interhash = {bdbd1272a934a46303dce19c382d9bd1},
  intrahash = {5a4851ad01d86e473f1ed6ba90b64c36},
  keywords = {Book Writing},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Writing for computer science},
  year = 2014
}

@book{kruse2012synergies,
  abstract = {In recent years there has been a growing interest to extend classical methods for data analysis.The aim is to allow a more flexible modeling of phenomena such as uncertainty, imprecision or ignorance.Such extensions of classical probability theory and statistics are useful in many real-life situations, since uncertainties in data are not only present in the form of randomness --- various types of incomplete or subjective information have to be handled.About twelve years ago the idea of strengthening the dialogue between the various research communities in the field of data analysis was born and resulted in the International Conference Series on Soft Methods in Probability and Statistics (SMPS).This book gathers contributions presented at the SMPS'2012 held in Konstanz, Germany. Its aim is to present recent results illustrating new trends in intelligent data analysis.It gives a comprehensive overview of current research into the fusion of soft computing methods with probability and statistics.Synergies of both fields might improve intelligent data analysis methods in terms of robustness to noise and applicability to larger datasets, while being able to efficiently obtain understandable solutions of real-world problems.},
  added-at = {2016-08-14T05:04:50.000+0200},
  address = {Berlin, Germany},
  author = {Kruse, Rudolf},
  biburl = {https://www.bibsonomy.org/bibtex/23680fd044e14d41bc0d87b3428ee0b5d/vngudivada},
  interhash = {21ff2968ecb96bf14227ba44938f93cb},
  intrahash = {3680fd044e14d41bc0d87b3428ee0b5d},
  keywords = {Book DataAnalytics Statistics},
  publisher = {Springer-Verlag},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Synergies of Soft Computing and Statistics for Intelligent Data Analysis},
  year = 2012
}

@book{ramakrishnan2003database,
  abstract = {While emphasizing conceptual and physical database design and tuning, this reference and guide also covers more advanced topics, including parallel and distributed database systems, transaction processing, decision support, object-relational systems, and active and deductive database systems. An essential professional source.},
  added-at = {2016-08-07T01:22:16.000+0200},
  address = {Boston},
  author = {Ramakrishnan, Raghu and Gehrke, Johannes},
  biburl = {https://www.bibsonomy.org/bibtex/29804c0f6770722a0a985bae7ac5e57d4/vngudivada},
  interhash = {3ea7ea3b6733d306ed97ca7240196656},
  intrahash = {9804c0f6770722a0a985bae7ac5e57d4},
  keywords = {Book DBMS},
  publisher = {McGraw-Hill},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Database management systems},
  year = 2003
}

@book{rosen2007discrete,
  abstract = {Discrete Mathematics and its Applications, Seventh Edition, is intended for one- or two-term introductory discrete mathematics courses taken by students from a wide variety of majors, including computer science, mathematics, and engineering. This renowned best-selling text, which has been used at over 500 institutions around the world, gives a focused introduction to the primary themes in a discrete mathematics course and demonstrates the relevance and practicality of discrete mathematics to a wide a wide variety of real-world applications...from computer science to data networking, to psychology, to chemistry, to engineering, to linguistics, to biology, to business, and to many other important fields.},
  added-at = {2016-08-09T21:22:31.000+0200},
  address = {Boston},
  author = {Rosen, Kenneth H.},
  biburl = {https://www.bibsonomy.org/bibtex/2928b1606b9bff6dee07c575d56fb14b9/vngudivada},
  interhash = {35a1fb550430cf5363b5f60959f0c99a},
  intrahash = {928b1606b9bff6dee07c575d56fb14b9},
  keywords = {Book DiscreteMathematics},
  publisher = {McGraw-Hill Higher Education},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Discrete mathematics and its applications},
  year = 2007
}

@book{aggarwal2015mining,
  abstract = {This textbook explores the different aspects of data mining from the fundamentals to the complex data types and their applications, capturing the wide diversity of problem domains for data mining issues. It goes beyond the traditional focus on data mining problems to introduce advanced data types such as text, time series, discrete sequences, spatial data, graph data, and social networks. Until now, no single book has addressed all these topics in a comprehensive and integrated way. The chapters of this book fall into one of three categories: Fundamental chapters: Data mining has four main problems, which correspond to clustering, classification, association pattern mining, and outlier analysis. These chapters comprehensively discuss a wide variety of methods for these problems. Domain chapters: These chapters discuss the specific methods used for different domains of data such as text data, time-series data, sequence data, graph data, and spatial data. Application chapters: These chapters study important applications such as stream mining, Web mining, ranking, recommendations, social networks, and privacy preservation. The domain chapters also have an applied flavor. Appropriate for both introductory and advanced data mining courses, Data Mining: The Textbook balances mathematical details and intuition. It contains the necessary mathematical details for professors and researchers, but it is presented in a simple and intuitive style to improve accessibility for students and industrial practitioners (including those with a limited mathematical background). Numerous illustrations, examples, and exercises are included, with an emphasis on semantically interpretable examples. Praise for Data Mining: The Textbook - llAs I read through this book, I have already decided to use it in my classes. lThis is a book written by an outstanding researcher who has made fundamental contributions to data mining, in a way that is both accessible and up to date. lThe book is complete with theory and practical use cases. lItlls a must-have for students and professors alike!"--Qiang Yang, Chair of Computer Science and Engineering at Hong Kong University of Science and Technology "This is the most amazing and comprehensive text book on data mining. It covers not only the fundamental problems, such as clustering, classification, outliers and frequent patterns, and different data types, including text, time series, sequences, spatial data and graphs, but also various applications, such as recommenders, Web, social network and privacy.l It is a great book for graduate students and researchers as well as practitioners." -- Philip S. Yu, UIC Distinguished Professor and Wexler Chair in Information Technology at University of Illinois at Chicago.},
  added-at = {2016-08-09T22:07:53.000+0200},
  author = {Aggarwal, Charu C.},
  biburl = {https://www.bibsonomy.org/bibtex/2ee09aa8a1a1e26056ee4bbd08e2a5e6e/vngudivada},
  interhash = {8252a499b7f9bd2c2374365f7f18b940},
  intrahash = {ee09aa8a1a1e26056ee4bbd08e2a5e6e},
  keywords = {Book DataMining ParExcellence},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Data mining: the textbook},
  year = 2015
}

@book{mastorakis2014computational,
  abstract = {This book provides readers with modern computational techniques for solving variety of problems from electrical, mechanical, civil and chemical engineering. Mathematical methods are presented in a unified manner, so they can be applied consistently to problems in applied electromagnetics, strength of materials, fluid mechanics, heat and mass transfer, environmental engineering, biomedical engineering, signal processing, automatic control and more.-Features contributions from distinguished researchers on significant aspects of current numerical methods and computational mathematics; -Presents actual results and innovative methods that provide numerical solutions, while minimizing computing times; -Includes new and advanced methods and modern variations of known techniques that can solve difficult scientific problems efficiently.},
  added-at = {2016-08-09T22:27:04.000+0200},
  author = {Mastorakis, Nikos E. and Mladenov, Valeri},
  biburl = {https://www.bibsonomy.org/bibtex/21d3eaeb6593d8ce0077d6e66cf753dc6/vngudivada},
  interhash = {0133b7b258781d702037f7b1f8efc1e6},
  intrahash = {1d3eaeb6593d8ce0077d6e66cf753dc6},
  keywords = {Book ComputationalProblems},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Computational problems in engineering},
  year = 2014
}

@book{yuk2014visualization,
  abstract = {A straightforward, full-color guide to showcasing data so your audience can see what you mean, not just read about it Big data is big news! Every company, industry, not-for-profit, and government agency wants and needs to analyze and leverage datasets that can quickly become ponderously large. Data visualization software enables different industries to present information in ways that are memorable and relevant to their mission. This full-color guide introduces you to a variety of ways to handle and synthesize data in much more interesting ways than mere columns and rows of numbers.},
  added-at = {2016-08-07T04:10:30.000+0200},
  author = {Yuk, Mico and Diamond, Stephanie},
  biburl = {https://www.bibsonomy.org/bibtex/20438026a8e2f1b54da2194fd3f3e59d5/vngudivada},
  interhash = {c6e81b0f6bea260f71e12a3879a27e40},
  intrahash = {0438026a8e2f1b54da2194fd3f3e59d5},
  keywords = {Book DataVisualization},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Data visualization for dummies},
  year = 2014
}

@inproceedings{bernard2012guided,
  abstract = {Visual cluster analysis provides valuable tools that help analysts to understand large data sets in terms of representative clusters and relationships thereof. Often, the found clusters are to be understood in context of belonging categorical, numerical or textual metadata which are given for the data elements. While often not part of the clustering process, such metadata play an important role and need to be considered during the interactive cluster exploration process. Traditionally, linked-views allow to relate (or loosely speaking: correlate) clusters with metadata or other properties of the underlying cluster data. Manually inspecting the distribution of metadata for each cluster in a linked-view approach is tedious, especially for large data sets, where a large search problem arises. Fully interactive search for potentially useful or interesting cluster to metadata relationships may constitute a cumbersome and long process. To remedy this problem, we propose a novel approach for guiding users in discovering interesting relationships between clusters and associated metadata. Its goal is to guide the analyst through the potentially huge search space. We focus in our work on metadata of categorical type, which can be summarized for a cluster in form of a histogram. We start from a given visual cluster representation, and compute certain measures of interestingness defined on the distribution of metadata categories for the clusters. These measures are used to automatically score and rank the clusters for potential interestingness regarding the distribution of categorical metadata. Identified interesting relationships are highlighted in the visual cluster representation for easy inspection by the user. We present a system implementing an encompassing, yet extensible, set of interestingness scores for categorical metadata, which can also be extended to numerical metadata. Appropriate visual representations are provided for showing the visual correlations, as well as the calculated ranking scores. Focusing on clusters of time series data, we test our approach on a large real-world data set of time-oriented scientific research data, demonstrating how specific interesting views are automatically identified, supporting the analyst discovering interesting and visually understandable relationships.},
  added-at = {2016-08-11T05:04:19.000+0200},
  address = {New York, NY, USA},
  author = {Bernard, J\"{u}rgen and Ruppert, Tobias and Scherer, Maximilian and Schreck, Tobias and Kohlhammer, J\"{o}rn},
  biburl = {https://www.bibsonomy.org/bibtex/2677727e32726f5e22722a0bb4c974b41/vngudivada},
  booktitle = {Proceedings of the 12th International Conference on Knowledge Management and Knowledge Technologies},
  interhash = {5c1ee828aa92599ee021633b76d38d54},
  intrahash = {677727e32726f5e22722a0bb4c974b41},
  keywords = {GuidedDiscovery TimeSeriesAnalysis VisualExploration},
  pages = {22:1--22:8},
  publisher = {ACM},
  series = {i-KNOW '12},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Guided Discovery of Interesting Relationships Between Time Series Clusters and Metadata Properties},
  url = {http://doi.acm.org/10.1145/2362456.2362485},
  year = 2012
}

@inproceedings{liu2012assocexplorer,
  abstract = {We present a system called AssocExplorer to support exploratory data analysis via association rule visualization and exploration. AssocExplorer is designed by following the visual information-seeking mantra: overview first, zoom and filter, then details on demand. It effectively uses coloring to deliver information so that users can easily detect things that are interesting to them. If users find a rule interesting, they can explore related rules for further analysis, which allows users to find interesting phenomenon that are difficult to detect when rules are examined separately. Our system also allows users to compare rules and inspect rules with similar item composition but different statistics so that the key factors that contribute to the difference can be isolated.},
  added-at = {2016-08-11T14:19:50.000+0200},
  address = {New York, NY, USA},
  author = {Liu, Guimei and Suchitra, Andre and Zhang, Haojun and Feng, Mengling and Ng, See-Kiong and Wong, Limsoon},
  biburl = {https://www.bibsonomy.org/bibtex/2ac0b9558bd0e87fd3d4001dbd2e0156a/vngudivada},
  booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  interhash = {de1f24d02d6eea65c09202f693933e6a},
  intrahash = {ac0b9558bd0e87fd3d4001dbd2e0156a},
  keywords = {EDA RulesEngine Visualization},
  pages = {1536--1539},
  publisher = {ACM},
  series = {KDD '12},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {AssocExplorer: An Association Rule Visualization System for Exploratory Data Analysis},
  url = {http://doi.acm.org/10.1145/2339530.2339774},
  year = 2012
}

@book{quinlan1993programs,
  abstract = {This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use, the source code (about 8,800 lines), and implementation notes.},
  added-at = {2016-08-11T04:51:39.000+0200},
  address = {San Mateo, Calif.},
  author = {Quinlan, J. R.},
  biburl = {https://www.bibsonomy.org/bibtex/23a696d9a735dec7206923a6456a8c9de/vngudivada},
  description = {Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available for download (see below).



C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies.



This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.},
  interhash = {1a265267f55efc59cd96ecb93a69b520},
  intrahash = {3a696d9a735dec7206923a6456a8c9de},
  keywords = {Book ML MachineLearning},
  publisher = {Morgan Kaufmann Publishers},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {C4.5 : programs for machine learning},
  year = 1993
}

@book{devries2012dummies,
  abstract = {This accessible guide is the ideal introduction to R for complete beginners. Learn to master the programming language of choice among statisticians and data analysts worldwide.},
  added-at = {2016-08-09T20:36:40.000+0200},
  address = {Chichester},
  author = {De Vries, Andrie and Meys, Joris},
  biburl = {https://www.bibsonomy.org/bibtex/2a4d7e0fa87f041ef45f1639c31bcd286/vngudivada},
  description = {Requiring no prior programming experience and packed with practical examples, easy, step-by-step exercises, and sample code, this extremely accessible guide is the ideal introduction to R for complete beginners. It also covers many concepts that intermediate-level programmers will find extremely useful. Master your R ABCs ? get up to speed in no time with the basics, from installing and configuring R to writing simple scripts and performing simultaneous calculations on many variables. Put data in its place ? get to know your way around lists, data frames, and other R data structures while learning to interact with other programs, such as Microsoft Excel. Make data dance to your tune ? learn how to reshape and manipulate data, merge data sets, split and combine data, perform calculations on vectors and arrays, and much more. Visualize it ? learn to use R's powerful data visualization features to create beautiful and informative graphical presentations of your data. Get statistical ? find out how to do simple statistical analysis, summarize your variables, and conduct classic statistical tests, such as t-tests. Expand and customize R ? get the lowdown on how to find, install, and make the most of add-on packages created by the global R community for a wide variety of purposes. Open the book and find: Help downloading, installing, and configuring R, Tips for getting data in and out of R, Ways to use data frames and lists to organize data, How to manipulate and process data, Advice on fitting regression models and ANOVA, Helpful hints for working with graphics, How to code in R, What R mailing lists and forums can do for you.},
  interhash = {6bdf98cb0af5008b4ef1c5e224c8c8f0},
  intrahash = {a4d7e0fa87f041ef45f1639c31bcd286},
  keywords = {Book Dummies R},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {R for dummies},
  year = 2012
}

@book{gopi2014digital,
  abstract = {Digital Speech Processing Using Matlab deals with digital speech pattern recognition, speech production model, speech feature extraction, and speech compression. The book is written in a manner that is suitable for beginners pursuing basic research in digital speech processing. Matlab illustrations are provided for most topics to enable better understanding of concepts. This book also deals with the basic pattern recognition techniques (illustrated with speech signals using Matlab) such as PCA, LDA, ICA, SVM, HMM, GMM, BPN, and KSOM.},
  added-at = {2016-08-09T23:27:13.000+0200},
  author = {Gopi, E. S.},
  biburl = {https://www.bibsonomy.org/bibtex/2f31a0dafd935f62d81c4adf4bbcadda1/vngudivada},
  interhash = {4435ff69d0763baa8dcb1ee7a31e642e},
  intrahash = {f31a0dafd935f62d81c4adf4bbcadda1},
  keywords = {Book NLU SpeechProcessing},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Digital speech processing using Matlab},
  year = 2014
}

@book{wan2006harnessing,
  abstract = {Collecting all the results on the particular types of inequalities, the coverage of this book is unique among textbooks in the literature. The book focuses on the historical development of the Carlson inequalities and their many generalizations and variations. As well as almost all known results concerning these inequalities and all known proof techniques, a number of open questions suitable for further research are considered. Two chapters are devoted to clarifying the close connection between interpolation theory and this type of inequality. Other applications are also included, in addition to a historical note on Fritz Carlson himself.},
  added-at = {2016-08-07T00:51:34.000+0200},
  address = {Hackensack, NJ},
  author = {Wan, Henry Y.},
  biburl = {https://www.bibsonomy.org/bibtex/2192dc86c6e5a08875743fc3b2341c026/vngudivada},
  description = {Immersing students in Java and the Java Virtual Machine (JVM), Introduction to Compiler Construction in a Java World enables a deep understanding of the Java programming language and its implementation. The text focuses on design, organization, and testing, helping students learn good software engineering skills and become better programmers. The book covers all of the standard compiler topics, including lexical analysis, parsing, abstract syntax trees, semantic analysis, code generation, and register allocation. The authors also demonstrate how JVM code can be translated to a register machine, specifically the MIPS architecture. In addition, they discuss recent strategies, such as just-in-time compiling and hotspot compiling, and present an overview of leading commercial compilers. Each chapter includes a mix of written exercises and programming projects. By working with and extending a real, functional compiler, students develop a hands-on appreciation of how compilers work, how to write compilers, and how the Java language behaves. They also get invaluable practice working with a non-trivial Java program of more than 30,000 lines of code. Fully documented Java code for the compiler is accessible at http://www.cs.umb.edu/j--/},
  interhash = {d60f76e1c3748d563e1a723fb9acd649},
  intrahash = {192dc86c6e5a08875743fc3b2341c026},
  keywords = {Book CompilerConstruction Java},
  publisher = {World Scientific},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Harnessing globalization : a review of East Asian case histories},
  year = 2006
}

@book{zumel2014practical,
  abstract = {Practical Data Science with R lives up to its name. It explains basic principles without the theoretical mumbo-jumbo and jumps right to the real use cases you'll face as you collect, curate, and analyze the data crucial to the success of your business. You'll apply the R programming language and statistical analysis techniques to carefully explained examples based in marketing, business intelligence, and decision support. Business analysts and developers are increasingly collecting, curating, analyzing, and reporting on crucial business data. The R language and its associated tools provide a straightforward way to tackle day-to-day data science tasks without a lot of academic theory or advanced mathematics. Practical Data Science with R shows you how to apply the R programming language and useful statistical techniques to everyday business situations. Using examples from marketing, business intelligence, and decision support, it shows you how to design experiments (such as A/B tests), build predictive models, and present results to audiences of all levels. This book is accessible to readers without a background in data science. Some familiarity with basic statistics, R, or another scripting language is assumed. What's inside: Data science for the business professional; Statistical analysis using the R language; Project lifecycle, from planning to delivery; Numerous instantly familiar use cases; Keys to effective data presentations--Publisher website.},
  added-at = {2016-08-07T04:03:30.000+0200},
  author = {Zumel, Nina and Mount, John},
  biburl = {https://www.bibsonomy.org/bibtex/24bf3b541b2e979725a24b5babebe3be0/vngudivada},
  interhash = {866c2b19fde87a91daeb13684fde7d87},
  intrahash = {4bf3b541b2e979725a24b5babebe3be0},
  keywords = {Book DataScience ParExcellence R},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Practical data science with R},
  year = 2014
}

@book{cooper2012engineering,
  abstract = {This entirely revised second edition of Engineering a Compiler is full of technical updates and new material covering the latest developments in compiler technology. In this comprehensive text you will learn important techniques for constructing a modern compiler. Leading educators and researchers Keith Cooper and Linda Torczon combine basic principles with pragmatic insights from their experience building state-of-the-art compilers. They will help you fully understand important techniques such as compilation of imperative and object-oriented languages, construction of static single assignment forms, instruction scheduling, and graph-coloring register allocation. In-depth treatment of algorithms and techniques used in the front end of a modern compiler. Focus on code optimization and code generation, the primary areas of recent research and development. Improvements in presentation including conceptual overviews for each chapter, summaries and review questions for sections, and prominent placement of definitions for new terms. Examples drawn from several different programming languages.},
  added-at = {2016-08-07T01:09:33.000+0200},
  address = {Boston, MA},
  author = {Cooper, Keith D. and Torczon, Linda},
  biburl = {https://www.bibsonomy.org/bibtex/259c8575535e70b1151f891c27c405c56/vngudivada},
  edition = {Second},
  interhash = {21380d52a59c60e7faa5420a8cc13018},
  intrahash = {59c8575535e70b1151f891c27c405c56},
  keywords = {Book CompilerConstruction},
  publisher = {Elsevier/Morgan Kaufmann},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Engineering a compiler},
  year = 2012
}

@book{halloway2012programming,
  abstract = {If you want to keep up with the significant changes in this important language, you need the second edition of Programming Clojure. Stu and Aaron describe the modifications to the numerics system in Clojure 1.3, explain new Clojure concepts such as Protocols and Datatypes, and teach you how to think in Clojure.Programming Clojure, 2nd Edition is a significant update to the classic book on the Clojure language. You'll get thorough coverage of all the new features of Clojure 1.3, and enjoy reorganized and rewritten chapters that reflect the significance of new Clojure concepts. Many code examples have been rewritten or replaced, and every page has been reevaluated in the light of Clojure 1.3. As Aaron and Stu show you how to build an application from scratch, you'll get a rich view into a complete Clojure workflow. And you'll get an invaluable education in thinking in Clojure as you work out solutions to the various parts of a problem. Clojure is becoming the language of choice for many who are moving to functional programming or dealing with the challenges of concurrency. Clojure offers: The simplicity of an elegantly designed language The power of Lisp The virtues of concurrency and functional style The reach of the JVM The speed of hand-written Java code It's the combination of these features that makes Clojure sparkle. Programming Clojure, 2nd Edition shows you how to think in Clojure, and to take advantage of these combined strengths to build powerful programs quickly. What You Need: Oracle JDK 6 A text editor},
  added-at = {2016-08-08T23:30:40.000+0200},
  author = {Halloway, Stuart Dabbs and Bedra, Aaron},
  biburl = {https://www.bibsonomy.org/bibtex/2dcb495da524a0773a96af678e261df4a/vngudivada},
  edition = {Second},
  interhash = {69db24157f528850cfbdbbd6706cf03d},
  intrahash = {dcb495da524a0773a96af678e261df4a},
  keywords = {Book Clojure},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Programming Clojure},
  year = 2012
}

@book{motulsky1995intuitive,
  abstract = {Intuitive Biostatistics takes a non-technical, non-quantitative approach to statistics and emphasizes interpretation of statistical results rather than the computational strategies for generating statistical data. This makes the text especially useful for those in health-science fields who have not taken a biostatistics course before. The text is also an excellent resource for professionals in labs, acting as a conceptually oriented and accessible biostatistics guide. With an engaging and conversational tone, Intuitive Biostatistics provides a clear introduction to statistics for undergraduate and graduate students and also serves as a statistics refresher for working scientists.},
  added-at = {2016-08-07T00:58:40.000+0200},
  address = {New York},
  author = {Motulsky, Harvey},
  biburl = {https://www.bibsonomy.org/bibtex/2888862da0b20d8b92a7f85d25acee085/vngudivada},
  interhash = {2d34f1d35a8ba68da071e87b7754e171},
  intrahash = {888862da0b20d8b92a7f85d25acee085},
  keywords = {Biostatistics Book},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Intuitive biostatistics},
  year = 1995
}

@inproceedings{huang2014spacefilling,
  abstract = {We introduce a new Space-Filling Multidimensional Data Visualization (SFMDVis) that can be used to facilitate the viewing, interaction and analysis of the multidimensional data with a fully utilized display space. The existing multidimensional visualizations typically create visual clutter and over-plotting that make it difficult for interaction with data items directly. Our new space filling technique uses horizontal lines to represent multidimensional data items. Each line is logically divided into segments based on color mapping in order to denote the data item with its value. The proposed visualization is space efficient and also avoids the visual clutter and over-plotting problems as we have often observed in other visualizations. In addition, we allow user to interact directly with data on the display which is more intuitive and efficient than other means.},
  added-at = {2016-08-11T14:02:23.000+0200},
  address = {New York, NY, USA},
  author = {Huang, Tze-Haw and Huang, Mao Lin and Nguyen, Quang Vinh and Zhao, Laiping},
  biburl = {https://www.bibsonomy.org/bibtex/2c713750727c7784eb743bc4083698050/vngudivada},
  booktitle = {Proceedings of the 7th International Symposium on Visual Information Communication and Interaction},
  interhash = {04ec96cfc91e5fef3df2332cf9b63f5b},
  intrahash = {c713750727c7784eb743bc4083698050},
  keywords = {EDA SpaceFillingMultidimensionalVisualization Visualization},
  pages = {19:19--19:28},
  publisher = {ACM},
  series = {VINCI '14},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {A Space-Filling Multidimensional Visualization (SFMDVis) for Exploratory Data Analysis},
  url = {http://doi.acm.org/10.1145/2636240.2636841},
  year = 2014
}

@book{russ2011image,
  abstract = {"This guide clearly explains the acquisition and use of digital images in a wide variety of scientific fields. This sixth edition features new sections on selecting a camera with resolution appropriate for use on light microscopes, on the ability of current cameras to capture raw images with high dynamic range, and on imaging in more than two dimensions. It discusses Dmax for X-ray images and combining images with different exposure settings to further extend the dynamic range. This edition also includes a new chapter on shape measurements, a review of new developments in image file searching, and a wide range of new examples and diagrams"--},
  added-at = {2016-08-07T01:20:52.000+0200},
  address = {Boca Raton, FL},
  author = {Russ, John C.},
  biburl = {https://www.bibsonomy.org/bibtex/2a8407b2a63e25d4c6186f84fdd762fff/vngudivada},
  description = {Whether obtained by microscopes, space probes, or the human eye, the same basic tools can be applied to acquire, process, and analyze the data contained in images. Ideal for self study, The Image Processing Handbook, Sixth Edition, first published in 1992, raises the bar once again as the gold-standard reference on this subject. Using extensive new illustrations and diagrams, it offers a logically organized exploration of the important relationship between 2D images and the 3D structures they reveal. Provides Hundreds of Visual Examples in FULL COLOR! The author focuses on helping readers visualize and compare processing and measurement operations and how they are typically combined in fields ranging from microscopy and astronomy to real-world scientific, industrial, and forensic applications. Presenting methods in the order in which they would be applied in a typical workflowfrom acquisition to interpretationthis book compares a wide range of algorithms used to: Improve the appearance, printing, and transmission of an image Prepare images for measurement of the features and structures they reveal Isolate objects and structures, and measure their size, shape, color, and position Correct defects and deal with limitations in images Enhance visual content and interpretation of details This handbook avoids dense mathematics, instead using new practical examples that better convey essential principles of image processing. This approach is more useful to develop readers grasp of how and why to apply processing techniques and ultimately process the mathematical foundations behind them. Much more than just an arbitrary collection of algorithms, this is the rare book that goes beyond mere image improvement, presenting a wide range of powerful example images that illustrate techniques involved in color processing and enhancement. Applying his 50-year experience as a scientist, educator, and industrial consultant, John Russ offers the benefit of his image processing expertise for fields ranging from astronomy and biomedical research to food science and forensics. His valuable insights and guidance continue to make this handbook a must-have reference.},
  interhash = {dbef4bee0ff4b373ac449b267531baf9},
  intrahash = {a8407b2a63e25d4c6186f84fdd762fff},
  keywords = {Handbook ImageProcessing},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {The image processing handbook},
  year = 2011
}

@book{subramaniam2014functional,
  abstract = {
Intermediate level, for programmers fairly familiar with Java, but new to the functional style of programming and lambda expressions.Get ready to program in a whole new way. Functional Programming in Java will help you quickly get on top of the new, essential Java 8 language features and the functional style that will change and improve your code. This short, targeted book will help you make the paradigm shift from the old imperative way to a less error-prone, more elegant, and concise coding style that's also a breeze to parallelize. You'll explore the syntax and semantics of lambda expressions, method and constructor references, and functional interfaces. You'll design and write applications better using the new standards in Java 8 and the JDK. Lambda expressions are lightweight, highly concise anonymous methods backed by functional interfaces in Java 8. You can use them to leap forward into a whole new world of programming in Java. With functional programming capabilities, which have been around for decades in other languages, you can now write elegant, concise, less error-prone code using standard Java. This book will guide you though the paradigm change, offer the essential details about the new features, and show you how to transition from your old way of coding to an improved style.In this book you'll see popular design patterns, such as decorator, builder, and strategy, come to life to solve common design problems, but with little ceremony and effort. With these new capabilities in hand, Functional Programming in Java will help you pick up techniques to implement designs that were beyond easy reach in earlier versions of Java. You'll see how you can reap the benefits of tail call optimization, memoization, and effortless parallelization techniques.Java 8 will change the way you write applications. If you're eager to take advantage of the new features in the language, this is the book for you.What you need:Java 8 with support for lambda expressions and the JDK is required to make use of the concepts and the examples in this book.},
  added-at = {2016-08-08T23:23:52.000+0200},
  author = {Subramaniam, Venkat},
  biburl = {https://www.bibsonomy.org/bibtex/29949e7be8cc9b4c096710ac6f4a7485e/vngudivada},
  interhash = {0695c0eaa4ba76d078a203543a3b2478},
  intrahash = {9949e7be8cc9b4c096710ac6f4a7485e},
  keywords = {Book FunctionalProgramming Java},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Functional programming in Java: harnessing the power of Java 8 Lambda expressions},
  year = 2014
}

@book{sayood2012introduction,
  abstract = {Each edition of Introduction to Data Compression has widely been considered the best introduction and reference text on the art and science of data compression, and the fourth edition continues in this tradition. Data compression techniques and technology are ever-evolving with new applications in image, speech, text, audio, and video. The fourth edition includes all the cutting edge updates the reader will need during the work day and in class. Khalid Sayood provides an extensive introduction to the theory underlying today's compression techniques with detailed instruction for their applications using several examples to explain the concepts. Encompassing the entire field of data compression, Introduction to Data Compression includes lossless and lossy compression, Huffman coding, arithmetic coding, dictionary techniques, context based compression, scalar and vector quantization. Khalid Sayood provides a working knowledge of data compression, giving the reader the tools to develop a complete and concise compression package upon completion of his book. New content added to include a more detailed description of the JPEG 2000 standard New content includes speech coding for internet applications Explains established and emerging standards in depth including JPEG 2000, JPEG-LS, MPEG-2, H.264, JBIG 2, ADPCM, LPC, CELP, MELP, and iLBC Source code provided via companion web site that gives readers the opportunity to build their own algorithms, choose and implement techniques in their own applications.},
  added-at = {2016-08-07T01:01:12.000+0200},
  address = {Waltham, MA},
  author = {Sayood, Khalid},
  biburl = {https://www.bibsonomy.org/bibtex/26336000d1b2050f417e370cbcc194383/vngudivada},
  interhash = {6b4ed8f0a97dd04c8c9b82ba2628927d},
  intrahash = {6336000d1b2050f417e370cbcc194383},
  keywords = {Book DataCompression},
  publisher = {Morgan Kaufmann},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Introduction to data compression},
  year = 2012
}

@book{gennick2011pocket,
  abstract = {Looks at the key aspects of SQL, covering such topics as data types, datetime conversions, datetime functions, numeric conversions, and tables.},
  added-at = {2016-08-09T21:31:18.000+0200},
  address = {Sebastopol, CA},
  author = {Gennick, Jonathan},
  biburl = {https://www.bibsonomy.org/bibtex/2f21968718bb691667d354a8359d85cec/vngudivada},
  interhash = {de23ba51cedbf24577c6091db7c1acbf},
  intrahash = {f21968718bb691667d354a8359d85cec},
  keywords = {Book PocketGuide SQL},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {SQL pocket guide},
  year = 2011
}

@book{carstairsmccarthy2002introduction,
  abstract = {What exactly are words? Are they the things that get listed in dictionaries, or are they the basic units of sentence structure? Andrew Carstairs-McCarthy explores the implications of these different approaches to words in English. He explains the various ways in which words are related to one another, and shows how the history of the English language has affected word structure. Topics include: words, sentences and dictionaries; a word and its parts (roots and affixes); a word and its forms (inflection); a word and its relatives (derivation); compound words; word structure; productivity; and the historical sources of English word formation.},
  added-at = {2016-08-08T22:53:10.000+0200},
  address = {Edinburgh},
  author = {Carstairs-McCarthy, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/28efb4deb88452293dca451bdbe73234a/vngudivada},
  interhash = {35e1c585dcc057a077d1c583b7929721},
  intrahash = {8efb4deb88452293dca451bdbe73234a},
  keywords = {Book EnglishMorphology Morphology},
  publisher = {Edinburgh University Press},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {An introduction to English morphology: words and their structure},
  url = {http://logic.sysu.edu.cn/ebookfull/UploadFiles_7160/200905/2009050617095579.pdf},
  year = 2002
}

@book{zhao2013mining,
  abstract = {This book introduces using R for data mining. Data mining techniques are widely used in government agencies, banks, insurance, retail, telecom, medicine and research. Recently, there is an increasing tendency to do data mining with R, a free software environment for statistical computing and graphics. According to a poll by KDnuggets.com in early 2011, R is the 2nd popular tool for data mining work. By introducing using R for data mining, this book will have a broad audience from both academia and industry. It targets researchers in the field of data mining, postgraduate students who are interested in data mining, and data miners and analysts from industry. For example, many universities have courses on data mining, and the proposed book will be a useful reference for students learning data mining in those courses. There are also many training courses on data mining in industry, such as training by SAS and IBM on data mining. The book will be of interest to the course learners as well. Presents an introduction into using R for data mining applications, covering most popular data mining techniques. Provides code examples and data so that readers can easily learn the techniques. Features case studies in real-world applications to help readers apply the techniques in their work.},
  added-at = {2016-08-07T00:34:34.000+0200},
  address = {[Place of publication not identified]},
  author = {Zhao, Yanchang},
  biburl = {https://www.bibsonomy.org/bibtex/2ee76913bf55f7ee1890194ca0f6c09db/vngudivada},
  interhash = {5d91a3f6b618f3c0c6c4e697c18891e1},
  intrahash = {ee76913bf55f7ee1890194ca0f6c09db},
  keywords = {Book CaseStudy DataQuality R},
  publisher = {Academic Press},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {R and data mining: examples and case studies},
  year = 2013
}

@book{imhoff2003mastering,
  abstract = {A cutting-edge response to Ralph Kimball's challenge to the data warehouse community that answers some tough questions about the effectiveness of the relational approach to data warehousing Written by one of the best-known exponents of the Bill Inmon approach to data warehousing Addresses head-on the tough issues raised by Kimball and explains how to choose the best modeling technique for solving common data warehouse design problems Weighs the pros and cons of relational vs. dimensional modeling techniques Focuses on tough modeling problems, including creating and maintaining keys and modelin.},
  added-at = {2016-08-07T02:01:26.000+0200},
  address = {Indianapolis, Ind.},
  author = {Imhoff, Claudia and Galemmo, Nicholas and Geiger, Jonathan G.},
  biburl = {https://www.bibsonomy.org/bibtex/2a74ed3991bd52f53d7536c1090c9d2e0/vngudivada},
  interhash = {5d4df95401cf84ed35f6bb1631fd993e},
  intrahash = {a74ed3991bd52f53d7536c1090c9d2e0},
  keywords = {Book DataWarehouse Design OLAP},
  note = {Misleading book, prefer Kimball's books},
  publisher = {Wiley Pub.},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Mastering data warehouse design: relational and dimensional techniques},
  year = 2003
}

@book{milovanovic2015python,
  abstract = {Python Data Visualization Cookbook will progress the reader from the point of installing and setting up a Python environment for data manipulation and visualization all the way to 3D animations using Python libraries. Readers will benefit from over 60 precise and reproducible recipes that will guide the reader towards a better understanding of data concepts and the building blocks for subsequent and sometimes more advanced concepts.

Python Data Visualization Cookbook starts by showing how to set up matplotlib and the related libraries that are required for most parts of the book, before moving on to discuss some of the lesser-used diagrams and charts such as Gantt Charts or Sankey diagrams. Initially it uses simple plots and charts to more advanced ones, to make it easy to understand for readers. As the readers will go through the book, they will get to know about the 3D diagrams and animations. Maps are irreplaceable for displaying geo-spatial data, so this book will also show how to build them. In the last chapter, it includes explanation on how to incorporate matplotlib into different environments, such as a writing system, LaTeX, or how to create Gantt charts using Python.},
  added-at = {2016-08-08T23:00:08.000+0200},
  author = {Milovanovic, Igor and Vettigli, Giuseppe and Foures, Dimitry},
  biburl = {https://www.bibsonomy.org/bibtex/2c16aa682be166c7185c2bc4b74568845/vngudivada},
  edition = {Second},
  interhash = {637bc23ad7c212a27469ce6feb25215f},
  intrahash = {c16aa682be166c7185c2bc4b74568845},
  keywords = {Book Cookbook Python Visualization},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Python data visualization cookbook: over 70 recipes, based on the principal concepts of data visualization, to get you started with popular Python libraries},
  year = 2015
}

@techreport{sas2016visualization,
  abstract = {Your business is not static. And neither is your data. But a spreadsheet filled with rows and tables? I think you can see where I'm going.

Data visualization, on the other hand, shows movement. It is movement. You can see where things are headed, whether you’re hitting your goals and what to change if you’re not. You can reveal hidden insights clearly and share the results in a way that everyone understands.

But to achieve data visualization's full potential, there are a few important considerations. Read this paper to learn how to:

Understand the data you are trying to visualize.
Determine what you are trying to visualize and what you want to convey.
Know your users and how they process visual information.
Select the best visual based on the data.},
  added-at = {2016-08-09T15:53:15.000+0200},
  author = {SAS},
  biburl = {https://www.bibsonomy.org/bibtex/256a618adcc4c738e1a3f99bf08466a20/vngudivada},
  interhash = {5cbb3b9714e5ce06edb0893f69d05ed9},
  intrahash = {56a618adcc4c738e1a3f99bf08466a20},
  keywords = {VisualAnalytics Visualization},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Data Visualization Techniques: From Basics to Big Data With SAS® Visual Analytics},
  year = 2016
}

@article{hodge2004survey,
  abstract = {Outlier detection has been used for centuries to detect and, where appropriate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behavior, fraudulent behavior, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics. In this paper, we introduce a survey of contemporary techniques for outlier detection. We identify their respective motivations and distinguish their advantages and disadvantages in a comparative review.},
  added-at = {2016-08-07T01:16:07.000+0200},
  author = {Hodge, Victoria J.},
  biburl = {https://www.bibsonomy.org/bibtex/28b3c8c00157a1a55bef807144a9a34e6/vngudivada},
  interhash = {2c1d880a5fc18d22e6d5f93ce4be2891},
  intrahash = {8b3c8c00157a1a55bef807144a9a34e6},
  keywords = {OutlierDetection Survey},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {A Survey of Outlier Detection Methodologies},
  year = 2004
}

@inproceedings{liu2009interactive,
  abstract = {We are building an interactive, visual text analysis tool that aids users in analyzing a large collection of text. Unlike existing work in text analysis, which focuses either on developing sophisticated text analytic techniques or inventing novel visualization metaphors, ours is tightly integrating state-of-the-art text analytics with interactive visualization to maximize the value of both. In this paper, we focus on describing our work from two aspects. First, we present the design and development of a time-based, visual text summary that effectively conveys complex text summarization results produced by the Latent Dirichlet Allocation (LDA) model. Second, we describe a set of rich interaction tools that allow users to work with a created visual text summary to further interpret the summarization results in context and examine the text collection from multiple perspectives. As a result, our work offers two unique contributions. First, we provide an effective visual metaphor that transforms complex and even imperfect text summarization results into a comprehensible visual summary of texts. Second, we offer users a set of flexible visual interaction tools as the alternatives to compensate for the deficiencies of current text summarization techniques. We have applied our work to a number of text corpora and our evaluation shows the promise of the work, especially in support of complex text analyses.},
  added-at = {2016-08-12T15:08:36.000+0200},
  address = {New York, NY, USA},
  author = {Liu, Shixia and Zhou, Michelle X. and Pan, Shimei and Qian, Weihong and Cai, Weijia and Lian, Xiaoxiao},
  biburl = {https://www.bibsonomy.org/bibtex/26bc1059afe8729de2187c3a9c84bd59c/vngudivada},
  booktitle = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
  interhash = {1c0e2c83dfb366c9d6910e0eee9ba11b},
  intrahash = {6bc1059afe8729de2187c3a9c84bd59c},
  keywords = {TextAnalytics TextVisualization TopicModel VisualAnalytics VisualExploration},
  pages = {543--552},
  publisher = {ACM},
  series = {CIKM '09},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Interactive, Topic-based Visual Text Summarization and Analysis},
  url = {http://doi.acm.org/10.1145/1645953.1646023},
  year = 2009
}

@inproceedings{wei2010tiara,
  abstract = {In this paper, we present a novel exploratory visual analytic system called TIARA (Text Insight via Automated Responsive Analytics), which combines text analytics and interactive visualization to help users explore and analyze large collections of text. Given a collection of documents, TIARA first uses topic analysis techniques to summarize the documents into a set of topics, each of which is represented by a set of keywords. In addition to extracting topics, TIARA derives time-sensitive keywords to depict the content evolution of each topic over time. To help users understand the topic-based summarization results, TIARA employs several interactive text visualization techniques to explain the summarization results and seamlessly link such results to the original text. We have applied TIARA to several real-world applications, including email summarization and patient record analysis. To measure the effectiveness of TIARA, we have conducted several experiments. Our experimental results and initial user feedback suggest that TIARA is effective in aiding users in their exploratory text analytic tasks.},
  added-at = {2016-08-11T04:30:12.000+0200},
  address = {New York, NY, USA},
  author = {Wei, Furu and Liu, Shixia and Song, Yangqiu and Pan, Shimei and Zhou, Michelle X. and Qian, Weihong and Shi, Lei and Tan, Li and Zhang, Qiang},
  biburl = {https://www.bibsonomy.org/bibtex/292cad4ca9b0f59b3e26835cf1b015160/vngudivada},
  booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  interhash = {63e65bf6ca5190c0b3a58ab496617d0d},
  intrahash = {92cad4ca9b0f59b3e26835cf1b015160},
  keywords = {EDA ExploratoryDataAnalysis TextVisualization VisualAnalytics VisualExploration},
  pages = {153--162},
  publisher = {ACM},
  series = {KDD '10},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {TIARA: A Visual Exploratory Text Analytic System},
  year = 2010
}

@inproceedings{sedlmair2011cardiogram,
  abstract = {We present Cardiogram, a visual analytics system that supports automotive engineers in debugging masses of traces each consisting of millions of recorded messages from in-car communication networks. With their increasing complexity, ensuring these safety-critical networks to be error-free has become a major task and challenge for automotive engineers. To overcome shortcomings of current analysis tools, Cardiogram combines visualization techniques with a data preprocessing approach to automatically reduce complexity based on engineers' domain knowledge. In this paper, we provide the findings from an exploratory, three-year field study within a large automotive company, studying current practices of engineers, the challenges they meet and the characteristics for integrating novel visual analytics tools into their work practices. We then introduce Cardiogram, discuss how our field analysis influenced our design decisions, and present a qualitative, long-term, in-depth evaluation. Results of this study showed that our participants successfully used Cardiogram to increase the amount of analyzable information, to externalize domain knowledge, and to provide new insights into trace data. Our design approach finally led to the adoption of Cardiogram into engineers' daily practices.},
  added-at = {2016-08-11T05:14:21.000+0200},
  address = {New York, NY, USA},
  author = {Sedlmair, Michael and Isenberg, Petra and Baur, Dominikus and Mauerer, Michael and Pigorsch, Christian and Butz, Andreas},
  biburl = {https://www.bibsonomy.org/bibtex/282fabc702d9feb9dbafa28e85cff5c44/vngudivada},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  interhash = {9474d87169cd6204e50dcccfcc7b2926},
  intrahash = {82fabc702d9feb9dbafa28e85cff5c44},
  keywords = {AutomativeEngineer EDA VisualAnalytics},
  pages = {1727--1736},
  publisher = {ACM},
  series = {CHI '11},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Cardiogram: Visual Analytics for Automotive Engineers},
  url = {http://doi.acm.org/10.1145/1978942.1979194},
  year = 2011
}

@book{hebert2013learn,
  abstract = {Erlang is a functional programming language used by high paying companies such as Facebook and Amazon. Erlang is growing in popularity because it uniquely allows programmers to manage and modify running code live, thereby avoiding costly downtime. Erlang is also one of the few languages designed for concurrency, meaning it takes full advantage of all the processors in a single machine, or multiple machines connected in a cluster. In the spirit of Learn You a Haskell for Great Good!, Learn You Some Erlang for Great Good! is the highly anticipated book version of author Fred Hebert's extremely.},
  added-at = {2016-08-08T23:37:55.000+0200},
  address = {San Francisco, CA},
  author = {Hébert, Fred},
  biburl = {https://www.bibsonomy.org/bibtex/2d0787c1ec41eb3480f4eafd46a605803/vngudivada},
  interhash = {5a3b701746df5786d15d5350fb489ca8},
  intrahash = {d0787c1ec41eb3480f4eafd46a605803},
  keywords = {Book Erlang},
  publisher = {No Starch Press},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Learn you some Erlang for great good! a beginner's guide},
  year = 2013
}

@book{silverman2013pocket,
  abstract = {Are you looking for a new version control system? Perhaps what you're using now is too cumbersome, or you just want to try something new to manage a pet project. With Git by Ryan Hodson, you can get up and running with one of the fastest-spreading revision control systems out there. Complete with vivid diagrams, clear code samples, and a careful walk-through of primary features, this free e-book is your quick guide to how Git operates, what its advantages are, and how you can incorporate it into your own workflow. This updated and expanded second edition of Book provides a user-friendly introduction to the subject, Taking a clear structural framework, it guides the reader through the subject's core elements. A flowing writing style combines with the use of illustrations and diagrams throughout the text to ensure the reader understands even the most complex of concepts. This succinct and enlightening overview is a required reading for all those interested in the subject . We hope you find this book useful in shaping your future career & Business.},
  added-at = {2016-08-09T21:36:49.000+0200},
  address = {Sebastopol, CA},
  author = {Silverman, Richard E.},
  biburl = {https://www.bibsonomy.org/bibtex/26798f7a9082baf14cd1dda6d51527bef/vngudivada},
  interhash = {40fc2dbba16855249ba8bcdd1271df75},
  intrahash = {6798f7a9082baf14cd1dda6d51527bef},
  keywords = {Book Git PocketGuide},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Git pocket guide},
  year = 2013
}

@book{yu2015automatic,
  abstract = {This book provides a comprehensive overview of the recent advancement in the field of automatic speech recognition with a focus on deep learning models including deep neural networks and many of their variants. This is the first automatic speech recognition book dedicated to the deep learning approach. In addition to the rigorous mathematical treatment of the subject, the book also presents insights and theoretical foundation of a series of highly successful deep learning models.},
  added-at = {2016-08-09T22:17:06.000+0200},
  author = {Yu, Dong and Deng, Li},
  biburl = {https://www.bibsonomy.org/bibtex/24b55800490328c13ab0d7fe8b0ef8bbe/vngudivada},
  interhash = {fbaf07e52074d95a31a2d8b43a4e6f91},
  intrahash = {4b55800490328c13ab0d7fe8b0ef8bbe},
  keywords = {Book SpeechRecognition},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Automatic recognition : speech a deep learning approach},
  year = 2015
}

@book{rossant2014ipython,
  abstract = {With its widely acclaimed web-based notebook, IPython is an ideal gateway to data analysis and numerical computing in Python. This book contains many ready-to-use focused recipes for high-performance scientific computing and data analysis. You will learn how to: code better by writing high-quality, readable, and well-tested programs; profiling and optimizing your code, and conducting reproducible interactive computing experiments; master all of the new features of the IPython notebook, including the interactive HTML/JavaScript widgets; analyze data with Bayesian and frequentist statistics (Pandas, PyMC, and R), and learn from data with machine learning (scikit-learn); gain insight into signals, images, and sounds with SciPy, scikit-image, and OpenCV; write blazingly fast Python programs with NumPy, PyTables, ctypes, Numba, Cython, OpenMP, GPU programming (CUDA and OpenCL), parallel IPython, MPI, and many more. --},
  added-at = {2016-08-07T04:13:47.000+0200},
  author = {Rossant, Cyrille},
  biburl = {https://www.bibsonomy.org/bibtex/2b7656aa6fa88e5f84d362080358d74dd/vngudivada},
  interhash = {91bc138966aa1fb477a7ecc496215df3},
  intrahash = {b7656aa6fa88e5f84d362080358d74dd},
  keywords = {Book DataVisualization IPython},
  publisher = {Packt Publishing},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {IPython Interactive Computing and Visualization Cookbook},
  year = 2014
}

@book{ghedira2013constraint,
  abstract = {A Constraint Satisfaction Problem (CSP) consists of a set ofvariables, a domain of values for each variable and a set ofconstraints. The objective is to assign a value for each variablesuch that all constraints are satisfied. CSPs continue to receiveincreased attention because of both their high complexity and theiromnipresence in academic, industrial and even reallife problems.This is why they are the subject of intense research in bothartificial intelligence and operations research. This bookintroduces the classic CSP and details severalextensions/improvements of both formalisms and techniques in orderto tackle a large variety of problems. Consistency, flexible,dynamic, distributed and learning aspects are discussed andillustrated using simple examples such as the nqueen problem. Contents 1. Foundations of CSP.2. Consistency Reinforcement Techniques.3. CSP Solving Algorithms.4. Search Heuristics.5. Learning Techniques.6. Maximal Constraint Satisfaction Problems.7. Constraint Satisfaction and Optimization Problems.8. Distibuted Constraint Satisfaction Problems. About the Authors Khaled Ghedira is the general managing director of the TunisScience City in Tunisia, Professor at the University of Tunis, aswell as the founding president of the Tunisian Association ofArtificial Intelligence and the founding director of the SOIEresearch laboratory. His research areas include MAS, CSP, transportand production logistics, metaheuristics and security inM/Egovernment. He has led several national and internationalresearch projects, supervised 30 PhD theses and more than 50Master s theses, coauthored about 300 journal, conferenceand book research papers, written two text books on metaheuristicsand production logistics and coauthored three others.},
  added-at = {2016-08-07T00:44:35.000+0200},
  address = {Hoboken, NJ},
  author = {Ghédira, Khaled},
  biburl = {https://www.bibsonomy.org/bibtex/2805ca5aa31eb764ca34ff618ffa538ef/vngudivada},
  interhash = {a9b4a426b8499683e0537c2ff8689c66},
  intrahash = {805ca5aa31eb764ca34ff618ffa538ef},
  keywords = {Book ConstraintProgramming ConstraintSatisfactionProblem},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Constraint satisfaction problems CSP formalisms and techniques},
  year = 2013
}

@inproceedings{vatrapu2011towards,
  abstract = {The focus of this paper is to delineate and discuss design considerations for supporting teachers' dynamic diagnostic decision-making in classrooms of the 21st century. Based on the Next Generation Teaching Education and Learning for Life (NEXT-TELL) European Commission integrated project, we envision classrooms of the 21st century to (a) incorporate 1:1 computing, (b) provide computational as well as methodological support for teachers to design, deploy and assess learning activities and (c) immerse students in rich, personalized and varied learning activities in information ecologies resulting in high-performance, high-density, high-bandwidth, and data-rich classrooms. In contrast to existing research in educational data mining and learning analytics, our vision is to employ visual analytics techniques and tools to support teachers dynamic diagnostic pedagogical decision-making in real-time and in actual classrooms. The primary benefits of our vision is that learning analytics becomes an integral part of the teaching profession so that teachers can provide timely, meaningful, and actionable formative assessments to on-going learning activities in-situ. Integrating emerging developments in visual analytics and the established methodological approach of design-based research (DBR) in the learning sciences, we introduce a new method called "Teaching Analytics" and explore a triadic model of teaching analytics (TMTA). TMTA adapts and extends the Pair Analytics method in visual analytics which in turn was inspired by the pair programming model of the extreme programming paradigm. Our preliminary vision of TMTA consists of a collocated collaborative triad of a Teaching Expert (TE), a Visual Analytics Expert (VAE), and a Design-Based Research Expert (DBRE) analyzing, interpreting and acting upon real-time data being generated by students' learning activities by using a range of visual analytics tools. We propose an implementation of TMTA using open learner models (OLM) and conclude with an outline of future work},
  added-at = {2016-08-11T14:33:54.000+0200},
  address = {New York, NY, USA},
  author = {Vatrapu, Ravi and Teplovs, Chris and Fujita, Nobuko and Bull, Susan},
  biburl = {https://www.bibsonomy.org/bibtex/24b0469b4a1cac2a9eda8d32f3aa940ed/vngudivada},
  booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
  interhash = {eb639c3520f6a7eb558f881085062fad},
  intrahash = {4b0469b4a1cac2a9eda8d32f3aa940ed},
  keywords = {DiagnosticAnalytics EDM LearningAnalytics VisualAnalytics},
  pages = {93--98},
  publisher = {ACM},
  series = {LAK '11},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Towards Visual Analytics for Teachers' Dynamic Diagnostic Pedagogical Decision-making},
  url = {http://doi.acm.org/10.1145/2090116.2090129},
  year = 2011
}

@book{gaddis2016starting,
  abstract = {KEY BENEFIT: Gaddis introduces Java with an accessible, step-by-step style that helps beginners understand how to become skilled programmers. KEY TOPICS: Introduction to Computers and Java; Java Fundamentals; Decision Structures; Loops and Files; Methods; A First Look at Classes; A First Look at GUI Applications; Arrays and the ArrayList Class; A Second Look at Classes and Objects; Text Processing and More About Wrapper Clauses; Inheritance; Exceptions and Advanced File I/O; Advanced GUI Applications; Applets and More; Recursion. MARKET: Ideal for beginners to Java programming.},
  added-at = {2016-08-09T20:33:16.000+0200},
  author = {Gaddis, Tony},
  biburl = {https://www.bibsonomy.org/bibtex/20ae8c64c865a2a49dbfb3c70028b519c/vngudivada},
  interhash = {97290a5d1e29577e4f37ab4af6159025},
  intrahash = {0ae8c64c865a2a49dbfb3c70028b519c},
  keywords = {Book Java},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Starting out with Java},
  year = 2016
}

@book{peck2013tableau,
  abstract = {Present a unified view of complex BI using the entire Tableau 8 toolkit Create and distribute dynamic, feature-rich data visualizations and highly interactive BI dashboards-quickly and easily! Tableau 8: The Official Guide provides the hands-on instruction and best practices you need to meet your business intelligence objectives and drive better decision making. Discover how to work from the Tableau GUI, load BI from disparate sources, drag and drop to analyze data, set up custom visualizations, and build robust dashboards. This practical guide shows you, step by step, how to design and publish meaningful business communications to end users across your enterprise. Navigate the Tableau user interface and data window Connect to spreadsheets, databases, and other sources Select data fields and drag them to desired screen locations Work with pre-defined visualizations and sample workbooks Display background maps and perform geographic analysis Add calculated fields, graphs, charts, tables, and statistics Combine multiple data sources into real-time dashboards Export your visualizations to the Web or in various file formats Electronic content includes: Videos that demonstrate the techniques presented in the book Sample Tableau workbooks},
  added-at = {2016-08-17T00:23:15.000+0200},
  address = {San Francisco, CA},
  author = {Peck, George},
  biburl = {https://www.bibsonomy.org/bibtex/2af7ef7b1d0e4e3bf3fba72b95b255198/vngudivada},
  interhash = {ce268c5a77706eccb207b3b08ce18312},
  intrahash = {af7ef7b1d0e4e3bf3fba72b95b255198},
  keywords = {Book Tableau VisualAnalytics},
  publisher = {McGraw-Hill},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Tableau 8},
  year = 2013
}

@book{vanderhart2014clojure,
  abstract = {With more than 150 detailed recipes, this cookbook shows experienced Clojure developers how to solve a variety of programming tasks with this JVM language. The solutions cover everything from building dynamic websites and working with databases to network communication, cloud computing, and advanced testing strategies. And more than 60 of the world's best Clojurians contributed recipes. Each recipe includes code that you can use right away, along with a discussion on how and why the solution works, so you can adapt these patterns, approaches, and techniques to situations not specifically covered in this cookbook. Master built-in primitive and composite data structures Create, develop and publish libraries, using the Leiningen tool Interact with the local computer that's running your application Manage network communication protocols and libraries Use techniques for connecting to and using a variety of databases Build and maintain dynamic websites, using the Ring HTTP server library Tackle application tasks such as packaging, distributing, profiling, and logging Take on cloud computing and heavyweight distributed data crunching Dive into unit, integration, simulation, and property-based testing Clojure Cookbook is a collaborative project with contributions from some of the world's best Clojurians, whose backgrounds range from aerospace to social media, banking to robotics, AI research to e-commerce.},
  added-at = {2016-08-08T23:32:30.000+0200},
  address = {Sebastapol, CA},
  author = {VanderHart, Luke and Neufeld, Ryan},
  biburl = {https://www.bibsonomy.org/bibtex/29d63aba2e063c422adecb98ae49b9e90/vngudivada},
  interhash = {c247cc7ec34933f9fcefe44d8502424a},
  intrahash = {9d63aba2e063c422adecb98ae49b9e90},
  keywords = {Book Clojure},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Clojure cookbook},
  year = 2014
}

@inproceedings{gibson2014cognitive,
  abstract = {Incorporating a learner's level of cognitive processing into Learning Analytics presents opportunities for obtaining rich data on the learning process. We propose a framework called COPA that provides a basis for mapping levels of cognitive operation into a learning analytics system. We utilise Bloom's taxonomy, a theoretically respected conceptualisation of cognitive processing, and apply it in a flexible structure that can be implemented incrementally and with varying degree of complexity within an educational organisation. We outline how the framework is applied, and its key benefits and limitations. Finally, we apply COPA to a University undergraduate unit, and demonstrate its utility in identifying key missing elements in the structure of the course.},
  added-at = {2016-08-11T14:46:45.000+0200},
  address = {New York, NY, USA},
  author = {Gibson, Andrew and Kitto, Kirsty and Willis, Jill},
  biburl = {https://www.bibsonomy.org/bibtex/217c54f93fb9a909003d1b9ba4c4d9552/vngudivada},
  booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
  interhash = {eef5c79b0e539d6da0cd9fa56794a463},
  intrahash = {17c54f93fb9a909003d1b9ba4c4d9552},
  keywords = {CognitiveModel LearningAnalytics},
  pages = {212--216},
  publisher = {ACM},
  series = {LAK '14},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {A Cognitive Processing Framework for Learning Analytics},
  url = {http://doi.acm.org/10.1145/2567574.2567610},
  year = 2014
}

@book{fisher2014dictionary,
  abstract = {"The subjects of computer vision and image processing have advanced very much over the past 5 years, and keeping up to date in this fast-moving subject area can be very challenging.The Second Edition of the Dictionary of Computer Vision & Image Processing features over 3000 of the most commonly used terms in the field of computer vision, machine learning, image analysis and image processing, with approximately 1000 new terms having been identified for inclusion since the current edition was published. Revised to include an additional 1000 new terms to reflect current updates, which includes a significantly increased focus on image processing terms, as well as machine learning terms. Now includes citations to the terms to point to further reading about each term. Easy to follow structure with grouping of terms for ease of reference, comprehensive selection of terms, readers can use with confidence as written by leading researchers in the field. A new feature of this edition is the supplementary regrouping of related terms by concept. Illustrated throughout with associated diagrams and images provided to help clarify explanation. Fully revised and updated this Dictionary is ideal for Final year undergraduates, Masters and PhD students; and early stage career researchers in computer vision and image processing"--},
  added-at = {2016-08-07T00:54:59.000+0200},
  author = {Fisher, R. B.},
  biburl = {https://www.bibsonomy.org/bibtex/2c512be716b5cd9900cfb97ce5fcdae61/vngudivada},
  interhash = {bb2f1b3f6225c7b071e95bb28e99c030},
  intrahash = {c512be716b5cd9900cfb97ce5fcdae61},
  keywords = {Book ComputerVision Dictionary ImageProcessing},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Dictionary of computer vision and image processing},
  year = 2014
}

@book{solomon2008concise,
  abstract = {This clearly written book offers readers a succinct foundation to the most important topics in the field of data compression. Part I presents the basic approaches to data compression and describes a few popular techniques and methods that are commonly used to compress data. The reader will discover essential concepts. Part II concentrates on advanced techniques, such as arithmetic coding, orthogonal transforms, subband transforms and Burrows-Wheeler transform. This book is the perfect reference for advanced undergraduates in computer science and requires a minimum of mathematics. An author-maintained website provides errata and auxiliary material.},
  added-at = {2016-08-07T01:03:35.000+0200},
  author = {Solomon, D},
  biburl = {https://www.bibsonomy.org/bibtex/2197756814fba08caeb86619b5284fc25/vngudivada},
  interhash = {21f8c1c2e844404468b9a318ce61a70f},
  intrahash = {197756814fba08caeb86619b5284fc25},
  keywords = {Book DataCompression},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {A Concise Introduction to Data Compression},
  year = 2008
}

@book{ma2013advances,
  abstract = {This book covers a fast-growing topic in great depth and focuses on the technologies and applications of probabilistic data management. It aims to provide a single account of current studies in probabilistic data management. The objective of the book is to provide the state of the art information to researchers, practitioners, and graduate students of information technology of intelligent information processing, and at the same time serving the information technology professional faced with non-traditional applications that make the application of conventional approaches difficult or impossible.},
  added-at = {2016-08-09T22:34:22.000+0200},
  address = {New York, NY},
  author = {Ma, Zongmin and Yan, Li},
  biburl = {https://www.bibsonomy.org/bibtex/2f7bf1629dc9ef8f0876a423c0843ef4f/vngudivada},
  interhash = {88b6400140fa356160920162334738d7},
  intrahash = {f7bf1629dc9ef8f0876a423c0843ef4f},
  keywords = {Book DBMS ProbabilisticDatabase},
  publisher = {Springer},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Advances in probabilistic databases for uncertain information management},
  year = 2013
}

@book{mobus2014principles,
  abstract = {This pioneering text provides a comprehensive introduction to systems structure, function, and modeling as applied in all fields of science and engineering. Systems understanding is increasingly recognized as a key to a more holistic education and greater problem solving skills, and is also reflected in the trend toward interdisciplinary approaches to research on complex phenomena. While the concepts and components of systems science will continue to be distributed throughout the various disciplines, undergraduate degree programs in systems science are also being developed, including at the authors own institutions. However, the subject is approached, systems science as a basis for understanding the components and drivers of phenomena at all scales should be viewed with the same importance as a traditional liberal arts education.Principles of Systems Science contains many graphs, illustrations, side bars, examples, and problems to enhance understanding. From basic principles of organization, complexity, abstract representations, and behavior (dynamics) to deeper aspects such as the relations between information, knowledge, computation, and system control, to higher order aspects such as auto-organization, emergence and evolution, the book provides an integrated perspective on the comprehensive nature of systems. It ends with practical aspects such as systems analysis, computer modeling, and systems engineering that demonstrate how the knowledge of systems can be used to solve problems in the real world. Each chapter is broken into parts beginning with qualitative descriptions that stand alone for students who have taken intermediate algebra. The second part presents quantitative descriptions that are based on pre-calculus and advanced algebra, providing a more formal treatment for students who have the necessary mathematical background. Numerous examples of systems from every realm of life, including the physical and biological sciences, humanities, social sciences, engineering, pre-med and pre-law, are based on the fundamental systems concepts of boundaries, components as subsystems, processes as flows of materials, energy, and messages, work accomplished, functions performed, hierarchical structures, and more. Understanding these basics enables further understanding both of how systems endure and how they may become increasingly complex and exhibit new properties or characteristics.Serves as a textbook for teaching systems fundamentals in any discipline or for use in an introductory course in systems science degree programsAddresses a wide range of audiences with different levels of mathematical sophisticationIncludes open-ended questions in special boxes intended to stimulate integrated thinking and class discussion Describes numerous examples of systems in science and societyCaptures the trend towards interdisciplinary research and problem solving.},
  added-at = {2016-08-09T21:55:32.000+0200},
  author = {Mobus, George E. and Kalton, Michael C.},
  biburl = {https://www.bibsonomy.org/bibtex/23a3a3c707c3b874715775c460e1c23dd/vngudivada},
  interhash = {316d0d9636685b39f57a2f3bf997bb72},
  intrahash = {3a3a3c707c3b874715775c460e1c23dd},
  keywords = {Book ComplexSystem SystemsScience},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Principles of systems science},
  year = 2014
}

@book{rossant2013learning,
  abstract = {A practical hands-on guide which focuses on interactive programming, numerical computing, and data analysis with IPython. This book is for Python developers who use Python as a scripting language or for software development, and are interested in learning IPython for increasing their productivity during interactive sessions in the console. Knowledge of Python is required, whereas no knowledge of IPython is necessary.},
  added-at = {2016-08-07T04:14:49.000+0200},
  author = {Rossant, Cyrille},
  biburl = {https://www.bibsonomy.org/bibtex/207b56eec99a49c629b0891b1800ec61d/vngudivada},
  interhash = {96d66804f6721dacbf58e5884a4af6fe},
  intrahash = {07b56eec99a49c629b0891b1800ec61d},
  keywords = {Book DataVisualization IPython},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Learning IPython for interactive computing and data visualization},
  year = 2013
}

@misc{webster2016integrated,
  abstract = {Companies are collecting more data than ever. But, given how difficult it is to unify the many internal and external data streams they’ve built, more data doesn’t necessarily translate into better analytics. The real challenge is to provide deep and broad access to “a single source of truth” in their data that the typically slow ETL process for data warehousing cannot achieve. More than just fast access, analysts need the ability to explore data at a granular level.

In this O’Reilly report, author Courtney Webster presents a roadmap to data centralization that will help your organization make data accessible, flexible, and actionable. Building a genuine data-driven culture depends on your company’s ability to quickly act upon new findings.},
  added-at = {2016-08-07T00:37:13.000+0200},
  author = {Webster, Courtney},
  biburl = {https://www.bibsonomy.org/bibtex/2a5ddc4edb6bc77e3b98601cfabf934fa/vngudivada},
  interhash = {430e177bea2cf273f20389ff19a65beb},
  intrahash = {a5ddc4edb6bc77e3b98601cfabf934fa},
  keywords = {Analytics},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Integrated Analytics Platforms and Principles for Centralizing Your Data},
  year = 2016
}

@misc{cutler2016trees,
  added-at = {2016-08-07T00:40:22.000+0200},
  author = {Cutler, Adele},
  biburl = {https://www.bibsonomy.org/bibtex/2876e344483df387179d6fd829566b0c8/vngudivada},
  interhash = {b5390c65ad78cace4aee24f3a7543f0d},
  intrahash = {876e344483df387179d6fd829566b0c8},
  keywords = {DecisionTree RandomForest},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Trees and Random Forests},
  year = 2016
}

@book{pogue2014yosemite,
  abstract = {With Yosemite, Apple has unleashed the most innovative version of OS X yet—and once again, David Pogue brings his expertise and humor to the #1 bestselling Mac book. Mac OS X 10.10 includes more innovations from the iPad and adds a variety of new features throughout the operating system. This updated edition covers it all with something new on practically every page.},
  added-at = {2016-08-09T21:48:17.000+0200},
  address = {Sebastopol},
  author = {Pogue, David},
  biburl = {https://www.bibsonomy.org/bibtex/21eafaefe910e0c0765749fd908cbfa8d/vngudivada},
  interhash = {a4682b7855c25afbc5dd9612d293d5fb},
  intrahash = {1eafaefe910e0c0765749fd908cbfa8d},
  keywords = {Book MacOSX},
  publisher = {O'Reilly & Associates},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {OS X Yosemite: The Missing Manual},
  year = 2014
}

@book{murray2013interactive,
  abstract = {Author Scott Murray teaches you the fundamental concepts and methods of D3, a JavaScript library that lets you express data visually in a web browser -- "Create and publish your own interactive data visualization projects on the Web--even if you have little or no experience with data visualization or web development. It's easy and fun with this practical, hands-on introduction. Author Scott Murray teaches you the fundamental concepts and methods of D3, a JavaScript library that lets you express data visually in a web browser. Along the way, you'll expand your web programming skills, using tools such as HTML and JavaScript. This step-by-step guide is ideal whether you're a designer or visual artist with no programming experience, a reporter exploring the new frontier of data journalism, or anyone who wants to visualize and share data. Learn HTML, CSS, JavaScript, and SVG basics; dynamically generate web page elements from your data--and choose visual encoding rules to style them; create bar charts, scatter plots, pie charts, stacked bar charts, and force-directed layouts; use smooth, animated transitions to show changes in your data; introduce interactivity to help users explore data through different views; create customized geographic maps with data; and explore hands-on with downloadable code and over 100 examples."--Publisher's descirption.},
  added-at = {2016-08-07T04:11:43.000+0200},
  address = {Sebastopol, CA},
  author = {Murray, Scott},
  biburl = {https://www.bibsonomy.org/bibtex/25760fd389cef30f19d7fc673fb74af0c/vngudivada},
  interhash = {3907345a0739bf80eb4e31e2a5691ad1},
  intrahash = {5760fd389cef30f19d7fc673fb74af0c},
  keywords = {Book DataVisualization Web},
  publisher = {O'Reilly Media},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Interactive data visualization for the web},
  year = 2013
}

@book{elmasri2011database,
  abstract = {Clear explanations of theory and design, broad coverage of models and real systems, and an up-to-date introduction to modern database technologies result in a leading introduction to database systems. With fresh new problems and a new lab manual, students get more opportunities to practice the fundamentals of design and implementation. More real-world examples serve as engaging, practical illustrations of database concepts. The Fifth Edition maintains its coverage of the most popular database topics, including SQL, security, data mining, and contains a new chapter on web script programming for.},
  added-at = {2016-08-11T05:26:27.000+0200},
  address = {Boston, MA},
  author = {Elmasri, Ramez and Navathe, Sham},
  biburl = {https://www.bibsonomy.org/bibtex/2751da71c795ce30c25e21c8c64f03de2/vngudivada},
  interhash = {85efc5f85f9a0f17ea10265b248e7f67},
  intrahash = {751da71c795ce30c25e21c8c64f03de2},
  keywords = {Book DBMS},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Database systems: models, languages, design and application programming},
  year = 2011
}

@book{rasch2011optimal,
  abstract = {"Experimental design is often overlooked in the literature of applied and mathematical statistics: statistics is taught and understood as merely a collection of methods for analyzing data. Consequently, experimenters seldom think about optimal design, including prerequisites such as the necessary sample size needed for a precise answer for an experimental question. Providing a concise introduction to experimental design theory, Optimal Experimental Design with R: Introduces the philosophy of experimental design Provides an easy process for constructing experimental designs and calculating necessary sample size using R programs Teaches by example using a custom made R program package: OPDOE. Consisting of detailed, data-rich examples, this book introduces experimenters to the philosophy of experimentation, experimental design, and data collection. It gives researchers and statisticians guidance in the construction of optimum experimental designs using R programs, including sample size calculations, hypothesis testing, and confidence estimation. A final chapter of in-depth theoretical details is included for interested mathematical statisticians"--Back cover.},
  added-at = {2016-08-07T00:32:55.000+0200},
  address = {Boca Raton, FL},
  author = {Rasch, Dieter},
  biburl = {https://www.bibsonomy.org/bibtex/2e1d5ee9de99a215ba22970998b307530/vngudivada},
  interhash = {be763ef3daa035a247e27445ad6b0aad},
  intrahash = {e1d5ee9de99a215ba22970998b307530},
  keywords = {Book ExperimentalDesign R Statistics},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Optimal experimental design with R},
  year = 2011
}

@book{kimball2008warehouse,
  abstract = {'The Data Warehouse Lifecycle Toolkit' has become the industry standard for data warehouse design, development, and management. This second edition brings the content up-to-date to reflect the latest set of recommended best practices.},
  added-at = {2016-08-07T05:18:09.000+0200},
  address = {Indianapolis, IN},
  author = {Kimball, Ralph},
  biburl = {https://www.bibsonomy.org/bibtex/28e58e5316228eca6d398c1854c837408/vngudivada},
  description = {A thorough update to the industry standard for designing, developing, and deploying data warehouse and business intelligence systems

The world of data warehousing has changed remarkably since the first edition of The Data Warehouse Lifecycle Toolkit was published in 1998. In that time, the data warehouse industry has reached full maturity and acceptance, hardware and software have made staggering advances, and the techniques promoted in the premiere edition of this book have been adopted by nearly all data warehouse vendors and practitioners. In addition, the term "business intelligence" emerged to reflect the mission of the data warehouse: wrangling the data out of source systems, cleaning it, and delivering it to add value to the business.

Ralph Kimball and his colleagues have refined the original set of Lifecycle methods and techniques based on their consulting and training experience. The authors understand first-hand that a data warehousing/business intelligence (DW/BI) system needs to change as fast as its surrounding organization evolves. To that end, they walk you through the detailed steps of designing, developing, and deploying a DW/BI system. You'll learn to create adaptable systems that deliver data and analyses to business users so they can make better business decisions.

With substantial new and updated content, this second edition of The Data Warehouse Lifecycle Toolkit again sets the standard in data warehousing for the next decade. It shows you how to:

Identify and prioritize data warehouse opportunities
Create an architecture plan and select products
Design a powerful, flexible, dimensional model
Build a robust ETL system
Develop BI applications to deliver data to business users
Deploy and sustain a healthy DW/BI environment
The authors are members of the Kimball Group. Each has focused on data warehousing and business intelligence consulting and education for more than 15 years; most have written other books in the Toolkit series. Learn more about the Kimball Group and Kimball University at www.kimballgroup.com.},
  interhash = {dad647d3b4d439b7a955516d7a5c2c7b},
  intrahash = {8e58e5316228eca6d398c1854c837408},
  keywords = {Book DataWarehouse LifeCycle},
  publisher = {Wiley Pub.},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {The data warehouse lifecycle toolkit},
  year = 2008
}

@book{almudevar2014approximate,
  abstract = {Iterative algorithms often rely on approximate evaluation techniques, which may include statistical estimation, computer simulation or functional approximation. This volume presents methods for the study of approximate iterative algorithms, providing tools for the derivation of error bounds and convergence rates, and for the optimal design of such algorithms. Techniques of functional analysis are used to derive analytical relationships between approximation methods and convergence properties for general classes of algorithms. The volume provides the necessary background in functional analys.},
  added-at = {2016-08-07T00:31:36.000+0200},
  author = {Almudevar, Anthony},
  biburl = {https://www.bibsonomy.org/bibtex/22ad2a03ab84154fd11f91b031a772b91/vngudivada},
  interhash = {217e4dc7168fa16a9bd40093a68e9dde},
  intrahash = {2ad2a03ab84154fd11f91b031a772b91},
  keywords = {ApproximateAlgorithms Book IterativeAlgorithms},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Approximate iterative algorithms},
  year = 2014
}

@inproceedings{daquin2013assessing,
  abstract = {In this research note, we present a preliminary study of available web datasets related to education, providing an overview of this area and, more importantly, highlighting how such linked datasets form a globally addressable network of resources for education. As expected, a certain level of heterogeneity was found. We therefore also show how a minor integration effort can improve the global cohesion of such a network of educational web data.},
  added-at = {2016-08-14T02:24:34.000+0200},
  address = {New York, NY, USA},
  author = {d'Aquin, Mathieu and Adamou, Alessandro and Dietze, Stefan},
  biburl = {https://www.bibsonomy.org/bibtex/2b50c351f0ee55907a73c74abb79411e2/vngudivada},
  booktitle = {Proceedings of the 5th Annual ACM Web Science Conference},
  interhash = {6a370e858f0015949faba6c9e51ceec0},
  intrahash = {b50c351f0ee55907a73c74abb79411e2},
  keywords = {LearningAnalytics LinkedData},
  pages = {43--46},
  publisher = {ACM},
  series = {WebSci '13},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Assessing the Educational Linked Data Landscape},
  url = {http://doi.acm.org/10.1145/2464464.2464487},
  year = 2013
}

@book{knuth1997computer,
  abstract = {This first volume in the series begins with basic programming concepts and techniques, then focuses more particularly on information structures—the representation of information inside a computer, the structural relationships between data elements and how to deal with them efficiently. Elementary applications are given to simulation, numerical methods, symbolic computing, software and system design. Dozens of simple and important algorithms and techniques have been added to those of the previous edition. The section on mathematical preliminaries has been extensively revised to match present trends in research.},
  added-at = {2016-08-11T05:36:29.000+0200},
  address = {Reading, Massachusetts},
  author = {Knuth, Don},
  biburl = {https://www.bibsonomy.org/bibtex/2f78f6a6476c0d39c67cef6418a39aebe/vngudivada},
  edition = {Third},
  interhash = {94be5edf9a5006fca5be5ff9c63e1570},
  intrahash = {f78f6a6476c0d39c67cef6418a39aebe},
  keywords = {Algorithms Book Knuth},
  publisher = {Addison Wesley},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {The Art of Computer Programming: Fundamental Algorithms},
  year = 1997
}

@book{ohri2014cloud,
  abstract = {R for Cloud Computing looks at some of the tasks performed by business analysts on the desktop (PC era) and helps the user navigate the wealth of information in R and its 4000 packages as well as transition the same analytics using the cloud. With this information the reader can select both cloud vendors and the sometimes confusing cloud ecosystem as well as the R packages that can help process the analytical tasks with minimum effort and cost, and maximum usefulness and customization. The use of Graphical User Interfaces (GUI) and Step by Step screenshot tutorials is emphasized in this book to lessen the famous learning curve in learning R and some of the needless confusion created in cloud computing that hinders its widespread adoption. This will help you kick-start analytics on the cloud including chapters on cloud computing, R, common tasks performed in analytics, scrutiny of big data analytics, and setting up and navigating cloud providers. Readers are exposed to a breadth of cloud computing choices and analytics topics without being buried in needless depth. The included references and links allow the reader to pursue business analytics on the cloud easily. It is aimed at practical analytics and is easy to transition from existing analytical set up to the cloud on an open source system based primarily on R. This book is aimed at industry practitioners with basic programming skills and students who want to enter analytics as a profession. Note the scope of the book is neither statistical theory nor graduate level research for statistics, but rather it is for business analytics practitioners. It will also help researchers and academics but at a practical rather than conceptual level. The R statistical software is the fastest growing analytics platform in the world, and is established in both academia and corporations for robustness, reliability and accuracy. The cloud computing paradigm is firmly established as the next generation of computing from microprocessors to desktop PCs to cloud.},
  added-at = {2016-08-09T20:39:01.000+0200},
  author = {Ohri, A.},
  biburl = {https://www.bibsonomy.org/bibtex/20385e7a244c09a2e32f769069ba3a3bd/vngudivada},
  interhash = {82b719467522cd254ec85acf05149ec9},
  intrahash = {0385e7a244c09a2e32f769069ba3a3bd},
  keywords = {Book CloudComputing R},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {R for cloud computing : an approach for data scientists},
  year = 2014
}

@book{grune2008parsing,
  abstract = {This second edition of Grune and Jacobs brilliant work presents new developments and discoveries that have been made in the field. Parsing, also referred to as syntax analysis, has been and continues to be an essential part of computer science and linguistics. Parsing techniques have grown considerably in importance, both in computer science, ie. advanced compilers often use general CF parsers, and computational linguistics where such parsers are the only option. They are used in a variety of software products including Web browsers, interpreters in computer devices, and data compression programs; and they are used extensively in linguistics.},
  added-at = {2016-08-09T22:37:41.000+0200},
  address = {New York, NY},
  author = {Grune, Dick and Jacobs, Ceriel J. H.},
  biburl = {https://www.bibsonomy.org/bibtex/2d771b410490f499cbdb0aec86fc3398c/vngudivada},
  interhash = {c7286aa2623e235bcc53aa074acae655},
  intrahash = {d771b410490f499cbdb0aec86fc3398c},
  keywords = {Book Parsing SyntaxAnalysis},
  publisher = {Springer},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Parsing techniques: a practical guide},
  year = 2008
}

@book{wilson2014nodejs,
  abstract = {Get to the forefront of server-side JavaScript programming by writing compact, robust, fast, networked Node applications that scale. Ready to take JavaScript beyond the browser, explore dynamic languages features and embrace evented programming? Explore the fun, growing repository of Node modules provided by npm. Work with multiple protocols, load-balanced RESTful web services, express, 0MQ, Redis, CouchDB, and more. Develop production-grade Node applications fast. JavaScript is the backbone of the modern web, powering nearly every web app's user interface. Node.js is JavaScript for the server. This book shows you how to develop small, fast, low-profile, useful, networked applications. You'll write asynchronous, non-blocking code using Node's style and patterns. You'll cluster and load balance your services with Node core features and third-party tools. You'll work with many protocols, creating RESTful web services, TCP socket clients and servers, and more. This short book packs a hefty dose of Node.js. You'll test your code's functionality and performance under load. You'll learn important aspects of Node development--from its architecture and core, to its ecosystem of third-party modules. You'll discover how Node pairs a server-side event loop with a JavaScript runtime to produce screaming fast, non-blocking concurrency. Through a series of practical programming domains, you'll use the latest available ECMAScript Harmony features and harness key Node classes such as EventEmitter and Stream. Throughout the book, you'll develop real programs that are small, fast, low-profile, and useful. Get ready to join a smart community that's rapidly advancing the state of the art in web development.What You Need: Latest stable release of Node.js, this book was written with 0.12.x in mind. The 0MQ (ZeroMQ) library, version 3.2 or higher.},
  added-at = {2016-08-08T23:26:01.000+0200},
  address = {Frisco, TX},
  author = {Wilson, Jim R.},
  biburl = {https://www.bibsonomy.org/bibtex/2cd2e78688b7800bd0399e36eae970c89/vngudivada},
  interhash = {435fd893579cee44ad4f48cb24f23cc6},
  intrahash = {cd2e78688b7800bd0399e36eae970c89},
  keywords = {Book Node.js},
  publisher = {Pragmatic Programmers},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Node.js the right way practical, server-side JavaScript that scales},
  year = 2014
}

@inproceedings{schulz2013visualization,
  abstract = {Spatiotemporal data often relates to different levels of granularity in space, time, and data. Yet, bringing these levels together for an integrated visual exploration across levels poses a challenge up to this day. With this paper, we aim to provide a first solution approach to this challenge, which decomposes the data in its various levels to be able to show them side-by-side. Based on this decomposition, we derive a visual exploration approach that consists of a novel multilevel visualization, adjoined traditional spatial and temporal views, as well as of tailored exploration techniques for their concerted use. We exemplify the utility of this approach by case studies on election and poll data from Germany's various administrative levels and different time spans.},
  added-at = {2016-08-11T14:05:15.000+0200},
  address = {New York, NY, USA},
  author = {Schulz, Hans-J\"{o}rg and Hadlak, Steffen and Schumann, Heidrun},
  biburl = {https://www.bibsonomy.org/bibtex/2cd99cb261aefa7175cfd012a656f0cc8/vngudivada},
  booktitle = {Proceedings of the 13th International Conference on Knowledge Management and Knowledge Technologies},
  interhash = {15574ac862a1e63bf50d06ed9ff6aee0},
  intrahash = {cd99cb261aefa7175cfd012a656f0cc8},
  keywords = {EDA SpatioTemporalData Visualization},
  pages = {2:1--2:8},
  publisher = {ACM},
  series = {i-Know '13},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {A Visualization Approach for Cross-level Exploration of Spatiotemporal Data},
  url = {http://doi.acm.org/10.1145/2494188.2494199},
  year = 2013
}

@inproceedings{luo2010visualizing,
  abstract = {This paper describes the application of existing and novel adaptations of visualisation techniques to routinely collected health data. The aim of this case study is to examine the capacity for visualisation approaches to quickly and effectively inform clinical, policy, and fiscal decision making to improve healthcare provision. We demonstrate the use of interactive graphics, fluctuation plots, mosaic plots, time plots, heatmaps, and disease maps to visualise patient admission, transfer, in-hospital mortality, morbidity coding, execution of diagnosis and treatment guidelines, and the temporal and spatial variations of diseases. The relative effectiveness of these techniques and associated challenges are discussed.},
  added-at = {2016-08-11T13:59:36.000+0200},
  address = {Darlinghurst, Australia},
  author = {Luo, Wei and Gallagher, Marcus and O'Kane, Di and Connor, Jason and Dooris, Mark and Roberts, Col and Mortimer, Lachlan and Wiles, Janet},
  biburl = {https://www.bibsonomy.org/bibtex/213abc628aa33c8873982b9e5121d5b55/vngudivada},
  booktitle = {Proceedings of the Fourth Australasian Workshop on Health Informatics and Knowledge Management - Volume 108},
  interhash = {2e90c3a282a80136b49737324d3d1f5f},
  intrahash = {13abc628aa33c8873982b9e5121d5b55},
  keywords = {EDA Healthcare Visualization},
  pages = {45--52},
  publisher = {Australian Computer Society, Inc.},
  series = {HIKM '10},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Visualizing a State-wide Patient Data Collection: A Case Study to Expand the Audience for Healthcare Data},
  year = 2010
}

@book{ojeda2014practical,
  abstract = {If you are an aspiring data scientist who wants to learn data science and numerical programming concepts through hands-on, real-world project examples, this is the book for you. Whether you are brand new to data science or you are a seasoned expert, you will benefit from learning about the structure of data science projects, the steps in the data science pipeline, and the programming examples presented in this book. Since the book is formatted to walk you through the projects with examples and explanations along the way, no prior programming experience is required.},
  added-at = {2016-08-07T04:06:03.000+0200},
  address = {Birmingham},
  author = {Ojeda, Tony and Murphy, Sean Patrick and Bengfort, Benjamin and Dasgupta, Abhijit},
  biburl = {https://www.bibsonomy.org/bibtex/2453fc6361d0e101885c53066d2eeafb1/vngudivada},
  description = {89 hands-on recipes to help you complete real-world data science projects in R and Python},
  interhash = {94449c1f14196d70b5105180ef4a7a0e},
  intrahash = {453fc6361d0e101885c53066d2eeafb1},
  keywords = {Book Cookbook DataScience Python R},
  publisher = {Packt Publishing},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Practical data science cookbook},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=1800641},
  year = 2014
}

@book{langtangen2011primer,
  abstract = {The book serves as a first introduction to computer programming of scientific applications, using the high-level Python language. The exposition is example- and problem-oriented, where the applications are taken from mathematics, numerical calculus, statistics, physics, biology, and finance. The book teaches "Matlab-style" and procedural programming as well as object-oriented programming. High school mathematics is a required background, and it is advantageous to study classical and numerical one-variable calculus in parallel with reading this book. Besides learning how to program computers, the reader will also learn how to solve mathematical problems, arising in various branches of science and engineering, with the aid of numerical methods and programming. By blending programming, mathematics and scientific applications, the book lays a solid foundation for practicing computational science.},
  added-at = {2016-08-09T23:22:34.000+0200},
  address = {New York, NY},
  author = {Langtangen, Hans Petter},
  biburl = {https://www.bibsonomy.org/bibtex/2c5c7c9849e177a04c30dd31b0b5615f7/vngudivada},
  interhash = {4f65ea09a895327619fc695fee47745b},
  intrahash = {c5c7c9849e177a04c30dd31b0b5615f7},
  keywords = {Book Python ScientificProgramming},
  publisher = {Springer},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {A primer on scientific programming with Python},
  year = 2011
}

@book{kshemkalyani2008distributed,
  abstract = {This comprehensive textbook covers the principles and models underlying the theory, algorithms and systems aspects of distributed computing.},
  added-at = {2016-08-07T00:48:54.000+0200},
  address = {Cambridge},
  author = {Kshemkalyani, Ajay D. and Singhal, Mukesh},
  biburl = {https://www.bibsonomy.org/bibtex/2c79705c3c6d1e4b23d11344e0eb6d474/vngudivada},
  description = {
Designing distributed computing systems is a complex process requiring a solid understanding of the design problems and the theoretical and practical aspects of their solutions. This comprehensive textbook covers the fundamental principles and models underlying the theory, algorithms and systems aspects of distributed computing. Broad and detailed coverage of the theory is balanced with practical systems-related issues such as mutual exclusion, deadlock detection, authentication, and failure recovery. Algorithms are carefully selected, lucidly presented, and described without complex proofs. Simple explanations and illustrations are used to elucidate the algorithms. Important emerging topics such as peer-to-peer networks and network security are also considered. With vital algorithms, numerous illustrations, examples and homework problems, this textbook is suitable for advanced undergraduate and graduate students of electrical and computer engineering and computer science. Practitioners in data networking and sensor networks will also find this a valuable resource. Additional resources are available online at www.cambridge.org/9780521876346.},
  interhash = {bd0ef516000970a4a4960c127e006f09},
  intrahash = {c79705c3c6d1e4b23d11344e0eb6d474},
  keywords = {Book DistributedComputing},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Distributed computing: principles, algorithms, and systems},
  year = 2008
}

@book{mcdaniel2014rapid,
  abstract = {Tired of boring spreadsheets and data overload from confusing graphs? Master the art of visualization with Rapid Graphs with Tableau 8! Tableau insiders Stephen and Eileen McDaniel expertly provide a hands-on case study approach and more than 225 illustrations that will teach you how to quickly explore and understand your data to make informed decisions in a wide variety of real-world situations. Rapid Graphs with Tableau 8 includes best practices of visual analytics for ideas on how to communicate your findings with audience-friendly graphs, tables and maps. "A picture is worth a thousand words" is a common saying that is more relevant today than ever as data volumes grow and the need for easy access to answers becomes more critical. This book covers the core of Tableau capabilities in easy-to-follow examples, updated and expanded for Version 8. Learn how to be successful with Tableau from the team that started the original training program as the founding Tableau Education Partner! A must read for anyone interested in Tableau. Clear explanations, practical advice and beautiful examples! Elissa Fink Chief Marketing Officer, Tableau Software.},
  added-at = {2016-08-17T00:28:28.000+0200},
  address = {New York, NY},
  author = {McDaniel, Stephen and McDaniel, Eileen},
  biburl = {https://www.bibsonomy.org/bibtex/278f9d7c6bbd8716b3396d27d00b09bd1/vngudivada},
  interhash = {b6fe593586485e475b5e69808afb390e},
  intrahash = {78f9d7c6bbd8716b3396d27d00b09bd1},
  keywords = {Book Graphs Tableau Visualization},
  publisher = {Apress},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Rapid graphs with Tableau 8 the original guide for the accidental analyst},
  year = 2014
}

@book{kimball2004warehouse,
  abstract = {Cowritten by Ralph Kimball, the world's leading data warehousing authority, whose previous books have sold more than 150,000 copies
Delivers real-world solutions for the most time- and labor-intensive portion of data warehousing-data staging, or the extract, transform, load (ETL) process
Delineates best practices for extracting data from scattered sources, removing redundant and inaccurate data, transforming the remaining data into correctly formatted data structures, and then loading the end product into the data warehouse
Offers proven time-saving ETL techniques, comprehensive guidance on building dimensional structures, and crucial advice on ensuring data quality},
  added-at = {2016-08-07T05:11:49.000+0200},
  address = {Indianapolis, IN},
  author = {Kimball, Ralph and Caserta, Joe},
  biburl = {https://www.bibsonomy.org/bibtex/2a9f53b1823e78832b7ef8e29134edd5b/vngudivada},
  interhash = {2168c745c19cf5558d6f4bd29c9170e8},
  intrahash = {a9f53b1823e78832b7ef8e29134edd5b},
  keywords = {Book DataWarehouse ETL},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {The data warehouse ETL toolkit: practical techniques for extracting, cleaning, conforming, and delivering data},
  year = 2004
}

@book{lipovaca2011learn,
  abstract = {Learn You a Haskell for Great Good! (LYAH!) is a fun, illustrated guide to learning Haskell, a functional programming language that's growing in popularity. LYAH! introduces programmers familiar with imperative languages (such as C++, Java, or Python) to the unique aspects of functional programming. Packed with jokes, pop culture references, and the author's own hilarious artwork, LYAH! eases the learning curve of this complex language, and is a perfect starting point for any programmer looking to expand their horizons. The well-known web tutorial on which this book is based is widely regarded.},
  added-at = {2016-08-08T23:40:44.000+0200},
  address = {San Francisco, Calif.},
  author = {Lipovaca, Miran},
  biburl = {https://www.bibsonomy.org/bibtex/2460be6b8ef3ebe51dfd047d4bf339897/vngudivada},
  description = {It's all in the name: Learn You a Haskell for Great Good! is a hilarious, illustrated guide to this complex functional language. Packed with the author's original artwork, pop culture references, and most importantly, useful example code, this book teaches functional fundamentals in a way you never thought possible.You'll start with the kid stuff: basic syntax, recursion, types and type classes. Then once you've got the basics down, the real black belt master-class begins: you'll learn to use applicative functors, monads, zippers, and all the other mythical Haskell constructs you've only read about in storybooks.As you work your way through the author's imaginative (and occasionally insane) examples, you'll learn to: Laugh in the face of side effects as you wield purely functional programming techniques Use the magic of Haskell's "laziness" to play with infinite sets of data Organize your programs by creating your own types, type classes, and modules Use Haskell's elegant input/output system to share the genius of your programs with the outside world Short of eating the author's brain, you will not find a better way to learn this powerful language than reading Learn You a Haskell for Great Good! Excerpt from the Introduction Haskell is fun, and thats what its all about! This book is aimed at people who have experience programming in imperative languagessuch as C++, Java, and Pythonand now want to try out Haskell. But even if you dont have any significant programming experience, Ill bet a smart person like you will be able to follow along and learn Haskell. My first reaction to Haskell was that the language was just too weird. But after getting over that initial hurdle, it was smooth sailing. Even if Haskell seems strange to you at first, dont give up. Learning Haskell is almost like learning to program for the first time all over again. Its fun, and it forces you to think differently. NOTE If you ever get really stuck, the IRC channel #haskell on the freenode network is a great place to ask questions. The people there tend to be nice, patient, and understanding. Theyre a great resource for Haskell newbies. So, What's Haskell? Haskell is a purely functional programming language. In imperative programming languages, you give the computer a sequence of tasks, which it then executes. While executing them, the computer can change state. For instance, you can set the variable a to 5 and then do some stuff that might change the value of a. There are also flow-control structures for executing instructions several times, such as for and while loops. Purely functional programming is different. You dont tell the computer what to doyou tell it what stuff is. For instance, you can tell the computer that the factorial of a number is the product of every integer from 1 to that number or that the sum of a list of numbers is the first number plus the sum of the remaining numbers. You can express both of these operations as functions. > Read the Introduction (PDF) in its entirety.},
  interhash = {9b55346ea7eee0684bbcd3569347948f},
  intrahash = {460be6b8ef3ebe51dfd047d4bf339897},
  keywords = {Book Haskell},
  publisher = {No Starch Press},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Learn you a Haskell for great good! : a beginner's guide},
  year = 2011
}

@misc{kriegel2010outlier,
  abstract = {This tutorial provides a comprehensive and comparative overview of a broad range of state-of-the-art algorithms for finding outliers in massive datasets. It sketches important applications of the introduced methods, and presents a taxonomy of existing approaches. In addition, relationships between the algorithmic approaches of each category of the taxonomy are discussed. Last but not least, at least one algorithm of each category is used for an empirical evaluation of the different approaches for outlier detection. The intended audience of this tutorial ranges from novice researchers to advanced experts as well as practitioners from any application domain where outlier detection methods are required.},
  added-at = {2016-08-07T00:41:50.000+0200},
  author = {Kriegel, Hans-Peter},
  biburl = {https://www.bibsonomy.org/bibtex/2344b4dffa48b611e7f2f3635f6ff2888/vngudivada},
  interhash = {403b39063d938909908a781c1f301aee},
  intrahash = {344b4dffa48b611e7f2f3635f6ff2888},
  keywords = {OutlierDetection Tutorial},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Outlier Detection Techniques},
  year = 2010
}

@book{parr2013definitive,
  abstract = {Programmers run into parsing problems all the time. Whether it's a data format like JSON, a network protocol like SMTP, a server configuration file for Apache, a PostScript/PDF file, or a simple spreadsheet macro language--ANTLR v4 and this book will demystify the process. ANTLR v4 has been rewritten from scratch to make it easier than ever to build parsers and the language applications built on top. This completely rewritten new edition of the bestselling Definitive ANTLR Reference shows you how to take advantage of these new features.

Build your own languages with ANTLR v4, using ANTLR's new advanced parsing technology. In this book, you'll learn how ANTLR automatically builds a data structure representing the input (parse tree) and generates code that can walk the tree (visitor). You can use that combination to implement data readers, language interpreters, and translators.

You'll start by learning how to identify grammar patterns in language reference manuals and then slowly start building increasingly complex grammars. Next, you'll build applications based upon those grammars by walking the automatically generated parse trees. Then you'll tackle some nasty language problems by parsing files containing more than one language (such as XML, Java, and Javadoc). You'll also see how to take absolute control over parsing by embedding Java actions into the grammar.

You'll learn directly from well-known parsing expert Terence Parr, the ANTLR creator and project lead. You'll master ANTLR grammar construction and learn how to build language tools using the built-in parse tree visitor mechanism. The book teaches using real-world examples and shows you how to use ANTLR to build such things as a data file reader, a JSON to XML translator, an R parser, and a Java class->interface extractor. This book is your ticket to becoming a parsing guru!},
  added-at = {2016-08-07T01:24:21.000+0200},
  author = {Parr, Terence},
  biburl = {https://www.bibsonomy.org/bibtex/25cd36c2175922b92e84da2ada6c2c96a/vngudivada},
  interhash = {d00798f134910fec8b9d1df7a0729849},
  intrahash = {5cd36c2175922b92e84da2ada6c2c96a},
  keywords = {ANTLR Book},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {The definitive ANTLR 4 reference},
  year = 2013
}

@book{kimball2016kimball,
  abstract = {Offers a collection of recommended guidelines for data warehousing and business intelligence. Features articles covering the complete lifecycle of data warehousing, including project planning, requirements gathering, dimensional modeling, ETL, and business intelligence and analytics.},
  added-at = {2016-08-07T05:24:52.000+0200},
  author = {Kimball, Ralph and Ross, Margy and Becker, Bob and Mundy, Joy and Thornthwaite, Warren},
  biburl = {https://www.bibsonomy.org/bibtex/27e0cc4fb78a8811d660c479041562bf1/vngudivada},
  interhash = {a70238b33321b8eab1fcd1f3cabcf6b0},
  intrahash = {7e0cc4fb78a8811d660c479041562bf1},
  keywords = {Book BusinessIntelligence DataWarehouse Tools},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {The Kimball Group reader: relentlessly practical tools for data warehousing and business intelligence},
  year = 2016
}

@techreport{sas2016management,
  abstract = {What does SAS Data Management do?

SAS Data Management helps transform, integrate, govern and secure data while improving its overall quality and reliability.

Why is SAS Data Management important?

Whether it’s traditional data in operational systems or big data in a Hadoop cluster, data is an asset that every organization has. And managing that data is no longer a convenience – it’s a necessity. SAS Data Management is the answer to solving your data integration and data quality challenges.

For whom is SAS Data Management designed?

SAS Data Management is designed for IT organizations that need to address performance and functional improvements in their data management infrastructure.},
  added-at = {2016-08-09T16:52:21.000+0200},
  author = {SAS},
  biburl = {https://www.bibsonomy.org/bibtex/28f16a119a301ba30f3a47089c65146f1/vngudivada},
  interhash = {162e25923b32f4e97ac3a3da97a807c5},
  intrahash = {8f16a119a301ba30f3a47089c65146f1},
  keywords = {DataManagement DataQuality SAS},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {SAS Data Management Fact Sheet},
  year = 2016
}

@article{nicol2016rethinking,
  added-at = {2016-08-07T01:26:29.000+0200},
  author = {Nicol, David and Macfarlane-Dick, Debra},
  biburl = {https://www.bibsonomy.org/bibtex/2f5bb219d5dae28481e59ff0518cd87f7/vngudivada},
  interhash = {83d48edf3dd7e26c83586de415f67c4f},
  intrahash = {f5bb219d5dae28481e59ff0518cd87f7},
  keywords = {FormativeAssessment LearningAssessment},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Rethinking Formative Assessment in HE: a theoretical model and seven principles of good feedback practice},
  year = 2016
}

@book{murray2013tableau,
  abstract = {Best practices and step-by-step instructions for using the Tableau Software toolset Although the Tableau Desktop interface is relatively intuitive, this book goes beyond the simple mechanics of the interface to show best practices for creating effective visualizations for specific business intelligence objectives. It illustrates little-known features and techniques for getting the most from the Tableau toolset, supporting the needs of the business analysts who use the product as well as the data and IT managers who support it. This comprehensive guide covers the core feature set.},
  added-at = {2016-08-17T00:14:58.000+0200},
  author = {Murray, Dan},
  biburl = {https://www.bibsonomy.org/bibtex/2a3896be30b6c668c8042d9b27dcaeb6c/vngudivada},
  interhash = {06c59192705a94a1632ff26099e683e5},
  intrahash = {a3896be30b6c668c8042d9b27dcaeb6c},
  keywords = {Book Tableau VisualAnalytics Visualization},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Tableau your data: fast and easy visual analysis with tableau software},
  url = {http://www.123library.org/book_details/?id=11497},
  year = 2013
}

@book{anderson2015science,
  abstract = {Discover how data science can help you gain indepth insight into your business the easy way! Jobs in data science abound, but few people have the data science skills needed to fill these increasingly important roles in organizations. Data Science For Dummies is the perfect starting point for IT professionals and students interested in making sense of their organization s massive data sets and applying their findings to realworld business scenarios. From uncovering rich data sources to managing large amounts of data within hardware and software limitations, ensuring consistency in reporting, merging various data sources, and beyond, you ll develop the knowhow you need to effectively interpret data and tell a story that can be understood by anyone in your organization. Provides a background in data science fundamentals before moving on to working with relational databases and unstructured data and preparing your data for analysis Details different data visualization techniques that can be used to showcase and summarize your data Explains both supervised and unsupervised machine learning, including regression, model validation, and clustering techniques Includes coverage of big data processing tools like MapReduce, Hadoop, Dremel, Storm, and Spark It s a big, big data world out there let Data Science For Dummies help you harness its power and gain a competitive edge for your organization.},
  added-at = {2016-08-07T04:09:17.000+0200},
  address = {Hoboken, NJ},
  author = {Anderson, Carl},
  biburl = {https://www.bibsonomy.org/bibtex/2fa27febe5d456a9ba548e25783bb9a4d/vngudivada},
  interhash = {67f7e785b610a700653d003abf77aacf},
  intrahash = {fa27febe5d456a9ba548e25783bb9a4d},
  keywords = {Book DataScience},
  publisher = {John Wiley & Sons},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Data Science for Dummies},
  year = 2015
}

@misc{godsey1017think,
  abstract = {Think Like a Data Scientist teaches you a step-by-step approach to solving real-world data-centric problems. By breaking down carefully crafted examples, you'll learn to combine analytic, programming, and business perspectives into a repeatable process for extracting real knowledge from data. As you read, you'll discover (or remember) valuable statistical techniques and explore powerful data science software. More importantly, you'll put this knowledge together using a structured process for data science. When you've finished, you'll have a strong foundation for a lifetime of data science learning and practice.},
  added-at = {2016-08-09T20:49:51.000+0200},
  author = {Godsey, Brian},
  biburl = {https://www.bibsonomy.org/bibtex/25f704163faaee0fddad97027e5906d2a/vngudivada},
  interhash = {944966ffb04e779a44eb0a08cf5a359a},
  intrahash = {5f704163faaee0fddad97027e5906d2a},
  keywords = {Book DataScience},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Think Like a Data Scientist},
  year = 1017
}

@book{johnson2014designing,
  abstract = {In this completely updated and revised edition of Designing with the Mind in Mind, Jeff Johnson provides you with just enough background in perceptual and cognitive psychology that user interface (UI) design guidelines make intuitive sense rather than being just a list or rules to follow. Early UI practitioners were trained in cognitive psychology, and developed UI design rules based on it. But as the field has evolved since the first edition of this book, designers enter the field from many disciplines. Practitioners today have enough experience in UI design that they have been exposed to design rules, but it is essential that they understand the psychology behind the rules in order to effectively apply them. In this new edition, you'll find new chapters on human choice and decision making, hand-eye coordination and attention, as well as new examples, figures, and explanations throughout. Provides an essential source for user interface design rules and how, when, and why to apply themArms designers with the science behind each design rule, allowing them to make informed decisions in projects, and to explain those decisions to others Equips readers with the knowledge to make educated tradeoffs between competing rules, project deadlines, and budget pressuresCompletely updated and revised, including additional coverage on human choice and decision making, hand-eye coordination and attention, and new mobile and touch-screen examples throughout.},
  added-at = {2016-08-07T01:13:03.000+0200},
  author = {Johnson, Jeff},
  biburl = {https://www.bibsonomy.org/bibtex/2766d022175db272567b92cb124db8153/vngudivada},
  interhash = {4cae576e026e96bbdacb8bd97df7e022},
  intrahash = {766d022175db272567b92cb124db8153},
  keywords = {Book CognitiveScience InterfaceDesign},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Designing with the mind in mind : simple guide to understanding user interface design guidelines},
  year = 2014
}

@book{toomey2014science,
  abstract = {Learn and explore the fundamentals of data science with R About This BookFamiliarize yourself with R programming packages and learn how to utilize them effectivelyLearn how to detect different types of data mining sequencesA step-by-step guide to understanding R scripts and the ramifications of your changesWho This Book Is ForIf you are a data analyst who has a firm grip on some advanced data analysis techniques and wants to learn how to leverage the features of R, this is the book for you. You should have some basic knowledge of the R language and should know about some data science topics. In Detail R is a powerful, open source, functional programming language. It can be used for a wide range of programming tasks and is best suited to produce data and visual analytics through customizable scripts and commands.The purpose of the book is to explore the core topics that data scientists are interested in. This book draws from a wide variety of data sources and evaluates this data using existing publicly available R functions and packages. In many cases, the resultant data can be displayed in a graphical form that is more intuitively understood. You will also learn about the often needed and frequently used analysis techniques in the industry.By the end of the book, you will know how to go about adopting a range of data science techniques with R.},
  added-at = {2016-08-07T04:02:08.000+0200},
  author = {Toomey, Dan},
  biburl = {https://www.bibsonomy.org/bibtex/27491f2159ecd282afea72e8017037852/vngudivada},
  interhash = {ee6c0c2315277184b1536d2ddb7ef641},
  intrahash = {7491f2159ecd282afea72e8017037852},
  keywords = {Book DataScience R},
  publisher = {Packt Publishing},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {R for Data Science},
  year = 2014
}

@book{buhlmann2016handbook,
  abstract = {Features

Depicts the current landscape of big data analysis
Emphasizes computational statistics and machine learning
Strikes the right balance not only between statistical theory and applications in computer science but also between the breadth of topics and the depth to which each topic is explored
Summary

Handbook of Big Data provides a state-of-the-art overview of the analysis of large-scale datasets. Featuring contributions from well-known experts in statistics and computer science, this handbook presents a carefully curated collection of techniques from both industry and academia. Thus, the text instills a working understanding of key statistical and computing ideas that can be readily applied in research and practice.

Offering balanced coverage of methodology, theory, and applications, this handbook:

Describes modern, scalable approaches for analyzing increasingly large datasets
Defines the underlying concepts of the available analytical tools and techniques
Details intercommunity advances in computational statistics and machine learning
Handbook of Big Data also identifies areas in need of further development, encouraging greater communication and collaboration between researchers in big data sub-specialties such as genomics, computational biology, and finance.},
  added-at = {2016-08-07T01:46:18.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/296ce6a251f57166389a1c85e706891a5/vngudivada},
  description = {Table of Contents

GENERAL PERSPECTIVES ON BIG DATA

The Advent of Data Science: Some Considerations on the Unreasonable Effectiveness of Data
Richard Starmans

Big n versus Big p in Big Data
Norman Matloff

DATA-CENTRIC, EXPLORATORY METHODS

Divide and Recombine: Approach for Detailed Analysis and Visualization of Large Complex Data
Ryan Hafen

Integrate Big Data for Better Operation, Control, and Protection of Power Systems
Guang Lin

Interactive Visual Analysis of Big Data
Carlos Scheidegger

A Visualization Tool for Mining Large Correlation Tables: The Association Navigator
Andreas Buja, Abba M. Krieger, and Edward I. George

EFFICIENT ALGORITHMS

High-Dimensional Computational Geometry
Alexandr Andoni

IRLBA: Fast Partial SVD Method
James Baglama

Structural Properties Underlying High-Quality Randomized Numerical Linear Algebra Algorithms
Michael W. Mahoney and Petros Drineas

Something for (Almost) Nothing: New Advances in Sublinear-Time Algorithms
Ronitt Rubinfeld and Eric Blais

GRAPH APPROACHES

Networks
Elizabeth L. Ogburn and Alexander Volfovsky

Mining Large Graphs
David F. Gleich and Michael W. Mahoney

MODEL FITTING AND REGULARIZATION

Estimator and Model Selection Using Cross-Validation
Iván Díaz

Stochastic Gradient Methods for Principled Estimation with Large Datasets
Panos Toulis and Edoardo M. Airoldi

Learning Structured Distributions
Ilias Diakonikolas

Penalized Estimation in Complex Models
Jacob Bien and Daniela Witten

High-Dimensional Regression and Inference
Lukas Meier

ENSEMBLE METHODS

Divide and Recombine Subsemble, Exploiting the Power of Cross-Validation
Stephanie Sapp and Erin LeDell

Scalable Super Learning
Erin LeDell

CAUSAL INFERENCE

Tutorial for Causal Inference
Laura Balzer, Maya Petersen, and Mark van der Laan

A Review of Some Recent Advances in Causal Inference
Marloes H. Maathuis and Preetam Nandy

TARGETED LEARNING

Targeted Learning for Variable Importance
Sherri Rose

Online Estimation of the Average Treatment Effect
Sam Lendle

Mining with Inference: Data-Adaptive Target Parameters
Alan Hubbard and Mark van der Laan},
  editor = {Bühlmann, Peter and Drineas, Petros and Kane, Michael and van der Laan, Mark},
  interhash = {c28a9013dc16c5c35a972e4f488257fa},
  intrahash = {96ce6a251f57166389a1c85e706891a5},
  isbn = {9781482249088 1482249081},
  keywords = {BigData Book Handbook},
  refid = {939597365},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Handbook of big data},
  url = {http://www.crcnetbase.com/isbn/978-1-4822-4907-1},
  year = 2016
}

@book{karwin2010antipatterns,
  abstract = {"Bill Karwin has helped thousands of people write better SQL and build stronger relational databases. Now he's sharing his collection of antipatterns--the most common errors he's identified in those thousands of requests for help. Each chapter in this book helps you identify, explain, and correct a unique and dangerous antipattern. The four parts of the book group the antipatterns in terms of logical database design, physical database design, queries, and application development. The chances are good that your application's database layer already contains problems such as Index Shotgun, Keyless Entry, Fear of the Unknown, and Spaghetti Query. This book will help you and your team find them. Even better, it will also show you how to fix them, and how to avoid these and other problems in the future."--Back cover.},
  added-at = {2016-08-08T23:28:20.000+0200},
  address = {Raleigh, N.C.},
  author = {Karwin, Bill},
  biburl = {https://www.bibsonomy.org/bibtex/2280a737e88682f2c30be98d42be285e9/vngudivada},
  interhash = {eef20df1d0d5ce4f99d3a4cb97dc63a3},
  intrahash = {280a737e88682f2c30be98d42be285e9},
  keywords = {Book SQL},
  publisher = {Pragmatic Bookshelf},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {SQL antipatterns: avoiding the pitfalls of database programming},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=978193435655},
  year = 2010
}

@inproceedings{ordonez2011interactive,
  abstract = {An OLAP cube is typically explored with multiple aggregations selecting different subsets of cube dimensions to analyze trends or to discover unexpected results. Unfortunately, such analytic process is generally manual and fails to statistically explain results. In this work, we propose to combine dimension lattice traversal and parametric statistical tests to identify significant metric differences between cube cells. We present a 2D interactive visualization of the OLAP cube based on a checkerboard that enables isolating and interpreting significant measure differences between two similar cuboids, which differ in one dimension and have the same values on the remaining dimensions. Cube exploration and visualization is performed by automatically generated SQL queries. An experimental evaluation with a medical data set presents statistically significant results and interactive visualizations, which link risk factors and degree of disease.},
  added-at = {2016-08-07T02:16:39.000+0200},
  address = {New York, NY, USA},
  author = {Ordonez, Carlos and Chen, Zhibo and Garc\'{\i}a-Garc\'{\i}a, Javier},
  biburl = {https://www.bibsonomy.org/bibtex/20c3abcaa4732c54a1b353e2d66627944/vngudivada},
  booktitle = {Proceedings of the ACM 14th International Workshop on Data Warehousing and OLAP},
  interhash = {45e4f257a83a8ed5524410089a01449b},
  intrahash = {0c3abcaa4732c54a1b353e2d66627944},
  keywords = {ExploratoryDataAnalysis OLAP Visualization},
  pages = {83--88},
  publisher = {ACM},
  series = {DOLAP '11},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Interactive Exploration and Visualization of OLAP Cubes},
  year = 2011
}

@book{carlson2012mountain,
  abstract = {This low-priced, handy guide is packed with practical guidance for people who want to jump in and start using the OS X Mountain Lion. Written by Jeff Carlson, this essential companion features snappy writing, eye-catching graphics, and a clean design to help readers get the most out of OS X Mountain Lion. Jeff guides readers through Apple's OS X Mountain Lion, showing you how to: Download, set up, and starting using Mountain Lion. Manage files with iCloud. Swipe, pinch, and scroll: Master Mountain Lion's Mult-Touch gestures. Install applications from the Mac App Store, and stay safe with Gatekeeper. Stay in touch: Enjoy video calls with family and friends with FaceTime and chat them up with Messages. Don't miss another email, calendar alert, or friend request with Notification Center. Show off your gaming skills through Game Center. In addition, Jeff offers plenty of tips and tricks for getting the most from Mountain Lion.},
  added-at = {2016-08-09T21:34:56.000+0200},
  address = {Berkeley; Old Tappan},
  author = {Carlson, Jeff},
  biburl = {https://www.bibsonomy.org/bibtex/260c6fd61e5a9499b42741b83caa70785/vngudivada},
  interhash = {252c42746dd10dcf84a126dc62546e69},
  intrahash = {60c6fd61e5a9499b42741b83caa70785},
  keywords = {Book MacOSX PocketGuide},
  publisher = {Peachpit Press Pearson Education [distributor]},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {The OS X Mountain Lion},
  year = 2012
}

@book{armstrong2007programming,
  abstract = {Erlang solves one of the most pressing problems facing developers today: how to write reliable, concurrent, high-performance systems. It's used worldwide by companies who need to produce reliable, efficient, and scalable applications. Invest in learning Erlang now. Moore's Law is the observation that the amount you can do on a single chip doubles every two years. But Moore's Law is taking a detour. Rather than producing faster and faster processors, companies such as Intel and AMD are producing multi-core devices: single chips containing two, four, or more processors. If your programs aren't concurrent, they'll only run on a single processor at a time. Your users will think that your code is slow. Erlang is a programming language designed for building highly parallel, distributed, fault-tolerant systems. It has been used commercially for many years to build massive fault-tolerated systems that run for years with minimal failures. Erlang programs run seamlessly on multi-core computers: this means your Erlang program should run a lot faster on a 4 core processor than on a single core processor, all without you having to change a line of code. Erlang combines ideas from the world of functional programming with techniques for building fault-tolerant systems to make a powerful language for building the massively parallel, networked applications of the future. This book presents Erlang and functional programming in the familiar Pragmatic style. And it's written by Joe Armstrong, one of the creators of Erlang. It includes example code you'll be able to build upon. In addition, the book contains the full source code for two interesting applications:A SHOUTcast server which you can use to stream music to every computer in your house, and a full-text indexing and search engine that can index gigabytes of data. Learn how to write programs that run on dozens or even hundreds of local and remote processors. See how to write robust applications that run even in the face of network and hardware failure, using the Erlang programming language.},
  added-at = {2016-08-08T23:34:45.000+0200},
  address = {Raleigh, N.C.},
  author = {Armstrong, Joe},
  biburl = {https://www.bibsonomy.org/bibtex/2452eb0fd73e38fafb6bc2449ce2a3b8e/vngudivada},
  interhash = {b5f50061e70e0765403d98179681c9f0},
  intrahash = {452eb0fd73e38fafb6bc2449ce2a3b8e},
  keywords = {Book Erlang},
  publisher = {Pragmatic Bookshelf},
  timestamp = {2019-03-25T17:12:29.000+0100},
  title = {Programming erlang : software for a concurrent world},
  year = 2007
}

@book{olive2003maths,
  abstract = {The second edition of this highly successful textbook has been completely revised and now includes a new chapter on vectors. Mathematics is the basis of all science and engineering degrees, and a source of difficulty for some students. Jenny Olive helps resolve this problem by presenting the core mathematics needed by students starting science or engineering courses in user-friendly comprehensible terms},
  added-at = {2016-08-29T00:27:09.000+0200},
  address = {New York, NY},
  author = {Olive, Jenny},
  biburl = {https://www.bibsonomy.org/bibtex/218de913052eff6ed5442d5e2e853db55/vngudivada},
  interhash = {b8b1a1c995e41d18d100369f6064a6dc},
  intrahash = {18de913052eff6ed5442d5e2e853db55},
  keywords = {Book Mathematics},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Maths: a student's survival guide},
  url = {http://dx.doi.org/10.1017/CBO9780511810794},
  year = 2003
}

@book{hoggar2006mathematics,
  abstract = {"Compression, restoration and recognition are three of the key components of digital imaging. The mathematics needed to understand and carry out all these components is here explained in a textbook that is at once rigorous and practical with many worked examples, exercises with solutions, pseudocode, and sample calculations on images. The introduction lists fast tracks to special topics such as Principal Component Analysis, and ways into and through the book, which abounds with illustrations."--Jacket.},
  added-at = {2016-08-26T01:20:18.000+0200},
  address = {Cambridge; New York},
  author = {Hoggar, S. G.},
  biburl = {https://www.bibsonomy.org/bibtex/29ef1a06d4365f44657ed10b271e9fd16/vngudivada},
  description = {This major revision of the author's popular book still focuses on foundations and proofs, but now exhibits a shift away from Topology to Probability and Information Theory (with Shannon's source and channel encoding theorems) which are used throughout. Three vital areas for the digital revolution are tackled (compression, restoration and recognition), establishing not only what is true, but why, to facilitate education and research. It will remain a valuable book for computer scientists, engineers and applied mathematicians.},
  interhash = {12bb1f1309bdd5fd2f5b9debe5b79937},
  intrahash = {9ef1a06d4365f44657ed10b271e9fd16},
  keywords = {Book DIP},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Mathematics of digital images: creation, compression, restoration, recognition},
  year = 2006
}

@book{huang2001spoken,
  abstract = {Spoken Language Processing draws on the latest advances and techniques from multiple fields: computer science, electrical engineering, acoustics, linguistics, mathematics, psychology, and beyond. To illustrate the book's methods, the authors present detailed case studies based on state-of-the-art systems, including Microsoft's Whisper speech recognizer, Whistler text-to-speech system, Dr. Who dialog system, and the MiPad handheld device},
  added-at = {2016-09-10T13:44:53.000+0200},
  address = {Upper Saddle River, NJ},
  author = {Huang, Xuedong and Acero, Alejandro and Hon, Hsiao-Wuen},
  biburl = {https://www.bibsonomy.org/bibtex/234eb2d8c93740477858c789ce66776ee/vngudivada},
  description = {New advances in spoken language processing: theory and practice

In-depth coverage of speech processing, speech recognition, speech synthesis, spoken language understanding, and speech interface design

Many case studies from state-of-the-art systems, including examples from Microsoft's advanced research labs Spoken Language Processing draws on the latest advances and techniques from multiple fields: computer science, electrical engineering, acoustics, linguistics, mathematics, psychology, and beyond.

Starting with the fundamentals, it presents all this and more:

Essential background on speech production and perception, probability and information theory, and pattern recognition

Extracting information from the speech signal: useful representations and practical compression solutions

Modern speech recognition techniques: hidden Markov models, acoustic and language modeling, improving resistance to environmental noises, search algorithms, and large vocabulary speech recognition

Text-to-speech: analyzing documents, pitch and duration controls; trainable synthesis, and more

Spoken language understanding: dialog management, spoken language applications, and multimodal interfaces.},
  interhash = {864f6cbc89c9b2e0092f6bb9268c6f41},
  intrahash = {34eb2d8c93740477858c789ce66776ee},
  keywords = {Book SpokenLanguage SpokenLanguageInterface SpokenLanguageRecognition},
  publisher = {Prentice Hall PTR},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Spoken language processing: a guide to theory, algorithm, and system development},
  year = 2001
}

@article{koedinger2013potentials,
  abstract = {Increasing widespread use of educational technologies is producing vast amounts of data. Such data can be used to help advance our understanding of student learning and enable more intelligent, interactive, engaging, and effective education. In this article, we discuss the status and prospects of this new and powerful opportunity for datadriven development and optimization of educational technologies, focusing on intelligent tutoring systems. We provide examples of use of a variety of techniques to develop or optimize the select, evaluate, suggest, and update functions of intelligent tutors, including probabilistic grammar learning, rule induction, Markov decision process, classification, and integrations of symbolic search and statistical inference. },
  added-at = {2016-08-17T17:00:06.000+0200},
  author = {Koedinger, Kenneth R. and Brunskill, Emma and de Baker, Ryan Shaun Joazeiro and McLaughlin, Elizabeth A. and Stamper, John C.},
  biburl = {https://www.bibsonomy.org/bibtex/2c880b7f36d8a3fa819036974e9ecd40e/vngudivada},
  ee = {http://www.aaai.org/ojs/index.php/aimagazine/article/view/2484},
  interhash = {128890a58c8355d4df919be2f73a58a8},
  intrahash = {c880b7f36d8a3fa819036974e9ecd40e},
  journal = {AI Magazine},
  keywords = {DataScience ITS},
  number = 3,
  pages = {27-41},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {New Potentials for Data-Driven Intelligent Tutoring System Development and Optimization.},
  url = {http://dblp.uni-trier.de/db/journals/aim/aim34.html#KoedingerBBMS13},
  volume = 34,
  year = 2013
}

@book{proakis2007digital,
  abstract = {A significant revision of a best-selling text for the introductory digital signal processing course. This book presents the fundamentals of discrete-time signals, systems, and modern digital processing and applications for students in electrical engineering, computer engineering, and computer science.The book is suitable for either a one-semester or a two-semester undergraduate level course in discrete systems and digital signal processing. It is also intended for use in a one-semester first-year graduate-level course in digital signal processing.},
  added-at = {2016-09-20T03:13:44.000+0200},
  address = {Upper Saddle River, NJ},
  author = {Proakis, John G. and Manolakis, Dimitris G.},
  biburl = {https://www.bibsonomy.org/bibtex/216dc8715cb0d981c84b1b509a0fe110c/vngudivada},
  edition = {Fourth},
  interhash = {c6819ac962c2f89126d82e212949a79a},
  intrahash = {16dc8715cb0d981c84b1b509a0fe110c},
  keywords = {Book DSP ParExcellence},
  publisher = {Pearson Education},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Digital signal processing: Principles, Algorithms, and Applications},
  year = 2007
}

@book{jaynes2003probability,
  abstract = {A comprehensive introduction to the role of probability theory in general scientific endeavour. This book provides an original interpretation of probability theory, showing the subject to be an extension of logic, and presenting new results and applications. Ideal for scientists working in any area involving inference from incomplete information.},
  added-at = {2016-08-29T00:33:01.000+0200},
  address = {New York, NY},
  author = {Jaynes, E. T. and Bretthorst, G. Larry},
  biburl = {https://www.bibsonomy.org/bibtex/2e6b5505c41b7ddaffc5c48f3a2cae02c/vngudivada},
  description = {Going beyond the conventional mathematics of probability theory, this study views the subject in a wider context. It discusses new results, along with applications of probability theory to a variety of problems. The book contains many exercises and is suitable for use as a textbook on graduate-level courses involving data analysis. Aimed at readers already familiar with applied mathematics at an advanced undergraduate level or higher, it is of interest to scientists concerned with inference from incomplete information.},
  interhash = {5637e120537dc7e09dfb41cde090df45},
  intrahash = {e6b5505c41b7ddaffc5c48f3a2cae02c},
  keywords = {Book ParExcellence Probability},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Probability theory: the logic of science},
  url = {http://dx.doi.org/10.1017/CBO9780511790423},
  year = 2003
}

@book{vugt2009beginning,
  abstract = {This is Linux for those of us who don t mind typing. All Linux users and administrators tend to like the flexibility and speed of Linux administration from the command line in byte sized chunks, instead of fairly standard GUIs. Beginning the Linux Command Line follows a task oriented approach and is distribution agnostic. * Work with files and directories. * Administer users and security. * Understand how Linux is organized.},
  added-at = {2016-08-26T04:05:09.000+0200},
  address = {Berkeley, Calif.},
  author = {Vugt, Sander van},
  biburl = {https://www.bibsonomy.org/bibtex/2540ab3199bbdd60626d51aad6bb6c861/vngudivada},
  interhash = {586140e4262dc731863545066c3109fe},
  intrahash = {540ab3199bbdd60626d51aad6bb6c861},
  keywords = {Book Linux},
  publisher = {Apress},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Beginning the Linux command line},
  year = 2009
}

@book{tucker2014graduate,
  abstract = {Suitable for a graduate course in analytic probability, this text requires only a limited background in real analysis. Topics include probability spaces and distributions, stochastic independence, basic limiting options, strong limit theorems for independent random variables, central limit theorem, conditional expectation and Martingale theory, and an introduction to stochastic processes},
  added-at = {2016-08-29T00:37:10.000+0200},
  author = {Tucker, Howard G.},
  biburl = {https://www.bibsonomy.org/bibtex/260a10d28e67b551a9074273435033668/vngudivada},
  interhash = {a00594385b5ded93943ff34f09c377a3},
  intrahash = {60a10d28e67b551a9074273435033668},
  keywords = {Book Probability},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {A graduate course in probability},
  year = 2014
}

@book{culicover2017basics,
  abstract = {Learning a language involves so much more than just rote memorization of rules. Basics of Language for Language Learners, 2nd edition, by Peter W. Culicover and Elizabeth V. Hume, systematically explores all the aspects of language central to second language learning: the sounds of language, the different grammatical structures, the social functions of communication, and the psychology of language learning and use.

	Unlike books specific to one single language, Basics of Language will help students of all languages. Readers will gain insight into the structure and use of their own language and will therefore see more clearly how the language they are learning differs from their first language. Language instructors will find the approach provocative, and the book will stimulate many new and effective ideas for teaching. Both a textbook and a reference work, Basics of Language will enhance the learning experience for anyone taking a foreign language course as well as the do-it-yourself learner.

	A new section, ``Tools and Strategies for Language Learning,'' has been added to this second edition. It comprises three chapters that focus on brain training, memory and using a dictionary. In addition, the section ``Thinking Like a Native Speaker'' has been substantially updated to include more discussion of errors made by language learners.},
  added-at = {2016-08-30T17:17:06.000+0200},
  address = {Columbus, Ohio},
  author = {Culicover, Peter W. and Hume, Elizabeth V.},
  biburl = {https://www.bibsonomy.org/bibtex/2cb91d0841d1aa4b1331530d8257ae4f4/vngudivada},
  description = {Peter W. Culicover and Elizabeth V. Hume’s book Basics of Language for Language Learners is a must-read for language teachers as well as language learners. The accessible explanations and examples provide fodder for teacher-facilitated discussions related to the Comparisons Standards in Pre-K through university world language classrooms.” —Deborah W. Robinson, Ph.D., President, National Council of State Supervisors for Languages},
  edition = {Second},
  interhash = {3deac80146f83022e012f622951b3398},
  intrahash = {cb91d0841d1aa4b1331530d8257ae4f4},
  keywords = {Book Language Linguistics},
  publisher = {Ohio State University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Basics of language for language learners},
  year = 2017
}

@book{shalevshwartz2014understanding,
  abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering},
  added-at = {2016-09-10T01:42:07.000+0200},
  author = {Shalev-Shwartz, Shai and Ben-David, Shai},
  biburl = {https://www.bibsonomy.org/bibtex/26b363ca2d55ddd20f20ed3d110b23e5b/vngudivada},
  description = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.

Provides a principled development of the most important machine learning tools
Describes a wide range of state-of-the-art algorithms
Promotes understanding of when machine learning is relevant, what the prerequisites for a successful application of ML algorithms are, and which algorithms to use for any given task},
  interhash = {125d708c7b440a3cfeb6146e83ab5de3},
  intrahash = {6b363ca2d55ddd20f20ed3d110b23e5b},
  keywords = {Book MachineLearning},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Understanding machine learning: from theory to algorithms},
  year = 2014
}

@misc{kulkarni2011statistical,
  abstract = {In this article, we provide a tutorial overview of some aspects of statistical learning theory, which also goes by other names such as statistical pattern recognition, nonparametric classification and estimation, and supervised learning. We focus on the problem of two-class pattern classification for various reasons. This problem is rich enough to capture many of the interesting aspects that are present in the cases of more than two classes and in the problem of estimation, and many of the results can be extended to these cases. Focusing on two-class pattern classification simplifies our discussion, and yet it is directly applicable to a wide range of practical settings.


We begin with a description of the two class pattern recognition problem. We then discuss various classical and state of the art approaches to this problem, with a focus on fundamental formulations, algorithms, and theoretical results. In particular, we describe nearest neighbor methods, kernel methods, multilayer perceptrons, VC theory, support vector machines, and boosting.},
  added-at = {2016-09-11T14:48:57.000+0200},
  author = {Kulkarni, Sanjeev R. and Harman, Gilbert},
  biburl = {https://www.bibsonomy.org/bibtex/276641bb58086db3931de3abd9a023bb5/vngudivada},
  interhash = {5c393cf620839c0fbc26301623835f89},
  intrahash = {76641bb58086db3931de3abd9a023bb5},
  keywords = {MachineLearning StatisticalLearning Tutorial},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Statistical Learning Theory: A Tutorial},
  year = 2011
}

@book{carrano2013abstraction,
  abstract = {Data Abstraction and Problem Solving with C++: Walls Mirrors provides a firm foundation in data abstraction that emphasizes the distinction between specifications and implementation as the basis for an object-oriented approach. Software engineering principles and concepts as well as UML diagrams are used to enhance student understanding.},
  added-at = {2016-08-26T03:19:56.000+0200},
  address = {Boston},
  author = {Carrano, Frank M. and Henry, Timothy},
  biburl = {https://www.bibsonomy.org/bibtex/245aabb67f5e1a3590fa75837cbf05857/vngudivada},
  interhash = {3d97b144bace1a3e19795f25ffe2edd3},
  intrahash = {45aabb67f5e1a3590fa75837cbf05857},
  keywords = {Book C++ DataStructures},
  publisher = {Pearson},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Data abstraction & problem solving with C++: walls and mirrors},
  year = 2013
}

@book{hernandez1996first,
  abstract = {This unique book is an excellent introduction to the basic properties of wavelets. The fundamental construction of these functions by means of "multiresolution analyses" is presented; in particular, this method is used for introducing the spline wavelets and the compactly supported wavelets. An important feature of this book, however, is the use of the Fourier transform for studying wavelets on the real line. A simple characterization of all wavelets is presented which is most useful for the construction of new families of wavelets. This technique can also be used for obtaining characterizations of low pass filters and scaling functions. Another feature is the use of wavelets for representing those function spaces that are most often encountered in analysis: the Lebesgue spaces, Hardy spaces, and more generally, the Besov spaces, the Sobolev, and the Lipschitz spaces. Other topics, some related to applications, are also included: the Fast Fourier Transform, wavelet packets, frames, local cosine and sine bases and their discrete versions are just some examples.},
  added-at = {2016-08-29T00:38:48.000+0200},
  address = {Boca Raton},
  author = {Hernández, Eugenio and Weiss, Guido L.},
  biburl = {https://www.bibsonomy.org/bibtex/25c38521da98d2de505f712cc75a9c385/vngudivada},
  interhash = {bed6a95c5ba319063c02a26661dd7ec3},
  intrahash = {5c38521da98d2de505f712cc75a9c385},
  keywords = {Book Wavelets},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {A first course on wavelets},
  year = 1996
}

@article{roussey2011introduction,
  abstract = {In the last decades, the use of ontologies in information systems has become more and more popular in various fields, such as web technologies, database integration, multi agent systems, Natural Language Processing, etc. At the beginning, Artificial Intelligent researchers borrow the word “ontology” from Philosophy, but now this word spread in many scientific domain and ontologies are now used in several developments. The main goal of this chapter is to answer generic questions about ontologies, such as: Which are the different kinds of ontologies? What is the purpose of the use of ontologies in an application? Which methods can I use to build an ontology?},
  added-at = {2016-08-29T04:48:37.000+0200},
  author = {{Roussey}, Catherine and {Pinet}, François and {Kang}, Myoung-Ah and {Corcho}, Oscar},
  biburl = {https://www.bibsonomy.org/bibtex/229741134cb426bd524f82db9b4bfbad6/vngudivada},
  interhash = {6eac9448cbddf8b5f1ce0287c528fbf9},
  intrahash = {29741134cb426bd524f82db9b4bfbad6},
  journal = {Advanced Information and Knowledge Processing},
  keywords = {Ontology},
  month = jul,
  note = {Ontologies in Urban Development Projects},
  pages = {9-38},
  publisher = {Springer},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {{An Introduction to Ontologies and Ontology Engineering}},
  volume = 1,
  year = 2011
}

@book{george2015artificial,
  abstract = {A small book that introduces key Artificial Intelligence (AI) concepts in an easy-to-read format with examples and illustrations. Someone with basic knowledge in Computer Science can have a quick overview of AI (heuristic searches, genetic algorithms, expert systems, game trees, fuzzy expert systems, natural language processing, super intelligence, etc.) with everyday examples. If you are taking a basic AI course or want to learn AI and find the traditional AI textbooks difficult to read, you may choose this book.},
  added-at = {2016-09-11T00:00:47.000+0200},
  author = {George, Binto and Carmichael, Gail and Mathai, Susan S. and Carmichael, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/236e04d137f9b51752f2d4d5883f1a440/vngudivada},
  interhash = {7c6dd0e2c9f5bb13e3a2f5f638520c66},
  intrahash = {36e04d137f9b51752f2d4d5883f1a440},
  keywords = {ArtificialIntelligence Book},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Artificial intelligence simplified: understanding basic concepts},
  year = 2015
}

@book{yule2016study,
  abstract = {This bestselling textbook provides an engaging and user-friendly introduction to the study of language. Assuming no prior knowledge of the subject, Yule presents information in bite-sized sections, clearly explaining the major concepts in linguistics through all the key elements of language. This sixth edition has been revised and updated throughout, with substantial changes made to the chapters on phonetics, grammar and syntax, and eighty new study questions. To increase student engagement and to foster problem-solving and critical thinking skills, the book also includes twenty new tasks. An expanded and revised online study guide provides students with further resources, including answers and tutorials for all tasks, while encouraging lively and proactive learning. This is the most fundamental and easy-to-use introduction to the study of language.},
  added-at = {2016-08-28T15:48:22.000+0200},
  address = {Cambridge, UK},
  author = {Yule, George},
  biburl = {https://www.bibsonomy.org/bibtex/2ca059206bf9ec3de905a50a7b0cd93e0/vngudivada},
  edition = {Sixth},
  interhash = {b5ee847aa123830a9dc9312427a598b2},
  intrahash = {ca059206bf9ec3de905a50a7b0cd93e0},
  isbn = {978-1316606759},
  keywords = {Book Linguistics},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The study of language},
  year = 2016
}

@book{shenoi2006introduction,
  abstract = {A practical and accessible guide to understanding digital signal processingIntroduction to Digital Signal Processing and Filter Design was developed and fine-tuned from the author's twenty-five years of experience teaching classes in digital signal processing. Following a step-by-step approach, students and professionals quickly master the fundamental concepts and applications of discrete-time signals and systems as well as the synthesis of these systems to meet specifications in the time and frequency domains. Striking the right balance between mathematical derivations and theory, the book features:* Discrete-time signals and systems* Linear difference equations* Solutions by recursive algorithms* Convolution* Time and frequency domain analysis* Discrete Fourier series* Design of FIR and IIR filters* Practical methods for hardware implementationA unique feature of this book is a complete chapter on the use of a MATLAB(r) tool, known as the FDA (Filter Design and Analysis) tool, to investigate the effect of finite word length and different formats of quantization, different realization structures, and different methods for filter design. This chapter contains material of practical importance that is not found in many books used in academic courses. It introduces students in digital signal processing to what they need to know to design digital systems using DSP chips currently available from industry.With its unique, classroom-tested approach, Introduction to Digital Signal Processing and Filter Design is the ideal text for students in electrical and electronic engineering, computer science, and applied mathematics, and an accessible introduction or refresher for engineers and scientists in the field.An Instructor's Manual presenting detailed solutions to all the problems in the book is available online from the Wiley editorial department.An Instructor Support FTP site is also available.},
  added-at = {2016-09-20T02:52:00.000+0200},
  address = {Hoboken, N.J.},
  author = {Shenoi, Belle A.},
  biburl = {https://www.bibsonomy.org/bibtex/2305dd0e897da9a7141f1efe356c68f94/vngudivada},
  interhash = {54a358d8bb13b775d95300eda8614616},
  intrahash = {305dd0e897da9a7141f1efe356c68f94},
  keywords = {Book DSP},
  publisher = {Wiley-Interscience},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Introduction to digital signal processing and filter design},
  year = 2006
}

@inproceedings{mahedero2005natural,
  abstract = {We report experiments on the use of standard natural language processing (NLP) tools for the analysis of music lyrics. A significant amount of music audio has lyrics. Lyrics encode an important part of the semantics of a song, therefore their analysis complements that of acoustic and cultural metadata and is fundamental for the development of complete music information retrieval systems. Moreover, a textual analysis of a song can generate ground truth data that can be used to validate results from purely acoustic methods. Preliminary results on language identification, structure extraction, categorization and similarity searches suggests that a lot of profit can be gained from the analysis of lyrics.},
  added-at = {2016-09-10T13:48:46.000+0200},
  address = {New York, NY, USA},
  author = {Mahedero, Jose P. G. and Mart\'{I}nez, \'{A}lvaro and Cano, Pedro and Koppenberger, Markus and Gouyon, Fabien},
  biburl = {https://www.bibsonomy.org/bibtex/24f5c679992b506bdc8da528cf829e9f2/vngudivada},
  booktitle = {Proceedings of the 13th Annual ACM International Conference on Multimedia},
  interhash = {e53d7ba24550a3718c132d051003980a},
  intrahash = {4f5c679992b506bdc8da528cf829e9f2},
  keywords = {Lyric Song},
  pages = {475--478},
  publisher = {ACM},
  series = {MULTIMEDIA '05},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Natural Language Processing of Lyrics},
  url = {http://doi.acm.org/10.1145/1101149.1101255},
  year = 2005
}

@misc{zenczykowska2012teaching,
  added-at = {2016-09-22T13:16:39.000+0200},
  author = {Zenczykowska, Alicja},
  biburl = {https://www.bibsonomy.org/bibtex/214cd240cb29af624710ef312c3c81aec/vngudivada},
  interhash = {38d173fe76697f6521543753d6467fb0},
  intrahash = {14cd240cb29af624710ef312c3c81aec},
  keywords = {IPA},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Teaching the IPA},
  year = 2012
}

@book{mitchell1997machine,
  abstract = {The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, recognize faces or spoken speech, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. It discusses many methods based in different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining, in order to present a unified treatment of machine learning problems and solutions. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The book can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods. After an introduction that defines machine learning and gives examples of machine learning applications, the book covers supervised learning, Bayesian decision theory, parametric methods, multivariate methods, dimensionality reduction, clustering, nonparametric methods, decision trees, linear discrimination, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, combining multiple learners, and reinforcement learning.

Mitchell covers the field of machine learning, the study of algorithms that allow computer programs to automatically improve through experience and that automatically infer general laws from specific data.},
  added-at = {2016-09-10T00:12:16.000+0200},
  address = {New York, NY},
  author = {Mitchell, Tom M.},
  biburl = {https://www.bibsonomy.org/bibtex/23e79734ee1a6e49aee02ffd108224d1c/vngudivada},
  description = {This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning--including probability and statistics, artificial intelligence, and neural networks--unifying them all in a logical and coherent manner. Machine Learning serves as a useful reference tool for software developers and researchers, as well as an outstanding text for college students.},
  edition = {First},
  interhash = {479a66c32badb3a455fbdcf8e6633a5d},
  intrahash = {3e79734ee1a6e49aee02ffd108224d1c},
  isbn = {978-0071154673},
  keywords = {Book MachineLearning},
  publisher = {McGraw-Hill},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Machine Learning},
  year = 1997
}

@book{oram2007beautiful,
  abstract = {Leading computer scientists discuss how they found unusual, carefully designed solutions to difficult problems. The text lets readers look over the shoulder of major coding and design experts to see problems through their eyes.},
  added-at = {2016-08-26T01:38:22.000+0200},
  address = {Sebastopol, Calif.},
  author = {Oram, Andrew and Wilson, Greg},
  biburl = {https://www.bibsonomy.org/bibtex/290b0dff4b66f2fb45a7bdf0cbaae8cf9/vngudivada},
  interhash = {2fe64b61cbb20b3765dccccf5a1d8f14},
  intrahash = {90b0dff4b66f2fb45a7bdf0cbaae8cf9},
  keywords = {Book Code ComputerScience},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Beautiful code},
  year = 2007
}

@book{subramuniyaswami2002merging,
  abstract = {Merging with Siva, Hinduism's Contemporary Metaphysics is a guide for one who is ready to diligently walk the spiritual path. Great new vistas open up throughout its 365 daily lessons as Gurudeva shares, in the clearest terms, deep metaphysical insights into the nature of God, soul and world, mind, emotions, ultimate realizations, chakras, purpose of life on earth and much, much more. Simple but effective practices are taught: how to remould our nature and karmas, calm the mind, develop self-esteem, begin to meditate, clear up the past and create a bright future.},
  added-at = {2016-08-28T03:58:54.000+0200},
  author = {Subramuniyaswami, Satguru Sivaya},
  biburl = {https://www.bibsonomy.org/bibtex/2e24c239f35bcc9f294f96b1e93333c73/vngudivada},
  interhash = {6d163c29514051d29dcab777531ef6cd},
  intrahash = {e24c239f35bcc9f294f96b1e93333c73},
  keywords = {Book Shiva},
  publisher = {Himalayan Academy},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Merging with Siva: Hinduism's contemporary metaphysics},
  year = 2002
}

@book{pinker1994language,
  abstract = {Everyone has questions about language. Some are from everyday experience: Why do immigrants struggle with a new language, only to have their fluent children ridicule their grammatical errors? Why can't computers converse with us? Why is the hockey team in Toronto called the Maple Leafs, not the Maple Leaves? Some are from popular science: Have scientists really reconstructed the first language spoken on earth? Are there genes for grammar? Can chimpanzees learn sign language? And some are from our deepest ponderings about the human condition: Does our language control our thoughts? How could language have evolved? Is language deteriorating? Today laypeople can chitchat about black holes and dinosaur extinictions, but their curiosity about their own speech has been left unsatisfied - until now. In The Language Instinct, Steven Pinker, one of the world's leading scientists of language and the mind, lucidly explains everything you always wanted to know about language: how it works, how children learn it, how it changes, how the brain computes it, how it evolved. But The Language Instinct is no encyclopedia. With wit, erudition, and deft use of everyday examples of humor and wordplay, Pinker weaves our vast knowledge of language into a compelling theory: that language is a human instinct, wired into our brains by evolution like web spinning in spiders or sonar in bats. The theory not only challenges conventional wisdom about language itself (especially from the self-appointed "experts" who claim to be safe-guarding the language but who understand it less well than a typical teenager). It is part of a whole new vision of the human mind: not a general-purpose computer, but a collection of instincts adapted to solving evolutionarily significant problems - the mind as a Swiss Army knife. Entertaining, insightful, provocative, The Language Instinct will change the way you talk about talking and think about thinking.},
  added-at = {2016-08-28T15:18:51.000+0200},
  address = {New York, NY},
  author = {Pinker, Steven},
  biburl = {https://www.bibsonomy.org/bibtex/2cf0b5b42e554651d1fa5934f91d99d56/vngudivada},
  description = {In this classic, the world's expert on language and mind lucidly explains everything you always wanted to know about language: how it works, how children learn it, how it changes, how the brain computes it, and how it evolved. With deft use of examples of humor and wordplay, Steven Pinker weaves our vast knowledge of language into a compelling story: language is a human instinct, wired into our brains by evolution. The Language Instinct received the William James Book Prize from the American Psychological Association and the Public Interest Award from the Linguistics Society of America.},
  interhash = {359b4d877ac054b05985bb743bea5dd5},
  intrahash = {cf0b5b42e554651d1fa5934f91d99d56},
  isbn = {978-0061336461},
  keywords = {Bppk Linguistics},
  publisher = {Harper Perennial Modern Classics},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The language instinct: how the mind creates language},
  year = 1994
}

@article{chater2006probabilistic,
  abstract = {Probabilistic methods are providing new explanatory approaches to fundamental cognitive science questions of how humans structure, process and acquire language. This review examines probabilistic models defined over traditional symbolic structures. Language comprehension and production involve probabilistic inference in such models; and acquisition involves choosing the best model, given innate constraints and linguistic and other input. Probabilistic models can account for the learning and processing of language, while maintaining the sophistication of symbolic models. A recent burgeoning of theoretical developments and online corpus creation has enabled large models to be tested, revealing probabilistic constraints in processing, undermining acquisition arguments based on a perceived poverty of the stimulus, and suggesting fruitful links with probabilistic theories of categorization and ambiguity resolution in perception.},
  added-at = {2016-08-28T15:33:36.000+0200},
  author = {Chater, Nick and Manning, Christopher D.},
  biburl = {https://www.bibsonomy.org/bibtex/26bce51c242f8f9c1d0a1dfcb04a3b28e/vngudivada},
  interhash = {288592e0fdcda8b18f72d7a08a07b3af},
  intrahash = {6bce51c242f8f9c1d0a1dfcb04a3b28e},
  journal = {TRENDS in Cognitive Sciences},
  keywords = {LanguageAcquisition LanguageProcessing NLP ProbabilisticModel},
  number = 7,
  pages = {335 - 344},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Probabilistic models of language processing and acquisition},
  volume = 10,
  year = 2006
}

@book{rickerson2012fiveminute,
  abstract = {The Five-Minute Linguist has been a popular introduction to the subject of language because it is succinct, clear, accurate, -- and fun to read. Now in its second edition, updated and expanded, the book has been warmly received by readers across the globe because it offers quick and reliable answers to questions about language that most of us have, such as: How many languages are there? What was the first language? What causes foreign accents? Are dialects dying? The book is the work of experts, authoritative and full of facts, but not technical or aimed at an audience of scholars. It is used by beginning students of linguistics and anthropology, and has broad appeal for general readers, people who read for enjoyment as well as knowledge. It has a conversational style that feels more like a series of fireside chats than a college textbook, because it started life as a series of five-minute radio broadcasts. Its chapters are short, suitable for browsing or reading on the run. But although it is intentionally light in tone, the book is full of up-to-date information, written by a cross-section of leading linguists in the U.S. and abroad. The second edition of the book was produced under the sponsorship of the Linguistic Society of America and the (U.S.) National Museum of Language.},
  added-at = {2016-08-28T15:59:54.000+0200},
  address = {Oakville, Conn.},
  author = {Rickerson, E. M. and Hilton, Barry},
  biburl = {https://www.bibsonomy.org/bibtex/22e7aa8ba0c3786c6bdecb5fe6f7b78a0/vngudivada},
  edition = {Second},
  interhash = {26286e70b2c445742c7ad17f65ce48b1},
  intrahash = {2e7aa8ba0c3786c6bdecb5fe6f7b78a0},
  keywords = {Book Linguistics},
  publisher = {Equinox Pub.},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The five-minute linguist: bite-sized essays on language and languages},
  year = 2012
}

@book{velleman2006prove,
  abstract = {This new edition of Velleman's successful text will prepare students to make the transition from solving problems to proving theorems by teaching them the techniques needed to read and write proofs." "The book begins with the basic concepts of logic and set theory to familiarize students with the language of mathematics and how it is interpreted. These concepts are used as the basis for a step-by-step breakdown of the most important techniques used in constructing proofs. The author shows how complex proofs are built up from these smaller steps, using detailed "scratch work" sections to expose the machinery of proofs for the natural numbers, relations, functions, and infinite sets. To give students the opportunity to construct their own proofs, this new edition contains more than 200 new exercises, selected solutions, and an introduction to Proof Designer software." "No background beyond standard high school mathematics is assumed. This book will be useful to anyone interested in logic and proofs: computer scientists, philosophers, linguists, and of course mathematicians.},
  added-at = {2016-08-29T00:14:24.000+0200},
  address = {New York, NY},
  author = {Velleman, Daniel J.},
  biburl = {https://www.bibsonomy.org/bibtex/20727397ad5f945fa588fbda16f302940/vngudivada},
  interhash = {055227c1388f205b69e5df93b73fdde8},
  intrahash = {0727397ad5f945fa588fbda16f302940},
  keywords = {Book ProofTechniques},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {How to prove it: a structured approach},
  year = 2006
}

@inproceedings{rivers2015designing,
  abstract = {Intelligent Tutoring Systems are highly effective at helping students learn, but have required intensive amounts of development time in the past, keeping teachers from making their own. Data-driven tutoring has made it possible to build these tutors more efficiently. For my thesis work, I intend to build an authoring tool for data-driven tutors that is designed to be used by computer science teachers. I plan to design this system based on data gathered in interviews with CS educators and evaluate it on its usability for new users.},
  added-at = {2016-08-17T16:48:57.000+0200},
  address = {New York, NY, USA},
  author = {Rivers, Kelly},
  biburl = {https://www.bibsonomy.org/bibtex/2a992b06564f1a3e78a1113eb3e00a964/vngudivada},
  booktitle = {Proceedings of the Eleventh Annual International Conference on International Computing Education Research},
  interhash = {102c85950cac4001b01cfd059cb13ffe},
  intrahash = {a992b06564f1a3e78a1113eb3e00a964},
  keywords = {DataScience ITS},
  pages = {277--278},
  publisher = {ACM},
  series = {ICER '15},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Designing a Data-Driven Tutor Authoring Tool for CS Educators},
  url = {http://doi.acm.org/10.1145/2787622.2787749},
  year = 2015
}

@article{berkhin2006survey,
  abstract = {Clustering is the division of data into groups of similar objects. In clustering, some details are disregarded in exchange for data simplification. Clustering can be viewed as a data modeling technique that provides for concise summaries of the data. Clusteringis therefore related to many disciplines and plays an important role in a broad range of applications. The applications ofclustering usually deal with large datasets and data with many attributes. Exploration of such data is a subject of data mining.This survey concentrates on clustering algorithms from a data mining perspective.},
  added-at = {2016-08-28T19:09:33.000+0200},
  author = {Berkhin, P.},
  biburl = {https://www.bibsonomy.org/bibtex/2748cbddfea81f1e77927cb72532c5875/vngudivada},
  interhash = {fc69b0651d1395d91e6cdcbcf80bcd7b},
  intrahash = {748cbddfea81f1e77927cb72532c5875},
  journal = {Grouping Multidimensional Data},
  keywords = {Clustering DataMining Survey},
  pages = {25--71},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {A Survey of Clustering Data Mining Techniques},
  url = {http://dx.doi.org/10.1007/3-540-28349-8_2},
  year = 2006
}

@book{cyganek2013object,
  abstract = {Object detection, tracking and recognition in images are key problems in computer vision. This book provides the reader with a balanced treatment between the theory and practice of selected methods in these areas to make the book accessible to a range of researchers, engineers, developers and postgraduate students working in computer vision and related fields. Key features: Explains the main theoretical ideas behind each method (which are augmented with a rigorous mathematical derivation of the formulas), their implementation (in C++) and demonstrated working in real applications. Places an emphasis on tensor and statistical based approaches within object detection and recognition. Provides an overview of image clustering and classification methods which includes subspace and kernel based processing, mean shift and Kalman filter, neural networks, and kmeans methods. Contains numerous case study examples of mainly automotive applications. Includes a companion website hosting full C++ implementation, of topics presented in the book as a software library, and an accompanying manual to the software platform.},
  added-at = {2016-08-26T01:21:57.000+0200},
  author = {Cyganek, Boguslaw},
  biburl = {https://www.bibsonomy.org/bibtex/2035b92a02a8b3640959ae070bd163e66/vngudivada},
  interhash = {f1dc79b43099019da8200a58662e050d},
  intrahash = {035b92a02a8b3640959ae070bd163e66},
  keywords = {Book DIP ObjectRecognition},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Object detection and recognition in digital images: theory and practice},
  year = 2013
}

@proceedings{moore2016spoken,
  abstract = {Recent years have seen significant market penetration for voice-based personal assistants such as Apple's Siri. However, despite this success, user take-up is frustratingly low. This position paper argues that there is a habitability gap caused by the inevitable mismatch between the capabilities and expectations of human users and the features and benefits provided by contemporary technology. Suggestions are made as to how such problems might be mitigated, but a more worrisome question emerges: "is spoken language all-or-nothing"? The answer, based on contemporary views on the special nature of (spoken) language, is that there may indeed be a fundamental limit to the interaction that can take place between mismatched interlocutors (such as humans and machines). However, it is concluded that interactions between native and non-native speakers, or between adults and children, or even between humans and dogs, might provide critical inspiration for the design of future speech-based human-machine interaction.},
  added-at = {2016-09-10T13:56:30.000+0200},
  author = {Moore, R. K.},
  biburl = {https://www.bibsonomy.org/bibtex/2163333333988b7426e0fcdd0965f706e/vngudivada},
  edition = {In K. Jokinen \& G. Wilcock (Eds.)},
  interhash = {d81a3630b8fef340eeb2afdaa6a15dcb},
  intrahash = {163333333988b7426e0fcdd0965f706e},
  journal = {Dialogues with Social Robots {\textendash} Enablements, Analyses, and Evaluation. },
  keywords = {SpokenLanguage SpokenLanguageInterface},
  month = { (in press)},
  publisher = {Springer Lecture Notes in in Electrical Engineering (LNEE)},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction.},
  url = {http://arxiv.org/abs/1607.05174},
  year = 2016
}

@book{greitzer2011cognitive,
  abstract = {In this report, we provide an overview of scientific/technical literature on information visualization and VA. Topics discussed include an update and overview of the extensive literature search conducted for this study, the nature and purpose of the field, major research thrusts, and scientific foundations. We review methodologies for evaluating and measuring the impact of VA technologies as well as taxonomies that have been proposed for various purposes to support the VA community. A cognitive science perspective underlies each of these discussions.},
  added-at = {2016-08-17T00:38:26.000+0200},
  author = {Greitzer, F.L. and Noonan, C.F. and Franklin, L.R.},
  biburl = {https://www.bibsonomy.org/bibtex/27c34fe0ea1e1d5c21e95d64cecac61e0/vngudivada},
  interhash = {ee4215c9fc870e8e013ee786f20b593c},
  intrahash = {7c34fe0ea1e1d5c21e95d64cecac61e0},
  keywords = {Book CognitiveModel VisualAnalytics},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Cognitive Foundations for Visual Analytics},
  year = 2011
}

@book{ofhawaii2016upanishads,
  added-at = {2016-08-30T13:24:40.000+0200},
  author = {of Hawaii, University},
  biburl = {https://www.bibsonomy.org/bibtex/23fb6039e0914e439cabbe0da3071a7b4/vngudivada},
  interhash = {7834fe79470c1580f5b761435268d867},
  intrahash = {3fb6039e0914e439cabbe0da3071a7b4},
  keywords = {Book Spirituality Upanishads},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The Upanishads},
  year = 2016
}

@book{gollapudi2016practical,
  abstract = {
Tackle the real-world complexities of modern machine learning with innovative, cutting-edge, techniques

Fully-coded working examples using a wide range of machine learning libraries and tools, including Python, R, Julia, and Spark

Comprehensive practical solutions taking you into the future of machine learning

Go a step further and integrate your machine learning projects with Hadoop

Who This Book Is For

This book has been created for data scientists who want to see machine learning in action and explore its real-world application. With guidance on everything from the fundamentals of machine learning and predictive analytics to the latest innovations set to lead the big data revolution into the future, this is an unmissable resource for anyone dedicated to tackling current big data challenges. Knowledge of programming (Python and R) and mathematics is advisable if you want to get started immediately.


What You Will Learn

Implement a wide range of algorithms and techniques for tackling complex data * Get to grips with some of the most powerful languages in data science, including R, Python, and Julia

Harness the capabilities of Spark and Hadoop to manage and process data successfully * Apply the appropriate machine learning technique to address real-world problems

Get acquainted with Deep learning and find out how neural networks are being used at the cutting-edge of machine learning

Explore the future of machine learning and dive deeper into polyglot persistence, semantic data, and more.

In Detail

Finding meaning in increasingly larger and more complex datasets is a growing demand of the modern world. Machine learning and predictive analytics have become the most important approaches to uncover data gold mines. Machine learning uses complex algorithms to make improved predictions of outcomes based on historical patterns and the behavior of data sets. Machine learning can deliver dynamic insights into trends, patterns, and relationships within data, immensely valuable to business growth and development. This book explores an extensive range of machine learning techniques uncovering hidden tricks and tips for several types of data using practical and real-world examples.

While machine learning can be highly theoretical, this book offers a refreshing hands-on approach without losing sight of the underlying principles. Inside, a full exploration of the various algorithms gives you high-quality guidance so you can begin to see just how effective machine learning is at tackling contemporary challenges of big data. This is the only book you need to implement a whole suite of open source tools, frameworks, and languages in machine learning. We will cover the leading data science languages, Python and R, and the underrated but powerful Julia, as well as a range of other big data platforms including Spark, Hadoop, and Mahout.

Practical Machine Learning is an essential resource for the modern data scientists who want to get to grips with its real-world application. With this book, you will not only learn the fundamentals of machine learning but dive deep into the complexities of real world data before moving on to using Hadoop and its wider ecosystem of tools to process and manage your structured and unstructured data. You will explore different machine learning techniques for both supervised and unsupervised learning; from decision trees to Naive Bayes classifiers and linear and clustering methods, you will learn strategies for a truly advanced approach to the statistical analysis of data. The book also explores the cutting-edge advancements in machine learning, with worked examples and guidance on deep learning and reinforcement learning, providing you with practical demonstrations and samples that help take the theory-and mystery-out of even the most advanced machine learning methodologies.

Style and approach

A practical data science tutorial designed to give you an insight into the practical application of machine learning, this book takes you through complex concepts and tasks in an accessible way. Featuring information on a wide range of data science techniques, Practical Machine Learning is a comprehensive data science resource.},
  added-at = {2016-09-09T23:43:46.000+0200},
  author = {Gollapudi, Sunila and Laxmikanth, V.},
  biburl = {https://www.bibsonomy.org/bibtex/214244e583aa01dfd4262a5a46560150a/vngudivada},
  interhash = {5852abc9467cd24b8c427550cbb8b286},
  intrahash = {14244e583aa01dfd4262a5a46560150a},
  keywords = {Book MachineLearning},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Practical machine learning : tackle the real-world complexities of modern machine learning with innovative and cutting-edge techniques},
  year = 2016
}

@article{halevy2009unreasonable,
  abstract = {At Brown University, there is excitement of having access to the Brown Corpus, containing one million English words. Since then, we have seen several notable corpora that are about 100 times larger, and in 2006, Google released a trillion-word corpus with frequency counts for all sequences up to five words long. In some ways this corpus is a step backwards from the Brown Corpus: it's taken from unfiltered Web pages and thus contains incomplete sentences, spelling errors, grammatical errors, and all sorts of other errors. It's not annotated with carefully hand-corrected part-of-speech tags. But the fact that it's a million times larger than the Brown Corpus outweighs these drawbacks. A trillion-word corpus - along with other Web-derived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions - captures even very rare aspects of human behavior. So, this corpus could serve as the basis of a complete model for certain tasks - if only we knew how to extract the model from the data.},
  added-at = {2016-09-10T04:08:54.000+0200},
  author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
  biburl = {https://www.bibsonomy.org/bibtex/29b974812cff8a48e95f595c5293cd4c1/vngudivada},
  doi = {10.1109/mis.2009.36},
  interhash = {d08a7d9e7d72d8a7f5fe1e602f79409c},
  intrahash = {9b974812cff8a48e95f595c5293cd4c1},
  journal = {{IEEE} Intell. Syst.},
  keywords = {DataScience},
  month = mar,
  number = 2,
  pages = {8--12},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The Unreasonable Effectiveness of Data},
  url = {http://dx.doi.org/10.1109/MIS.2009.36},
  volume = 24,
  year = 2009
}

@book{tolstov1976fourier,
  abstract = {Richard A. Silverman's series of translations of outstanding Russian textbooks and monographs is well-known to people in the fields of mathematics, physics, and engineering. The present book is another excellent text from this series, a valuable addition to the English-language literature on Fourier series.
This edition is organized into nine well-defined chapters: Trigonometric Fourier Series, Orthogonal Systems, Convergence of Trigonometric Fourier Series, Trigonometric Series with Decreasing Coefficients, Operations on Fourier Series, Summation of Trigonometric Fourier Series, Double Fourier Series and the Fourier Integral, Bessel Functions and Fourier-Bessel Series, and the Eigenfunction Method and its Applications to Mathematical Physics. Every chapter moves clearly from topic to topic and theorem to theorem, with many theorem proofs given. A total of 107 problems will be found at the ends of the chapters, including many specially added to this English-language edition, and answers are given at the end of the text.
Richard Silverman's excellent translation makes this book readily accessible to mathematicians and math students, as well as workers and students in the fields of physics and engineering. He has also added a bibliography, containing suggestions for collateral and supplementary reading. 1962 edition.},
  added-at = {2016-09-20T03:06:37.000+0200},
  address = {New York, NY},
  author = {Tolstov, G. P. and Silverman, Richard A.},
  biburl = {https://www.bibsonomy.org/bibtex/22934d465cc6017c7b6fdf50d10bc5187/vngudivada},
  interhash = {46d142ffbb15db06b8cf8a025f3e43a6},
  intrahash = {2934d465cc6017c7b6fdf50d10bc5187},
  keywords = {Book DSP FourierSeries},
  publisher = {Dover Publications},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Fourier series},
  year = 1976
}

@book{segaran2009beautiful,
  abstract = {In this insightful book, you'll learn from the best data practitioners in the field just how wide-ranging -- and beautiful -- working with data can be. Join 39 contributors as they explain how they developed simple and elegant solutions on projects ranging from the Mars lander to a Radiohead video.

With Beautiful Data, you will:

Explore the opportunities and challenges involved in working with the vast number of datasets made available by the Web
Learn how to visualize trends in urban crime, using maps and data mashups
Discover the challenges of designing a data processing system that works within the constraints of space travel
Learn how crowdsourcing and transparency have combined to advance the state of drug research
Understand how new data can automatically trigger alerts when it matches or overlaps pre-existing data
Learn about the massive infrastructure required to create, capture, and process DNA data},
  added-at = {2016-09-10T04:14:59.000+0200},
  address = {Sebastopol, CA},
  author = {Segaran, Toby and Hammerbacher, Jeff},
  biburl = {https://www.bibsonomy.org/bibtex/26df8a15eb43882ce08095d2fd5028688/vngudivada},
  interhash = {5d680f0dd4d47817adc8ab71a1086cf7},
  intrahash = {6df8a15eb43882ce08095d2fd5028688},
  keywords = {Book DataScience DataVisualization},
  publisher = {O'Reilly},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Beautiful data},
  year = 2009
}

@misc{donoho2015years,
  abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a $100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.},
  added-at = {2016-09-10T03:15:24.000+0200},
  author = {Donoho, David},
  biburl = {https://www.bibsonomy.org/bibtex/2d4e6dcdf1dd5850c5df640a517f23b2f/vngudivada},
  interhash = {44ff1834e6c5d14ad678a62bf06f78b3},
  intrahash = {d4e6dcdf1dd5850c5df640a517f23b2f},
  keywords = {DataScience},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {50 years of Data Science},
  year = 2015
}

@book{smola2008introduction,
  abstract = {The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data.
This book is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. In order to present a unified treatment of machine learning problems and solutions, it discusses many methods from different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining.
All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The text covers such topics as supervised learning, Bayesian decision theory, parametric methods, multivariate methods, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, and reinforcement learning.},
  added-at = {2016-09-10T03:10:21.000+0200},
  author = {Smola, Alex and Vishwanathan, S. V. N.},
  biburl = {https://www.bibsonomy.org/bibtex/27eb7e1039b899c6d201646e879640df9/vngudivada},
  interhash = {964a5681be9cedf788833015260f51a1},
  intrahash = {7eb7e1039b899c6d201646e879640df9},
  keywords = {Book MachineLearning},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Introduction to Machine Learning},
  year = 2008
}

@book{coutinho1999mathematics,
  abstract = {"What do prime numbers, multiplication, and factoring have in common? They are all part of The Mathematics of Ciphers, code-building at its best."--BOOK JACKET. "Revised and updated since its publication in Portuguese in 1997, this book is an introduction to the algorithmic aspects of number theory and its applications to cryptography. Accompanied by historical anecdotes, the familiar topics of number theory are defined and explored. The author takes the reader on a leisurely journey through this fascinating field, culminating in a visit to the RSA cryptosystem."--Jacket.},
  added-at = {2016-08-26T01:18:27.000+0200},
  address = {Natick, Mass.},
  author = {Coutinho, S. C.},
  biburl = {https://www.bibsonomy.org/bibtex/241e911c78b8f8ecfeae46da497485ac6/vngudivada},
  description = {This book is an introduction to the algorithmic aspects of number theory and its applications to cryptography, with special emphasis on the RSA cryptosys-tem. It covers many of the familiar topics of elementary number theory, all with an algorithmic twist. The text also includes many interesting historical notes.},
  interhash = {4680f22b04bd6dcf98a50bb87b5ece32},
  intrahash = {41e911c78b8f8ecfeae46da497485ac6},
  keywords = {Book Cipher Cryptography RSA},
  publisher = {A K Peters},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The mathematics of ciphers: number theory and RSA cryptography},
  year = 1999
}

@book{liu2015computational,
  abstract = {This book provides an introduction to computational trust models from a machine learning perspective. After reviewing traditional computational trust models, it discusses a new trend of applying formerly unused machine learning methodologies, such as supervised learning. The application of various learning algorithms, such as linear regression, matrix decomposition, and decision trees, illustrates how to translate the trust modeling problem into a (supervised) learning problem. The book also shows how novel machine learning techniques can improve the accuracy of trust assessment compared to traditional approaches},
  added-at = {2016-09-10T00:07:23.000+0200},
  author = {Liu, Xin and Datta, Anwitaman and Lim, Ee-Peng},
  biburl = {https://www.bibsonomy.org/bibtex/253e1815cc95daad813a44f78b2e3ef2d/vngudivada},
  description = {Computational Trust Models and Machine Learning provides a detailed introduction to the concept of trust and its application in various computer science areas, including multi-agent systems, online social networks, and communication systems. Identifying trust modeling challenges that cannot be addressed by traditional approaches, this book: Explains how reputation-based systems are used to determine trust in diverse online communities Describes how machine learning techniques are employed to build robust reputation systems Explores two distinctive approaches to determining credibility of resourcesone where the human role is implicit, and one that leverages human input explicitly Shows how decision support can be facilitated by computational trust models Discusses collaborative filtering-based trust aware recommendation systems Defines a framework for translating a trust modeling problem into a learning problem Investigates the objectivity of human feedback, emphasizing the need to filter out outlying opinions Computational Trust Models and Machine Learning effectively demonstrates how novel machine learning techniques can improve the accuracy of trust assessment.},
  interhash = {6fefd6c363bcd101132b3b25a7ea0778},
  intrahash = {53e1815cc95daad813a44f78b2e3ef2d},
  keywords = {Book MachineLearning},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Computational trust models and machine learning},
  year = 2015
}

@inproceedings{obitko2004ontology,
  abstract = {Ontologies, often defined as an explicit specification of conceptualization, are necessary for knowledge representation and knowledge exchange. Usually this means that ontology describes concepts and relations that exist in a domain. To enable knowledge exchange, it is necessary to describe these concepts and relations in a better way than just ordering them in taxonomy. However, ontology design usually starts and stops with designing taxonomies. We present a method that is based on formal concept analysis, which is a theory of data analysis which identifies conceptual structures among data sets. This method allows for discovering necessity for new concepts and relations in an ontology, which leads to an ontology that has these entities described in a way suitable for knowledge exchange.},
  added-at = {2016-08-29T04:54:58.000+0200},
  author = {Obitko, Marek and Snásel, Václav and Smid, Jan},
  biburl = {https://www.bibsonomy.org/bibtex/2408c59a616f7d40ef7106d9a215d2844/vngudivada},
  booktitle = {CLA},
  editor = {Snásel, Václav and Belohlávek, Radim},
  interhash = {2495aa0cc496cd658219523c5c070b0a},
  intrahash = {408c59a616f7d40ef7106d9a215d2844},
  keywords = {Ontology},
  pages = {111 - 119},
  series = {CEUR Workshop Proceedings},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Ontology Design with Formal Concept Analysis},
  year = 2004
}

@inproceedings{miller2007elements,
  abstract = {In many settings, such as home care or mobile environments, demands on users' attention, or users' anticipated level of formal training, or other on-site conditions will make standard keyboard-and monitor-based robot programming interfaces impractical. In such cases, a spoken language interface may be preferable. However, the open-ended task of programming a machine is very different from the sort of closed-vocabulary, data-rich applications (e.g. call routing) for which most speaker-independent spoken language interfaces are designed. This paper will describe some of the challenges of designing a spoken language programming interface for robots, and will present an approach that uses these semantic-level resources as extensively as possible in order to address these challenges.},
  added-at = {2016-09-09T05:42:08.000+0200},
  address = {New York, NY, USA},
  author = {Miller, Tim and Exley, Andy and Schuler, William},
  biburl = {https://www.bibsonomy.org/bibtex/2fd76c316f3627f235b15094d47e55131/vngudivada},
  booktitle = {Proceedings of the ACM/IEEE International Conference on Human-robot Interaction},
  interhash = {c24019278bcd47b4ae77ac769ff76702},
  intrahash = {fd76c316f3627f235b15094d47e55131},
  keywords = {SpokenLanguageInterface},
  pages = {231--237},
  publisher = {ACM},
  series = {HRI '07},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Elements of a Spoken Language Programming Interface for Robots},
  url = {http://doi.acm.org/10.1145/1228716.1228748},
  year = 2007
}

@book{shotts2012linux,
  abstract = {You've experienced the shiny, point-and-click surface of your Linux computernow dive below and explore its depths with the power of the command line.The Linux Command Line takes you from your very first terminal keystrokes to writing full programs in Bash, the most popular Linux shell. Along the way you'll learn the timeless skills handed down by generations of gray-bearded, mouse-shunning gurus: file navigation, environment configuration, command chaining, pattern matching with regular expressions, and more.In addition to that practical knowledge, author William Shotts reveals the philosophy behind these tools and the rich heritage that your desktop Linux machine has inherited from Unix supercomputers of yore.As you make your way through the book's short, easily-digestible chapters, you'll learn how to:Create and delete files, directories, and symlinks Administer your system, including networking, package installation, and process management Use standard input and output, redirection, and pipelines Edit files with Vi, the world's most popular text editor Write shell scripts to automate common or boring tasks Slice and dice text files with cut, paste, grep, patch, and sed Once you overcome your initial "shell shock," you'll find that the command line is a natural and expressive way to communicate with your computer. Just don't be surprised if your mouse starts to gather dust.},
  added-at = {2016-08-26T04:00:04.000+0200},
  address = {New York, NY},
  author = {Shotts, William E.},
  biburl = {https://www.bibsonomy.org/bibtex/2f0883431a7d784b6966d7c969cf31d54/vngudivada},
  interhash = {3b0311f80bfae15b86f338b192da94f6},
  intrahash = {f0883431a7d784b6966d7c969cf31d54},
  keywords = {Book Linux},
  publisher = {John Wiley & Sons},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The Linux Command Line: A Complete Introduction},
  year = 2012
}

@book{martin2009clean,
  abstract = {Even bad code can function. But if code isn't clean, it can bring a development organization to its knees. Every year, countless hours and significant resources are lost because of poorly written code. But it doesn't have to be that way. Noted software expert Robert C. Martin presents a revolutionary paradigm with Clean Code: A Handbook of Agile Software Craftsmanship . Martin has teamed up with his colleagues from Object Mentor to distill their best agile practice of cleaning code 'on the fly' into a book that will instill within you the values of a software craftsman and make you a better programmer-but only if you work at it. What kind of work will you be doing? You'll be reading code-lots of code. And you will be challenged to think about what's right about that code, and what's wrong with it. More importantly, you will be challenged to reassess your professional values and your commitment to your craft. Clean Code is divided into three parts. The first describes the principles, patterns, and practices of writing clean code. The second part consists of several case studies of increasing complexity. Each case study is an exercise in cleaning up code-of transforming a code base that has some problems into one that is sound and efficient. The third part is the payoff: a single chapter containing a list of heuristics and 'smells' gathered while creating the case studies. The result is a knowledge base that describes the way we think when we write, read, and clean code. Readers will come away from this book understanding How to tell the difference between good and bad code How to write good code and how to transform bad code into good code How to create good names, good functions, good objects, and good classes How to format code for maximum readability How to implement complete error handling without obscuring code logic How to unit test and practice test-driven development This book is a must for any developer, software engineer, project manager, team lead, or systems analyst with an interest in producing better code.},
  added-at = {2016-08-26T01:39:27.000+0200},
  address = {Upper Saddle River, NJ},
  author = {Martin, Robert C.},
  biburl = {https://www.bibsonomy.org/bibtex/288a5b23df2b2e80fde579f0ab62dbfef/vngudivada},
  interhash = {9e17563e02239539bb1774828ced166b},
  intrahash = {88a5b23df2b2e80fde579f0ab62dbfef},
  keywords = {Book Code},
  publisher = {Prentice Hall},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Clean code: a handbook of agile software craftsmanship},
  year = 2009
}

@book{russell2010artificial,
  abstract = {In this third edition, the authors have updated the treatment of all major areas. A new organizing principle--the representational dimension of atomic, factored, and structured models--has been added. Significant new material has been provided in areas such as partially observable search, contingency planning, hierarchical planning, relational and first-order probability models, regularization and loss functions in machine learning, kernel methods, Web search engines, information extraction, and learning in vision and robotics. The book also includes hundreds of new exercises.},
  added-at = {2016-09-10T03:12:52.000+0200},
  address = {Upper Saddle River, NJ},
  author = {Russell, Stuart J. and Norvig, Peter and Davis, Ernest},
  biburl = {https://www.bibsonomy.org/bibtex/2612986e7325b8ab0b01939109654ce79/vngudivada},
  interhash = {b24c7d6a4b73d125adc1327484d4e70f},
  intrahash = {612986e7325b8ab0b01939109654ce79},
  keywords = {ArtificialIntelligence Book},
  publisher = {Prentice Hall},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Artificial intelligence: a modern approach},
  year = 2010
}

@article{breiman2001statistical,
  abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  added-at = {2016-08-20T03:46:37.000+0200},
  author = {Breiman, L.},
  biburl = {https://www.bibsonomy.org/bibtex/271fe1961c73e2502acf6d7ce2795d493/vngudivada},
  interhash = {8be8035d3393decab43719e0c18ef386},
  intrahash = {71fe1961c73e2502acf6d7ce2795d493},
  journal = {Statistical Science},
  keywords = {MachineLearning StatisticalLearning Statistics},
  number = 3,
  pages = {199--215},
  publisher = {JSTOR},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Statistical modeling: The two cultures},
  volume = 16,
  year = 2001
}

@article{breiman2001random,
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Freund and Schapire[1996]), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  added-at = {2016-08-20T03:57:01.000+0200},
  address = {Hingham, MA},
  author = {Breiman, Leo},
  biburl = {https://www.bibsonomy.org/bibtex/2b8187107bf870043f2f93669958858f1/vngudivada},
  interhash = {4450d2e56555e7cb8f3817578e1dd4da},
  intrahash = {b8187107bf870043f2f93669958858f1},
  journal = {Machine Learning},
  keywords = {EnsembleLearning MachineLearning RandomForest},
  month = oct,
  number = 1,
  pages = {5--32},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Random Forests},
  volume = 45,
  year = 2001
}

@book{matthews2011linguistics,
  abstract = {Linguistics falls in the gap between arts and science, on the edges of which the most fascinating discoveries and the most important problems are found. Beginning at the 'arts' end of the subject with the common origins of languages, and finishing at the 'science' end with the newest discoveries regarding language in the brain, this stimulating guide covers all the major aspects of linguistics from a refreshing and insightful angle.

	About the Series: Combining authority with wit, accessibility, and style, Very Short Introductions offer an introduction to some of life's most interesting topics. Written by experts for the newcomer, they demonstrate the finest contemporary thinking about the central problems and issues in hundreds of key topics, from philosophy to Freud, quantum theory to Islam.},
  added-at = {2016-09-09T05:46:09.000+0200},
  address = {Oxford, UK},
  author = {Matthews, P. H.},
  biburl = {https://www.bibsonomy.org/bibtex/2a0876febb826fbe6f388da71f6ce761d/vngudivada},
  interhash = {d50078d30cce1c0ffd02d61cefc6eb23},
  intrahash = {a0876febb826fbe6f388da71f6ce761d},
  isbn = {978-0192801487},
  keywords = {Book Linguistics},
  publisher = {Oxford University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Linguistics: A Very Short Introduction},
  year = 2011
}

@book{fromkin2014introduction,
  abstract = {Assuming no prior knowledge of linguistics, AN INTRODUCTION TO LANGUAGE, Tenth Edition, is appropriate for a variety of fields--including education, languages, psychology, cognitive science, anthropology, English, and teaching English as a Second Language (TESL)--at both the undergraduate and graduate levels. This completely updated edition retains the clear descriptions, humor, and seamless pedagogy that have made the book a perennial best-seller, while adding new information and exercises that render each topic fresh, engaging, and current.},
  added-at = {2016-09-08T18:45:20.000+0200},
  author = {Fromkin, Victoria and Rodman, Robert and Hyams, Nina M.},
  biburl = {https://www.bibsonomy.org/bibtex/2db6b8d9b661f4399dcf4bd40e96334a2/vngudivada},
  edition = {Tenth},
  interhash = {f3bdc3d435c5c24934cdcdc23bac5406},
  intrahash = {db6b8d9b661f4399dcf4bd40e96334a2},
  isbn = {978-1133310686},
  keywords = {Book Language Linguistics},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {An Introduction to Language},
  year = 2014
}

@book{downey2013think,
  abstract = {If you know how to program with Python and also know a little about probability, you're ready to tackle Bayesian statistics. With this book, you'll learn how to solve statistical problems with Python code instead of mathematical notation, and use discrete probability distributions instead of continuous mathematics. Once you get the math out of the way, the Bayesian fundamentals will become clearer, and you'll begin to apply these techniques to real-world problems. Bayesian statistical methods are becoming more common and more important, but not many resources are available to help beginners. Based on undergraduate classes taught by author Allen Downey, this book's computational approach helps you get a solid start. Use your existing programming skills to learn and understand Bayesian statistics Work with problems involving estimation, prediction, decision analysis, evidence, and hypothesis testing Get started with simple examples, using coins, MMs, Dungeons Dragons dice, paintball, and hockey Learn computational methods for solving real-world problems, such as interpreting SAT scores, simulating kidney tumors, and modeling the human microbiome.},
  added-at = {2016-08-26T01:30:10.000+0200},
  address = {Sebastopol, California},
  author = {Downey, Allen},
  biburl = {https://www.bibsonomy.org/bibtex/240427dcb657e64e2cb80d10d189c9f67/vngudivada},
  interhash = {844f98cb0839531b5c8bbe9f991bd7a3},
  intrahash = {40427dcb657e64e2cb80d10d189c9f67},
  keywords = {Bayes Book Statistics},
  publisher = {O'Reilly Media},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Think Bayes},
  year = 2013
}

@book{jeffries2006discovering,
  abstract = {This text provides students with the basic descriptive knowledge they are required to master before moving on to more advanced study. The book is organized under thematic headings, which are thoroughly cross-referenced, enabling students and teachers to use the book as required--either as a course text or to help with individual aspects of language. Each section includes an introduction, worked examples, "in context" sections relating the topic to real text examples, suggestions for further reading and analysis and a summary.},
  added-at = {2016-09-08T19:43:59.000+0200},
  address = {New York, NY},
  author = {Jeffries, Lesley},
  biburl = {https://www.bibsonomy.org/bibtex/241a644c6425d5b66755cc5000c235333/vngudivada},
  interhash = {7e7d5646efad7a1ce1f3215d649a5897},
  intrahash = {41a644c6425d5b66755cc5000c235333},
  keywords = {Book Language Linguistics},
  publisher = {Palgrave Macmillan},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Discovering language: the structure of modern English},
  year = 2006
}

@article{chandrasekaran1999ontologies,
  abstract = {This survey provides a conceptual introduction to ontologies and their role in information systems and AI. The authors also discuss how ontologies clarify the domain's structure of knowledge and enable knowledge sharing},
  added-at = {2016-08-29T04:45:24.000+0200},
  author = {Chandrasekaran, B. and Josephson, J. R. and Benjamins, V. R.},
  biburl = {https://www.bibsonomy.org/bibtex/21f7a212b846024983160bee5cff1af94/vngudivada},
  interhash = {c4a181bdeb87e30196c6161628fcdadc},
  intrahash = {1f7a212b846024983160bee5cff1af94},
  journal = {IEEE Intelligent Systems and Their Applications},
  keywords = {Ontology},
  number = 1,
  pages = {20--26},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {What are ontologies, and why do we need them?},
  volume = 14,
  year = 1999
}

@misc{noy2001ontology,
  abstract = {Ontologies have become core components of many large applications yet the training material has not kept pace with the growing interest. This paper addresses the issues of why one would build an ontology and presents a methodology for creating ontologies based on declarative knowledge representation systems. It leverages the two authors experiences building and maintaining ontologies in a number of ontology environments including Protege-2000, Ontolingua, and Chimaera. It presents the methodology by example utilizing a tutorial wines knowledge base example. While it is aimed at users of frame-based systems, it can be useful for building ontologies in any object-centered system.},
  added-at = {2016-08-29T04:51:49.000+0200},
  author = {Noy, Natalya F. and {mcguinness}, Deborah L.},
  biburl = {https://www.bibsonomy.org/bibtex/2dc68dbc549f9f8e4c54b20ca0224531f/vngudivada},
  howpublished = {Online},
  interhash = {2a496bb63d728d96264a50c0a4960e9a},
  intrahash = {dc68dbc549f9f8e4c54b20ca0224531f},
  keywords = {Ontology},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Ontology Development 101: A Guide to Creating Your First Ontology},
  year = 2001
}

@book{lander2014everyone,
  abstract = {Using the open source R language, you can build powerful statistical models to answer many of your most challenging questions. R has traditionally been difficult for non-statisticians to learn, and most R books assume far too much knowledge to be of help. R for Everyone is the solution.
Drawing on his unsurpassed experience teaching new users, professional data scientist Jared P. Lander has written the perfect tutorial for anyone new to statistical programming and modeling. Organized to make learning easy and intuitive, this guide focuses on the 20 percent of R functionality you’ll need to accomplish 80 percent of modern data tasks.
Lander’s self-contained chapters start with the absolute basics, offering extensive hands-on practice and sample code. You’ll download and install R; navigate and use the R environment; master basic program control, data import, and manipulation; and walk through several essential tests. Then, building on this foundation, you’ll construct several complete models, both linear and nonlinear, and use some data mining techniques.
By the time you’re done, you won’t just know how to write R programs, you’ll be ready to tackle the statistical problems you care about most.

COVERAGE INCLUDES
• Exploring R, RStudio, and R packages
• Using R for math: variable types, vectors, calling functions, and more
• Exploiting data structures, including data.frames, matrices, and lists
• Creating attractive, intuitive statistical graphics
• Writing user-defined functions
• Controlling program flow with if, ifelse, and complex checks
• Improving program efficiency with group manipulations
• Combining and reshaping multiple datasets
• Manipulating strings using R’s facilities and regular expressions
• Creating normal, binomial, and Poisson probability distributions
• Programming basic statistics: mean, standard deviation, and t-tests
• Building linear, generalized linear, and nonlinear models
• Assessing the quality of models and variable selection
• Preventing overfitting, using the Elastic Net and Bayesian methods
• Analyzing univariate and multivariate time series data
• Grouping data via K-means and hierarchical clustering
• Preparing reports, slideshows, and web pages with knitr
• Building reusable R packages with devtools and Rcpp
• Getting involved with the R global community},
  added-at = {2016-09-11T18:07:25.000+0200},
  author = {Lander, Jared P.},
  biburl = {https://www.bibsonomy.org/bibtex/2946d78c81f63926dd45ba1a5833b97b5/vngudivada},
  interhash = {ce14bf12f49bb114ab70c7c5bd4bfacb},
  intrahash = {946d78c81f63926dd45ba1a5833b97b5},
  keywords = {Book BuyHardCopy ParExcellence R Statistics},
  publisher = {Addison-Wesley Educational Publishers},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {R for Everyone: Advanced Analytics and Graphics},
  year = 2014
}

@book{mihalicek2011language,
  abstract = {Since its inception, Language Files has become one of the most widely adopted, consulted, and authoritative introductory textbooks to linguistics ever written. The scope of the text makes it suitable for use in a wide range of courses, while its unique organization into student-friendly, self-contained sections allows for tremendous flexibility in course design.

The eleventh edition has been revised, clarified, and updated in many places. This edition includes an all-new syntax chapter, new files on language and culture and on writing systems, restructured semantics files, and many new examples, exercises, and activities. Additional readings have been added to all chapters, and the number of cross-references among chapters has been increased. In addition, the accompanying Language Files webpage has links to online resources and websites related to language and linguistics that instructors and students may find interesting.},
  added-at = {2016-08-28T15:16:29.000+0200},
  address = {Columbus, Ohio},
  author = {Mihalicek, Vedrana and Wilson, Christin},
  biburl = {https://www.bibsonomy.org/bibtex/270858aac1bf86defca906a675a98a28c/vngudivada},
  interhash = {de5856bb1a9755b3ba3f56045e0bb649},
  intrahash = {70858aac1bf86defca906a675a98a28c},
  keywords = {Book Linguistics ParExcellence},
  publisher = {Ohio State University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Language files: materials for an introduction to language and linguistics},
  year = 2011
}

@book{dua2011mining,
  abstract = {Introducing basic concepts of machine learning and data mining methodologies for cyber security, this book provides a unified reference for specific machine learning solutions and cybersecurity problems. The authors focus on how to apply machine learning methodologies in cybersecurity, categorizing methods for detecting, scanning, profiling, intrusions, and anomalies. The text presents challenges and solutions in machine learning along with cybersecurity fundamentals. It also describes advanced problems in cybersecurity in the machine learning domain and examines privacy-preserving data mining methods as a proactive security solution"-- "This interdisciplinary assessment is especially useful for students, who typically learn cybersecurity, machine learning, and data mining in independent courses. Machine learning and data mining play significant roles in cybersecurity, especially as more challenges appear with the rapid development of information discovery techniques, such as those originating from the sheer dimensionality and heterogeneous nature of the network data, the dynamic change of threats, and the severe imbalanced classes of normal and anomalous behaviors"--},
  added-at = {2016-09-10T00:14:56.000+0200},
  address = {Boca Raton, FL},
  author = {Dua, Sumeet and Du, Xian},
  biburl = {https://www.bibsonomy.org/bibtex/249205d4c38f2eb24be4de678a8a1f153/vngudivada},
  description = {With the rapid advancement of information discovery techniques, machine learning and data mining continue to play a significant role in cybersecurity. Although several conferences, workshops, and journals focus on the fragmented research topics in this area, there has been no single interdisciplinary resource on past and current works and possible paths for future research in this area. This book fills this need. From basic concepts in machine learning and data mining to advanced problems in the machine learning domain, Data Mining and Machine Learning in Cybersecurity provides a unified reference for specific machine learning solutions to cybersecurity problems. It supplies a foundation in cybersecurity fundamentals and surveys contemporary challengesdetailing cutting-edge machine learning and data mining techniques. It also: Unveils cutting-edge techniques for detecting new attacks Contains in-depth discussions of machine learning solutions to detection problems Categorizes methods for detecting, scanning, and profiling intrusions and anomalies Surveys contemporary cybersecurity problems and unveils state-of-the-art machine learning and data mining solutions Details privacy-preserving data mining methods This interdisciplinary resource includes technique review tables that allow for speedy access to common cybersecurity problems and associated data mining methods. Numerous illustrative figures help readers visualize the workflow of complex techniques and more than forty case studies provide a clear understanding of the design and application of data mining and machine learning techniques in cybersecurity.},
  interhash = {de7469e0adb3d15b1d40b84ce5ca75f2},
  intrahash = {49205d4c38f2eb24be4de678a8a1f153},
  keywords = {Book Cybersecurity DataMining MachineLearning},
  publisher = {Taylor & Francis},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Data mining and machine learning in cybersecurity},
  year = 2011
}

@book{bueno2015lauren,
  abstract = {Lauren, a clever girl lost in Userland, applies logic and problem solving skills to find her way home, encountering along the way such concepts as timing attacks, algorithm design, and the traveling salesman problem.},
  added-at = {2016-08-26T03:45:55.000+0200},
  author = {Bueno, Carlos and Lipovaca, Miran},
  biburl = {https://www.bibsonomy.org/bibtex/2c70f41e0e20159bb8d3032aa1872c351/vngudivada},
  interhash = {0b990a6f31c3eee06ebda39ea7608c04},
  intrahash = {c70f41e0e20159bb8d3032aa1872c351},
  keywords = {Book CS0 ComputerScience},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Lauren Ipsum: a story about computer science and other improbable things},
  year = 2015
}

@article{boyce2007developing,
  abstract = {Ontologies have the potential to play an important role in instructional design and the development of course content. They can be used to represent knowledge about content, supporting instructors in creating content or learners in accessing content in a knowledge-guided way. While ontologies exist for many subject domains, their quality and suitability for the educational context might be unclear. For numerous subjects, ontologies do not exist. We present a method for domain experts rather than ontology engineers to develop ontologies for use in the delivery of courseware content. We will focus in particular on relationship types that allow us to model rich domains adequately.},
  added-at = {2016-08-29T04:42:56.000+0200},
  author = {Boyce, Sinéad and Pahl, Claus},
  biburl = {https://www.bibsonomy.org/bibtex/2d4aa20a899ecd5bae41e0c7dc2794202/vngudivada},
  interhash = {57c1a2d1a6ea1e15f9b8bdd3ef98ca10},
  intrahash = {d4aa20a899ecd5bae41e0c7dc2794202},
  journal = {Educational Technology & Society},
  keywords = {Ontology},
  number = 3,
  pages = {275-288},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Developing Domain Ontologies for Course Content.},
  volume = 10,
  year = 2007
}

@book{szpiro2006secret,
  abstract = {Most of us picture mathematicians laboring before a chalkboard, scribbling numbers and obscure symbols as they mutter unintelligibly. This lighthearted (but realistic) sneak-peak into the everyday world of mathematicians turns that stereotype on its head. Most people have little idea what mathematicians do or how they think. It's often difficult to see how their seemingly arcane and esoteric work applies to our own everyday lives. But mathematics also holds a special allure for many people. We are drawn to its inherent beauty and fascinated by its complexity - but often intimidated by its presumed difficulty. "The Secret Life of Numbers" opens our eyes to the joys of mathematics, introducing us to the charming, often whimsical side, of the discipline. Divided into several parts, the book looks at interesting and largely unknown historical tidbits, introduces the larger than - life practitioners of mathematics through the ages, profiles some of the most significant unsolved conjectures, and describes problems and puzzles that have already been solved. Rounding out the table of contents is a host of mathematical miscellany - all of which add up to 50 fun, sometimes cheeky, short takes on the field. Chock full of stories, anecdotes, and entertaining vignettes, "The Secret Life of Numbers" shows us how mathematics really does affect almost every aspect of life - from the law to geography, elections to botany - and we come to appreciate the delight and gratification that mathematics holds for all of us.},
  added-at = {2016-08-26T01:24:25.000+0200},
  address = {Washington, D.C.},
  author = {Szpiro, George},
  biburl = {https://www.bibsonomy.org/bibtex/2210776197158b0e39ff0f00f7d71e971/vngudivada},
  interhash = {d9685aa14e3c629b55a3921d9a793a9e},
  intrahash = {210776197158b0e39ff0f00f7d71e971},
  keywords = {Book Mathematics Number},
  publisher = {Joseph Henry Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The secret life of numbers: 50 easy pieces on how mathematicians work and think},
  year = 2006
}

@book{bell2014machine,
  abstract = {Specifically designed for non-mathematicians, this useful guide presents a breakdown of each variant of machine learning, with examples and working code. You'll learn the various algorithms, data preparation techniques, trees, and networks, and get acquainted with the tools that help you get more from your data. Learn the languages of machine learning: Weka, Mahout, Spark, and R. Make the right data storage and cleaning decisions, tailored to your desired output. Understand decision trees, Bayesian networks, artificial neural networks, and association rule learning. Implement support vector machines knowing the relevant advantages and limitations. Apply Big Data processing techniques with Hadoop, Mahout, and MapReduce. Use Spring XD to capture streaming data and learn in real time. Access the tools you need to plan your project and acquire and process data.},
  added-at = {2016-09-10T00:17:01.000+0200},
  address = {Indianapolis, IN},
  author = {Bell, Jason},
  biburl = {https://www.bibsonomy.org/bibtex/26363ef6e5e2b50fdf63a2ebb976f1070/vngudivada},
  interhash = {4a12cc4353e72d9eef98b6b74fd96ada},
  intrahash = {6363ef6e5e2b50fdf63a2ebb976f1070},
  keywords = {Book MachineLearning},
  publisher = {Wiley},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Machine Learning: Hands-On for Developers and Technical Professionals},
  year = 2014
}

@book{pinker2015words,
  abstract = {How does language work? How do children learn their mother tongue? Why do languages change over time, making Shakespearean English difficult for us and Chaucer’s English almost incomprehensible? Why do languages have so many quirks and irregularities? Are they all fundamentally alike? How are new words created? Where in the brain does language reside?

In Words and Rules, Steven Pinker answers these and many other questions. His book shares the wit and style of his classic, The Language Instinct, but explores language in a completely different way. In Words and Rules, Pinker explains the profound mysteries of language by picking a deceptively simple phenomenon and examining it from every angle. The phenomenon—regular and irregular verbs—connects an astonishing array of topics in the sciences and humanities: the history of languages, the theories of Noam Chomsky and his critics; the attempts to simulate language using computer simulations of neural networks; the illuminating errors of children as they begin to speak; the nature of human concepts; the peculiarities of the English language; major ideas in the history of Western philosophy; the latest techniques in identifying genes and imaging the living brain.

Pinker makes sense of all of this with the help of a single, powerful idea: that language comprises a mental dictionary of memorized words and a mental grammar of creative rules. The idea extends beyond language and offers insight into the very nature of the human mind. This is a sparkling, eye-opening and utterly original book by one of the world’s leading cognitive scientists.},
  added-at = {2016-08-28T15:40:17.000+0200},
  address = {New York, NY},
  author = {Pinker, Steven},
  biburl = {https://www.bibsonomy.org/bibtex/2db018a46011eab1a8547c9d78ada065c/vngudivada},
  interhash = {235c215a343884a01defe92d8864b4bf},
  intrahash = {db018a46011eab1a8547c9d78ada065c},
  isbn = {978-0465072699},
  keywords = {Book Linguistics},
  publisher = {Basic Books},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Words and rules: the ingredients of language},
  year = 2015
}

@book{payne2006exploring,
  abstract = {Designed for those beginning to study linguistics, this is a lively introduction to two key aspects of the structure of language: syntax (the structure of sentences) and morphology (the structure of words). It shows students in a step-by-step fashion how to analyze the syntax and morphology of any language, by clearly describing the basic methods and techniques, and providing almost 100 practical exercises based on data from a rich variety of the world's languages. Written in an engaging style and complete with a comprehensive glossary, Exploring language structure explains linguistic concepts by using clear analogies from everyday life.},
  added-at = {2016-09-23T01:15:05.000+0200},
  address = {New York, NY},
  author = {Payne, Thomas Edward},
  biburl = {https://www.bibsonomy.org/bibtex/25a68ea532b2374e7fa46345db4f2ac54/vngudivada},
  interhash = {3ee8c5c93b974b6117d6c210498c2d20},
  intrahash = {5a68ea532b2374e7fa46345db4f2ac54},
  isbn = {052185542X 0521671507 9780521855426 9780521671507},
  keywords = {Book Language},
  publisher = {Cambridge University Press},
  refid = {60743102},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Exploring language structure: a student's guide},
  year = 2006
}

@book{subramuniyaswami1999merging,
  abstract = {Spirituality in the New Age has tended to reflect our technological obsessions. It is, usually, verbose, complicated and complex. The problem with this approach is that it works with the mind, even perhaps the emotions, but does not speak to the essential self. So much modern spiritual literature works to compete in the world with secular and mainstream writing, hence rather than really reaching us it simply presents another theory, another technique, a new form of yoga or meditation. This penchant for novelty may fill a consumer need, but does not necessarily feed the soul.
"Merging with Siva" is a very different sort of book. Its heart is comprised of 365 short sections which can be read in a year, spending around 10 minutes a day. Each section is very easy to read, simple and written in a direct style. It is only throughout the next day as you think over what you have read that you begin to realize the strength of each message. "Merging with Siva" embodies the concept that there is immense power in simplicity. Each section while flowing effortlessly seems to bypass emotions, intellect and body and touch something much deeper. While it may only take 10 minutes to read, one wonders if there is not a lifetime of meaning condensed into each passage. Condensing and distilling--each of these words describe "Merging with Siva." It is as though through a unique alchemical process the secrets of life have been condensed into simple but poignant and powerful expressions. The beauty of teaching in bite sized packages is the way in which quite complex subjects can be slowly unveiled.

"Merging with Siva" is like building a house, each brick is placed onto another until the whole edifice is revealed. This allows us to contemplate individual concepts which interconnect as the whole picture is unveiled. Accordingly, difficult and complex matters are taught with an ease which would make many school teachers jealous!

I personally find the Self God, which is included at the start of the book very meaningful. I first came across it many, many years ago in a small booklet I found at a second hand store. It was an ecstatic experience as I read what seemed to be like nectar, juice squeezed from the fruit of spiritual experience. This one booklet led me on a journey that culminated in my personal contact with the Himalayan Academy. It is included at the start of "Merging with Siva," printed in the handwriting of Subramuniyaswami. At first this seems annoying, even a nuisance. It cannot be quickly read or glossed over. Here, then, is the importance and power of the text. As you work to read each work, phrase and paragraph its message and vision begins to unfold. It is a great way to begin your journey through Merging with Siva.

There is so much more in this book that it is hard to summarize. There is, of course, good solid background information on the tradition of Saiva Siddhanta: its theology and practise and certainly more than enough to provide one of the better introductions to Shaivism as a living spiritual tradition. But what makes Merging with Siva so unique is its ability to bring that living tradition within reach of the seeker. It slowly draws the reader into a deep understanding of themselves, the universe and the great "spiritual chain of being" of which we are part. It does so simply, succinctly and yet with great erudition and wisdom.

Merging with Siva is a modest text. There is no new age hype, no outrageous claims, no UFO abductions nor calls to the bizarre or garish. There is a simple and honest presentation of the distilled wisdom of a lifetime and this makes Merging with Siva very rare and quite unique. In my personal collection I consider it one of the most important books I have. Its presentation matches its content. Superb. It is beautifully illustrated, easy on the eye and professionally presented. I commend Merging with Siva to you.},
  added-at = {2016-08-30T13:27:47.000+0200},
  author = {Subramuniyaswami, Satguru Sivaya},
  biburl = {https://www.bibsonomy.org/bibtex/23da25a7b8cff68f0e0df7d1909def8a5/vngudivada},
  description = {Merging with Siva is among the richest, most soul-touching statements on meditation and God Realization written. Yet, it's down-to-earth, nonacademic, user-friendly, easy to follow! This book is the essence of Satguru Sivaya Subramuniyaswami's fifty years of yogic realizations and inspired teaching. Merging with Siva is a huge volume: 1,408 pages, beautifully illustrated, containing 365 lessons, one for each day of the year, plus 99 aphorisms. In Merging with Siva Gurudeva's speaks directly to you about your soul and God, the mystical realm of 21 chakras, the human aura, karma, force fields, meditation, the two paths, samadhi and so much more. Lavishly illustrated.},
  interhash = {77a208c86e3a8e0a14a05d3f417189b6},
  intrahash = {3da25a7b8cff68f0e0df7d1909def8a5},
  keywords = {Book Spirituality},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Merging with Siva: Hinduism's Contemporary Metaphysics},
  year = 1999
}

@book{smith1999scientist,
  abstract = {In the early 1980s, DSP was taught as a graduate level course in electrical engineering. A decade later, DSP had become a standard part of the undergraduate curriculum. Today, DSP is a basic skill needed by scientists and engineers in many fields. Unfortunately, DSP education has been slow to adapt to this change. Nearly all DSP textbooks are still written in the traditional electrical engineering style of detailed and rigorous mathematics. DSP is incredibly powerful, but if you can't understand it, you can't use it! This book was written for scientists and engineers in a wide variety of fields: physics, bioengineering, geology, oceanography, mechanical and electrical engineering, to name just a few. The goal is to present practical techniques while avoiding the barriers of detailed mathematics and abstract theory. This book is primarily intended for a one year course in practical DSP, with the students being drawn from a wide variety of science and engineering fields. This book was also written with the practicing professional in mind. Many everyday DSP applications are discussed: digital filters, neural networks, data compression, audio and image processing, etc. As much as possible, these chapters stand on their own, not requiring the reader to review the entire book to solve a specific problem.},
  added-at = {2016-09-20T02:49:32.000+0200},
  address = {San Diego, CA},
  author = {Smith, Steven W.},
  biburl = {https://www.bibsonomy.org/bibtex/2e77c46b7ffcffc51f514bb0ab07573ae/vngudivada},
  interhash = {52b919646823b7eff7a5546afae69127},
  intrahash = {e77c46b7ffcffc51f514bb0ab07573ae},
  keywords = {Book DSP ParExcellence},
  publisher = {California Technical Pub.},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The scientist and engineer's guide to digital signal processing},
  year = 1999
}

@book{negus2015linux,
  abstract = {The industry favorite Linux guide, updated for Red Hat Enterprise Linux 7 and the cloud Linux Bible, 9th Edition is the ultimate hands-on Linux user guide, whether you're a true beginner or a more advanced user navigating recent changes. This updated ninth edition covers the latest versions of Red Hat Enterprise Linux 7 (RHEL 7), Fedora 21, and Ubuntu 10.04 LTS, and includes new information on cloud computing and development with guidance on Openstack and Cloudforms. With a focus on RHEL 7, this practical guide gets you up to speed quickly on the new enhancements for enterprise-quality file.},
  added-at = {2016-08-26T03:56:47.000+0200},
  author = {Negus, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/2bd93ddb532dfa4387e23192dc4da5725/vngudivada},
  interhash = {636d70b64812646f660eaea4e9e53831},
  intrahash = {bd93ddb532dfa4387e23192dc4da5725},
  keywords = {Book Linux},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Linux bible},
  year = 2015
}

@misc{dobrovolskyphoneticsphonetics,
  added-at = {2016-09-22T13:21:03.000+0200},
  author = {Dobrovolsky, Michael and Katamba, Francis},
  biburl = {https://www.bibsonomy.org/bibtex/2bc16bc6ed009cf9597c23f9dced6185e/vngudivada},
  interhash = {c3a8d42986bcedfa031f5fb7626366ba},
  intrahash = {bc16bc6ed009cf9597c23f9dced6185e},
  keywords = {2016},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Phonetics: The Sounds of Language},
  year = {Phonetics}
}

@inbook{moore2014spoken,
  abstract = {Over the past thirty years, the field of spoken language processing has made impressive progress from simple laboratory demonstrations to mainstream consumer products. However, commercial applications such as Siri highlight the fact that there is still some way to go in creating Autonomous Social Agents that are truly capable of conversing effectively with their human counterparts in real-world situations. This paper suggests that it may be time for the spoken language processing community to take an interest in the potentially important developments that are occurring in related fields such as cognitive neuroscience, intelligent systems and developmental robotics. It then gives an insight into how such ideas might be integrated into a novel Mutual Beliefs Desires Intentions Actions and Consequences (MBDIAC) framework that places a focus on generative models of communicative behaviour which are recruited for interpreting the behaviour of others.},
  added-at = {2016-09-10T13:53:55.000+0200},
  author = {Moore, RogerK.},
  biburl = {https://www.bibsonomy.org/bibtex/243447e5368b78fd4567397ac5ea2bbaf/vngudivada},
  booktitle = {Statistical Language and Speech Processing},
  editor = {Besacier, Laurent and Dediu, Adrian-Horia and Mart{\'\i}n-Vide, Carlos},
  interhash = {1359d8fc23e55fdc2940ed62d9518050},
  intrahash = {43447e5368b78fd4567397ac5ea2bbaf},
  keywords = {SpokenLanguage},
  organization = {Springer International Publishing},
  pages = {21-36},
  publisher = {Springer International Publishing},
  series = {Lecture Notes in Computer Science},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Spoken Language Processing: Time to Look Outside?},
  url = {http://dx.doi.org/10.1007/978-3-319-11397-5_2},
  year = 2014
}

@inproceedings{nagy2015mining,
  abstract = {Constructing complex queries in SQL sometimes necessitates the use of language constructs and the invocation of internal functions which inexperienced developers find hard to comprehend or which are unknown to them. In the worst case, bad usage of these constructs might lead to errors, to ineffective queries, or hamper developers in their tasks. This paper presents a mining technique for Stack Overflow to identify error-prone patterns in SQL queries. Identifying such patterns can help developers to avoid the use of error-prone constructs, or if they have to use such constructs, the Stack Overflow posts can help them to properly utilize the language. Hence, our purpose is to provide the initial steps towards a recommendation system that supports developers in constructing SQL queries. Our current implementation supports the MySQL dialect, and Stack Overflow has over 300,000 questions tagged with the MySQL flag in its database. It provides a huge knowledge base where developers can ask questions about real problems. Our initial results indicate that our technique is indeed able to identify patterns among them.},
  added-at = {2016-08-27T00:52:11.000+0200},
  author = {Nagy, Csaba and Cleve, Anthony},
  biburl = {https://www.bibsonomy.org/bibtex/21f30d9a92df6e2f510edc06dbd28ec8a/vngudivada},
  booktitle = {{IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
  doi = {10.1109/icsm.2015.7332505},
  interhash = {44e723a1855e5c39f1d447283aa2f493},
  intrahash = {1f30d9a92df6e2f510edc06dbd28ec8a},
  keywords = {SQL SQLErrors},
  month = sep,
  publisher = {IEEE},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Mining Stack Overflow for discovering error patterns in {SQL} queries},
  year = 2015
}

@book{kabacoff2015action,
  abstract = {R in Action, Second Edition presents both the R language and the examples that make it so useful for business developers. Focusing on practical solutions, the book offers a crash course in statistics and covers elegant methods for dealing with messy and incomplete data that are difficult to analyze using traditional methods. You'll also master R's extensive graphical capabilities for exploring and presenting data visually. And this expanded second edition includes new chapters on time series analysis, cluster analysis, and classification methodologies, including decision trees, random forests, and support vector machines.

Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.

About the Technology

Business pros and researchers thrive on data, and R speaks the language of data analysis. R is a powerful programming language for statistical computing. Unlike general-purpose tools, R provides thousands of modules for solving just about any data-crunching or presentation challenge you're likely to face. R runs on all important platforms and is used by thousands of major corporations and institutions worldwide.

About the Book

R in Action, Second Edition teaches you how to use the R language by presenting examples relevant to scientific, technical, and business developers. Focusing on practical solutions, the book offers a crash course in statistics, including elegant methods for dealing with messy and incomplete data. You'll also master R's extensive graphical capabilities for exploring and presenting data visually. And this expanded second edition includes new chapters on forecasting, data mining, and dynamic report writing.

What's Inside

Complete R language tutorial
Using R to manage, analyze, and visualize data
Techniques for debugging programs and creating packages
OOP in R
Over 160 graphs
About the Author

Dr. Rob Kabacoff is a seasoned researcher and teacher who specializes in data analysis. He also maintains the popular Quick-R website at statmethods.net.

Table of Contents

PART 1 GETTING STARTED
Introduction to R
Creating a dataset
Getting started with graphs
Basic data management
Advanced data management
PART 2 BASIC METHODS
Basic graphs
Basic statistics
PART 3 INTERMEDIATE METHODS
Regression
Analysis of variance
Power analysis
Intermediate graphs
Resampling statistics and bootstrapping
PART 4 ADVANCED METHODS
Generalized linear models
Principal components and factor analysis
Time series
Cluster analysis
Classification
Advanced methods for missing data
PART 5 EXPANDING YOUR SKILLS
Advanced graphics with ggplot2
Advanced programming
Creating a package
Creating dynamic reports
Advanced graphics with the lattice package available online only from manning.com/kabacoff2},
  added-at = {2016-09-11T18:34:33.000+0200},
  author = {Kabacoff, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/2a0c37fb39e30b8a8fcfa66e5ad26a4c8/vngudivada},
  interhash = {329a34f41e234a408ff9865bc9ca7f94},
  intrahash = {a0c37fb39e30b8a8fcfa66e5ad26a4c8},
  keywords = {Book R Statistics},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {R in action: data analysis and graphics with R},
  year = 2015
}

@book{edmonds2008think,
  abstract = {This book presents insights, notations, and analogies to help the novice describe and think about algorithms like an expert. Jeff Edmonds provides both the big picture and easy step-by-step methods for developing algorithms, while avoiding the common pitfalls. Paradigms such as loop invariants and recursion help to unify a huge range of algorithms into a few meta-algorithms. Part of the goal is to teach the students to think abstractly.},
  added-at = {2016-08-26T01:32:19.000+0200},
  address = {Cambridge; New York},
  author = {Edmonds, Jeff},
  biburl = {https://www.bibsonomy.org/bibtex/2fdfe0253b09b3c12e5bfa999745b9b50/vngudivada},
  description = {There are many algorithm texts that provide lots of well-polished code and proofs of correctness. This book is not one of them. Instead, this book presents insights, notations, and analogies to help the novice describe and think about algorithms like an expert. By looking at both the big picture and easy step-by-step methods for developing algorithms, the author helps students avoid the common pitfalls. He stresses paradigms such as loop invariants and recursion to unify a huge range of algorithms into a few meta-algorithms. Part of the goal is to teach the students to think abstractly. Without getting bogged with formal proofs, the book fosters a deeper understanding of how and why each algorithm works. These insights are presented in a slow and clear manner accessible to second- or third-year students of computer science, preparing them to find their own innovative ways to solve problems.},
  interhash = {621580d574137e8f85a013edc700490c},
  intrahash = {fdfe0253b09b3c12e5bfa999745b9b50},
  keywords = {Algorithms Book},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {How to think about algorithms},
  year = 2008
}

@book{negus2013ubuntu,
  abstract = {This updated bestseller from Linux guru Chris Negus is packed with an array of new and revised material As a longstanding bestseller, Ubuntu Linux Toolbox has taught you how to get the most out Ubuntu, the world's most popular Linux distribution. With this anticipated new edition, Christopher Negus returns with a host of new and expanded coverage on tools for managing file systems, ways to connect to networks, techniques for securing Ubuntu systems, and a look at the latest Long Term Support (LTS) release of Ubuntu, all aimed at getting you up and running with Ubuntu Linux quickly. Covers installation, configuration, shell primer, the desktop, administrations, servers, and security Delves into coverage of popular applications for the web, productivity suites, and e-mail Highlights setting up a server (Apache, Samba, CUPS) Boasts a handy trim size so that you can take it with you on the go Ubuntu Linux Toolbox, Second Edition prepares you with a host of updated tools for today's environment, as well as expanded coverage on everything you know to confidently start using Ubuntu today.},
  added-at = {2016-08-26T03:54:01.000+0200},
  address = {New York, NY},
  author = {Negus, Chris},
  biburl = {https://www.bibsonomy.org/bibtex/214c364401b387abb31dfa4c75b13914c/vngudivada},
  interhash = {0603225e40a4673ac4e4412266cf7d93},
  intrahash = {14c364401b387abb31dfa4c75b13914c},
  keywords = {Book Linux},
  publisher = {John Wiley & Sons},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Ubuntu Linux toolbox: 1000+ commands for Ubuntu and Debian power users},
  year = 2013
}

@book{zhang2012ensemble,
  abstract = {It is common wisdom that gathering a variety of views and inputs improves the process of decision making, and, indeed, underpins a democratic society. Dubbed ensemble learning by researchers in computational intelligence and machine learning, it is known to improve a decision systems robustness and accuracy. Now, fresh developments are allowing researchers to unleash the power of ensemble learning in an increasing range of real-world applications. Ensemble learning algorithms such as boosting and random forest facilitate solutions to key computational issues such as face recognition and are now being applied in areas as diverse as object tracking and bioinformatics.Responding to a shortage of literature dedicated to the topic, this volume offers comprehensive coverage of state-of-the-art ensemble learning techniques, including the random forest skeleton tracking algorithm in the Xbox Kinect sensor, which bypasses the need for game controllers. At once a solid theoretical study and a practical guide, the volume is a windfall for researchers and practitioners alike.},
  added-at = {2016-09-09T23:39:42.000+0200},
  address = {New York, NY},
  author = {Zhang, Cha and Ma, Yunqian},
  biburl = {https://www.bibsonomy.org/bibtex/28097f53b1585bbd1ac16e9281dde8663/vngudivada},
  interhash = {ebcf4c096c07bcf3c907358b9ad9e107},
  intrahash = {8097f53b1585bbd1ac16e9281dde8663},
  keywords = {Book EnsembleLearning MachineLearning},
  publisher = {Springer},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Ensemble machine learning: methods and applications},
  year = 2012
}

@book{parker2010digital,
  abstract = {Digital Signal Processing 101: Everything You Need to Know to Get Started provides a basic tutorial on digital signal processing (DSP). Beginning with discussions of numerical representation and complex numbers and exponentials, it goes on to explain difficult concepts such as sampling, aliasing, imaginary numbers, and frequency response. It does so using easy-to-understand examples and a minimum of mathematics. In addition, there is an overview of the DSP functions and implementation used in several DSP-intensive fields or applications, from error correction to CDMA mobile communication to airborne radar systems. This book is intended for those who have absolutely no previous experience with DSP, but are comfortable with high-school-level math skills. It is also for those who work in or provide components for industries that are made possible by DSP. Sample industries include wireless mobile phone and infrastructure equipment, broadcast and cable video, DSL modems, satellite communications, medical imaging, audio, radar, sonar, surveillance, and electrical motor control.
Dismayed when presented with a mass of equations as an explanation of DSP? This is the book for you!
Clear examples and a non-mathematical approach gets you up to speed with DSP
Includes an overview of the DSP functions and implementation used in typical DSP-intensive applications, including error correction, CDMA mobile communication, and radar systems},
  added-at = {2016-09-20T02:36:15.000+0200},
  address = {Burlington, MA},
  author = {Parker, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/27e1fee892c6a6f0871e27e4107685b52/vngudivada},
  interhash = {c21a5cc0eba21c1ab982c3ccad72ec06},
  intrahash = {7e1fee892c6a6f0871e27e4107685b52},
  keywords = {Book DSP},
  publisher = {Newnes},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Digital signal processing 101 : everything you need to know to get started},
  year = 2010
}

@book{wiley2015beginning,
  abstract = {Beginning R, Second Edition is a hands-on book showing how to use the R language, write and save R scripts, read in data files, and write custom statistical functions as well as use built in functions. This book shows the use of R in specific cases such as one-way ANOVA analysis, linear and logistic regression, data visualization, parallel processing, bootstrapping, and more. It takes a hands-on, example-based approach incorporating best practices with clear explanations of the statistics being done. It has been completely re-written since the first edition to make use of the latest packages and features in R version 3. R is a powerful open-source language and programming environment for statistics and has become the de facto standard for doing, teaching, and learning computational statistics. R is both an object-oriented language and a functional language that is easy to learn, easy to use, and completely free. A large community of dedicated R users and programmers provides an excellent source of R code, functions, and data sets, with a constantly evolving ecosystem of packages providing new functionality for data analysis. R has also become popular in commercial use at companies such as Microsoft, Google, and Oracle. Your investment in learning R is sure to pay off in the long term as R continues to grow into the go to language for data analysis and research. What You Will Learn: How to acquire and install R Hot to import and export data and scripts How to analyze data and generate graphics How to program in R to write custom functions Hot to use R for interactive statistical explorations How to conduct bootstrapping and other advanced techniques.},
  added-at = {2016-09-11T15:20:05.000+0200},
  author = {Wiley, Joshua F. and Pace, Larry A.},
  biburl = {https://www.bibsonomy.org/bibtex/2956c58f742685bc48aa05c2fa8ddaf40/vngudivada},
  interhash = {7c57eb3daf78b7239350c3c348ac4a82},
  intrahash = {956c58f742685bc48aa05c2fa8ddaf40},
  keywords = {Book R},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Beginning R: an introduction to statistical programming},
  year = 2015
}

@article{fodor1988connectionism,
  abstract = {This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a ‘language of thought’: i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the ‘systematicity’ of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or ‘abstract neurological’) structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation. },
  added-at = {2016-08-18T00:07:50.000+0200},
  author = {Fodor, Jerry A. and Pylyshyn, Zenon W.},
  biburl = {https://www.bibsonomy.org/bibtex/2385c1a5556dc843672de6751e248e866/vngudivada},
  interhash = {caece44f9b638e3ba3f1089540e77dd5},
  intrahash = {385c1a5556dc843672de6751e248e866},
  journal = {Cognition },
  keywords = {CognitiveArchitecture Connectionism},
  number = {1–2},
  pages = {3 - 71},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Connectionism and cognitive architecture: A critical analysis },
  volume = 28,
  year = 1988
}

@book{dawson2016language,
  abstract = {Language Files: Materials for an Introduction to Language and Linguistics has become one of the most widely adopted, consulted, and authoritative introductory textbooks to linguistics ever written. The scope of the text makes it suitable for use in a wide range of courses, while its unique organization into student-friendly, self-contained sections allows for tremendous flexibility in course design.

The twelfth edition has been significantly revised, clarified, and updated throughout—with particular attention to the chapters on phonetics, phonology, pragmatics, and especially psycholinguistics. The restructured chapter on psycholinguistics makes use of recent research on language in the brain and includes expanded coverage of language processing disorders, introducing students to current models of speech perception and production and cutting-edge research techniques. In addition, exercises have been updated, and icons have been added to the text margins throughout the book, pointing instructors and students to useful and engaging audio files, videos, and other online resources on the accompanying Language Files website, which has also been significantly expanded.},
  added-at = {2016-09-25T17:17:29.000+0200},
  address = {Columbus, Ohio},
  biburl = {https://www.bibsonomy.org/bibtex/292138be517b34f7876dffef2551c907c/vngudivada},
  edition = {Twelfth},
  editor = {Dawson, Hope and Phelan, Michael},
  interhash = {8f915bba28e3bfb4950dd07dae165bb1},
  intrahash = {92138be517b34f7876dffef2551c907c},
  keywords = {Language Linguistics},
  publisher = {The Ohio State University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Language files: materials for an introduction to language and linguistics},
  year = 2016
}

@article{costello2005practices,
  abstract = {Exploratory factor analysis (EFA) is a complex, multi-step process. The goal of this paper is to collect, in one article, information that will allow researchers and practitioners to understand the various choices available through popular software packages, and to make decisions about “best practices” in exploratory factor analysis. In particular, this paper provides practical information on making decisions regarding (a) extraction, (b) rotation, (c) the number of factors to interpret, and (d) sample size.},
  added-at = {2016-10-23T04:59:20.000+0200},
  author = {Costello, Anna B. and Osborne, Jason W.},
  biburl = {https://www.bibsonomy.org/bibtex/29cdfeb0a48d0bedf30701f69d77c639d/vngudivada},
  interhash = {c854a95218102a419f6d2c2ab0a217a8},
  intrahash = {9cdfeb0a48d0bedf30701f69d77c639d},
  journal = {Practical Assessment, Research \& Evaluation},
  keywords = {EFA PCA},
  number = 7,
  pages = {1 - 9},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Best Practices in Exploratory Factor Analysis: Four Recommendations for Getting the Most From Your Analysis},
  url = {/brokenurl# http://pareonline.net/getvn.asp?v=10&n=7},
  volume = 10,
  year = 2005
}

@book{osborne2013practices,
  abstract = {Many researchers jump straight from data collection to data analysis without realizing how analyses and hypothesis tests can go profoundly wrong without clean data. This book provides a clear, step-by-step process to examining and cleaning data in order to decrease error rates and increase both the power and replicability of results. Jason W. Osborne, author of Best Practices in Quantitative Methods (SAGE, 2008) provides easily-implemented suggestions that are research-based and will motivate change in practice by empirically demonstrating for each topic the benefits of following best practices and the potential consequences of not following these guidelines. If your goal is to do the best research you can do, draw conclusions that are most likely to be accurate representations of the population(s) you wish to speak about, and report results that are most likely to be replicated by other researchers, then this basic guidebook is indispensable.},
  added-at = {2016-10-17T06:07:05.000+0200},
  address = {Thousand Oaks, CA},
  author = {Osborne, Jason W.},
  biburl = {https://www.bibsonomy.org/bibtex/2274aa599f8a7638b1246671b8cb41c2f/vngudivada},
  interhash = {64e306783892b92d21c42c73be64d317},
  intrahash = {274aa599f8a7638b1246671b8cb41c2f},
  keywords = {Book DataCleaning},
  publisher = {SAGE},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Best practices in data cleaning: a complete guide to everything you need to do before and after collecting your data},
  year = 2013
}

@book{tabachnick2013using,
  abstract = {A Practical Approach to using Multivariate Analyses

Using Multivariate Statistics, 6th edition provides advanced undergraduate as well as graduate students with a timely and comprehensive introduction to today's most commonly encountered statistical and multivariate techniques, while assuming only a limited knowledge of higher-level mathematics. This text’s practical approach focuses on the benefits and limitations of applications of a technique to a data set – when, why, and how to do it.

Learning Goals
Upon completing this book, readers should be able to:
Learn to conduct numerous types of multivariate statistical analyses
Find the best technique to use
Understand Limitations to applications
Learn how to use SPSS and SAS syntax and output},
  added-at = {2016-10-17T23:44:54.000+0200},
  address = {Boston},
  author = {Tabachnick, Barbara G. and Fidell, Linda S.},
  biburl = {https://www.bibsonomy.org/bibtex/2a92109e580fd76d14993926edbbbc736/vngudivada},
  interhash = {0c594a49b10629625cfc81825ff8d79d},
  intrahash = {a92109e580fd76d14993926edbbbc736},
  keywords = {Book MultivariateStatistics ParExcellence Statistics},
  publisher = {Pearson Education},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Using multivariate statistics},
  year = 2013
}

@book{chomsky2006language,
  abstract = {This is the long-awaited third edition of Chomsky's outstanding collection of essays on language and mind. The first six chapters, originally published in the 1960s, made a groundbreaking contribution to linguistic theory. This new edition complements them with an additional chapter and a new preface, bringing Chomsky's influential approach into the twenty-first century. Chapters 1-6 present Chomsky's early work on the nature and acquisition of language as a genetically endowed, biological system (Universal Grammar), through the rules and principles of which we acquire an internalized knowledge (I-language). Over the past fifty years, this framework has sparked an explosion of inquiry into a wide range of languages, and has yielded some major theoretical questions. The final chapter revisits the key issues, reviewing the 'biolinguistic' approach that has guided Chomsky's work from its origins to the present day, and raising some novel and exciting challenges for the study of language and mind},
  added-at = {2016-09-24T15:53:33.000+0200},
  address = {Cambridge; New York},
  author = {Chomsky, Noam},
  biburl = {https://www.bibsonomy.org/bibtex/29ae65882eff72fa63b6c9051b53c76b4/vngudivada},
  description = {This is the third edition of Chomsky's outstanding collection of essays on language and mind, first published in 2006. The first six chapters, originally published in the 1960s, made a groundbreaking contribution to linguistic theory. This edition complements them with an additional chapter and a new preface, bringing Chomsky's influential approach into the twenty-first century. Chapters 1-6 present Chomsky's early work on the nature and acquisition of language as a genetically endowed, biological system (Universal Grammar), through the rules and principles of which we acquire an internalized knowledge (I-language). Over the past fifty years, this framework has sparked an explosion of inquiry into a wide range of languages, and has yielded some major theoretical questions. The final chapter revisits the key issues, reviewing the 'biolinguistic' approach that has guided Chomsky's work from its origins to the present day, and raising some novel and exciting challenges for the study of language and mind.},
  interhash = {c5b62feb45ebdc3fcf5e0d73be12b842},
  intrahash = {9ae65882eff72fa63b6c9051b53c76b4},
  keywords = {CognitiveLinguistics Language},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Language and mind},
  year = 2006
}

@inproceedings{neven2002reusable,
  abstract = {In this paper, we survey the field of learning object repositories. Learning objects are typically relatively small content components that are meant to be reusable in different contexts. Associated to these learning objects are metadata, so that they can be managed, searched, etc. As the international standardization in this area is making important progress, the number of these repositories is growing rapidly, and the whole field of learning objects is rapidly maturing as a research area in its own right.},
  acmid = {641067},
  added-at = {2016-12-06T04:05:52.000+0100},
  address = {New York, NY, USA},
  author = {Neven, Filip and Duval, Erik},
  biburl = {https://www.bibsonomy.org/bibtex/238b9d663092bd71399e64cef670375ba/vngudivada},
  booktitle = {Proceedings of the Tenth ACM International Conference on Multimedia},
  doi = {10.1145/641007.641067},
  interhash = {8be54437c55b84556b577b34fd766b72},
  intrahash = {38b9d663092bd71399e64cef670375ba},
  isbn = {1-58113-620-X},
  keywords = {sys:relevantfor:ecu-cc-research LOM LOR LearningObjectMetadata LearningObjectRepository ReusableLearningObject},
  location = {Juan-les-Pins, France},
  numpages = {4},
  pages = {291--294},
  publisher = {ACM},
  series = {MULTIMEDIA '02},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Reusable Learning Objects: A Survey of LOM-based Repositories},
  url = {http://doi.acm.org/10.1145/641007.641067},
  year = 2002
}

@book{wray2012projects,
  abstract = {Projects in Linguistics and Language Studies, Third Edition, is your essential guide when embarking on a research project in linguistics or English language. This third edition is clearly divided into the subject areas that most appeal to you as a student: psycholinguistics; first- and second-language acquisition; structure and meaning; sociolinguistics; language and gender; accents and dialects; and the history of English. New to this edition is a chapter on computer-mediated communication (CMC), to help you get to grips with how the latest technology can make your project even more of a success. This third edition of Projects in Linguistics and Language Studies offers you practical advice on: Identifying a topic ; Making your background reading even more effective -planning and designing a project ; Collecting and analysing data ; Writing up and presenting your findings. With over 320 project ideas that you can use directly or adapt to suit different contexts and interests, and with chapters on how to reference effectively and how to avoid plagiarism, this third edition of Projects in Linguistics and Language Studies is a reference guide that you will use again and again during your studies.},
  added-at = {2016-09-24T15:41:51.000+0200},
  address = {London},
  author = {Wray, Alison and Bloomer, Aileen and Wray, Alison},
  biburl = {https://www.bibsonomy.org/bibtex/2f33b4211d92f1c95698eb1dda9f84bc1/vngudivada},
  interhash = {53a23ee53cd56c20b945e3a6f889ef38},
  intrahash = {f33b4211d92f1c95698eb1dda9f84bc1},
  keywords = {Book Language Linguistics},
  publisher = {Hodder Education},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Projects in linguistics and language studies: a practical guide to researching language},
  year = 2012
}

@article{finch2012distribution,
  abstract = {Abstract
	The presence of outliers can very problematic in data analysis, leading statisticians to develop a wide variety of methods for identifying them in both the univariate and multivariate contexts.  In case of the latter, perhaps the most popular approach has been Mahalanobis distance, where large values suggest an observation that is unusual as compared to the center of the data.  However, researchers have identified problems with the application of this metric such that its utility may be limited in some situations.  As a consequence, other methods for detecting outlying observations have been developed and studied.  However, a number of these approaches, while apparently robust and useful have not made their way into general practice in the social sciences.  Thus, the goal of this study was to describe some of these methods and demonstrate them using a well known dataset from a popular multivariate textbook widely used in the social sciences.  Results demonstrated that the methods do indeed result in datasets with very different distributional characteristics.  These results are discussed in light of how they might be used by researchers and practitioners.
},
  added-at = {2016-10-16T22:01:02.000+0200},
  author = {Finch, Holmes},
  biburl = {https://www.bibsonomy.org/bibtex/28b9dc7d32e9f86d587471e834bf5b247/vngudivada},
  interhash = {45569c2d5f7966ea318d0efaf76ed2a4},
  intrahash = {8b9dc7d32e9f86d587471e834bf5b247},
  journal = {Frontiers in Psychology},
  keywords = {OutlierDetection RobustStatistics},
  pages = 211,
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Distribution of Variables by Method of Outlier Detection},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00211},
  volume = 3,
  year = 2012
}

@book{aggarwal2013outlier,
  abstract = {With the increasing advances in hardware technology for data collection, and advances in software technology (databases) for data organization, computer scientists have increasingly participated in the latest advancements of the outlier analysis field. Computer scientists, specifically, approach this field based on their practical experiences in managing large amounts of data, and with far fewer assumptions– the data can be of any type, structured or unstructured, and may be extremely large. Outlier Analysis is a comprehensive exposition, as understood by data mining experts, statisticians and computer scientists. The book has been organized carefully, and emphasis was placed on simplifying the content, so that students and practitioners can also benefit. Chapters will typically cover one of three areas: methods and techniques  commonly used in outlier analysis, such as linear methods, proximity-based methods, subspace methods, and supervised methods; data  domains, such as, text, categorical, mixed-attribute, time-series, streaming, discrete sequence, spatial and network data; and key applications of these methods as applied to diverse domains such as  credit card fraud detection, intrusion detection, medical diagnosis, earth science, web log analytics, and social network analysis are covered.},
  added-at = {2016-11-30T02:30:30.000+0100},
  address = {New York, NY},
  author = {Aggarwal, Charu C.},
  biburl = {https://www.bibsonomy.org/bibtex/27864548cf7b0b98f20dc8f47c01420c6/vngudivada},
  interhash = {7705e4281511a62d9afa8c864436cde3},
  intrahash = {7864548cf7b0b98f20dc8f47c01420c6},
  keywords = {Book OutlierAnalysis Outliers Statistics},
  publisher = {Springer},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Outlier Analysis},
  url = {https://www.amazon.com/Outlier-Analysis-Charu-C-Aggarwal/dp/1461463955/ref=pd_sbs_14_1?_encoding=UTF8&pd_rd_i=1461463955&pd_rd_r=0W2KNHJ835EQ3EER36PG&pd_rd_w=PhN6m&pd_rd_wg=NBQF7&psc=1&refRID=0W2KNHJ835EQ3EER36PG},
  year = 2013
}

@article{wilcox2012modern,
  abstract = {During the last half century, hundreds of papers published in statistical journals have documented general conditions where reliance on least squares regression and Pearson's correlation can result in missing even strong associations between variables. Moreover, highly misleading conclusions can be made, even when the sample size is large. There are, in fact, several fundamental concerns related to non-normality, outliers, heteroscedasticity, and curvature that can result in missing a strong association. Simultaneously, a vast array of new methods has been derived for effectively dealing with these concerns. The paper (i) reviews why least squares regression and classic inferential methods can fail, (ii) provides an overview of the many modern strategies for dealing with known problems, including some recent advances, and (iii) illustrates that modern robust methods can make a practical difference in our understanding of data. Included are some general recommendations regarding how modern methods might be used. Copyright © 2011 John Wiley & Sons, Ltd.},
  added-at = {2016-10-18T02:50:24.000+0200},
  author = {Wilcox, Rand R. and Keselman, H. J.},
  biburl = {https://www.bibsonomy.org/bibtex/25cb0f68b04fc15147be7c4e09a18cb23/vngudivada},
  interhash = {024c39fef5a8611126bb5d394ae418c0},
  intrahash = {5cb0f68b04fc15147be7c4e09a18cb23},
  journal = {European Journal of Personality},
  keywords = {Regression StatisticalPower Statistics},
  number = 3,
  pages = {165--174},
  publisher = {John Wiley \& Sons, Ltd},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Modern Regression Methods that can Substantially Increase Power and Provide a more Accurate Understanding of Associations},
  url = {http://dx.doi.org/10.1002/per.860},
  volume = 26,
  year = 2012
}

@inproceedings{sathiyamurthy2012approach,
  abstract = {With the energetic growth of internet usage in today's world, every user spends their time in internet, hence to make users interested in learning the eLearning technology are widely used. eLearning aims at replacing old-fashioned pre-determined learning with on demand process of learning. The learning objects are used for eLearning process, which are used as a reusable component and it is interoperable to make the learning environment easy and makes education content made available in any format the user needs. To have an effective courseware presentation, the ontologies are used to provide learners with the correct learning content. The learning objects are stored in a repository. The main objective of this work is focused towards providing efficient learning environment by assembling the Learning Objects (LO) dynamically for a given user query supported by domain, and pedagogy ontology to generate courseware. In this work granularity computation is applied to assemble the learning objects.},
  acmid = {2345587},
  added-at = {2016-12-06T04:03:23.000+0100},
  address = {New York, NY, USA},
  author = {Sathiyamurthy, K. and Geetha, T. V. and Senthilvelan, M.},
  biburl = {https://www.bibsonomy.org/bibtex/258ce81458357967b4a6f450914dee30c/vngudivada},
  booktitle = {Proceedings of the International Conference on Advances in Computing, Communications and Informatics},
  doi = {10.1145/2345396.2345587},
  interhash = {27e8fac1baa25c10032b0c9215696f91},
  intrahash = {58ce81458357967b4a6f450914dee30c},
  isbn = {978-1-4503-1196-0},
  keywords = {sys:relevantfor:ecu-cc-research LOM LOR LearningObjectMetadata LearningObjectRepository Ontology Pedagogy ReusableLearningObject},
  location = {Chennai, India},
  numpages = {6},
  pages = {1193--1198},
  publisher = {ACM},
  series = {ICACCI '12},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {An Approach Towards Dynamic Assembling of Learning Objects},
  url = {http://doi.acm.org/10.1145/2345396.2345587},
  year = 2012
}

@book{cardona1997background,
  abstract = {This is the first volume of a study of Panini`s work, its antecedents and the traditions of interpretation and analysis to which it gave rise. This revised second edition included the text of Panini`s Astadhyayi with indications of changes that were introduced to this text and a discussion concerning such changes. Subsequent volumes take up in full detail issues of interpretation, method, and theory associated with the Astadhyayi. This first volume is meant to provide a basic for such detailed discussions.},
  added-at = {2016-09-25T14:22:40.000+0200},
  address = {Delhi, India},
  author = {Cardona, George},
  biburl = {https://www.bibsonomy.org/bibtex/22cb6f44cbd84a05941836a0611c6971b/vngudivada},
  interhash = {68cd3257684368ccb1eb1e7c49b34ea7},
  intrahash = {2cb6f44cbd84a05941836a0611c6971b},
  keywords = {Book Grammar Panini Snaskrit},
  publisher = {Motilal Banarsidass},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Background and introduction},
  year = 1997
}

@article{chervela1981medial,
  abstract = {This study is an attempt to trace the acquisition of consonant clusters by Telugu children. Phonemic inventories are given for each of the children for a clear picture of their acquisitional stage. It was found that reduction, substitution and assimilation played major roles in cluster acquisition. Co-occurrence restrictions and hierarchical application of the three phonological processes were noted. Results of this study are compared with similar works on Indian languages and English, and differences are pointed out.},
  added-at = {2016-10-07T21:44:51.000+0200},
  author = {Chervela, Nirmala},
  biburl = {https://www.bibsonomy.org/bibtex/2a4e22948074588811cc903d9e1770575/vngudivada},
  doi = {10.1017/s0305000900003019},
  interhash = {aa7eebf50fac108185f8985705f337ea},
  intrahash = {a4e22948074588811cc903d9e1770575},
  journal = {Journal of Child Language},
  keywords = {ConsonantAcquisition Telugu},
  month = feb,
  number = 01,
  publisher = {Cambridge University Press ({CUP})},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Medial consonant cluster acquisition by Telugu children},
  url = {http://dx.doi.org/10.1017/S0305000900003019},
  volume = 8,
  year = 1981
}

@book{loshin2010practitioners,
  abstract = {Business problems are directly related to missed data quality expectations. Flawed information production processes introduce risks preventing the successful achievement of critical business objectives. However, these flaws are mitigated through data quality management and control: controlling the quality of the information production process from beginning to end to ensure that any imperfections are identified early, prioritized, and remediated before material impacts can be incurred. The Practitioner's Guide to Data Quality Improvement shares the fundamentals for understanding the impacts of poor data quality, and guides practitioners and managers alike in socializing, gaining sponsorship for, planning, and establishing a data quality program. This book shares templates and processes for business impact analysis, defining data quality metrics, inspection and monitoring, remediation, and using data quality tools. Never shying away from the difficult topics or subjects, this is the seminal book that offers advice on how to actually get the job done.

Offers a comprehensive look at data quality for business and IT, encompassing people, process, and technology.
Shows how to institute and run a data quality program, from first thoughts and justifications to maintenance and ongoing metrics.
Includes an in-depth look at the use of data quality tools, including business case templates, and tools for analysis, reporting, and strategic planning.},
  added-at = {2016-12-01T01:13:30.000+0100},
  address = {Burlington, Massachusetts},
  author = {Loshin, David},
  biburl = {https://www.bibsonomy.org/bibtex/2958a0eceb91a062bb6765af78e765e75/vngudivada},
  interhash = {62223fe0e828e49d2ef94efb27a886a5},
  intrahash = {958a0eceb91a062bb6765af78e765e75},
  keywords = {Book DataQuality},
  publisher = {Morgan Kaufmann},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The Practitioner's Guide to Data Quality Improvement},
  year = 2010
}

@book{mcgilvray2008executing,
  abstract = {Information is currency. Recent studies show that data quality problems are costing businesses billions of dollars each year, with poor data linked to waste and inefficiency, damaged credibility among customers and suppliers, and an organizational inability to make sound decisions. In this important and timely new book, Danette McGilvray presents her ¿Ten Steps? approach to information quality, a proven method for both understanding and creating information quality in the enterprise. Her trademarked approach-in which she has trained Fortune 500 clients and hundreds of workshop attendees-applies to all types of data and to all types of organizations.

* Includes numerous templates, detailed examples, and practical advice for executing every step of the ¿Ten Steps? approach.

* Allows for quick reference with an easy-to-use format highlighting key concepts and definitions, important checkpoints, communication activities, and best practices.

* A companion Web site includes links to numerous data quality resources, including many of the planning and information-gathering templates featured in the text, quick summaries of key ideas from the Ten Step methodology, and other tools and information available online.},
  added-at = {2016-10-09T19:22:04.000+0200},
  address = {San Francisco, CA, USA},
  author = {McGilvray, Danette},
  biburl = {https://www.bibsonomy.org/bibtex/268d0cf3003cdf131f50367a13d1e5ef4/vngudivada},
  description = {Table of Contents

Introduction
The Reason for This Book
Intended Audiences
Structure of This Book
How to Use This Book
Acknowledgements

Chapter 1 Overview
Impact of Information and Data Quality
About the Methodology
Approaches to Data Quality in Projects
Engaging Management

Chapter 2 Key Concepts
Introduction
Framework for Information Quality (FIQ)
Information Life Cycle
Data Quality Dimensions
Business Impact Techniques
Data Categories
Data Specifications
Data Governance and Stewardship
The Information and Data Quality Improvement Cycle
The Ten Steps¿ Process
Best Practices and Guidelines

Chapter 3 The Ten Steps
1. Define Business Need and Approach
2. Analyze Information Environment
3. Assess Data Quality
4. Assess Business Impact
5. Identify Root Causes
6. Develop Improvement Plans
7. Prevent Future Data Errors
8. Correct Current Data Errors
9. Implement Controls
10. Communicate Actions and Results

Chapter 4 Structuring Your Project
Projects and The Ten Steps
Data Quality Project Roles
Project Timing

Chapter 5 Other Techniques and Tools
Introduction
Information Life Cycle Approaches
Capture Data
Analyze and Document Results
Metrics
Data Quality Tools
The Ten Steps and Six Sigma

Chapter 6 A Few Final Words

Appendix Quick References
Framework for Information Quality
POSMAD Interaction Matrix Detail
POSMAD Phases and Activities
Data Quality Dimensions
Business Impact Techniques
The Ten Steps¿ Overview
Definitions of Data Categories},
  interhash = {81fca3713b8e0073048d02eea0d280b8},
  intrahash = {68d0cf3003cdf131f50367a13d1e5ef4},
  keywords = {Book DataQuality},
  publisher = {Morgan Kaufmann Publishers Inc.},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Executing Data Quality Projects: Ten Steps to Quality Data and Trusted InformationTM},
  year = 2008
}

@book{crystal2010cambridge,
  abstract = {This new, thoroughly revised edition of the acclaimed Cambridge Encyclopedia of Language incorporates the major developments in language study which have taken place since the mid 1990s. Two main new areas have been added: the rise of electronic communication in all its current forms from email to texting, and the crisis affecting the world's languages, of which half are thought to be so seriously endangered that they will die out this century. • All language statistics have been updated, and additional information provided about their linguistic affiliation • All topics involving technology have been revised to take account of recent developments, notably in phonetics, language disability, and computing • Maps have been revised to include new countries or country names • Special attention has been paid to fast-moving areas such as language teaching and learning • The text design has been completely updated with many new illustrations throughout},
  added-at = {2016-10-01T18:50:30.000+0200},
  author = {Crystal, D.},
  biburl = {https://www.bibsonomy.org/bibtex/2342fe44e3704ffdb8094baaa11199bc6/vngudivada},
  description = {Table of Contents

Preface to 1st edition
Preface to 2nd edition
Preface to 3rd edition
Part I. Popular Ideas About Language:
1. The prescriptive tradition
2. The equality of languages
3. The magic of language
4. The functions of language
5. Language and thought
Part II. Language and Identity:
6. Physical identity
7. Psychological identity
8. Geographical identity
9. Ethnic and national identity
10. Social identity
11 Contextual identity
12. Stylistic identity and literature
Part III. The Structure of Language:
13. Linguistic levels
14. Typology and universals
15. The statistical structure of language
16. Grammar
17. Semantics
18. Dictionaries
19. Names
20. Discourse and text
21. Pragmatics
Part IV. The Medium of Language: Speaking and Listening:
22. The anatomy and physiology of speech
23. The acoustics of speech
24. The instrumental analysis of speech
25. Speech reception
26. Speech interaction with machines
27. The sounds of speech
28. The linguistic use of sound
29. Suprasegmentals
30. Sound symbolism
Part V. The Medium of Language: Writing and Reading:
31. Written and spoken language
32. Graphic expression
33. Graphology
34. The process of reading and writing
Part VI. The Medium of Language: Signing and Seeing:
35. Sign language
36. Sign language structure
37. Types of sign language
Part VII. Child Language Acquisition:
38. Investigating children's language
39. The first year
40. Phonological development
41. Grammatical development
42. Semantic development
43. Pragmatic development
44. Language development in school
Part VIII. Language, Brain, and Disability:
45. Language and the brain
46. Language disability
Part IX. The Languages of the World:
47. How many languages?
48. How many speakers?
49. The origins of language
50. Families of language
51. The Indo-European family
52. Other families
53. Language isolates
54. Language change
55. Pidgins and creoles
Part X. Language in the World:
56. The language barrier
57. Translating and interpreting
58. Artificial languages
59. World languages
60. Multilingualism
61. Language planning
62. Foreign language teaching and learning
63. Language for special purposes
Part XI. Language and Communication:
64. Language and other communication systems
65. Linguistics
Appendices: I. Glossary
II. Special symbols and abbreviations
III. Table of the world's languages
IV. Further reading
V. Index of languages, families, dialects, and scripts
VI. Index of authors and personalities
VII. Index of topics
Acknowledgements.},
  edition = {Third},
  interhash = {f52151f53f8fbe4e69bc081382dfcab0},
  intrahash = {342fe44e3704ffdb8094baaa11199bc6},
  keywords = {Book Encyclopedia Language},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The Cambridge encyclopedia of language},
  year = 2010
}

@book{mcwhorter2004story,
  abstract = {Professor McWhorter covers a wealth of material, enlivened with wit and personal observations: How did different languages come to be?, Why isn't there just a single language? How does a language change, and when it does, is that change indicative of decay or growth?, How does a language become extinct? Professor McWhorter addresses these and other questions as he takes you on an in-depth, 36-lecture tour of the development of human language, showing how a single tongue spoken 150,000 years ago has evolved into the estimated 6,000 languages used around the world today.},
  added-at = {2016-10-01T17:55:45.000+0200},
  author = {McWhorter, John},
  biburl = {https://www.bibsonomy.org/bibtex/2f9d824ae2da3228027bb0d43b7b36e69/vngudivada},
  interhash = {8a0a7b8bfe5a9d4ca19413c45ab56a4b},
  intrahash = {f9d824ae2da3228027bb0d43b7b36e69},
  keywords = {Book GreatCourses Language},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The Story of Human Language - Course Guidebook},
  year = 2004
}

@book{kroeger2005analyzing,
  abstract = {Analyzing Grammar is a clear introductory textbook on grammatical analysis, designed to equip students with the tools and methods needed to analyze grammatical patterns in any language. Standard notational devices such as phrase structure trees and word-formation rules are shown, and a range of problem sets and exercises are provided.},
  added-at = {2016-09-23T01:25:53.000+0200},
  address = {New York, NY},
  author = {Kroeger, Paul},
  biburl = {https://www.bibsonomy.org/bibtex/2aefb2133979606abc55957a12f0a1cfc/vngudivada},
  description = {Covering both syntax (the structure of phrases and sentences) and morphology (the structure of words), this book equips students with the tools and methods needed to analyze grammatical patterns in any language. Students are shown how to use standard notational devices such as phrase structure trees and word-formation rules, as well as prose descriptions. Emphasis is placed on comparing the different grammatical systems of the world's languages, and students are encouraged to practice the analyses through a diverse range of problem sets and exercises.

Draws on data from a wide range of non-European languages
Provides a glossary of technical terms
Data-analysis based exercises are included in every chapter},
  interhash = {3b2a48b58b2069d1bdaed4815dd8fef3},
  intrahash = {aefb2133979606abc55957a12f0a1cfc},
  keywords = {Book Grammar Language},
  publisher = {Cambridge University Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Analyzing grammar: an introduction},
  url = {http://dx.doi.org/10.1017/CBO9780511801679},
  year = 2005
}

@misc{closs2016modern,
  added-at = {2016-12-07T00:52:13.000+0100},
  author = {Closs, Sissi},
  biburl = {https://www.bibsonomy.org/bibtex/23fb2dfee0befe63febfc43f470479365/vngudivada},
  interhash = {0ed2a624d6bd6bafd0b642a020f48822},
  intrahash = {3fb2dfee0befe63febfc43f470479365},
  keywords = {DITA SoftwareDocumentation},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {The Modern Way of Writing Software Documentation},
  year = 2016
}

@book{finch2003study,
  abstract = {Intended primarily for newcomers to the subject, but with new material designed to help the more advanced reader, How to Study Linguistics is written in a refreshing and engaging style. It assumes no prior knowledge and contains many useful suggestions for developing a secure understanding of the subject. Chapters discuss strategies for studying phonology, syntax, and semantics, and for pursuing branches of linguistics, such as sociolinguistics, stylistics, and psycholinguistics, as well as practical advice on writing essays. The book also includes a glossary to aid learning and revision.},
  added-at = {2016-10-01T17:42:43.000+0200},
  address = {New York, NY},
  author = {Finch, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/278587044a0165df4999ffb275139e317/vngudivada},
  interhash = {0d9bc7c084b34295bc9a987a16fb2bad},
  intrahash = {78587044a0165df4999ffb275139e317},
  keywords = {Book Introductory Linguistics},
  publisher = {Palgrave Macmillan},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {How to study linguistics: a guide to understanding language},
  year = 2003
}

@article{bickerton2007language,
  abstract = {For the benefit of linguists new to the field of language evolution, the author sets out the issues that need to be distinguished in any research on it. He offers a guided tour of contemporary approaches, including the work of linguists (Bickerton, Carstairs-McCarthy, Chomsky, Hurford, Jackendoff, Pinker, Wray), animal behaviour experts (Dunbar, Hauser, Premack, Savage-Rumbaugh), neurophysiologists (Arbib, Calvin), psychologists (Corballis, Donald), archaeologists (Davidson), and computer modellers (Batali, Kirby, Steels). He criticises the expectation that recent discoveries such as ‘mirror neurons’ and the FOXP2 gene will provide easy answers. He emphasises the extremely interdisciplinary nature of this field, and also the importance of involvement in it by linguists, after more than a century of neglect.},
  added-at = {2016-10-01T17:49:17.000+0200},
  author = {Bickerton, Derek},
  biburl = {https://www.bibsonomy.org/bibtex/24d66e722b16116cb5b0260e9f0b4d35c/vngudivada},
  doi = {10.1016/j.lingua.2005.02.006},
  interhash = {1f56ecabed5ab0c9a3ee6218d504c623},
  intrahash = {4d66e722b16116cb5b0260e9f0b4d35c},
  journal = {Lingua},
  keywords = {LanguageEvolution},
  number = 3,
  pages = {510--526},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Language evolution: A brief guide for linguists},
  url = {http://www.sciencedirect.com/science/article/pii/S0024384105000975},
  volume = 117,
  year = 2007
}

@book{aggarwal2015classification,
  abstract = {Features

Integrates different perspectives from the pattern recognition, database, data mining, and machine learning communities
	Presents an overview of the core methods in data classification
	Covers recent problem domains, such as graphs and social networks
	Discusses advanced methods for enhancing the quality of the underlying classification results
	Summary

	Comprehensive Coverage of the Entire Area of Classification

	Research on the problem of classification tends to be fragmented across such areas as pattern recognition, database, data mining, and machine learning. Addressing the work of these different communities in a unified way, Data Classification: Algorithms and Applications explores the underlying algorithms of classification as well as applications of classification in a variety of problem domains, including text, multimedia, social network, and biological data.

	This comprehensive book focuses on three primary aspects of data classification:

	Methods: The book first describes common techniques used for classification, including probabilistic methods, decision trees, rule-based methods, instance-based methods, support vector machine methods, and neural networks.

	Domains: The book then examines specific methods used for data domains such as multimedia, text, time-series, network, discrete sequence, and uncertain data. It also covers large data sets and data streams due to the recent importance of the big data paradigm.

	Variations: The book concludes with insight on variations of the classification process. It discusses ensembles, rare-class learning, distance function learning, active learning, visual learning, transfer learning, and semi-supervised learning as well as evaluation aspects of classifiers.},
  added-at = {2016-11-30T02:35:06.000+0100},
  address = {Boca Raton, Florida},
  biburl = {https://www.bibsonomy.org/bibtex/218393881f99b79b0708bf2ad29b5f06e/vngudivada},
  editor = {Aggarwal, Charu C.},
  interhash = {80fe1e80f2490ada34471ecea77fda2d},
  intrahash = {18393881f99b79b0708bf2ad29b5f06e},
  isbn = {978-1466586741},
  keywords = {Book Classification MachineLearning Statistics},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Data Classification: Algorithms and Applications},
  url = {https://www.crcpress.com/Data-Classification-Algorithms-and-Applications/Aggarwal/p/book/9781466586741?source=igodigital},
  year = 2015
}

@book{wilcox2016introduction,
  abstract = {Key Features

Extensive revisions to cover the latest developments in robust regression
Covers latest improvements in ANOVA
Includes newest rank-based methods
Describes and illustrated easy to use software
Description

Introduction to Robust Estimating and Hypothesis Testing, 4th Editon, is a ‘how-to’ on the application of robust methods using available software. Modern robust methods provide improved techniques for dealing with outliers, skewed distribution curvature and heteroscedasticity that can provide substantial gains in power as well as a deeper, more accurate and more nuanced understanding of data. Since the last edition, there have been numerous advances and improvements. They include new techniques for comparing groups and measuring effect size as well as new methods for comparing quantiles. Many new regression methods have been added that include both parametric and nonparametric techniques. The methods related to ANCOVA have been expanded considerably. New perspectives related to discrete distributions with a relatively small sample space are described as well as new results relevant to the shift function. The practical importance of these methods is illustrated using data from real world studies. The R package written for this book now contains over 1200 functions.
New to this edition
35% revised content
Covers many new and improved R functions
New techniques that deal with a wide range of situations
Readership

The book is relevant to anyone dealing with methods for studying associations, comparing groups, or analyzing multivariate data. The book assumes the reader has had some basic training in statistics},
  added-at = {2016-10-16T21:44:49.000+0200},
  address = {Boston, MA},
  author = {Wilcox, Rand R.},
  biburl = {https://www.bibsonomy.org/bibtex/2612c9d6b9e9b9af032132510bafc9a7c/vngudivada},
  description = {Introduction to Robust Estimation and Hypothesis Testing, 4th Edition

1. Introduction
2. A Foundation for Robust Methods
3. Estimating Measures of Location and Scale
4. Confidence Intervals in the One-Sample Case
5. Comparing Two Groups
6. Some Multivariate Methods
7. One-Way and Higher Designs for Independent Groups
8. Comparing Multiple Dependent Groups
9. Correlation and Tests Of Independence
10. Robust Regression
11. More Regression Methods
12. ANCOVA},
  edition = {Fourth},
  interhash = {cad9f7b8dff93ed00e9ddc0f05edb080},
  intrahash = {612c9d6b9e9b9af032132510bafc9a7c},
  keywords = {Book RobustStatistics Statistics},
  publisher = {Academic Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Introduction to robust estimation and hypothesis testing},
  year = 2016
}

@book{aggarwal2013clustering,
  abstract = {Features

Presents core methods for data clustering, including probabilistic, density- and grid-based, and spectral clustering
Explores various problems and scenarios pertaining to multimedia, text, biological, categorical, network, streams, and uncertain data
Offers in-depth insight on the clustering process, including different ways to cluster the same data set
Includes an extensive bibliography at the end of each chapter
Summary

Research on the problem of clustering tends to be fragmented across the pattern recognition, database, data mining, and machine learning communities. Addressing this problem in a unified way, Data Clustering: Algorithms and Applications provides complete coverage of the entire area of clustering, from basic methods to more refined and complex data clustering approaches. It pays special attention to recent issues in graphs, social networks, and other domains.

The book focuses on three primary aspects of data clustering:

Methods, describing key techniques commonly used for clustering, such as feature selection, agglomerative clustering, partitional clustering, density-based clustering, probabilistic clustering, grid-based clustering, spectral clustering, and nonnegative matrix factorization
Domains, covering methods used for different domains of data, such as categorical data, text data, multimedia data, graph data, biological data, stream data, uncertain data, time series clustering, high-dimensional clustering, and big data
Variations and Insights, discussing important variations of the clustering process, such as semisupervised clustering, interactive clustering, multiview clustering, cluster ensembles, and cluster validation
In this book, top researchers from around the world explore the characteristics of clustering problems in a variety of application areas. They also explain how to glean detailed insight from the clustering process—including how to verify the quality of the underlying clusters—through supervision, human intervention, or the automated generation of alternative clusters.},
  added-at = {2016-11-30T02:33:00.000+0100},
  address = {Boca Raton, Florida},
  biburl = {https://www.bibsonomy.org/bibtex/2210467e4ae76131679bea7a05192d82e/vngudivada},
  editor = {Aggarwal, Charu C.},
  interhash = {a6733ca294c5f393a30c0c602205c74c},
  intrahash = {210467e4ae76131679bea7a05192d82e},
  isbn = {978-1466558212},
  keywords = {Book Clustering MachineLearning Statistics},
  publisher = {CRC Press},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Data Clustering: Algorithms and Applications},
  url = {https://www.amazon.com/Data-Clustering-Algorithms-Applications-Aggarwal/dp/B010BAR6QE/ref=sr_1_1?s=books&ie=UTF8&qid=1480469149&sr=1-1&keywords=Charu+C.+Aggarwal+CRC+Press},
  year = 2013
}

@book{maydanchik2007quality,
  abstract = {DATA QUALITY ASSESSMENT is a must read for anyone who needs to understand, correct, or prevent data quality issues in their organization. Skipping theory and focusing purely on what is practical and what works, this text contains a proven approach to identifying, warehousing, and analyzing data errors – the first step in any data quality program. Master techniques in:
Data profiling and gathering metadata
Identifying, designing, and implementing data quality rules
Organizing rule and error catalogues
Ensuring accuracy and completeness of the data quality assessment
Constructing the dimensional data quality scorecard
Executing a recurrent data quality assessment},
  added-at = {2016-10-09T03:33:37.000+0200},
  address = {Bradley Beach, N.J.},
  author = {Maydanchik, Arkady},
  biburl = {https://www.bibsonomy.org/bibtex/242d2425ef8393821c375bc615aa66630/vngudivada},
  interhash = {6e28eed9482b5d7dd50dd30805477e31},
  intrahash = {42d2425ef8393821c375bc615aa66630},
  keywords = {Book DataQualityAssessment},
  publisher = {Technics Publications},
  timestamp = {2019-03-25T17:12:15.000+0100},
  title = {Data quality assessment},
  year = 2007
}
